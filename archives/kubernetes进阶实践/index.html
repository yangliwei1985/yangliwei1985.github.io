<!DOCTYPE html>
<html lang="zh" dir="auto">

<head><meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta name="robots" content="index, follow">
<title>Kubernetes进阶实践 | ylw&#39;s blog</title>
<meta name="keywords" content="docker, k8s" />
<meta name="description" content="第三天 Kubernetes进阶实践 本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。
 ETCD数据的访问 kube-scheduler调度策略实践  预选与优选流程 生产中常用的调度配置实践   k8s集群网络模型  CNI介绍及集群网络选型 Flannel网络模型的实现  vxlan Backend hostgw Backend     集群认证与授权  APIServer安全控制模型 Kubectl的认证授权 RBAC kubelet的认证授权 Service Account   使用Helm管理复杂应用的部署  Helm工作原理详解 Helm的模板开发 实战：使用Helm部署Harbor仓库   kubernetes对接分部式存储  pv、pvc介绍 k8s集群如何使用cephfs作为分布式存储后端 利用storageClass实现动态存储卷的管理 实战：使用分部署存储实现有状态应用的部署   本章知识梳理及回顾  ETCD常用操作 拷贝etcdctl命令行工具：
$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看etcd集群的成员节点：
$ export ETCDCTL_API=3 $ etcdctl --endpoints=https://[127.">
<meta name="author" content="">
<link rel="canonical" href="https://iblog.zone/archives/kubernetes%E8%BF%9B%E9%98%B6%E5%AE%9E%E8%B7%B5/" />
<link crossorigin="anonymous" href="/assets/css/stylesheet.min.7af5e4f048c756d5896f15b3f7cd7ee898ea1b3b101fcb40abfb5216ca230ecf.css" integrity="sha256-evXk8EjHVtWJbxWz981&#43;6JjqGzsQH8tAq/tSFsojDs8=" rel="preload stylesheet" as="style">
<script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
<script defer crossorigin="anonymous" src="/assets/js/highlight.min.4dcb3c4f38462f66c6b6137227726f5543cb934cca9788f041c087e374491df2.js" integrity="sha256-Tcs8TzhGL2bGthNyJ3JvVUPLk0zKl4jwQcCH43RJHfI="
    onload="hljs.initHighlightingOnLoad();"></script>
<link rel="icon" href="https://iblog.zone/images/favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="https://iblog.zone/%3Clink%20/%20abs%20url%3E">
<link rel="icon" type="image/png" sizes="32x32" href="https://iblog.zone/%3Clink%20/%20abs%20url%3E">
<link rel="apple-touch-icon" href="https://iblog.zone/%3Clink%20/%20abs%20url%3E">
<link rel="mask-icon" href="https://iblog.zone/%3Clink%20/%20abs%20url%3E">
<meta name="theme-color" content="#2e2e33">
<meta name="msapplication-TileColor" content="#2e2e33">
<noscript>
    <style>
        #theme-toggle,
        .top-link {
            display: none;
        }

    </style>
    <style>
        @media (prefers-color-scheme: dark) {
            :root {
                --theme: rgb(29, 30, 32);
                --entry: rgb(46, 46, 51);
                --primary: rgb(218, 218, 219);
                --secondary: rgb(155, 156, 157);
                --tertiary: rgb(65, 66, 68);
                --content: rgb(196, 196, 197);
                --hljs-bg: rgb(46, 46, 51);
                --code-bg: rgb(55, 56, 62);
                --border: rgb(51, 51, 51);
            }

            .list {
                background: var(--theme);
            }

            .list:not(.dark)::-webkit-scrollbar-track {
                background: 0 0;
            }

            .list:not(.dark)::-webkit-scrollbar-thumb {
                border-color: var(--theme);
            }
        }

    </style>
</noscript><link rel='stylesheet' href='//cdn.jsdelivr.net/npm/hack-font@3.3.0/build/web/hack.css'>

<meta property="og:title" content="Kubernetes进阶实践" />
<meta property="og:description" content="第三天 Kubernetes进阶实践 本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。
 ETCD数据的访问 kube-scheduler调度策略实践  预选与优选流程 生产中常用的调度配置实践   k8s集群网络模型  CNI介绍及集群网络选型 Flannel网络模型的实现  vxlan Backend hostgw Backend     集群认证与授权  APIServer安全控制模型 Kubectl的认证授权 RBAC kubelet的认证授权 Service Account   使用Helm管理复杂应用的部署  Helm工作原理详解 Helm的模板开发 实战：使用Helm部署Harbor仓库   kubernetes对接分部式存储  pv、pvc介绍 k8s集群如何使用cephfs作为分布式存储后端 利用storageClass实现动态存储卷的管理 实战：使用分部署存储实现有状态应用的部署   本章知识梳理及回顾  ETCD常用操作 拷贝etcdctl命令行工具：
$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看etcd集群的成员节点：
$ export ETCDCTL_API=3 $ etcdctl --endpoints=https://[127." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://iblog.zone/archives/kubernetes%E8%BF%9B%E9%98%B6%E5%AE%9E%E8%B7%B5/" /><meta property="article:section" content="posts" />
<meta property="article:published_time" content="2022-01-05T11:13:03&#43;00:00" />
<meta property="article:modified_time" content="2022-01-05T11:13:03&#43;00:00" />

<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="Kubernetes进阶实践"/>
<meta name="twitter:description" content="第三天 Kubernetes进阶实践 本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。
 ETCD数据的访问 kube-scheduler调度策略实践  预选与优选流程 生产中常用的调度配置实践   k8s集群网络模型  CNI介绍及集群网络选型 Flannel网络模型的实现  vxlan Backend hostgw Backend     集群认证与授权  APIServer安全控制模型 Kubectl的认证授权 RBAC kubelet的认证授权 Service Account   使用Helm管理复杂应用的部署  Helm工作原理详解 Helm的模板开发 实战：使用Helm部署Harbor仓库   kubernetes对接分部式存储  pv、pvc介绍 k8s集群如何使用cephfs作为分布式存储后端 利用storageClass实现动态存储卷的管理 实战：使用分部署存储实现有状态应用的部署   本章知识梳理及回顾  ETCD常用操作 拷贝etcdctl命令行工具：
$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看etcd集群的成员节点：
$ export ETCDCTL_API=3 $ etcdctl --endpoints=https://[127."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "Posts",
      "item": "https://iblog.zone/posts/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Kubernetes进阶实践",
      "item": "https://iblog.zone/archives/kubernetes%E8%BF%9B%E9%98%B6%E5%AE%9E%E8%B7%B5/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "Kubernetes进阶实践",
  "name": "Kubernetes进阶实践",
  "description": "第三天 Kubernetes进阶实践 本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。\n ETCD数据的访问 kube-scheduler调度策略实践  预选与优选流程 生产中常用的调度配置实践   k8s集群网络模型  CNI介绍及集群网络选型 Flannel网络模型的实现  vxlan Backend hostgw Backend     集群认证与授权  APIServer安全控制模型 Kubectl的认证授权 RBAC kubelet的认证授权 Service Account   使用Helm管理复杂应用的部署  Helm工作原理详解 Helm的模板开发 实战：使用Helm部署Harbor仓库   kubernetes对接分部式存储  pv、pvc介绍 k8s集群如何使用cephfs作为分布式存储后端 利用storageClass实现动态存储卷的管理 实战：使用分部署存储实现有状态应用的部署   本章知识梳理及回顾  ETCD常用操作 拷贝etcdctl命令行工具：\n$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看etcd集群的成员节点：\n$ export ETCDCTL_API=3 $ etcdctl --endpoints=https://[127.",
  "keywords": [
    "docker", "k8s"
  ],
  "articleBody": "第三天 Kubernetes进阶实践 本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。\n ETCD数据的访问 kube-scheduler调度策略实践  预选与优选流程 生产中常用的调度配置实践   k8s集群网络模型  CNI介绍及集群网络选型 Flannel网络模型的实现  vxlan Backend hostgw Backend     集群认证与授权  APIServer安全控制模型 Kubectl的认证授权 RBAC kubelet的认证授权 Service Account   使用Helm管理复杂应用的部署  Helm工作原理详解 Helm的模板开发 实战：使用Helm部署Harbor仓库   kubernetes对接分部式存储  pv、pvc介绍 k8s集群如何使用cephfs作为分布式存储后端 利用storageClass实现动态存储卷的管理 实战：使用分部署存储实现有状态应用的部署   本章知识梳理及回顾  ETCD常用操作 拷贝etcdctl命令行工具：\n$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看etcd集群的成员节点：\n$ export ETCDCTL_API=3 $ etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list -w table  $ alias etcdctl='etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key'  $ etcdctl member list -w table 查看etcd集群节点状态：\n$ etcdctl endpoint status -w table  $ etcdctl endpoint health -w table 设置key值:\n$ etcdctl put luffy 1 $ etcdctl get luffy 查看所有key值：\n$ etcdctl get / --prefix --keys-only 查看具体的key对应的数据：\n$ etcdctl get /registry/pods/jenkins/sonar-postgres-7fc5d748b6-gtmsb 添加定时任务做数据快照（重要！）\n$ etcdctl snapshot save `hostname`-etcd_`date +%Y%m%d%H%M`.db 恢复快照：\n  停止etcd和apiserver\n  移走当前数据目录\n$ mv /var/lib/etcd/ /tmp   恢复快照\n$ etcdctl snapshot restore `hostname`-etcd_`date +%Y%m%d%H%M`.db --data-dir=/var/lib/etcd/   集群恢复\nhttps://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\n  Kubernetes调度 为何要控制Pod应该如何调度  集群中有些机器的配置高（SSD，更好的内存等），我们希望核心的服务（比如说数据库）运行在上面 某两个服务的网络传输很频繁，我们希望它们最好在同一台机器上 ……  Kubernetes Scheduler 的作用是将待调度的 Pod 按照一定的调度算法和策略绑定到集群中一个合适的 Worker Node 上，并将绑定信息写入到 etcd 中，之后目标 Node 中 kubelet 服务通过 API Server 监听到 Scheduler 产生的 Pod 绑定事件获取 Pod 信息，然后下载镜像启动容器。\n调度的过程 Scheduler 提供的调度流程分为预选 (Predicates) 和优选 (Priorities) 两个步骤：\n 预选，K8S会遍历当前集群中的所有 Node，筛选出其中符合要求的 Node 作为候选 优选，K8S将对候选的 Node 进行打分  经过预选筛选和优选打分之后，K8S选择分数最高的 Node 来运行 Pod，如果最终有多个 Node 的分数最高，那么 Scheduler 将从当中随机选择一个 Node 来运行 Pod。\n预选：\n优选：\nCordon $ kubectl cordon k8s-slave2 $ kubectl drain k8s-slave2 NodeSelector label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，POD 的调度可以根据节点的 label 进行特定的部署。\n查看节点的label：\n$ kubectl get nodes --show-labels 为节点打label：\n$ kubectl label node k8s-master disktype=ssd 当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在spec 字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label。\n... spec:  hostNetwork: true # 声明pod的网络模式为host模式，效果通docker run --net=host  volumes:  - name: mysql-data  hostPath:  path: /opt/mysql/data  nodeSelector: # 使用节点选择器将Pod调度到指定label的节点  component: mysql  containers:  - name: mysql  image: 192.168.136.10:5000/demo/mysql:5.7 ... nodeAffinity 节点亲和性 ， 比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合，不只是简单的相等匹配 。分为两种，硬策略和软策略。\nrequiredDuringSchedulingIgnoredDuringExecution ： 硬策略，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不会调度Pod。\npreferredDuringSchedulingIgnoredDuringExecution：软策略，如果你没有满足调度要求的节点的话，Pod就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有满足就忽略掉的策略。\n#要求 Pod 不能运行在128和132两个节点上，如果有节点满足disktype=ssd或者sas的话就优先调度到这类节点上 ... spec:  containers:  - name: demo  image: 192.168.136.10:5000/demo/myblog:v1  ports:  - containerPort: 8002  affinity:  nodeAffinity:  requiredDuringSchedulingIgnoredDuringExecution:  nodeSelectorTerms:  - matchExpressions:  - key: kubernetes.io/hostname  operator: NotIn  values:  - 172.21.51.698  - 192.168.136.132   preferredDuringSchedulingIgnoredDuringExecution:  - weight: 1  preference:  matchExpressions:  - key: disktype  operator: In  values:  - ssd  - sas ... 这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种：\n In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在  如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 Pod\n污点（Taints）与容忍（tolerations） 对于nodeAffinity无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度Pod。\nTaints(污点)是Node的一个属性，设置了Taints(污点)后，因为有了污点，所以Kubernetes是不会将Pod调度到这个Node上的。于是Kubernetes就给Pod设置了个属性Tolerations(容忍)，只要Pod能够容忍Node上的污点，那么Kubernetes就会忽略Node上的污点，就能够(不是必须)把Pod调度过去。\n场景一：私有云服务中，某业务使用GPU进行大规模并行计算。为保证性能，希望确保该业务对服务器的专属性，避免将普通业务调度到部署GPU的服务器。\n场景二：用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 Pod，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：\n设置污点：\n$ kubectl taint node [node_name] key=value:[effect]  其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ]  NoSchedule：一定不能被调度。  PreferNoSchedule：尽量不要调度。  NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。  示例：kubectl taint node k8s-slave1 smoke=true:NoSchedule 去除污点：\n去除指定key及其effect：  kubectl taint nodes [node_name] key:[effect]- #这里的key不用指定value   去除指定key所有的effect:  kubectl taint nodes node_name key-   示例：  kubectl taint node k8s-master smoke=true:NoSchedule  kubectl taint node k8s-master smoke:NoExecute-  kubectl taint node k8s-master smoke- 污点演示：\n## 给k8s-slave1打上污点，smoke=true:NoSchedule $ kubectl taint node k8s-slave1 smoke=true:NoSchedule $ kubectl taint node k8s-slave2 drunk=true:NoSchedule   ## 扩容myblog的Pod，观察新Pod的调度情况 $ kuebctl -n luffy scale deploy myblog --replicas=3 $ kubectl -n luffy get po -w ## pending Pod容忍污点示例：myblog/deployment/deploy-myblog-taint.yaml\n... spec:  containers:  - name: demo  image: 192.168.136.10:5000/demo/myblog:v1  tolerations: #设置容忍性  - key: \"smoke\"  operator: \"Equal\" #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal  value: \"true\"  effect: \"NoSchedule\"  - key: \"drunk\"  operator: \"Exists\" #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal  #意思是这个Pod要容忍的有污点的Node的key是smoke Equal true,效果是NoSchedule，  #tolerations属性下各值必须使用引号，容忍的值都是设置Node的taints时给的值。 $ kubectl apply -f deploy-myblog-taint.yaml spec:  containers:  - name: demo  image: 192.168.136.10:5000/demo/myblog  tolerations:  - operator: \"Exists\" 验证NoExecute效果\nKubernetes集群的网络实现 CNI介绍及集群网络选型, CSI\n容器网络接口（Container Network Interface），实现kubernetes集群的Pod网络通信及管理。包括：\n CNI Plugin负责给容器配置网络，它包括两个基本的接口： 配置网络: AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error) 清理网络: DelNetwork(net NetworkConfig, rt RuntimeConf) error IPAM Plugin负责给容器分配IP地址，主要实现包括host-local和dhcp。  以上两种插件的支持，使得k8s的网络可以支持各式各样的管理模式，当前在业界也出现了大量的支持方案，其中比较流行的比如flannel、calico等。\nkubernetes配置了cni网络插件后，其容器网络创建流程为：\n kubelet先创建pause容器生成对应的network namespace 调用网络driver，因为配置的是CNI，所以会调用CNI相关代码，识别CNI的配置目录为/etc/cni/net.d CNI driver根据配置调用具体的CNI插件，二进制调用，可执行文件目录为/opt/cni/bin,项目 CNI插件给pause容器配置正确的网络，pod中其他的容器都是用pause的网络  可以在此查看社区中的CNI实现，https://github.com/containernetworking/cni\n通用类型：flannel、calico等，部署使用简单\n其他：根据具体的网络环境及网络需求选择，比如\n 公有云机器，可以选择厂商与网络插件的定制Backend，如AWS、阿里、腾讯针对flannel均有自己的插件，也有AWS ECS CNI 私有云厂商，比如Vmware NSX-T等 网络性能等，MacVlan  Flannel网络模型实现剖析 flannel实现overlay，underlay网络通常有多种实现：\n udp vxlan host-gw …  不特殊指定的话，默认会使用vxlan技术作为Backend，可以通过如下查看：\n$ kubectl -n kube-system exec kube-flannel-ds-amd64-cb7hs cat /etc/kube-flannel/net-conf.json {  \"Network\": \"10.244.0.0/16\",  \"Backend\": {  \"Type\": \"vxlan\"  } } vxlan介绍及点对点通信的实现 VXLAN 全称是虚拟可扩展的局域网（ Virtual eXtensible Local Area Network），它是一种 overlay 技术，通过三层的网络来搭建虚拟的二层网络。\n它创建在原来的 IP 网络（三层）上，只要是三层可达（能够通过 IP 互相通信）的网络就能部署 vxlan。在每个端点上都有一个 vtep 负责 vxlan 协议报文的封包和解包，也就是在虚拟报文上封装 vtep 通信的报文头部。物理网络上可以创建多个 vxlan 网络，这些 vxlan 网络可以认为是一个隧道，不同节点的虚拟机能够通过隧道直连。每个 vxlan 网络由唯一的 VNI 标识，不同的 vxlan 可以不相互影响。\n VTEP（VXLAN Tunnel Endpoints）：vxlan 网络的边缘设备，用来进行 vxlan 报文的处理（封包和解包）。vtep 可以是网络设备（比如交换机），也可以是一台机器（比如虚拟化集群中的宿主机） VNI（VXLAN Network Identifier）：VNI 是每个 vxlan 的标识，一共有 2^24 = 16,777,216，一般每个 VNI 对应一个租户，也就是说使用 vxlan 搭建的公有云可以理论上可以支撑千万级别的租户  演示：在k8s-slave1和k8s-slave2两台机器间，利用vxlan的点对点能力，实现虚拟二层网络的通信\nk8s-slave1节点：\n# 创建vTEP设备，对端指向k8s-slave2节点，指定VNI及underlay网络使用的网卡 $ ip link add vxlan20 type vxlan id 20 remote 172.21.51.69 dstport 4789 dev eth0  $ ip -d link show vxlan20  # 启动设备 $ ip link set vxlan20 up  # 设置ip地址  ip addr add 10.0.136.11/24 dev vxlan20 k8s-slave2节点：\n# 创建VTEP设备，对端指向k8s-slave1节点，指定VNI及underlay网络使用的网卡 $ ip link add vxlan20 type vxlan id 20 remote 172.21.51.68 dstport 4789 dev eth0  # 启动设备 $ ip link set vxlan20 up  # 设置ip地址 $ ip addr add 10.0.136.12/24 dev vxlan20 在k8s-slave1节点：\n$ ping 10.0.136.12 隧道是一个逻辑上的概念，在 vxlan 模型中并没有具体的物理实体想对应。隧道可以看做是一种虚拟通道，vxlan 通信双方（图中的虚拟机）认为自己是在直接通信，并不知道底层网络的存在。从整体来说，每个 vxlan 网络像是为通信的虚拟机搭建了一个单独的通信通道，也就是隧道。\n实现的过程：\n虚拟机的报文通过 vtep 添加上 vxlan 以及外部的报文层，然后发送出去，对方 vtep 收到之后拆除 vxlan 头部然后根据 VNI 把原始报文发送到目的虚拟机。\n# 查看k8s-slave1主机路由 $ route -n 10.0.136.0 0.0.0.0 255.255.255.0 U 0 0 0 vxlan20  # 到了vxlan的设备后， $ ip -d link show vxlan20  vxlan id 20 remote 172.21.51.69 dev eth0 srcport 0 0 dstport 4789 ...  # 查看fdb地址表，主要由MAC地址、VLAN号、端口号和一些标志域等信息组成,vtep 对端地址为 172.21.51.69，换句话说，如果接收到的报文添加上 vxlan 头部之后都会发到 172.21.51.69 $ bridge fdb show|grep vxlan20 00:00:00:00:00:00 dev vxlan20 dst 172.21.51.69 via eth0 self permanent 在k8s-slave2机器抓包，查看vxlan封装后的包:\n# 在k8s-slave2机器执行 $ tcpdump -i eth0 host 172.21.51.68 -w vxlan.cap  # 在k8s-slave1机器执行 $ ping 10.0.136.12 使用wireshark分析ICMP类型的数据包\n跨主机容器网络的通信 思考：容器网络模式下，vxlan设备该接在哪里？\n基本的保证：目的容器的流量要通过vtep设备进行转发！\n演示：利用vxlan实现跨主机容器网络通信\n为了不影响已有的网络，因此创建一个新的网桥，创建容器接入到新的网桥来演示效果\n在k8s-slave1节点：\n$ docker network ls  # 创建新网桥，指定cidr段 $ docker network create --subnet 172.18.0.0/16 network-luffy $ docker network ls  # 新建容器，接入到新网桥 $ docker run -d --name vxlan-test --net network-luffy --ip 172.18.0.2 nginx:alpine  $ docker exec vxlan-test ifconfig  $ brctl show network-luffy 在k8s-slave2节点：\n# 创建新网桥，指定cidr段 $ docker network create --subnet 172.18.0.0/16 network-luffy  # 新建容器，接入到新网桥 $ docker run -d --name vxlan-test --net network-luffy --ip 172.18.0.3 nginx:alpine 此时执行ping测试：\n$ docker exec vxlan-test ping 172.18.0.3 分析：数据到了网桥后，出不去。结合前面的示例，因此应该将流量由vtep设备转发，联想到网桥的特性，接入到桥中的端口，会由网桥负责转发数据，因此，相当于所有容器发出的数据都会经过到vxlan的端口，vxlan将流量转到对端的vtep端点，再次由网桥负责转到容器中。\nk8s-slave1节点：\n# 删除旧的vtep $ ip link del vxlan20  # 新建vtep $ ip link add vxlan_docker type vxlan id 100 remote 172.21.51.69 dstport 4789 dev eth0 $ ip link set vxlan_docker up # 不用设置ip，因为目标是可以转发容器的数据即可  # 接入到网桥中 $ brctl addif br-904603a72dcd vxlan_docker k8s-slave2节点：\n# 删除旧的vtep $ ip link del vxlan20  # 新建vtep $ ip link add vxlan_docker type vxlan id 100 remote 172.21.51.68 dstport 4789 dev eth0 $ ip link set vxlan_docker up # 不用设置ip，因为目标是可以转发容器的数据即可  # 接入到网桥中 $ brctl addif br-c6660fe2dc53 vxlan_docker 再次执行ping测试：\n$ docker exec vxlan-test ping 172.18.0.3 Flannel的vxlan实现精讲 思考：k8s集群的网络环境和手动实现的跨主机的容器通信有哪些差别？\n CNI要求，集群中的每个Pod都必须分配唯一的Pod IP k8s集群内的通信不是vxlan点对点通信，因为集群内的所有节点之间都需要互联  没法创建点对点的vxlan模型    flannel如何为每个节点分配Pod地址段：\n$ kubectl -n kube-system exec kube-flannel-ds-amd64-cb7hs cat /etc/kube-flannel/net-conf.json {  \"Network\": \"10.244.0.0/16\",  \"Backend\": {  \"Type\": \"vxlan\"  } }  #查看节点的pod ip [root@k8s-master bin]# kd get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog-5d9ff54d4b-4rftt 1/1 Running 1 33h 10.244.2.19 k8s-slave2 myblog-5d9ff54d4b-n447p 1/1 Running 1 33h 10.244.1.32 k8s-slave1  #查看k8s-slave1主机分配的地址段 $ cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.1.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true  # kubelet启动容器的时候就可以按照本机的网段配置来为pod设置IP地址 vtep的设备在哪：\n$ ip -d link show flannel.1 # 没有remote ip，非点对点 Pod的流量如何转到vtep设备中\n$ brctl show cni0  # 每个Pod都会使用Veth pair来实现流量转到cni0网桥  $ route -n 10.244.0.0 10.244.0.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.1.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 10.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 vtep封包的时候，如何拿到目的vetp端的IP及MAC信息\n# flanneld启动的时候会需要配置--iface=eth0,通过该配置可以将网卡的ip及Mac信息存储到ETCD中， # 这样，flannel就知道所有的节点分配的IP段及vtep设备的IP和MAC信息，而且所有节点的flanneld都可以感知到节点的添加和删除操作，就可以动态的更新本机的转发配置 演示跨主机Pod通信的流量详细过程：\n$ kubectl -n luffy get po -o wide myblog-5d9ff54d4b-4rftt 1/1 Running 1 25h 10.244.2.19 k8s-slave2 myblog-5d9ff54d4b-n447p 1/1 Running 1 25h 10.244.1.32 k8s-slave1  $ kubectl -n luffy exec myblog-5d9ff54d4b-n447p -- ping 10.244.2.19 -c 2 PING 10.244.2.19 (10.244.2.19) 56(84) bytes of data. 64 bytes from 10.244.2.19: icmp_seq=1 ttl=62 time=0.480 ms 64 bytes from 10.244.2.19: icmp_seq=2 ttl=62 time=1.44 ms  --- 10.244.2.19 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 0.480/0.961/1.443/0.482 ms  # 查看路由 $ kubectl -n luffy exec myblog-5d9ff54d4b-n447p -- route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.244.1.1 0.0.0.0 UG 0 0 0 eth0 10.244.0.0 10.244.1.1 255.255.0.0 UG 0 0 0 eth0 10.244.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0  # 查看k8s-slave1 的veth pair 和网桥 $ brctl show bridge name bridge id STP enabled interfaces cni0 8000.6a9a0b341d88 no veth048cc253  veth76f8e4ce  vetha4c972e1 # 流量到了cni0后，查看slave1节点的route $ route -n Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.136.2 0.0.0.0 UG 100 0 0 eth0 10.0.136.0 0.0.0.0 255.255.255.0 U 0 0 0 vxlan20 10.244.0.0 10.244.0.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.1.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 10.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.136.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0  # 流量转发到了flannel.1网卡，查看该网卡，其实是vtep设备 $ ip -d link show flannel.1 4: flannel.1:  mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default  link/ether 8a:2a:89:4d:b0:31 brd ff:ff:ff:ff:ff:ff promiscuity 0  vxlan id 1 local 172.21.51.68 dev eth0 srcport 0 0 dstport 8472 nolearning ageing 300 noudpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535  # 该转发到哪里，通过etcd查询数据，然后本地缓存，流量不用走多播发送 $ bridge fdb show dev flannel.1 a6:64:a0:a5:83:55 dst 192.168.136.10 self permanent 86:c2:ad:4e:47:20 dst 172.21.51.69 self permanent  # 对端的vtep设备接收到请求后做解包，取出源payload内容，查看k8s-slave2的路由 $ route -n Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.136.2 0.0.0.0 UG 100 0 0 eth0 10.0.136.0 0.0.0.0 255.255.255.0 U 0 0 0 vxlan20 10.244.0.0 10.244.0.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.2.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.136.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0  #根据路由规则转发到cni0网桥,然后由网桥转到具体的Pod中 实际的请求图：\n k8s-slave1 节点中的 pod-a（10.244.2.19）当中的 IP 包通过 pod-a 内的路由表被发送到eth0，进一步通过veth pair转到宿主机中的网桥 cni0 到达 cni0 当中的 IP 包通过匹配节点 k8s-slave1 的路由表发现通往 10.244.2.19 的 IP 包应该交给 flannel.1 接口 flannel.1 作为一个 VTEP 设备，收到报文后将按照 VTEP 的配置进行封包，第一次会查询ETCD，知道10.244.2.19的vtep设备是k8s-slave2机器，IP地址是172.21.51.69，拿到MAC 地址进行 VXLAN 封包。 通过节点 k8s-slave2 跟 k8s-slave1之间的网络连接，VXLAN 包到达 k8s-slave2 的 eth0 接口 通过端口 8472，VXLAN 包被转发给 VTEP 设备 flannel.1 进行解包 解封装后的 IP 包匹配节点 k8s-slave2 当中的路由表（10.244.2.0），内核将 IP 包转发给cni0 cni0将 IP 包转发给连接在 cni0 上的 pod-b  利用host-gw模式提升集群网络性能 vxlan模式适用于三层可达的网络环境，对集群的网络要求很宽松，但是同时由于会通过VTEP设备进行额外封包和解包，因此给性能带来了额外的开销。\n网络插件的目的其实就是将本机的cni0网桥的流量送到目的主机的cni0网桥。实际上有很多集群是部署在同一二层网络环境下的，可以直接利用二层的主机当作流量转发的网关。这样的话，可以不用进行封包解包，直接通过路由表去转发流量。\n为什么三层可达的网络不直接利用网关转发流量？\n内核当中的路由规则，网关必须在跟主机当中至少一个 IP 处于同一网段。 由于k8s集群内部各节点均需要实现Pod互通，因此，也就意味着host-gw模式需要整个集群节点都在同一二层网络内。 修改flannel的网络后端：\n$ kubectl edit cm kube-flannel-cfg -n kube-system ... net-conf.json: |  {  \"Network\": \"10.244.0.0/16\",  \"Backend\": {  \"Type\": \"host-gw\"  }  } kind: ConfigMap ... 重建Flannel的Pod\n$ kubectl -n kube-system get po |grep flannel kube-flannel-ds-amd64-5dgb8 1/1 Running 0 15m kube-flannel-ds-amd64-c2gdc 1/1 Running 0 14m kube-flannel-ds-amd64-t2jdd 1/1 Running 0 15m  $ kubectl -n kube-system delete po kube-flannel-ds-amd64-5dgb8 kube-flannel-ds-amd64-c2gdc kube-flannel-ds-amd64-t2jdd  # 等待Pod新启动后，查看日志，出现Backend type: host-gw字样 $ kubectl -n kube-system logs -f kube-flannel-ds-amd64-4hjdw I0704 01:18:11.916374 1 kube.go:126] Waiting 10m0s for node controller to sync I0704 01:18:11.916579 1 kube.go:309] Starting kube subnet manager I0704 01:18:12.917339 1 kube.go:133] Node controller sync successful I0704 01:18:12.917848 1 main.go:247] Installing signal handlers I0704 01:18:12.918569 1 main.go:386] Found network config - Backend type: host-gw I0704 01:18:13.017841 1 main.go:317] Wrote subnet file to /run/flannel/subnet.env 查看节点路由表：\n$ route -n Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.136.2 0.0.0.0 UG 100 0 0 eth0 10.244.0.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 10.244.1.0 172.21.51.68 255.255.255.0 UG 0 0 0 eth0 10.244.2.0 172.21.51.69 255.255.255.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.136.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0  k8s-slave1 节点中的 pod-a（10.244.2.19）当中的 IP 包通过 pod-a 内的路由表被发送到eth0，进一步通过veth pair转到宿主机中的网桥 cni0 到达 cni0 当中的 IP 包通过匹配节点 k8s-slave1 的路由表发现通往 10.244.2.19 的 IP 包应该使用172.21.51.69这个网关进行转发 包到达k8s-slave2节点（172.21.51.69）节点的eth0网卡，根据该节点的路由规则，转发给cni0网卡 cni0将 IP 包转发给连接在 cni0 上的 pod-b  Kubernetes认证与授权 APIServer安全控制   Authentication：身份认证\n 这个环节它面对的输入是整个http request，负责对来自client的请求进行身份校验，支持的方法包括:  basic auth client证书验证（https双向验证） jwt token(用于serviceaccount)   APIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐个使用这些方法对客户端请求进行验证， 只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功； 使用kubeadm引导启动的k8s集群，apiserver的初始配置中，默认支持client证书验证和serviceaccount两种身份验证方式。 证书认证通过设置--client-ca-file根证书以及--tls-cert-file和--tls-private-key-file来开启。 在这个环节，apiserver会通过client证书或 http header中的字段(比如serviceaccount的jwt token)来识别出请求的用户身份，包括”user”、”group”等，这些信息将在后面的authorization环节用到。    Authorization：鉴权，你可以访问哪些资源\n 这个环节面对的输入是http request context中的各种属性，包括：user、group、request path（比如：/api/v1、/healthz、/version等）、 request verb(比如：get、list、create等)。 APIServer会将这些属性值与事先配置好的访问策略(access policy）相比较。APIServer支持多种authorization mode，包括Node、RBAC、Webhook等。 APIServer启动时，可以指定一种authorization mode，也可以指定多种authorization mode，如果是后者，只要Request通过了其中一种mode的授权， 那么该环节的最终结果就是授权成功。在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，authorization-mode的默认配置是”Node,RBAC”。    Admission Control：准入控制，一个控制链(层层关卡)，用于拦截请求的一种方式。偏集群安全控制、管理方面。\n  为什么需要？\n认证与授权获取 http 请求 header 以及证书，无法通过body内容做校验。\nAdmission 运行在 API Server 的增删改查 handler 中，可以自然地操作 API resource\n  举个栗子\n  以NamespaceLifecycle为例， 该插件确保处于Termination状态的Namespace不再接收新的对象创建请求，并拒绝请求不存在的Namespace。该插件还可以防止删除系统保留的Namespace:default，kube-system，kube-public。\n  LimitRanger，若集群的命名空间设置了LimitRange对象，若Pod声明时未设置资源值，则按照LimitRange的定义来未Pod添加默认值\napiVersion: v1 kind: LimitRange metadata:  name: mem-limit-range  namespace: luffy spec:  limits:  - default:  memory: 512Mi  defaultRequest:  memory: 256Mi  type: Container --- apiVersion: v1 kind: Pod metadata:  name: default-mem-demo-2 spec:  containers:  - name: default-mem-demo-2-ctr  image: nginx:alpin   NodeRestriction， 此插件限制kubelet修改Node和Pod对象，这样的kubelets只允许修改绑定到Node的Pod API对象，以后版本可能会增加额外的限制 。开启Node授权策略后，默认会打开该项\n    怎么用？\nAPIServer启动时通过 --enable-admission-plugins --disable-admission-plugins 指定需要打开或者关闭的 Admission Controller\n  场景\n 自动注入sidecar容器或者initContainer容器 webhook admission，实现业务自定义的控制需求      kubectl的认证授权 kubectl的日志调试级别：\n   信息 描述     v=0 通常，这对操作者来说总是可见的。   v=1 当您不想要很详细的输出时，这个是一个合理的默认日志级别。   v=2 有关服务和重要日志消息的有用稳定状态信息，这些信息可能与系统中的重大更改相关。这是大多数系统推荐的默认日志级别。   v=3 关于更改的扩展信息。   v=4 调试级别信息。   v=6 显示请求资源。   v=7 显示 HTTP 请求头。   v=8 显示 HTTP 请求内容。   v=9 显示 HTTP 请求内容，并且不截断内容。    $ kubectl get nodes -v=7 I0329 20:20:08.633065 3979 loader.go:359] Config loaded from file /root/.kube/config I0329 20:20:08.633797 3979 round_trippers.go:416] GET https://192.168.136.10:6443/api/v1/nodes?limit=500 kubeadm init启动完master节点后，会默认输出类似下面的提示内容：\n... ... Your Kubernetes master has initialized successfully!  To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/config ... ... 这些信息是在告知我们如何配置kubeconfig文件。按照上述命令配置后，master节点上的kubectl就可以直接使用$HOME/.kube/config的信息访问k8s cluster了。 并且，通过这种配置方式，kubectl也拥有了整个集群的管理员(root)权限。\n很多K8s初学者在这里都会有疑问：\n 当kubectl使用这种kubeconfig方式访问集群时，Kubernetes的kube-apiserver是如何对来自kubectl的访问进行身份验证(authentication)和授权(authorization)的呢？ 为什么来自kubectl的请求拥有最高的管理员权限呢？  查看/root/.kube/config文件：\n前面提到过apiserver的authentication支持通过tls client certificate、basic auth、token等方式对客户端发起的请求进行身份校验， 从kubeconfig信息来看，kubectl显然在请求中使用了tls client certificate的方式，即客户端的证书。\n证书base64解码：\n$ echo xxxxxxxxxxxxxx |base64 -d  kubectl.crt 说明在认证阶段，apiserver会首先使用--client-ca-file配置的CA证书去验证kubectl提供的证书的有效性,基本的方式 ：\n$ openssl verify -CAfile /etc/kubernetes/pki/ca.crt kubectl.crt kubectl.crt: OK 除了认证身份，还会取出必要的信息供授权阶段使用，文本形式查看证书内容：\n$ openssl x509 -in kubectl.crt -text Certificate:  Data:  Version: 3 (0x2)  Serial Number: 4736260165981664452 (0x41ba9386f52b74c4)  Signature Algorithm: sha256WithRSAEncryption  Issuer: CN=kubernetes  Validity  Not Before: Feb 10 07:33:39 2020 GMT  Not After : Feb 9 07:33:40 2021 GMT  Subject: O=system:masters, CN=kubernetes-admin  ... 认证通过后，提取出签发证书时指定的CN(Common Name),kubernetes-admin，作为请求的用户名 (User Name), 从证书中提取O(Organization)字段作为请求用户所属的组 (Group)，group = system:masters，然后传递给后面的授权模块。\nkubeadm在init初始引导集群启动过程中，创建了许多默认的RBAC规则， 在k8s有关RBAC的官方文档中，我们看到下面一些default clusterrole列表:\n其中第一个cluster-admin这个cluster role binding绑定了system:masters group，这和authentication环节传递过来的身份信息不谋而合。 沿着system:masters group对应的cluster-admin clusterrolebinding“追查”下去，真相就会浮出水面。\n我们查看一下这一binding：\n$ kubectl describe clusterrolebinding cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true Role:  Kind: ClusterRole  Name: cluster-admin Subjects:  Kind Name Namespace  ---- ---- ---------  Group system:masters 我们看到在kube-system名字空间中，一个名为cluster-admin的clusterrolebinding将cluster-admin cluster role与system:masters Group绑定到了一起， 赋予了所有归属于system:masters Group中用户cluster-admin角色所拥有的权限。\n我们再来查看一下cluster-admin这个role的具体权限信息：\n$ kubectl describe clusterrole cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule:  Resources Non-Resource URLs Resource Names Verbs  --------- ----------------- -------------- -----  *.* [] [] [*]  [*] [] [*] 非资源类，如查看集群健康状态。\nRBAC Role-Based Access Control，基于角色的访问控制， apiserver启动参数添加–authorization-mode=RBAC 来启用RBAC认证模式，kubeadm安装的集群默认已开启。官方介绍\n查看开启：\n# master节点查看apiserver进程 $ ps aux |grep apiserver RBAC模式引入了4个资源类型：\n  Role，角色\n一个Role只能授权访问单个namespace\n## 示例定义一个名为pod-reader的角色，该角色具有读取default这个命名空间下的pods的权限 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:  namespace: default  name: pod-reader rules: - apiGroups: [\"\"] # \"\" indicates the core API group  resources: [\"pods\"]  verbs: [\"get\", \"watch\", \"list\"]  ## apiGroups: \"\",\"apps\", \"autoscaling\", \"batch\", kubectl api-versions ## resources: \"services\", \"pods\",\"deployments\"... kubectl api-resources ## verbs: \"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\", \"delete\", \"exec\"  ## https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/   ClusterRole\n一个ClusterRole能够授予和Role一样的权限，但是它是集群范围内的。\n## 定义一个集群角色，名为secret-reader，该角色可以读取所有的namespace中的secret资源 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  # \"namespace\" omitted since ClusterRoles are not namespaced  name: secret-reader rules: - apiGroups: [\"\"]  resources: [\"secrets\"]  verbs: [\"get\", \"watch\", \"list\"]  # User,Group,ServiceAccount   Rolebinding\n将role中定义的权限分配给用户和用户组。RoleBinding包含主题（users,groups,或service accounts）和授予角色的引用。对于namespace内的授权使用RoleBinding，集群范围内使用ClusterRoleBinding。\n## 定义一个角色绑定，将pod-reader这个role的权限授予给jane这个User，使得jane可以在读取default这个命名空间下的所有的pod数据 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: read-pods  namespace: default subjects: - kind: User  #这里可以是User,Group,ServiceAccount  name: jane   apiGroup: rbac.authorization.k8s.io roleRef:  kind: Role #这里可以是Role或者ClusterRole,若是ClusterRole，则权限也仅限于rolebinding的内部  name: pod-reader # match the name of the Role or ClusterRole you wish to bind to  apiGroup: rbac.authorization.k8s.io 注意：rolebinding既可以绑定role，也可以绑定clusterrole，当绑定clusterrole的时候，subject的权限也会被限定于rolebinding定义的namespace内部，若想跨namespace，需要使用clusterrolebinding\n## 定义一个角色绑定，将dave这个用户和secret-reader这个集群角色绑定，虽然secret-reader是集群角色，但是因为是使用rolebinding绑定的，因此dave的权限也会被限制在development这个命名空间内 apiVersion: rbac.authorization.k8s.io/v1 # This role binding allows \"dave\" to read secrets in the \"development\" namespace. # You need to already have a ClusterRole named \"secret-reader\". kind: RoleBinding metadata:  name: read-secrets  #  # The namespace of the RoleBinding determines where the permissions are granted.  # This only grants permissions within the \"development\" namespace.  namespace: development subjects: - kind: User  name: dave # Name is case sensitive  apiGroup: rbac.authorization.k8s.io - kind: ServiceAccount  name: dave # Name is case sensitive  namespace: luffy roleRef:  kind: ClusterRole  name: secret-reader  apiGroup: rbac.authorization.k8s.io 考虑一个场景： 如果集群中有多个namespace分配给不同的管理员，每个namespace的权限是一样的，就可以只定义一个clusterrole，然后通过rolebinding将不同的namespace绑定到管理员身上，否则就需要每个namespace定义一个Role，然后做一次rolebinding。\n  ClusterRolebingding\n允许跨namespace进行授权\napiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows anyone in the \"manager\" group to read secrets in any namespace. kind: ClusterRoleBinding metadata:  name: read-secrets-global subjects: - kind: Group  name: manager # Name is case sensitive  apiGroup: rbac.authorization.k8s.io roleRef:  kind: ClusterRole  name: secret-reader  apiGroup: rbac.authorization.k8s.io   kubelet的认证授权 查看kubelet进程\n$ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent  Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)  Drop-In: /usr/lib/systemd/system/kubelet.service.d  └─10-kubeadm.conf  Active: active (running) since Sun 2020-07-05 19:33:36 EDT; 1 day 12h ago  Docs: https://kubernetes.io/docs/  Main PID: 10622 (kubelet)  Tasks: 24  Memory: 60.5M  CGroup: /system.slice/kubelet.service  └─851 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf 查看/etc/kubernetes/kubelet.conf，解析证书：\n$ echo xxxxx |base64 -d kubelet.crt $ openssl x509 -in kubelet.crt -text Certificate:  Data:  Version: 3 (0x2)  Serial Number: 9059794385454520113 (0x7dbadafe23185731)  Signature Algorithm: sha256WithRSAEncryption  Issuer: CN=kubernetes  Validity  Not Before: Feb 10 07:33:39 2020 GMT  Not After : Feb 9 07:33:40 2021 GMT  Subject: O=system:nodes, CN=system:node:master-1 得到我们期望的内容：\nSubject: O=system:nodes, CN=system:node:k8s-master 我们知道，k8s会把O作为Group来进行请求，因此如果有权限绑定给这个组，肯定在clusterrolebinding的定义中可以找得到。因此尝试去找一下绑定了system:nodes组的clusterrolebinding\n$ kubectl get clusterrolebinding|awk 'NR1{print $1}'|xargs kubectl get clusterrolebinding -oyaml|grep -n10 system:nodes 98- roleRef: 99- apiGroup: rbac.authorization.k8s.io 100- kind: ClusterRole 101- name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 102- subjects: 103- - apiGroup: rbac.authorization.k8s.io 104- kind: Group 105: name: system:nodes 106-- apiVersion: rbac.authorization.k8s.io/v1 107- kind: ClusterRoleBinding 108- metadata: 109- creationTimestamp: \"2020-02-10T07:34:02Z\" 110- name: kubeadm:node-proxier 111- resourceVersion: \"213\" 112- selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-proxier  $ kubectl describe clusterrole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule:  Resources Non-Resource URLs Resource Names Verbs  --------- ----------------- -------------- -----  certificatesigningrequests.certificates.k8s.io/selfnodeclient [] [] [create] 结局有点意外，除了system:certificates.k8s.io:certificatesigningrequests:selfnodeclient外，没有找到system相关的rolebindings，显然和我们的理解不一样。 尝试去找资料，发现了这么一段 :\n   Default ClusterRole Default ClusterRoleBinding Description     system:kube-scheduler system:kube-scheduler user Allows access to the resources required by the schedulercomponent.   system:volume-scheduler system:kube-scheduler user Allows access to the volume resources required by the kube-scheduler component.   system:kube-controller-manager system:kube-controller-manager user Allows access to the resources required by the controller manager component. The permissions required by individual controllers are detailed in the controller roles.   system:node None Allows access to resources required by the kubelet, including read access to all secrets, and write access to all pod status objects. You should use the Node authorizer and NodeRestriction admission plugin instead of the system:node role, and allow granting API access to kubelets based on the Pods scheduled to run on them. The system:node role only exists for compatibility with Kubernetes clusters upgraded from versions prior to v1.8.   system:node-proxier system:kube-proxy user Allows access to the resources required by the kube-proxycomponent.    大致意思是说：之前会定义system:node这个角色，目的是为了kubelet可以访问到必要的资源，包括所有secret的读权限及更新pod状态的写权限。如果1.8版本后，是建议使用 Node authorizer and NodeRestriction admission plugin 来代替这个角色的。\n我们目前使用1.16，查看一下授权策略：\n$ ps axu|grep apiserver kube-apiserver --authorization-mode=Node,RBAC --enable-admission-plugins=NodeRestriction 查看一下官网对Node authorizer的介绍：\nNode authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.\nIn future releases, the node authorizer may add or remove permissions to ensure kubelets have the minimal set of permissions required to operate correctly.\nIn order to be authorized by the Node authorizer, kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:\nService Account及K8S Api调用 前面说，认证可以通过证书，也可以通过使用ServiceAccount（服务账户）的方式来做认证。大多数时候，我们在基于k8s做二次开发时都是选择通过ServiceAccount + RBAC 的方式。我们之前访问dashboard的时候，是如何做的？\n## 新建一个名为admin的serviceaccount，并且把名为cluster-admin的这个集群角色的权限授予新建的 #serviceaccount apiVersion: v1 kind: ServiceAccount metadata:  name: admin  namespace: kubernetes-dashboard --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata:  name: admin  annotations:  rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef:  kind: ClusterRole  name: cluster-admin  apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount  name: admin  namespace: kubernetes-dashboard 我们查看一下：\n$ kubectl -n kubernetes-dashboard get sa admin -o yaml apiVersion: v1 kind: ServiceAccount metadata:  creationTimestamp: \"2020-04-01T11:59:21Z\"  name: admin  namespace: kubernetes-dashboard  resourceVersion: \"1988878\"  selfLink: /api/v1/namespaces/kubernetes-dashboard/serviceaccounts/admin  uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f secrets: - name: admin-token-lfsrf 注意到serviceaccount上默认绑定了一个名为admin-token-lfsrf的secret，我们查看一下secret\n$ kubectl -n kubernetes-dashboard describe secret admin-token-lfsrf Name: admin-token-lfsrf Namespace: kubernetes-dashboard Labels:  Annotations: kubernetes.io/service-account.name: admin  kubernetes.io/service-account.uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f  Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 4 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLWxmc3JmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjM5ZWNjM2UtNzRkOS0xMWVhLWE1OWItMDAwYzI5ZGZkNzNmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlbW86YWRtaW4ifQ.ffGCU4L5LxTsMx3NcNixpjT6nLBi-pmstb4I-W61nLOzNaMmYSEIwAaugKMzNR-2VwM14WbuG04dOeO67niJeP6n8-ALkl-vineoYCsUjrzJ09qpM3TNUPatHFqyjcqJ87h4VKZEqk2qCCmLxB6AGbEHpVFkoge40vHs56cIymFGZLe53JZkhu3pwYuS4jpXytV30Ad-HwmQDUu_Xqcifni6tDYPCfKz2CZlcOfwqHeGIHJjDGVBKqhEeo8PhStoofBU6Y4OjObP7HGuTY-Foo4QindNnpp0QU6vSb7kiOiQ4twpayybH8PTf73dtdFt46UF6mGjskWgevgolvmO8A 演示role的权限：\n$ cat test-sa.yaml serviceaccount apiVersion: v1 kind: ServiceAccount metadata:  name: test  namespace: kubernetes-dashboard  --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata:  name: test  annotations:  rbac.authorization.kubernetes.io/autoupdate: \"true\" roleRef:  kind: ClusterRole  name: cluster-admin  apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount  name: test  namespace: kubernetes-dashboard curl演示\n$ curl -k -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InhXcmtaSG5ZODF1TVJ6dUcycnRLT2c4U3ZncVdoVjlLaVRxNG1wZ0pqVmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1xNXBueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImViZDg2ODZjLWZkYzAtNDRlZC04NmZlLTY5ZmE0ZTE1YjBmMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbiJ9.iEIVMWg2mHPD88GQ2i4uc_60K4o17e39tN0VI_Q_s3TrRS8hmpi0pkEaN88igEKZm95Qf1qcN9J5W5eqOmcK2SN83Dd9dyGAGxuNAdEwi0i73weFHHsjDqokl9_4RGbHT5lRY46BbIGADIphcTeVbCggI6T_V9zBbtl8dcmsd-lD_6c6uC2INtPyIfz1FplynkjEVLapp_45aXZ9IMy76ljNSA8Uc061Uys6PD3IXsUD5JJfdm7lAt0F7rn9SdX1q10F2lIHYCMcCcfEpLr4Vkymxb4IU4RCR8BsMOPIO_yfRVeYZkG4gU2C47KwxpLsJRrTUcUXJktSEPdeYYXf9w\" https://192.168.136.10:6443/api/v1/namespaces/luffy/pods?limit=500 通过HPA实现业务应用的动态扩缩容 HPA控制器介绍 当系统资源过高的时候，我们可以使用如下命令来实现 Pod 的扩缩容功能\n$ kubectl -n luffy scale deployment myblog --replicas=2 但是这个过程是手动操作的。在实际项目中，我们需要做到是的是一个自动化感知并自动扩容的操作。Kubernetes 也为提供了这样的一个资源对象：Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称HPA\n基本原理：HPA 通过监控分析控制器控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量\nHPA的实现有两个版本：\n autoscaling/v1，只包含了根据CPU指标的检测，稳定版本 autoscaling/v2beta1，支持根据memory或者用户自定义指标进行伸缩  如何获取Pod的监控数据？\n k8s 1.8以下：使用heapster，1.11版本完全废弃 k8s 1.8以上：使用metric-server  思考：为什么之前用 heapster ，现在废弃了项目，改用 metric-server ？\nheapster时代，apiserver 会直接将metric请求通过apiserver proxy 的方式转发给集群内的 hepaster 服务，采用这种 proxy 方式是有问题的：\n  http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/service_name[:port_name]/proxy   proxy只是代理请求，一般用于问题排查，不够稳定，且版本不可控\n  heapster的接口不能像apiserver一样有完整的鉴权以及client集成\n  pod 的监控数据是核心指标（HPA调度），应该和 pod 本身拥有同等地位，即 metric应该作为一种资源存在，如metrics.k8s.io 的形式，称之为 Metric Api\n  于是官方从 1.8 版本开始逐步废弃 heapster，并提出了上边 Metric api 的概念，而 metrics-server 就是这种概念下官方的一种实现，用于从 kubelet获取指标，替换掉之前的 heapster。\nMetrics Server 可以通过标准的 Kubernetes API 把监控数据暴露出来，比如获取某一Pod的监控数据：\nhttps://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces//pods/  # https://192.168.136.10:6443/api/v1/namespaces/luffy/pods?limit=500 目前的采集流程：\nMetric Server 官方介绍\n... Metric server collects metrics from the Summary API, exposed by Kubelet on each node.  Metrics Server registered in the main API server through Kubernetes aggregator, which was introduced in Kubernetes 1.7 ... 安装 官方代码仓库地址：https://github.com/kubernetes-sigs/metrics-server\nDepending on your cluster setup, you may also need to change flags passed to the Metrics Server container. Most useful flags:\n --kubelet-preferred-address-types - The priority of node address types used when determining an address for connecting to a particular node (default [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]) --kubelet-insecure-tls - Do not verify the CA of serving certificates presented by Kubelets. For testing purposes only. --requestheader-client-ca-file - Specify a root certificate bundle for verifying client certificates on incoming requests.  $ wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml 修改args参数：\n...  84 containers:  85 - name: metrics-server  86 image: registry.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6  87 imagePullPolicy: IfNotPresent  88 args:  89 - --cert-dir=/tmp  90 - --secure-port=4443  91 - --kubelet-insecure-tls  92 - --kubelet-preferred-address-types=InternalIP ... 执行安装：\n$ kubectl create -f components.yaml  $ kubectl -n kube-system get pods  $ kubectl top nodes kubelet的指标采集 无论是 heapster还是 metric-server，都只是数据的中转和聚合，两者都是调用的 kubelet 的 api 接口获取的数据，而 kubelet 代码中实际采集指标的是 cadvisor 模块，你可以在 node 节点访问 10250 端口获取监控数据：\n Kubelet Summary metrics: https://127.0.0.1:10250/metrics，暴露 node、pod 汇总数据 Cadvisor metrics: https://127.0.0.1:10250/metrics/cadvisor，暴露 container 维度数据  调用示例：\n$ curl -k -H \"Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InhXcmtaSG5ZODF1TVJ6dUcycnRLT2c4U3ZncVdoVjlLaVRxNG1wZ0pqVmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1xNXBueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImViZDg2ODZjLWZkYzAtNDRlZC04NmZlLTY5ZmE0ZTE1YjBmMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbiJ9.iEIVMWg2mHPD88GQ2i4uc_60K4o17e39tN0VI_Q_s3TrRS8hmpi0pkEaN88igEKZm95Qf1qcN9J5W5eqOmcK2SN83Dd9dyGAGxuNAdEwi0i73weFHHsjDqokl9_4RGbHT5lRY46BbIGADIphcTeVbCggI6T_V9zBbtl8dcmsd-lD_6c6uC2INtPyIfz1FplynkjEVLapp_45aXZ9IMy76ljNSA8Uc061Uys6PD3IXsUD5JJfdm7lAt0F7rn9SdX1q10F2lIHYCMcCcfEpLr4Vkymxb4IU4RCR8BsMOPIO_yfRVeYZkG4gU2C47KwxpLsJRrTUcUXJktSEPdeYYXf9w\" https://localhost:10250/metrics kubelet虽然提供了 metric 接口，但实际监控逻辑由内置的cAdvisor模块负责，早期的时候，cadvisor是单独的组件，从k8s 1.12开始，cadvisor 监听的端口在k8s中被删除，所有监控数据统一由Kubelet的API提供。\ncadvisor获取指标时实际调用的是 runc/libcontainer库，而libcontainer是对 cgroup文件 的封装，即 cadvsior也只是个转发者，它的数据来自于cgroup文件。\ncgroup文件中的值是监控数据的最终来源，如\n  mem usage的值，\n  对于docker容器来讲，来源于/sys/fs/cgroup/memory/docker/[containerId]/memory.usage_in_bytes\n  对于pod来讲，/sys/fs/cgroup/memory/kubepods/besteffort/pod[podId]/memory.usage_in_bytes或者\n/sys/fs/cgroup/memory/kubepods/burstable/pod[podId]/memory.usage_in_bytes\n    如果没限制内存，Limit = machine_mem，否则来自于 /sys/fs/cgroup/memory/docker/[id]/memory.limit_in_bytes\n  内存使用率 = memory.usage_in_bytes/memory.limit_in_bytes\n  Metrics数据流：\n思考：\nMetrics Server是独立的一个服务，只能服务内部实现自己的api，是如何做到通过标准的kubernetes 的API格式暴露出去的？\nkube-aggregator\nkube-aggregator聚合器及Metric-Server的实现 kube-aggregator是对 apiserver 的api的一种拓展机制，它允许开发人员编写一个自己的服务，并把这个服务注册到k8s的api里面，即扩展 API 。\n定义一个APIService对象：\napiVersion: apiregistration.k8s.io/v1 kind: APIService metadata:  name: v1beta1.luffy.k8s.io spec:  group: luffy.k8s.io  groupPriorityMinimum: 100  insecureSkipTLSVerify: true  service:  name: service-A  # 必须https访问  namespace: luffy  port: 443  version: v1beta1  versionPriority: 100 k8s会自动帮我们代理如下url的请求：\nproxyPath := \"/apis/\" + apiService.Spec.Group + \"/\" + apiService.Spec.Version 即：https://192.168.136.10:6443/apis/luffy.k8s.io/v1beta1/xxxx转到我们的service-A服务中，service-A中只需要实现 https://service-A/luffy.k8s.io/v1beta1/xxxx 即可。\n看下metric-server的实现：\n$ kubectl get apiservice NAME SERVICE AVAILABLE v1beta1.metrics.k8s.io kube-system/metrics-server\tTrue  $ kubectl get apiservice v1beta1.metrics.k8s.io -oyaml ... spec:  group: metrics.k8s.io  groupPriorityMinimum: 100  insecureSkipTLSVerify: true  service:  name: metrics-server  namespace: kube-system  port: 443  version: v1beta1  versionPriority: 100 ...  $ kubectl -n kube-system get svc metrics-server NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE metrics-server ClusterIP 10.110.111.146  443/TCP 11h  $ curl -k -H \"Authorization: Bearer xxxx\" https://10.110.111.146 {  \"paths\": [  \"/apis\",  \"/apis/metrics.k8s.io\",  \"/apis/metrics.k8s.io/v1beta1\",  \"/healthz\",  \"/healthz/healthz\",  \"/healthz/log\",  \"/healthz/ping\",  \"/healthz/poststarthook/generic-apiserver-start-informers\",  \"/metrics\",  \"/openapi/v2\",  \"/version\"  ]  # https://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces//pods/ #  $ curl -k -H \"Authorization: Bearer xxxx\" https://10.110.111.146/apis/metrics.k8s.io/v1beta1/namespaces/luffy/pods/myblog-5d9ff54d4b-4rftt  $ curl -k -H \"Authorization: Bearer xxxx\" https://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces/luffy/pods/myblog-5d9ff54d4b-4rftt HPA实践 基于CPU的动态伸缩 创建hpa对象：\n# 方式一 $ cat hpa-myblog.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata:  name: hpa-myblog-cpu  namespace: luffy spec:  maxReplicas: 3  minReplicas: 1  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: myblog  targetCPUUtilizationPercentage: 10  # 方式二 $ kubectl -n luffy autoscale deployment myblog --cpu-percent=10 --min=1 --max=3  Deployment对象必须配置requests的参数，不然无法获取监控数据，也无法通过HPA进行动态伸缩\n 验证：\n$ yum -y install httpd-tools $ kubectl -n luffy get svc myblog myblog ClusterIP 10.104.245.225  80/TCP 6d18h  # 为了更快看到效果，先调整副本数为1 $ kubectl -n luffy scale deploy myblog --replicas=1  # 模拟1000个用户并发访问页面10万次 $ ab -n 100000 -c 1000 http://10.104.245.225/blog/index/  $ kubectl get hpa $ kubectl -n luffy get pods 压力降下来后，会有默认5分钟的scaledown的时间，可以通过controller-manager的如下参数设置：\n--horizontal-pod-autoscaler-downscale-stabilization  The value for this option is a duration that specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed. The default value is 5 minutes (5m0s). 是一个逐步的过程，当前的缩放完成后，下次缩放的时间间隔，比如从3个副本降低到1个副本，中间大概会等待2*5min = 10分钟\n基于内存的动态伸缩 创建hpa对象\n$ cat hpa-demo-mem.yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: hpa-demo-mem  namespace: luffy spec:  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: hpa-demo-mem  minReplicas: 1  maxReplicas: 3  metrics:  - type: Resource  resource:  name: memory  targetAverageUtilization: 30 加压演示脚本：\n$ cat increase-mem-config.yaml apiVersion: v1 kind: ConfigMap metadata:  name: increase-mem-config  namespace: luffy data:  increase-mem.sh: |  #!/bin/bash   mkdir /tmp/memory  mount -t tmpfs -o size=40M tmpfs /tmp/memory  dd if=/dev/zero of=/tmp/memory/block  sleep 60  rm /tmp/memory/block  umount /tmp/memory  rmdir /tmp/memory 测试deployment：\n$ cat hpa-demo-mem-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: hpa-demo-mem  namespace: luffy spec:  selector:  matchLabels:  app: nginx  template:  metadata:  labels:  app: nginx  spec:  volumes:  - name: increase-mem-script  configMap:  name: increase-mem-config  containers:  - name: nginx  image: nginx:alpine  ports:  - containerPort: 80  volumeMounts:  - name: increase-mem-script  mountPath: /etc/script  resources:  requests:  memory: 50Mi  cpu: 50m  securityContext:  privileged: true 测试：\n$ kubectl create -f increase-mem-config.yaml $ kubectl create -f hpa-demo-mem.yaml $ kubectl create -f hpa-demo-mem-deploy.yaml  $ kubectl -n luffy exec -ti hpa-demo-mem-7fc75bf5c8-xx424 sh #/ sh /etc/script/increase-mem.sh   # 观察hpa及pod $ kubectl -n luffy get hpa $ kubectl -n luffy get po 基于自定义指标的动态伸缩 除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使用 Prometheus Adapter，Prometheus 用于监控应用的负载和集群本身的各种指标，Prometheus Adapter 可以帮我们使用 Prometheus 收集的指标并使用它们来制定扩展策略，这些指标都是通过 APIServer 暴露的，而且 HPA 资源对象也可以很轻易的直接使用。\n架构图：\nkubernetes对接分部式存储 PV与PVC快速入门 k8s存储的目的就是保证Pod重建后，数据不丢失。简单的数据持久化的下述方式：\n  emptyDir\napiVersion: v1 kind: Pod metadata:  name: test-pd spec:  containers:  - image: k8s.gcr.io/test-webserver  name: webserver  volumeMounts:  - mountPath: /cache  name: cache-volume  - image: k8s.gcr.io/test-redis  name: redis  volumeMounts:  - mountPath: /data  name: cache-volume volumes:  - name: cache-volume  emptyDir: {}  Pod内的容器共享卷的数据 存在于Pod的生命周期，Pod销毁，数据丢失 Pod内的容器自动重建后，数据不会丢失    hostPath\napiVersion: v1 kind: Pod metadata:  name: test-pd spec:  containers:  - image: k8s.gcr.io/test-webserver  name: test-container  volumeMounts:  - mountPath: /test-pd  name: test-volume  volumes:  - name: test-volume  hostPath:  # directory location on host  path: /data  # this field is optional  type: Directory 通常配合nodeSelector使用\n  nfs存储\n...  volumes:  - name: redisdata  #卷名称  nfs: #使用NFS网络存储卷  server: 192.168.31.241 #NFS服务器地址  path: /data/redis  #NFS服务器共享的目录  readOnly: false #是否为只读 ...   volume支持的种类众多（参考 https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes ），每种对应不同的存储后端实现，因此为了屏蔽后端存储的细节，同时使得Pod在使用存储的时候更加简洁和规范，k8s引入了两个新的资源类型，PV和PVC。\nPersistentVolume（持久化卷），是对底层的存储的一种抽象，它和具体的底层的共享存储技术的实现方式有关，比如 Ceph、GlusterFS、NFS 等，都是通过插件机制完成与共享存储的对接。如使用PV对接NFS存储：\napiVersion: v1 kind: PersistentVolume metadata:  name: nfs-pv spec:  capacity:  storage: 1Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain  nfs:  path: /data/k8s  server: 121.204.157.52  capacity，存储能力， 目前只支持存储空间的设置， 就是我们这里的 storage=1Gi，不过未来可能会加入 IOPS、吞吐量等指标的配置。 accessModes，访问模式， 是用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：  ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载     persistentVolumeReclaimPolicy，pv的回收策略, 目前只有 NFS 和 HostPath 两种类型支持回收策略  Retain（保留）- 保留数据，需要管理员手工清理数据 Recycle（回收）- 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/* Delete（删除）- 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务，比如 ASW EBS。    因为PV是直接对接底层存储的，就像集群中的Node可以为Pod提供计算资源（CPU和内存）一样，PV可以为Pod提供存储资源。因此PV不是namespaced的资源，属于集群层面可用的资源。Pod如果想使用该PV，需要通过创建PVC挂载到Pod中。\nPVC全写是PersistentVolumeClaim（持久化卷声明），PVC 是用户存储的一种声明，创建完成后，可以和PV实现一对一绑定。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。\napiVersion: v1 kind: PersistentVolumeClaim metadata:  name: pvc-nfs  namespace: default spec:  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 1Gi 然后Pod中通过如下方式去使用：\n...  spec:  containers:  - name: nginx  image: nginx:alpine  imagePullPolicy: IfNotPresent  ports:  - containerPort: 80  name: web  volumeMounts: #挂载容器中的目录到pvc nfs中的目录  - name: www  mountPath: /usr/share/nginx/html  volumes:  - name: www  persistentVolumeClaim: #指定pvc  claimName: pvc-nfs ... PV与PVC管理NFS存储卷实践 环境准备 服务端：121.204.157.52\n$ yum -y install nfs-utils rpcbind  # 共享目录 $ mkdir -p /data/k8s \u0026\u0026 chmod 755 /data/k8s  $ echo '/data/k8s *(insecure,rw,sync,no_root_squash)'/etc/exports  $ systemctl enable rpcbind \u0026\u0026 systemctl start rpcbind $ systemctl enable nfs \u0026\u0026 systemctl start nfs 客户端：k8s集群slave节点\n$ yum -y install nfs-utils rpcbind $ mkdir /nfsdata $ mount -t nfs 121.204.157.52:/data/k8s /nfsdata PV与PVC演示 $ cat pv-nfs.yaml apiVersion: v1 kind: PersistentVolume metadata:  name: nfs-pv spec:  capacity:  storage: 1Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain  nfs:  path: /data/k8s  server: 121.204.157.52  $ kubectl create -f pv-nfs.yaml  $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS nfs-pv 1Gi RWO Retain Available 一个 PV 的生命周期中，可能会处于4中不同的阶段：\n Available（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PV 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败  $ cat pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata:  name: pvc-nfs  namespace: default spec:  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 1Gi  $ kubectl create -f pvc.yaml $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs Bound nfs-pv 1Gi RWO 3s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM nfs-pv 1Gi RWO Retain Bound default/pvc-nfs  #访问模式，storage大小（pvc大小需要小于pv大小），以及 PV 和 PVC 的 storageClassName 字段必须一样，这样才能够进行绑定。  #PersistentVolumeController会不断地循环去查看每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与未绑定的 PVC 进行绑定，这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态。而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。  # 查看nfs数据目录 $ ls /nfsdata 创建Pod挂载pvc\n$ cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: nfs-pvc spec:  replicas: 1  selector: #指定Pod的选择器  matchLabels:  app: nginx  template:  metadata:  labels:  app: nginx  spec:  containers:  - name: nginx  image: nginx:alpine  imagePullPolicy: IfNotPresent  ports:  - containerPort: 80  name: web  volumeMounts: #挂载容器中的目录到pvc nfs中的目录  - name: www  mountPath: /usr/share/nginx/html  volumes:  - name: www  persistentVolumeClaim: #指定pvc  claimName: pvc-nfs   $ kubectl create -f deployment.yaml  # 查看容器/usr/share/nginx/html目录 storageClass实现动态挂载 创建pv及pvc过程是手动，且pv与pvc一一对应，手动创建很繁琐。因此，通过storageClass + provisioner的方式来实现通过PVC自动创建并绑定PV。\n部署： https://github.com/kubernetes-retired/external-storage\nprovisioner.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: nfs-client-provisioner  labels:  app: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: default spec:  replicas: 1  selector:  matchLabels:  app: nfs-client-provisioner  strategy:  type: Recreate  selector:  matchLabels:  app: nfs-client-provisioner  template:  metadata:  labels:  app: nfs-client-provisioner  spec:  serviceAccountName: nfs-client-provisioner  containers:  - name: nfs-client-provisioner  image: quay.io/external_storage/nfs-client-provisioner:latest  volumeMounts:  - name: nfs-client-root  mountPath: /persistentvolumes  env:  - name: PROVISIONER_NAME  value: luffy.com/nfs  - name: NFS_SERVER  value: 172.21.51.55  - name: NFS_PATH   value: /data/k8s  volumes:  - name: nfs-client-root  nfs:  server: 172.21.51.55  path: /data/k8s rbac.yaml kind: ServiceAccount apiVersion: v1 metadata:  name: nfs-client-provisioner  namespace: nfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: nfs-client-provisioner-runner  namespace: nfs-provisioner rules:  - apiGroups: [\"\"]  resources: [\"persistentvolumes\"]  verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"]  - apiGroups: [\"\"]  resources: [\"persistentvolumeclaims\"]  verbs: [\"get\", \"list\", \"watch\", \"update\"]  - apiGroups: [\"storage.k8s.io\"]  resources: [\"storageclasses\"]  verbs: [\"get\", \"list\", \"watch\"]  - apiGroups: [\"\"]  resources: [\"events\"]  verbs: [\"create\", \"update\", \"patch\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: run-nfs-client-provisioner  namespace: nfs-provisioner subjects:  - kind: ServiceAccount  name: nfs-client-provisioner  namespace: nfs-provisioner roleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: leader-locking-nfs-client-provisioner  namespace: nfs-provisioner rules:  - apiGroups: [\"\"]  resources: [\"endpoints\"]  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: leader-locking-nfs-client-provisioner  namespace: nfs-provisioner subjects:  - kind: ServiceAccount  name: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: nfs-provisioner roleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io storage-class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:  name: nfs provisioner: luffy.com/nfs pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: test-claim2 spec:  accessModes:  - ReadWriteMany  resources:  requests:  storage: 1Mi  storageClassName: nfs 对接Ceph存储实践 ceph的安装及使用参考 http://docs.ceph.org.cn/start/intro/\n# CephFS需要使用两个Pool来分别存储数据和元数据 ceph osd pool create cephfs_data 128 ceph osd pool create cephfs_meta 128 ceph osd lspools  # 创建一个CephFS ceph fs new cephfs cephfs_meta cephfs_data  # 查看 ceph fs ls  # rados -p cephfs_meta ls storageClass实现动态挂载 创建pv及pvc过程是手动，且pv与pvc一一对应，手动创建很繁琐。因此，通过storageClass + provisioner的方式来实现通过PVC自动创建并绑定PV。\n比如，针对cephfs，可以创建如下类型的storageclass：\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata:  name: dynamic-cephfs provisioner: ceph.com/cephfs parameters:  monitors: 121.204.157.52:6789  adminId: admin  adminSecretName: ceph-admin-secret  adminSecretNamespace: \"kube-system\"  claimRoot: /volumes/kubernetes NFS，ceph-rbd，cephfs均提供了对应的provisioner\n部署cephfs-provisioner\n$ cat external-storage-cephfs-provisioner.yaml apiVersion: v1 kind: ServiceAccount metadata:  name: cephfs-provisioner  namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: cephfs-provisioner rules:  - apiGroups: [\"\"]  resources: [\"persistentvolumes\"]  verbs: [\"get\", \"list\", \"watch\", \"create\", \"delete\"]  - apiGroups: [\"\"]  resources: [\"persistentvolumeclaims\"]  verbs: [\"get\", \"list\", \"watch\", \"update\"]  - apiGroups: [\"storage.k8s.io\"]  resources: [\"storageclasses\"]  verbs: [\"get\", \"list\", \"watch\"]  - apiGroups: [\"\"]  resources: [\"events\"]  verbs: [\"create\", \"update\", \"patch\"]  - apiGroups: [\"\"]  resources: [\"endpoints\"]  verbs: [\"get\", \"list\", \"watch\", \"create\", \"update\", \"patch\"]  - apiGroups: [\"\"]  resources: [\"secrets\"]  verbs: [\"create\", \"get\", \"delete\"] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: cephfs-provisioner subjects:  - kind: ServiceAccount  name: cephfs-provisioner  namespace: kube-system roleRef:  kind: ClusterRole  name: cephfs-provisioner  apiGroup: rbac.authorization.k8s.io  --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata:  name: cephfs-provisioner  namespace: kube-system rules:  - apiGroups: [\"\"]  resources: [\"secrets\"]  verbs: [\"create\", \"get\", \"delete\"] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata:  name: cephfs-provisioner  namespace: kube-system roleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: cephfs-provisioner subjects: - kind: ServiceAccount  name: cephfs-provisioner  namespace: kube-system  --- apiVersion: apps/v1 kind: Deployment metadata:  name: cephfs-provisioner  namespace: kube-system spec:  replicas: 1  selector:  matchLabels:  app: cephfs-provisioner  strategy:  type: Recreate  template:  metadata:  labels:  app: cephfs-provisioner  spec:  containers:  - name: cephfs-provisioner  image: \"quay.io/external_storage/cephfs-provisioner:latest\"  env:  - name: PROVISIONER_NAME  value: ceph.com/cephfs  imagePullPolicy: IfNotPresent  command:  - \"/usr/local/bin/cephfs-provisioner\"  args:  - \"-id=cephfs-provisioner-1\"  - \"-disable-ceph-namespace-isolation=true\"  serviceAccount: cephfs-provisioner 在ceph monitor机器中查看admin账户的key\n$ ceph auth ls $ ceph auth get-key client.admin AQAejeJbowvgMhAAsuloUOvepcj/TXEIoSrd7A== 创建secret\n$ echo -n AQAejeJbowvgMhAAsuloUOvepcj/TXEIoSrd7A==|base64 QVFBZWplSmJvd3ZnTWhBQXN1bG9VT3ZlcGNqL1RYRUlvU3JkN0E9PQ== $ cat ceph-admin-secret.yaml apiVersion: v1 data:  key: QVFBZWplSmJvd3ZnTWhBQXN1bG9VT3ZlcGNqL1RYRUlvU3JkN0E9PQ== kind: Secret metadata:  name: ceph-admin-secret  namespace: kube-system type: Opaque 创建storageclass\n$ cat cephfs-storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata:  name: dynamic-cephfs provisioner: ceph.com/cephfs parameters:  monitors: 36.111.140.31:6789  adminId: admin  adminSecretName: ceph-admin-secret  adminSecretNamespace: \"kube-system\"  claimRoot: /volumes/kubernetes 动态pvc验证及实现分析 使用流程： 创建pvc，指定storageclass和存储大小，即可实现动态存储。\n创建pvc测试自动生成pv\n$ cat cephfs-pvc-test.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: cephfs-claim spec:  accessModes:  - ReadWriteOnce  storageClassName: dynamic-cephfs  resources:  requests:  storage: 2Gi  $ kubectl create -f cephfs-pvc-test.yaml  $ kubectl get pv pvc-2abe427e-7568-442d-939f-2c273695c3db 2Gi RWO Delete Bound default/cephfs-claim dynamic-cephfs 1s 创建Pod使用pvc挂载cephfs数据盘\n$ cat test-pvc-cephfs.yaml apiVersion: v1 kind: Pod metadata:  name: nginx-pod  labels:  name: nginx-pod spec:  containers:  - name: nginx-pod  image: nginx:alpine  ports:  - name: web  containerPort: 80  volumeMounts:  - name: cephfs  mountPath: /usr/share/nginx/html  volumes:  - name: cephfs  persistentVolumeClaim:  claimName: cephfs-claim  $ kubectl create -f test-pvc-cephfs.yaml 我们所说的容器的持久化，实际上应该理解为宿主机中volume的持久化，因为Pod是支持销毁重建的，所以只能通过宿主机volume持久化，然后挂载到Pod内部来实现Pod的数据持久化。\n宿主机上的volume持久化，因为要支持数据漂移，所以通常是数据存储在分布式存储中，宿主机本地挂载远程存储（NFS，Ceph，OSS），这样即使Pod漂移也不影响数据。\nk8s的pod的挂载盘通常的格式为：\n/var/lib/kubelet/pods//volumes/kubernetes.io~/ 查看nginx-pod的挂载盘，\n$ df -TH /var/lib/kubelet/pods/61ba43c5-d2e9-4274-ac8c-008854e4fa8e/volumes/kubernetes.io~cephfs/pvc-2abe427e-7568-442d-939f-2c273695c3db/  $ findmnt /var/lib/kubelet/pods/61ba43c5-d2e9-4274-ac8c-008854e4fa8e/volumes/kubernetes.io~cephfs/pvc-2abe427e-7568-442d-939f-2c273695c3db/  36.111.140.31:6789:/volumes/kubernetes/kubernetes/kubernetes-dynamic-pvc-ffe3d84d-c433-11ea-b347-6acc3cf3c15f 使用Helm3管理复杂应用的部署 认识Helm   为什么有helm？\n  Helm是什么？\nkubernetes的包管理器，“可以将Helm看作Linux系统下的apt-get/yum”。\n 对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。  除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。\n  Helm的版本\n  helm2\nC/S架构，helm通过Tiller与k8s交互\n  helm3\n  从安全性和易用性方面考虑，移除了Tiller服务端，helm3直接使用kubeconfig文件鉴权访问APIServer服务器\n  由二路合并升级成为三路合并补丁策略（ 旧的配置，线上状态，新的配置 ）\nhelm install very_important_app ./very_important_app 这个应用的副本数量设置为 3 。现在，如果有人不小心执行了 kubectl edit 或：\nkubectl scale -replicas=0 deployment/very_important_app 然后，团队中的某个人发现 very_important_app 莫名其妙宕机了，尝试执行命令：\nhelm rollback very_important_app 在 Helm 2 中，这个操作将比较旧的配置与新的配置，然后生成一个更新补丁。由于，误操作的人仅修改了应用的线上状态（旧的配置并未更新）。Helm 在回滚时，什么事情也不会做。因为旧的配置与新的配置没有差别（都是 3 个副本）。然后，Helm 不执行回滚，副本数继续保持为 0\n  移除了helm serve本地repo仓库\n  创建应用时必须指定名字（或者–generate-name随机生成）\n      Helm的重要概念\n chart，应用的信息集合，包括各种对象的配置模板、参数定义、依赖关系、文档说明等 Repoistory，chart仓库，存储chart的地方，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。 release， 当 chart 被安装到 kubernetes 集群，就生成了一个 release ， 是 chart 的运行实例，代表了一个正在运行的应用    helm 是包管理工具，包就是指 chart，helm 能够：\n 从零创建chart 与仓库交互，拉取、保存、更新 chart 在kubernetes集群中安装、卸载 release 更新、回滚、测试 release  安装与快速入门实践 下载最新的稳定版本：https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz\n更多版本可以参考： https://github.com/helm/helm/releases\n# k8s-master节点 $ wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz $ tar -zxf helm-v3.2.4-linux-amd64.tar.gz  $ cp linux-amd64/helm /usr/local/bin/  # 验证安装 $ helm version version.BuildInfo{Version:\"v3.2.4\", GitCommit:\"0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\", GitTreeState:\"clean\", GoVersion:\"go1.13.12\"} $ helm env  # 添加仓库 $ helm repo add stable http://mirror.azure.cn/kubernetes/charts/ # 同步最新charts信息到本地 $ helm repo update 快速入门实践：\n示例一：使用helm安装mysql应用\n# helm 搜索chart包 $ helm search repo mysql  # 从仓库安装 $ helm install mysql stable/mysql  $ helm ls $ kubectl get all  # 从chart仓库中把chart包下载到本地 $ helm pull stable/mysql $ tree mysql 示例二：新建nginx的chart并安装\n$ helm create nginx  # 从本地安装 $ helm install nginx ./nginx  # 安装到别的命名空间luffy $ helm -n luffy install ./nginx  # 查看 $ helm ls $ helm -n luffy ls  # $ kubectl get all $ kubectl -n luffy get all Chart的模板语法及开发 nginx的chart实现分析 格式：\n$ tree nginx/ nginx/ ├── charts # 存放子chart ├── Chart.yaml # 该chart的全局定义信息 ├── templates # chart运行所需的资源清单模板，用于和values做渲染 │ ├── deployment.yaml │ ├── _helpers.tpl # 定义全局的命名模板，方便在其他模板中引入使用 │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt # helm安装完成后终端的提示信息 │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml # 模板使用的默认值信息 很明显，资源清单都在templates中，数据来源于values.yaml，安装的过程就是将模板与数据融合成k8s可识别的资源清单，然后部署到k8s环境中。\n分析模板文件的实现：\n  引用命名模板并传递作用域\n{{ include \"nginx.fullname\" . }} include从_helpers.tpl中引用命名模板，并传递顶级作用域.\n  内置对象\n.Values .Release.Name  Release：该对象描述了 release 本身的相关信息，它内部有几个对象：  Release.Name：release 名称 Release.Namespace：release 安装到的命名空间 Release.IsUpgrade：如果当前操作是升级或回滚，则该值为 true Release.IsInstall：如果当前操作是安装，则将其设置为 true Release.Revision：release 的 revision 版本号，在安装的时候，值为1，每次升级或回滚都会增加 Reelase.Service：渲染当前模板的服务，在 Helm 上，实际上该值始终为 Helm   Values：从 values.yaml 文件和用户提供的 values 文件传递到模板的 Values 值 Chart：获取 Chart.yaml 文件的内容，该文件中的任何数据都可以访问，例如 {{ .Chart.Name }}-{{ .Chart.Version}} 可以渲染成 mychart-0.1.0    模板定义\n{{- define \"nginx.fullname\" -}} {{- if .Values.fullnameOverride }} {{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }} {{- else }} {{- $name := default .Chart.Name .Values.nameOverride }} {{- if contains $name .Release.Name }} {{- .Release.Name | trunc 63 | trimSuffix \"-\" }} {{- else }} {{- printf \"%s-%s\" .Release.Name $name | trunc 63 | trimSuffix \"-\" }} {{- end }} {{- end }} {{- end }}   {{- 去掉左边的空格及换行，-}} 去掉右侧的空格及换行\n  示例\napiVersion: v1 kind: ConfigMap metadata:  name: {{ .Release.Name }}-configmap data:  myvalue: \"Hello World\"  drink: {{ .Values.favorite.drink | default \"tea\" | quote }}  food: {{ .Values.favorite.food | upper | quote }}  {{ if eq .Values.favorite.drink \"coffee\" }}  mug: true  {{ end }} 渲染完后是：\napiVersion: v1 kind: ConfigMap metadata:  name: mychart-1575971172-configmap data:  myvalue: \"Hello World\"  drink: \"coffee\"  food: \"PIZZA\"   mug: true     管道及方法\n  trunc表示字符串截取，63作为参数传递给trunc方法，trimSuffix表示去掉-后缀\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \"-\" }}   nindent表示前面的空格数\n selector:  matchLabels:  {{- include \"nginx.selectorLabels\" . | nindent 6 }}   lower表示将内容小写，quote表示用双引号引起来\nvalue: {{ include \"mytpl\" . | lower | quote }}     条件判断语句每个if对应一个end\n{{- if .Values.fullnameOverride }} ... {{- else }} ... {{- end }} 通常用来根据values.yaml中定义的开关来控制模板中的显示：\n{{- if not .Values.autoscaling.enabled }}  replicas: {{ .Values.replicaCount }} {{- end }}   定义变量，模板中可以通过变量名字去引用\n{{- $name := default .Chart.Name .Values.nameOverride }}   遍历values的数据\n {{- with .Values.nodeSelector }}  nodeSelector:  {{- toYaml . | nindent 8 }}  {{- end }} toYaml处理值中的转义及特殊字符， “kubernetes.io/role”=master ， name=“value1,value2” 类似的情况\n  default设置默认值\nimage: \"{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\"   Helm template\nhpa.yaml\n{{- if .Values.autoscaling.enabled }} apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: {{ include \"nginx.fullname\" . }}  labels:  {{- include \"nginx.labels\" . | nindent 4 }} spec:  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: {{ include \"nginx.fullname\" . }}  minReplicas: {{ .Values.autoscaling.minReplicas }}  maxReplicas: {{ .Values.autoscaling.maxReplicas }}  metrics:  {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}  - type: Resource  resource:  name: cpu  targetAverageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}  {{- end }}  {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}  - type: Resource  resource:  name: memory  targetAverageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}  {{- end }} {{- end }} 创建应用的时候赋值  set的方式  # 改变副本数和resource值 $ helm install nginx-2 ./nginx --set replicaCount=2 --set resources.limits.cpu=200m --set resources.limits.memory=256Mi   value文件的方式\n$ cat nginx-values.yaml resources:  limits:  cpu: 100m  memory: 128Mi  requests:  cpu: 100m  memory: 128Mi autoscaling:  enabled: true  minReplicas: 1  maxReplicas: 3  targetCPUUtilizationPercentage: 80 ingress:  enabled: true  hosts:  - host: chart-example.luffy.com  paths:  - /  $ helm install -f nginx-values.yaml nginx-3 ./nginx   更多语法参考：\nhttps://helm.sh/docs/topics/charts/\n部署mysql失败的问题\n实战：使用Helm部署Harbor镜像及chart仓库 harbor踩坑部署 架构 https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor\n Core，核心组件  API Server，接收处理用户请求 Config Manager ：所有系统的配置，比如认证、邮件、证书配置等 Project Manager：项目管理 Quota Manager ：配额管理 Chart Controller：chart管理 Replication Controller ：镜像副本控制器，可以与不同类型的仓库实现镜像同步  Distribution (docker registry) Docker Hub …   Scan Manager ：扫描管理，引入第三方组件，进行镜像安全扫描 Registry Driver ：镜像仓库驱动，目前使用docker registry   Job Service，执行异步任务，如同步镜像信息 Log Collector，统一日志收集器，收集各模块日志 GC Controller Chart Museum，chart仓库服务，第三方 Docker Registry，镜像仓库服务 kv-storage，redis缓存服务，job service使用，存储job metadata local/remote storage，存储服务，比较镜像存储 SQL Database，postgresl，存储用户、项目等元数据  通常用作企业级镜像仓库服务，实际功能强大很多。\n组件众多，因此使用helm部署\n# 添加harbor chart仓库 $ helm repo add harbor https://helm.goharbor.io  # 搜索harbor的chart $ helm search repo harbor  # 不知道如何部署，因此拉到本地 $ helm pull harbor/harbor --version 1.4.1 创建pvc\n$ kubectl create namespace harbor $ cat harbor-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: harbor-pvc  namespace: harbor spec:  accessModes:  - ReadWriteOnce  storageClassName: dynamic-cephfs  resources:  requests:  storage: 20Gi 修改harbor配置：\n 开启ingress访问 externalURL，web访问入口，和ingress的域名相同 持久化，使用PVC对接的cephfs harborAdminPassword: “Harbor12345”，管理员默认账户 admin/Harbor12345 开启chartmuseum clair和trivy漏洞扫描组件，暂不启用  helm创建：\n# 使用本地chart安装 $ helm install harbor ./harbor -n harbor 踩坑一：redis持久化数据目录权限导致无法登录\nredis数据目录，/var/lib/redis，需要设置redis的用户及用户组权限\n initContainers:  - name: \"change-permission-of-directory\"  image: {{ .Values.redis.internal.image.repository }}:{{ .Values.redis.internal.image.tag }}  imagePullPolicy: {{ .Values.imagePullPolicy }}  command: [\"/bin/sh\"]  args: [\"-c\", \"chown -R 999:999 /var/lib/redis\"]  securityContext:  runAsUser: 0  volumeMounts:  - name: data  mountPath: /var/lib/redis  subPath: {{ $redis.subPath }} 踩坑二：registry组件的镜像存储目录权限导致镜像推送失败\nregistry的镜像存储目录，需要设置registry用户的用户及用户组，不然镜像推送失败\n initContainers:  - name: \"change-permission-of-directory\"  securityContext:  runAsUser: 0  image: {{ .Values.registry.registry.image.repository }}:{{ .Values.registry.registry.image.tag }}  imagePullPolicy: {{ .Values.imagePullPolicy }}  command: [\"/bin/sh\"]  args: [\"-c\", \"chown -R 10000:10000 {{ .Values.persistence.imageChartStorage.filesystem.rootdirectory }}\"]  volumeMounts:  - name: registry-data  mountPath: {{ .Values.persistence.imageChartStorage.filesystem.rootdirectory }}  subPath: {{ .Values.persistence.persistentVolumeClaim.registry.subPath }} 踩坑三：chartmuseum存储目录权限，导致chart推送失败\n initContainers:  - name: \"change-permission-of-directory\"  image: {{ .Values.chartmuseum.image.repository }}:{{ .Values.chartmuseum.image.tag }}  imagePullPolicy: {{ .Values.imagePullPolicy }}  command: [\"/bin/sh\"]  args: [\"-c\", \"chown -R 10000:10000 /chart_storage\"]  securityContext:  runAsUser: 0  volumeMounts:  - name: chartmuseum-data  mountPath: /chart_storage  subPath: {{ .Values.persistence.persistentVolumeClaim.chartmuseum.subPath }} 更新内容后，执行更新release\n$ helm upgrade harbor -n harbor ./ 推送镜像到Harbor仓库 配置hosts及docker非安全仓库：\n$ cat /etc/hosts ... 192.168.136.10 k8s-master core.harbor.domain ...  $ cat /etc/docker/daemon.json {  \"insecure-registries\": [  \"192.168.136.10:5000\",  \"core.harbor.domain\"  ],  \"registry-mirrors\" : [  \"https://8xpk5wnt.mirror.aliyuncs.com\"  ] }  # $ systemctl restart docker  # 使用账户密码登录admin/Harbor12345 $ docker login core.harbor.domain  $ docker tag nginx:alpine core.harbor.domain/library/nginx:alpine $ docker push core.harbor.domain/library/nginx:alpine 推送chart到Harbor仓库 helm3默认没有安装helm push插件，需要手动安装。插件地址 https://github.com/chartmuseum/helm-push\n安装插件：\n$ helm plugin install https://github.com/chartmuseum/helm-push 离线安装：\n$ helm plugin install ./helm-push 添加repo\n$ helm repo add myharbor https://core.harbor.domain/chartrepo/library # x509错误  # 添加证书信任，根证书为配置给ingress使用的证书 $ kubectl get secret harbor-harbor-ingress -n harbor -o jsonpath=\"{.data.ca\\.crt}\" | base64 -d harbor.ca.crt  $ cp harbor.ca.crt /etc/pki/ca-trust/source/anchors $ update-ca-trust enable; update-ca-trust extract  # 再次添加 $ helm repo add myharbor https://core.harbor.domain/chartrepo/library --ca-file=harbor.ca.crt  $ helm repo ls 推送chart到仓库：\n$ helm push harbor myharbor --ca-file=harbor.ca.crt -u admin -p Harbor12345 查看harbor仓库的chart\n课程小结 使用k8s的进阶内容。\n  学习k8s在etcd中数据的存储，掌握etcd的基本操作命令\n  理解k8s调度的过程，预选及优先。影响调度策略的设置\n  Flannel网络的原理学习，了解网络的流向，帮助定位问题\n  认证与授权，掌握kubectl、kubelet、rbac及二次开发如何调度API\n  利用HPA进行业务动态扩缩容，通过metrics-server了解整个k8s的监控体系\n  PV + PVC\n  Helm\n  ",
  "wordCount" : "5861",
  "inLanguage": "zh",
  "datePublished": "2022-01-05T11:13:03Z",
  "dateModified": "2022-01-05T11:13:03Z",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://iblog.zone/archives/kubernetes%E8%BF%9B%E9%98%B6%E5%AE%9E%E8%B7%B5/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ylw's blog",
    "logo": {
      "@type": "ImageObject",
      "url": "https://iblog.zone/images/favicon.ico"
    }
  }
}
</script>
</head>

<body class="" id="top">
<script>
    if (localStorage.getItem("pref-theme") === "dark") {
        document.body.classList.add('dark');
    } else if (localStorage.getItem("pref-theme") === "light") {
        document.body.classList.remove('dark')
    } else if (window.matchMedia('(prefers-color-scheme: dark)').matches) {
        document.body.classList.add('dark');
    }

</script>

<header class="header">
    <nav class="nav">
        <div class="logo">
            <a href="https://iblog.zone" accesskey="h" title="ylw&#39;s blog (Alt + H)">ylw&#39;s blog</a>
            <span class="logo-switches">
                <button id="theme-toggle" accesskey="t" title="(Alt + T)">
                    <svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path>
                    </svg>
                    <svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24"
                        fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round"
                        stroke-linejoin="round">
                        <circle cx="12" cy="12" r="5"></circle>
                        <line x1="12" y1="1" x2="12" y2="3"></line>
                        <line x1="12" y1="21" x2="12" y2="23"></line>
                        <line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line>
                        <line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line>
                        <line x1="1" y1="12" x2="3" y2="12"></line>
                        <line x1="21" y1="12" x2="23" y2="12"></line>
                        <line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line>
                        <line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line>
                    </svg>
                </button>
            </span>
        </div>
        <ul id="menu">
            <li>
                <a href="https://iblog.zone/" title="Home">
                    <span>Home</span>
                </a>
            </li>
            <li>
                <a href="https://iblog.zone/archives/" title="Archives">
                    <span>Archives</span>
                </a>
            </li>
            <li>
                <a href="https://iblog.zone/categories/" title="Categories">
                    <span>Categories</span>
                </a>
            </li>
            <li>
                <a href="https://iblog.zone/tags/" title="Tags">
                    <span>Tags</span>
                </a>
            </li>
            <li>
                <a href="https://iblog.zone/search/" title="Search (Alt &#43; /)" accesskey=/>
                    <span>Search</span>
                </a>
            </li>
        </ul>
    </nav>
</header>
<main class="main">

<article class="post-single">
  <header class="post-header">
    <div class="breadcrumbs"><a href="https://iblog.zone">主页</a>&nbsp;»&nbsp;<a href="https://iblog.zone/posts/">Posts</a></div>
    <h1 class="post-title">
      Kubernetes进阶实践
    </h1>
    <div class="post-meta"><span title='2022-01-05 11:13:03 +0000 UTC'>2022-01-05</span>&nbsp;·&nbsp;28 分钟

</div>
  </header> <aside id="toc-container" class="toc-container wide">
    <div class="toc">
        <details  open>
            <summary accesskey="c" title="(Alt + C)">
                <span class="details">目录</span>
            </summary>

            <div class="inner"><ul>
                    <li>
                        <a href="#%e7%ac%ac%e4%b8%89%e5%a4%a9-kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5" aria-label="第三天 Kubernetes进阶实践">第三天 Kubernetes进阶实践</a><ul>
                            
                    <li>
                        <a href="#etcd%e5%b8%b8%e7%94%a8%e6%93%8d%e4%bd%9c" aria-label="ETCD常用操作">ETCD常用操作</a></li>
                    <li>
                        <a href="#kubernetes%e8%b0%83%e5%ba%a6" aria-label="Kubernetes调度">Kubernetes调度</a><ul>
                            <ul>
                            
                    <li>
                        <a href="#%e4%b8%ba%e4%bd%95%e8%a6%81%e6%8e%a7%e5%88%b6pod%e5%ba%94%e8%af%a5%e5%a6%82%e4%bd%95%e8%b0%83%e5%ba%a6" aria-label="为何要控制Pod应该如何调度">为何要控制Pod应该如何调度</a></li>
                    <li>
                        <a href="#%e8%b0%83%e5%ba%a6%e7%9a%84%e8%bf%87%e7%a8%8b" aria-label="调度的过程">调度的过程</a></li>
                    <li>
                        <a href="#cordon" aria-label="Cordon">Cordon</a></li>
                    <li>
                        <a href="#nodeselector" aria-label="NodeSelector">NodeSelector</a></li>
                    <li>
                        <a href="#nodeaffinity" aria-label="nodeAffinity">nodeAffinity</a></li>
                    <li>
                        <a href="#%e6%b1%a1%e7%82%b9taints%e4%b8%8e%e5%ae%b9%e5%bf%8dtolerations" aria-label="污点（Taints）与容忍（tolerations）">污点（Taints）与容忍（tolerations）</a></li></ul>
                        </ul>
                    </li>
                    <li>
                        <a href="#kubernetes%e9%9b%86%e7%be%a4%e7%9a%84%e7%bd%91%e7%bb%9c%e5%ae%9e%e7%8e%b0" aria-label="Kubernetes集群的网络实现">Kubernetes集群的网络实现</a><ul>
                            
                    <li>
                        <a href="#cni%e4%bb%8b%e7%bb%8d%e5%8f%8a%e9%9b%86%e7%be%a4%e7%bd%91%e7%bb%9c%e9%80%89%e5%9e%8b" aria-label="CNI介绍及集群网络选型,">CNI介绍及集群网络选型,</a></li>
                    <li>
                        <a href="#flannel%e7%bd%91%e7%bb%9c%e6%a8%a1%e5%9e%8b%e5%ae%9e%e7%8e%b0%e5%89%96%e6%9e%90" aria-label="Flannel网络模型实现剖析">Flannel网络模型实现剖析</a><ul>
                            
                    <li>
                        <a href="#vxlan%e4%bb%8b%e7%bb%8d%e5%8f%8a%e7%82%b9%e5%af%b9%e7%82%b9%e9%80%9a%e4%bf%a1%e7%9a%84%e5%ae%9e%e7%8e%b0" aria-label="vxlan介绍及点对点通信的实现">vxlan介绍及点对点通信的实现</a></li>
                    <li>
                        <a href="#%e8%b7%a8%e4%b8%bb%e6%9c%ba%e5%ae%b9%e5%99%a8%e7%bd%91%e7%bb%9c%e7%9a%84%e9%80%9a%e4%bf%a1" aria-label="跨主机容器网络的通信">跨主机容器网络的通信</a></li>
                    <li>
                        <a href="#flannel%e7%9a%84vxlan%e5%ae%9e%e7%8e%b0%e7%b2%be%e8%ae%b2" aria-label="Flannel的vxlan实现精讲">Flannel的vxlan实现精讲</a></li>
                    <li>
                        <a href="#%e5%88%a9%e7%94%a8host-gw%e6%a8%a1%e5%bc%8f%e6%8f%90%e5%8d%87%e9%9b%86%e7%be%a4%e7%bd%91%e7%bb%9c%e6%80%a7%e8%83%bd" aria-label="利用host-gw模式提升集群网络性能">利用host-gw模式提升集群网络性能</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#kubernetes%e8%ae%a4%e8%af%81%e4%b8%8e%e6%8e%88%e6%9d%83" aria-label="Kubernetes认证与授权">Kubernetes认证与授权</a><ul>
                            <ul>
                            
                    <li>
                        <a href="#apiserver%e5%ae%89%e5%85%a8%e6%8e%a7%e5%88%b6" aria-label="APIServer安全控制">APIServer安全控制</a></li>
                    <li>
                        <a href="#kubectl%e7%9a%84%e8%ae%a4%e8%af%81%e6%8e%88%e6%9d%83" aria-label="kubectl的认证授权">kubectl的认证授权</a></li>
                    <li>
                        <a href="#rbac" aria-label="RBAC">RBAC</a></li>
                    <li>
                        <a href="#kubelet%e7%9a%84%e8%ae%a4%e8%af%81%e6%8e%88%e6%9d%83" aria-label="kubelet的认证授权">kubelet的认证授权</a></li>
                    <li>
                        <a href="#service-account%e5%8f%8ak8s-api%e8%b0%83%e7%94%a8" aria-label="Service Account及K8S Api调用">Service Account及K8S Api调用</a></li></ul>
                        </ul>
                    </li>
                    <li>
                        <a href="#%e9%80%9a%e8%bf%87hpa%e5%ae%9e%e7%8e%b0%e4%b8%9a%e5%8a%a1%e5%ba%94%e7%94%a8%e7%9a%84%e5%8a%a8%e6%80%81%e6%89%a9%e7%bc%a9%e5%ae%b9" aria-label="通过HPA实现业务应用的动态扩缩容">通过HPA实现业务应用的动态扩缩容</a><ul>
                            
                    <li>
                        <a href="#hpa%e6%8e%a7%e5%88%b6%e5%99%a8%e4%bb%8b%e7%bb%8d" aria-label="HPA控制器介绍">HPA控制器介绍</a></li>
                    <li>
                        <a href="#metric-server" aria-label="Metric Server">Metric Server</a><ul>
                            
                    <li>
                        <a href="#%e5%ae%89%e8%a3%85" aria-label="安装">安装</a></li>
                    <li>
                        <a href="#kubelet%e7%9a%84%e6%8c%87%e6%a0%87%e9%87%87%e9%9b%86" aria-label="kubelet的指标采集">kubelet的指标采集</a></li>
                    <li>
                        <a href="#kube-aggregator%e8%81%9a%e5%90%88%e5%99%a8%e5%8f%8ametric-server%e7%9a%84%e5%ae%9e%e7%8e%b0" aria-label="kube-aggregator聚合器及Metric-Server的实现">kube-aggregator聚合器及Metric-Server的实现</a></li></ul>
                    </li>
                    <li>
                        <a href="#hpa%e5%ae%9e%e8%b7%b5" aria-label="HPA实践">HPA实践</a><ul>
                            
                    <li>
                        <a href="#%e5%9f%ba%e4%ba%8ecpu%e7%9a%84%e5%8a%a8%e6%80%81%e4%bc%b8%e7%bc%a9" aria-label="基于CPU的动态伸缩">基于CPU的动态伸缩</a></li>
                    <li>
                        <a href="#%e5%9f%ba%e4%ba%8e%e5%86%85%e5%ad%98%e7%9a%84%e5%8a%a8%e6%80%81%e4%bc%b8%e7%bc%a9" aria-label="基于内存的动态伸缩">基于内存的动态伸缩</a></li>
                    <li>
                        <a href="#%e5%9f%ba%e4%ba%8e%e8%87%aa%e5%ae%9a%e4%b9%89%e6%8c%87%e6%a0%87%e7%9a%84%e5%8a%a8%e6%80%81%e4%bc%b8%e7%bc%a9" aria-label="基于自定义指标的动态伸缩">基于自定义指标的动态伸缩</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#kubernetes%e5%af%b9%e6%8e%a5%e5%88%86%e9%83%a8%e5%bc%8f%e5%ad%98%e5%82%a8" aria-label="kubernetes对接分部式存储">kubernetes对接分部式存储</a><ul>
                            
                    <li>
                        <a href="#pv%e4%b8%8epvc%e5%bf%ab%e9%80%9f%e5%85%a5%e9%97%a8" aria-label="PV与PVC快速入门">PV与PVC快速入门</a></li>
                    <li>
                        <a href="#pv%e4%b8%8epvc%e7%ae%a1%e7%90%86nfs%e5%ad%98%e5%82%a8%e5%8d%b7%e5%ae%9e%e8%b7%b5" aria-label="PV与PVC管理NFS存储卷实践">PV与PVC管理NFS存储卷实践</a><ul>
                            
                    <li>
                        <a href="#%e7%8e%af%e5%a2%83%e5%87%86%e5%a4%87" aria-label="环境准备">环境准备</a></li>
                    <li>
                        <a href="#pv%e4%b8%8epvc%e6%bc%94%e7%a4%ba" aria-label="PV与PVC演示">PV与PVC演示</a></li>
                    <li>
                        <a href="#storageclass%e5%ae%9e%e7%8e%b0%e5%8a%a8%e6%80%81%e6%8c%82%e8%bd%bd" aria-label="storageClass实现动态挂载">storageClass实现动态挂载</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%af%b9%e6%8e%a5ceph%e5%ad%98%e5%82%a8%e5%ae%9e%e8%b7%b5" aria-label="对接Ceph存储实践">对接Ceph存储实践</a><ul>
                            
                    <li>
                        <a href="#storageclass%e5%ae%9e%e7%8e%b0%e5%8a%a8%e6%80%81%e6%8c%82%e8%bd%bd-1" aria-label="storageClass实现动态挂载">storageClass实现动态挂载</a></li>
                    <li>
                        <a href="#%e5%8a%a8%e6%80%81pvc%e9%aa%8c%e8%af%81%e5%8f%8a%e5%ae%9e%e7%8e%b0%e5%88%86%e6%9e%90" aria-label="动态pvc验证及实现分析">动态pvc验证及实现分析</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#%e4%bd%bf%e7%94%a8helm3%e7%ae%a1%e7%90%86%e5%a4%8d%e6%9d%82%e5%ba%94%e7%94%a8%e7%9a%84%e9%83%a8%e7%bd%b2" aria-label="使用Helm3管理复杂应用的部署">使用Helm3管理复杂应用的部署</a><ul>
                            
                    <li>
                        <a href="#%e8%ae%a4%e8%af%86helm" aria-label="认识Helm">认识Helm</a></li>
                    <li>
                        <a href="#%e5%ae%89%e8%a3%85%e4%b8%8e%e5%bf%ab%e9%80%9f%e5%85%a5%e9%97%a8%e5%ae%9e%e8%b7%b5" aria-label="安装与快速入门实践">安装与快速入门实践</a></li>
                    <li>
                        <a href="#chart%e7%9a%84%e6%a8%a1%e6%9d%bf%e8%af%ad%e6%b3%95%e5%8f%8a%e5%bc%80%e5%8f%91" aria-label="Chart的模板语法及开发">Chart的模板语法及开发</a><ul>
                            
                    <li>
                        <a href="#nginx%e7%9a%84chart%e5%ae%9e%e7%8e%b0%e5%88%86%e6%9e%90" aria-label="nginx的chart实现分析">nginx的chart实现分析</a></li>
                    <li>
                        <a href="#%e5%88%9b%e5%bb%ba%e5%ba%94%e7%94%a8%e7%9a%84%e6%97%b6%e5%80%99%e8%b5%8b%e5%80%bc" aria-label="创建应用的时候赋值">创建应用的时候赋值</a></li></ul>
                    </li>
                    <li>
                        <a href="#%e5%ae%9e%e6%88%98%e4%bd%bf%e7%94%a8helm%e9%83%a8%e7%bd%b2harbor%e9%95%9c%e5%83%8f%e5%8f%8achart%e4%bb%93%e5%ba%93" aria-label="实战：使用Helm部署Harbor镜像及chart仓库">实战：使用Helm部署Harbor镜像及chart仓库</a><ul>
                            
                    <li>
                        <a href="#harbor%e8%b8%a9%e5%9d%91%e9%83%a8%e7%bd%b2" aria-label="harbor踩坑部署">harbor踩坑部署</a></li>
                    <li>
                        <a href="#%e6%8e%a8%e9%80%81%e9%95%9c%e5%83%8f%e5%88%b0harbor%e4%bb%93%e5%ba%93" aria-label="推送镜像到Harbor仓库">推送镜像到Harbor仓库</a></li>
                    <li>
                        <a href="#%e6%8e%a8%e9%80%81chart%e5%88%b0harbor%e4%bb%93%e5%ba%93" aria-label="推送chart到Harbor仓库">推送chart到Harbor仓库</a></li></ul>
                    </li></ul>
                    </li>
                    <li>
                        <a href="#%e8%af%be%e7%a8%8b%e5%b0%8f%e7%bb%93" aria-label="课程小结">课程小结</a>
                    </li>
                </ul>
                </li>
                </ul>
            </div>
        </details>
    </div>
</aside>
<script data-cfasync="false">
    let activeElement;
    let elements;
    window.addEventListener('DOMContentLoaded', function (event) {
        checkTocPosition();

        elements = document.querySelectorAll('h1[id],h2[id],h3[id],h4[id],h5[id],h6[id]');
         
         activeElement = elements[0];
         const id = encodeURI(activeElement.getAttribute('id')).toLowerCase();
         document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
     }, false);

    window.addEventListener('resize', function(event) {
        checkTocPosition();
    }, false);

    window.addEventListener('scroll', () => {
        
        activeElement = Array.from(elements).find((element) => {
            if ((getOffsetTop(element) - window.pageYOffset) > 0 && 
                (getOffsetTop(element) - window.pageYOffset) < window.innerHeight/2) {
                return element;
            }
        }) || activeElement

        elements.forEach(element => {
             const id = encodeURI(element.getAttribute('id')).toLowerCase();
             if (element === activeElement){
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.add('active');
             } else {
                 document.querySelector(`.inner ul li a[href="#${id}"]`).classList.remove('active');
             }
         })
     }, false);

    const main = parseInt(getComputedStyle(document.body).getPropertyValue('--article-width'), 10);
    const toc = parseInt(getComputedStyle(document.body).getPropertyValue('--toc-width'), 10);
    const gap = parseInt(getComputedStyle(document.body).getPropertyValue('--gap'), 10);

    function checkTocPosition() {
        const width = document.body.scrollWidth;

        if (width - main - (toc * 2) - (gap * 4) > 0) {
            document.getElementById("toc-container").classList.add("wide");
        } else {
            document.getElementById("toc-container").classList.remove("wide");
        }
    }

    function getOffsetTop(element) {
        if (!element.getClientRects().length) {
            return 0;
        }
        let rect = element.getBoundingClientRect();
        let win = element.ownerDocument.defaultView;
        return rect.top + win.pageYOffset;   
    }
</script>


  <div class="post-content"><h3 id="第三天-kubernetes进阶实践">第三天 Kubernetes进阶实践<a hidden class="anchor" aria-hidden="true" href="#第三天-kubernetes进阶实践">#</a></h3>
<p>本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。</p>
<ul>
<li>ETCD数据的访问</li>
<li>kube-scheduler调度策略实践
<ul>
<li>预选与优选流程</li>
<li>生产中常用的调度配置实践</li>
</ul>
</li>
<li>k8s集群网络模型
<ul>
<li>CNI介绍及集群网络选型</li>
<li>Flannel网络模型的实现
<ul>
<li>vxlan Backend</li>
<li>hostgw Backend</li>
</ul>
</li>
</ul>
</li>
<li>集群认证与授权
<ul>
<li>APIServer安全控制模型</li>
<li>Kubectl的认证授权</li>
<li>RBAC</li>
<li>kubelet的认证授权</li>
<li>Service Account</li>
</ul>
</li>
<li>使用Helm管理复杂应用的部署
<ul>
<li>Helm工作原理详解</li>
<li>Helm的模板开发</li>
<li>实战：使用Helm部署Harbor仓库</li>
</ul>
</li>
<li>kubernetes对接分部式存储
<ul>
<li>pv、pvc介绍</li>
<li>k8s集群如何使用cephfs作为分布式存储后端</li>
<li>利用storageClass实现动态存储卷的管理</li>
<li>实战：使用分部署存储实现有状态应用的部署</li>
</ul>
</li>
<li>本章知识梳理及回顾</li>
</ul>
<h4 id="etcd常用操作">ETCD常用操作<a hidden class="anchor" aria-hidden="true" href="#etcd常用操作">#</a></h4>
<p>拷贝etcdctl命令行工具：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker exec -ti  etcd_container which etcdctl
</span></span><span style="display:flex;"><span>$ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl
</span></span></code></pre></div><p>查看etcd集群的成员节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ export ETCDCTL_API<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>$ etcdctl --endpoints<span style="color:#f92672">=</span>https://<span style="color:#f92672">[</span>127.0.0.1<span style="color:#f92672">]</span>:2379 --cacert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/ca.crt --cert<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/healthcheck-client.crt --key<span style="color:#f92672">=</span>/etc/kubernetes/pki/etcd/healthcheck-client.key member list -w table
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ alias etcdctl<span style="color:#f92672">=</span><span style="color:#e6db74">&#39;etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key&#39;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ etcdctl member list -w table
</span></span></code></pre></div><p>查看etcd集群节点状态：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ etcdctl endpoint status -w table
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ etcdctl endpoint health -w table
</span></span></code></pre></div><p>设置key值:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ etcdctl put luffy <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>$ etcdctl get luffy
</span></span></code></pre></div><p>查看所有key值：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$  etcdctl get / --prefix --keys-only
</span></span></code></pre></div><p>查看具体的key对应的数据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ etcdctl get /registry/pods/jenkins/sonar-postgres-7fc5d748b6-gtmsb
</span></span></code></pre></div><p>添加定时任务做数据快照（重要！）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ etcdctl snapshot save <span style="color:#e6db74">`</span>hostname<span style="color:#e6db74">`</span>-etcd_<span style="color:#e6db74">`</span>date +%Y%m%d%H%M<span style="color:#e6db74">`</span>.db
</span></span></code></pre></div><p>恢复快照：</p>
<ol>
<li>
<p>停止etcd和apiserver</p>
</li>
<li>
<p>移走当前数据目录</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ mv /var/lib/etcd/ /tmp
</span></span></code></pre></div></li>
<li>
<p>恢复快照</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ etcdctl snapshot restore <span style="color:#e6db74">`</span>hostname<span style="color:#e6db74">`</span>-etcd_<span style="color:#e6db74">`</span>date +%Y%m%d%H%M<span style="color:#e6db74">`</span>.db --data-dir<span style="color:#f92672">=</span>/var/lib/etcd/
</span></span></code></pre></div></li>
<li>
<p>集群恢复</p>
<p><a href="https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md">https://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md</a></p>
</li>
</ol>
<h4 id="kubernetes调度">Kubernetes调度<a hidden class="anchor" aria-hidden="true" href="#kubernetes调度">#</a></h4>
<h6 id="为何要控制pod应该如何调度">为何要控制Pod应该如何调度<a hidden class="anchor" aria-hidden="true" href="#为何要控制pod应该如何调度">#</a></h6>
<ul>
<li>集群中有些机器的配置高（SSD，更好的内存等），我们希望核心的服务（比如说数据库）运行在上面</li>
<li>某两个服务的网络传输很频繁，我们希望它们最好在同一台机器上</li>
<li>&hellip;&hellip;</li>
</ul>
<p>Kubernetes Scheduler 的作用是将待调度的 Pod 按照一定的调度算法和策略绑定到集群中一个合适的 Worker Node 上，并将绑定信息写入到 etcd 中，之后目标 Node 中 kubelet 服务通过 API Server 监听到 Scheduler 产生的 Pod 绑定事件获取 Pod 信息，然后下载镜像启动容器。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/kube-scheduler-1.jpg" alt=""  />
</p>
<h6 id="调度的过程">调度的过程<a hidden class="anchor" aria-hidden="true" href="#调度的过程">#</a></h6>
<p>Scheduler 提供的调度流程分为预选 (Predicates) 和优选 (Priorities) 两个步骤：</p>
<ul>
<li>预选，K8S会遍历当前集群中的所有 Node，筛选出其中符合要求的 Node 作为候选</li>
<li>优选，K8S将对候选的 Node 进行打分</li>
</ul>
<p>经过预选筛选和优选打分之后，K8S选择分数最高的 Node 来运行 Pod，如果最终有多个 Node 的分数最高，那么 Scheduler 将从当中随机选择一个 Node 来运行 Pod。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/kube-scheduler-process.png" alt=""  />
</p>
<p>预选：</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/kube-scheduler-pre.jpg" alt=""  />
</p>
<p>优选：</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/kube-scheduler-pro.jpg" alt=""  />
</p>
<h6 id="cordon">Cordon<a hidden class="anchor" aria-hidden="true" href="#cordon">#</a></h6>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl cordon k8s-slave2
</span></span><span style="display:flex;"><span>$ kubectl drain k8s-slave2
</span></span></code></pre></div><h6 id="nodeselector">NodeSelector<a hidden class="anchor" aria-hidden="true" href="#nodeselector">#</a></h6>
<p><code>label</code>是<code>kubernetes</code>中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，POD 的调度可以根据节点的 label 进行特定的部署。</p>
<p>查看节点的label：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl get nodes --show-labels
</span></span></code></pre></div><p>为节点打label：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl label node k8s-master disktype<span style="color:#f92672">=</span>ssd
</span></span></code></pre></div><p>当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在spec 字段中添加<code>nodeSelector</code>字段，里面是我们需要被调度的节点的 label。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">hostNetwork</span>: <span style="color:#66d9ef">true</span> <span style="color:#75715e"># 声明pod的网络模式为host模式，效果通docker run --net=host</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">volumes</span>: 
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysql-data</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">hostPath</span>: 
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/opt/mysql/data</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">nodeSelector</span>:   <span style="color:#75715e"># 使用节点选择器将Pod调度到指定label的节点</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">component</span>: <span style="color:#ae81ff">mysql</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mysql</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">192.168.136.10</span>:<span style="color:#ae81ff">5000</span><span style="color:#ae81ff">/demo/mysql:5.7</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><h6 id="nodeaffinity">nodeAffinity<a hidden class="anchor" aria-hidden="true" href="#nodeaffinity">#</a></h6>
<p>节点亲和性 ， 比上面的<code>nodeSelector</code>更加灵活，它可以进行一些简单的逻辑组合，不只是简单的相等匹配 。分为两种，硬策略和软策略。</p>
<p>requiredDuringSchedulingIgnoredDuringExecution ： 硬策略，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不会调度Pod。</p>
<p>preferredDuringSchedulingIgnoredDuringExecution：软策略，如果你没有满足调度要求的节点的话，Pod就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有满足就忽略掉的策略。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e">#要求 Pod 不能运行在128和132两个节点上，如果有节点满足disktype=ssd或者sas的话就优先调度到这类节点上</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">demo</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">192.168.136.10</span>:<span style="color:#ae81ff">5000</span><span style="color:#ae81ff">/demo/myblog:v1</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">8002</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">affinity</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">nodeAffinity</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">requiredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>                <span style="color:#f92672">nodeSelectorTerms</span>:
</span></span><span style="display:flex;"><span>                - <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>                    - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">kubernetes.io/hostname</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">NotIn</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#f92672">values</span>:
</span></span><span style="display:flex;"><span>                        - <span style="color:#ae81ff">172.21.51.698</span>
</span></span><span style="display:flex;"><span>                        - <span style="color:#ae81ff">192.168.136.132</span>
</span></span><span style="display:flex;"><span>                        
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">preferredDuringSchedulingIgnoredDuringExecution</span>:
</span></span><span style="display:flex;"><span>                - <span style="color:#f92672">weight</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>                  <span style="color:#f92672">preference</span>:
</span></span><span style="display:flex;"><span>                    <span style="color:#f92672">matchExpressions</span>:
</span></span><span style="display:flex;"><span>                    - <span style="color:#f92672">key</span>: <span style="color:#ae81ff">disktype</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#f92672">operator</span>: <span style="color:#ae81ff">In</span>
</span></span><span style="display:flex;"><span>                      <span style="color:#f92672">values</span>:
</span></span><span style="display:flex;"><span>                        - <span style="color:#ae81ff">ssd</span>
</span></span><span style="display:flex;"><span>                        - <span style="color:#ae81ff">sas</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>这里的匹配逻辑是 label 的值在某个列表中，现在<code>Kubernetes</code>提供的操作符有下面的几种：</p>
<ul>
<li>In：label 的值在某个列表中</li>
<li>NotIn：label 的值不在某个列表中</li>
<li>Gt：label 的值大于某个值</li>
<li>Lt：label 的值小于某个值</li>
<li>Exists：某个 label 存在</li>
<li>DoesNotExist：某个 label 不存在</li>
</ul>
<p><em>如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 Pod</em></p>
<h6 id="污点taints与容忍tolerations">污点（Taints）与容忍（tolerations）<a hidden class="anchor" aria-hidden="true" href="#污点taints与容忍tolerations">#</a></h6>
<p>对于<code>nodeAffinity</code>无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而<code>Taints</code>恰好与之相反，如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度Pod。</p>
<p>Taints(污点)是Node的一个属性，设置了Taints(污点)后，因为有了污点，所以Kubernetes是不会将Pod调度到这个Node上的。于是Kubernetes就给Pod设置了个属性Tolerations(容忍)，只要Pod能够容忍Node上的污点，那么Kubernetes就会忽略Node上的污点，就能够(不是必须)把Pod调度过去。</p>
<p>场景一：私有云服务中，某业务使用GPU进行大规模并行计算。为保证性能，希望确保该业务对服务器的专属性，避免将普通业务调度到部署GPU的服务器。</p>
<p>场景二：用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 Pod，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：</p>
<p>设置污点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl taint node <span style="color:#f92672">[</span>node_name<span style="color:#f92672">]</span> key<span style="color:#f92672">=</span>value:<span style="color:#f92672">[</span>effect<span style="color:#f92672">]</span>   
</span></span><span style="display:flex;"><span>      其中<span style="color:#f92672">[</span>effect<span style="color:#f92672">]</span> 可取值： <span style="color:#f92672">[</span> NoSchedule | PreferNoSchedule | NoExecute <span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>       NoSchedule：一定不能被调度。
</span></span><span style="display:flex;"><span>       PreferNoSchedule：尽量不要调度。
</span></span><span style="display:flex;"><span>       NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。
</span></span><span style="display:flex;"><span>  示例：kubectl taint node k8s-slave1 smoke<span style="color:#f92672">=</span>true:NoSchedule
</span></span></code></pre></div><p>去除污点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>去除指定key及其effect：
</span></span><span style="display:flex;"><span>     kubectl taint nodes <span style="color:#f92672">[</span>node_name<span style="color:#f92672">]</span> key:<span style="color:#f92672">[</span>effect<span style="color:#f92672">]</span>-    <span style="color:#75715e">#这里的key不用指定value</span>
</span></span><span style="display:flex;"><span>                
</span></span><span style="display:flex;"><span> 去除指定key所有的effect: 
</span></span><span style="display:flex;"><span>     kubectl taint nodes node_name key-
</span></span><span style="display:flex;"><span> 
</span></span><span style="display:flex;"><span> 示例：
</span></span><span style="display:flex;"><span>     kubectl taint node k8s-master smoke<span style="color:#f92672">=</span>true:NoSchedule
</span></span><span style="display:flex;"><span>     kubectl taint node k8s-master smoke:NoExecute-
</span></span><span style="display:flex;"><span>     kubectl taint node k8s-master smoke-
</span></span></code></pre></div><p>污点演示：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e">## 给k8s-slave1打上污点，smoke=true:NoSchedule</span>
</span></span><span style="display:flex;"><span>$ kubectl taint node k8s-slave1 smoke<span style="color:#f92672">=</span>true:NoSchedule
</span></span><span style="display:flex;"><span>$ kubectl taint node k8s-slave2 drunk<span style="color:#f92672">=</span>true:NoSchedule
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## 扩容myblog的Pod，观察新Pod的调度情况</span>
</span></span><span style="display:flex;"><span>$ kuebctl -n luffy scale deploy myblog --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>$ kubectl -n luffy get po -w    <span style="color:#75715e">## pending</span>
</span></span></code></pre></div><p>Pod容忍污点示例：<code>myblog/deployment/deploy-myblog-taint.yaml</code></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">demo</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">192.168.136.10</span>:<span style="color:#ae81ff">5000</span><span style="color:#ae81ff">/demo/myblog:v1</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">tolerations</span>: <span style="color:#75715e">#设置容忍性</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">key</span>: <span style="color:#e6db74">&#34;smoke&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">operator</span>: <span style="color:#e6db74">&#34;Equal&#34;</span>  <span style="color:#75715e">#如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">value</span>: <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">effect</span>: <span style="color:#e6db74">&#34;NoSchedule&#34;</span>
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">key</span>: <span style="color:#e6db74">&#34;drunk&#34;</span> 
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">operator</span>: <span style="color:#e6db74">&#34;Exists&#34;</span>  <span style="color:#75715e">#如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#意思是这个Pod要容忍的有污点的Node的key是smoke Equal true,效果是NoSchedule，</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e">#tolerations属性下各值必须使用引号，容忍的值都是设置Node的taints时给的值。</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">$ kubectl apply -f deploy-myblog-taint.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">demo</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">192.168.136.10</span>:<span style="color:#ae81ff">5000</span><span style="color:#ae81ff">/demo/myblog</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">tolerations</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">operator</span>: <span style="color:#e6db74">&#34;Exists&#34;</span>
</span></span></code></pre></div><p>验证NoExecute效果</p>
<h4 id="kubernetes集群的网络实现">Kubernetes集群的网络实现<a hidden class="anchor" aria-hidden="true" href="#kubernetes集群的网络实现">#</a></h4>
<h5 id="cni介绍及集群网络选型">CNI介绍及集群网络选型,<a hidden class="anchor" aria-hidden="true" href="#cni介绍及集群网络选型">#</a></h5>
<p>CSI</p>
<p>容器网络接口（Container Network Interface），实现kubernetes集群的Pod网络通信及管理。包括：</p>
<ul>
<li>CNI Plugin负责给容器配置网络，它包括两个基本的接口： 配置网络: AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error) 清理网络: DelNetwork(net NetworkConfig, rt RuntimeConf) error</li>
<li>IPAM Plugin负责给容器分配IP地址，主要实现包括host-local和dhcp。</li>
</ul>
<p>以上两种插件的支持，使得k8s的网络可以支持各式各样的管理模式，当前在业界也出现了大量的支持方案，其中比较流行的比如flannel、calico等。</p>
<p>kubernetes配置了cni网络插件后，其容器网络创建流程为：</p>
<ul>
<li>kubelet先创建pause容器生成对应的network namespace</li>
<li>调用网络driver，因为配置的是CNI，所以会调用CNI相关代码，识别CNI的配置目录为/etc/cni/net.d</li>
<li>CNI driver根据配置调用具体的CNI插件，二进制调用，可执行文件目录为/opt/cni/bin,<a href="https://github.com/containernetworking/plugins">项目</a></li>
<li>CNI插件给pause容器配置正确的网络，pod中其他的容器都是用pause的网络</li>
</ul>
<p>可以在此查看社区中的CNI实现，https://github.com/containernetworking/cni</p>
<p>通用类型：flannel、calico等，部署使用简单</p>
<p>其他：根据具体的网络环境及网络需求选择，比如</p>
<ul>
<li>公有云机器，可以选择厂商与网络插件的定制Backend，如AWS、阿里、腾讯针对flannel均有自己的插件，也有AWS ECS CNI</li>
<li>私有云厂商，比如Vmware NSX-T等</li>
<li>网络性能等，MacVlan</li>
</ul>
<h5 id="flannel网络模型实现剖析">Flannel网络模型实现剖析<a hidden class="anchor" aria-hidden="true" href="#flannel网络模型实现剖析">#</a></h5>
<p>flannel实现overlay，underlay网络通常有多种实现：</p>
<ul>
<li>udp</li>
<li>vxlan</li>
<li>host-gw</li>
<li>&hellip;</li>
</ul>
<p>不特殊指定的话，默认会使用vxlan技术作为Backend，可以通过如下查看：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl -n kube-system exec  kube-flannel-ds-amd64-cb7hs cat /etc/kube-flannel/net-conf.json
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;Network&#34;</span>: <span style="color:#e6db74">&#34;10.244.0.0/16&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;Backend&#34;</span>: <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Type&#34;</span>: <span style="color:#e6db74">&#34;vxlan&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span></code></pre></div><h6 id="vxlan介绍及点对点通信的实现">vxlan介绍及点对点通信的实现<a hidden class="anchor" aria-hidden="true" href="#vxlan介绍及点对点通信的实现">#</a></h6>
<p>VXLAN 全称是虚拟可扩展的局域网（ Virtual eXtensible Local Area Network），它是一种 overlay 技术，通过三层的网络来搭建虚拟的二层网络。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/vxlan.png" alt=""  />
</p>
<p>它创建在原来的 IP 网络（三层）上，只要是三层可达（能够通过 IP 互相通信）的网络就能部署 vxlan。在每个端点上都有一个 vtep 负责 vxlan 协议报文的封包和解包，也就是在虚拟报文上封装 vtep 通信的报文头部。物理网络上可以创建多个 vxlan 网络，这些 vxlan 网络可以认为是一个隧道，不同节点的虚拟机能够通过隧道直连。每个 vxlan 网络由唯一的 VNI 标识，不同的 vxlan 可以不相互影响。</p>
<ul>
<li>VTEP（VXLAN Tunnel Endpoints）：vxlan 网络的边缘设备，用来进行 vxlan 报文的处理（封包和解包）。vtep 可以是网络设备（比如交换机），也可以是一台机器（比如虚拟化集群中的宿主机）</li>
<li>VNI（VXLAN Network Identifier）：VNI 是每个 vxlan 的标识，一共有 2^24 = 16,777,216，一般每个 VNI 对应一个租户，也就是说使用 vxlan 搭建的公有云可以理论上可以支撑千万级别的租户</li>
</ul>
<p>演示：在k8s-slave1和k8s-slave2两台机器间，利用vxlan的点对点能力，实现虚拟二层网络的通信</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/vxlan-p2p-1.png" alt=""  />
</p>
<p>k8s-slave1节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 创建vTEP设备，对端指向k8s-slave2节点，指定VNI及underlay网络使用的网卡</span>
</span></span><span style="display:flex;"><span>$ ip link add vxlan20 type vxlan id <span style="color:#ae81ff">20</span> remote 172.21.51.69 dstport <span style="color:#ae81ff">4789</span> dev eth0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ ip -d link show vxlan20
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 启动设备</span>
</span></span><span style="display:flex;"><span>$ ip link set vxlan20 up 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置ip地址</span>
</span></span><span style="display:flex;"><span> ip addr add 10.0.136.11/24 dev vxlan20
</span></span></code></pre></div><p>k8s-slave2节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 创建VTEP设备，对端指向k8s-slave1节点，指定VNI及underlay网络使用的网卡</span>
</span></span><span style="display:flex;"><span>$ ip link add vxlan20 type vxlan id <span style="color:#ae81ff">20</span> remote 172.21.51.68 dstport <span style="color:#ae81ff">4789</span> dev eth0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 启动设备</span>
</span></span><span style="display:flex;"><span>$ ip link set vxlan20 up 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 设置ip地址</span>
</span></span><span style="display:flex;"><span>$ ip addr add 10.0.136.12/24 dev vxlan20
</span></span></code></pre></div><p>在k8s-slave1节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ ping 10.0.136.12
</span></span></code></pre></div><p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/vxlan-p2p.png" alt=""  />
</p>
<p>隧道是一个逻辑上的概念，在 vxlan 模型中并没有具体的物理实体想对应。隧道可以看做是一种虚拟通道，vxlan 通信双方（图中的虚拟机）认为自己是在直接通信，并不知道底层网络的存在。从整体来说，每个 vxlan 网络像是为通信的虚拟机搭建了一个单独的通信通道，也就是隧道。</p>
<p>实现的过程：</p>
<p>虚拟机的报文通过 vtep 添加上 vxlan 以及外部的报文层，然后发送出去，对方 vtep 收到之后拆除 vxlan 头部然后根据 VNI 把原始报文发送到目的虚拟机。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 查看k8s-slave1主机路由</span>
</span></span><span style="display:flex;"><span>$ route -n
</span></span><span style="display:flex;"><span>10.0.136.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> vxlan20
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 到了vxlan的设备后，</span>
</span></span><span style="display:flex;"><span>$ ip -d link show vxlan20
</span></span><span style="display:flex;"><span>    vxlan id <span style="color:#ae81ff">20</span> remote 172.21.51.69 dev eth0 srcport <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span> dstport <span style="color:#ae81ff">4789</span> ...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看fdb地址表，主要由MAC地址、VLAN号、端口号和一些标志域等信息组成,vtep 对端地址为 172.21.51.69，换句话说，如果接收到的报文添加上 vxlan 头部之后都会发到 172.21.51.69</span>
</span></span><span style="display:flex;"><span>$ bridge fdb show|grep vxlan20
</span></span><span style="display:flex;"><span>00:00:00:00:00:00 dev vxlan20 dst 172.21.51.69 via eth0 self permanent
</span></span></code></pre></div><p>在k8s-slave2机器抓包，查看vxlan封装后的包:</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 在k8s-slave2机器执行</span>
</span></span><span style="display:flex;"><span>$ tcpdump -i eth0 host 172.21.51.68 -w vxlan.cap
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 在k8s-slave1机器执行</span>
</span></span><span style="display:flex;"><span>$ ping 10.0.136.12
</span></span></code></pre></div><p>使用wireshark分析ICMP类型的数据包</p>
<h6 id="跨主机容器网络的通信">跨主机容器网络的通信<a hidden class="anchor" aria-hidden="true" href="#跨主机容器网络的通信">#</a></h6>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/vxlan-docker.png" alt=""  />
</p>
<p>思考：容器网络模式下，vxlan设备该接在哪里？</p>
<p>基本的保证：目的容器的流量要通过vtep设备进行转发！</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/vxlan-docker-mul.png" alt=""  />
</p>
<p>演示：利用vxlan实现跨主机容器网络通信</p>
<p>为了不影响已有的网络，因此创建一个新的网桥，创建容器接入到新的网桥来演示效果</p>
<p>在k8s-slave1节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker network ls
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建新网桥，指定cidr段</span>
</span></span><span style="display:flex;"><span>$ docker network create --subnet 172.18.0.0/16  network-luffy
</span></span><span style="display:flex;"><span>$ docker network ls
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 新建容器，接入到新网桥</span>
</span></span><span style="display:flex;"><span>$ docker run -d --name vxlan-test --net network-luffy --ip 172.18.0.2 nginx:alpine
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ docker exec vxlan-test ifconfig
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ brctl show network-luffy
</span></span></code></pre></div><p>在k8s-slave2节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 创建新网桥，指定cidr段</span>
</span></span><span style="display:flex;"><span>$ docker network create --subnet 172.18.0.0/16  network-luffy
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 新建容器，接入到新网桥</span>
</span></span><span style="display:flex;"><span>$ docker run -d --name vxlan-test --net network-luffy --ip 172.18.0.3 nginx:alpine
</span></span></code></pre></div><p>此时执行ping测试：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker exec vxlan-test ping 172.18.0.3
</span></span></code></pre></div><p>分析：数据到了网桥后，出不去。结合前面的示例，因此应该将流量由vtep设备转发，联想到网桥的特性，接入到桥中的端口，会由网桥负责转发数据，因此，相当于所有容器发出的数据都会经过到vxlan的端口，vxlan将流量转到对端的vtep端点，再次由网桥负责转到容器中。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/vxlan-docker-mul-all.png" alt=""  />
</p>
<p>k8s-slave1节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 删除旧的vtep</span>
</span></span><span style="display:flex;"><span>$ ip link del vxlan20
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 新建vtep</span>
</span></span><span style="display:flex;"><span>$ ip link add vxlan_docker type vxlan id <span style="color:#ae81ff">100</span> remote 172.21.51.69 dstport <span style="color:#ae81ff">4789</span> dev eth0
</span></span><span style="display:flex;"><span>$ ip link set vxlan_docker up
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 不用设置ip，因为目标是可以转发容器的数据即可</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 接入到网桥中</span>
</span></span><span style="display:flex;"><span>$ brctl addif br-904603a72dcd vxlan_docker
</span></span></code></pre></div><p>k8s-slave2节点：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 删除旧的vtep</span>
</span></span><span style="display:flex;"><span>$ ip link del vxlan20
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 新建vtep</span>
</span></span><span style="display:flex;"><span>$ ip link add vxlan_docker type vxlan id <span style="color:#ae81ff">100</span> remote 172.21.51.68 dstport <span style="color:#ae81ff">4789</span> dev eth0
</span></span><span style="display:flex;"><span>$ ip link set vxlan_docker up
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 不用设置ip，因为目标是可以转发容器的数据即可</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 接入到网桥中</span>
</span></span><span style="display:flex;"><span>$ brctl addif br-c6660fe2dc53 vxlan_docker
</span></span></code></pre></div><p>再次执行ping测试：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ docker exec vxlan-test ping 172.18.0.3
</span></span></code></pre></div><h6 id="flannel的vxlan实现精讲">Flannel的vxlan实现精讲<a hidden class="anchor" aria-hidden="true" href="#flannel的vxlan实现精讲">#</a></h6>
<p>思考：k8s集群的网络环境和手动实现的跨主机的容器通信有哪些差别？</p>
<ol>
<li>CNI要求，集群中的每个Pod都必须分配唯一的Pod IP</li>
<li>k8s集群内的通信不是vxlan点对点通信，因为集群内的所有节点之间都需要互联
<ul>
<li>没法创建点对点的vxlan模型</li>
</ul>
</li>
</ol>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/flannel.png" alt=""  />
</p>
<p>flannel如何为每个节点分配Pod地址段：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl -n kube-system exec kube-flannel-ds-amd64-cb7hs cat /etc/kube-flannel/net-conf.json
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;Network&#34;</span>: <span style="color:#e6db74">&#34;10.244.0.0/16&#34;</span>,
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;Backend&#34;</span>: <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;Type&#34;</span>: <span style="color:#e6db74">&#34;vxlan&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#查看节点的pod ip</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">[</span>root@k8s-master bin<span style="color:#f92672">]</span><span style="color:#75715e"># kd get po -o wide</span>
</span></span><span style="display:flex;"><span>NAME                      READY   STATUS    RESTARTS   AGE     IP            NODE        
</span></span><span style="display:flex;"><span>myblog-5d9ff54d4b-4rftt   1/1     Running   <span style="color:#ae81ff">1</span>          33h     10.244.2.19   k8s-slave2  
</span></span><span style="display:flex;"><span>myblog-5d9ff54d4b-n447p   1/1     Running   <span style="color:#ae81ff">1</span>          33h     10.244.1.32   k8s-slave1
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#查看k8s-slave1主机分配的地址段</span>
</span></span><span style="display:flex;"><span>$ cat /run/flannel/subnet.env
</span></span><span style="display:flex;"><span>FLANNEL_NETWORK<span style="color:#f92672">=</span>10.244.0.0/16
</span></span><span style="display:flex;"><span>FLANNEL_SUBNET<span style="color:#f92672">=</span>10.244.1.1/24
</span></span><span style="display:flex;"><span>FLANNEL_MTU<span style="color:#f92672">=</span><span style="color:#ae81ff">1450</span>
</span></span><span style="display:flex;"><span>FLANNEL_IPMASQ<span style="color:#f92672">=</span>true
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># kubelet启动容器的时候就可以按照本机的网段配置来为pod设置IP地址</span>
</span></span></code></pre></div><p>vtep的设备在哪：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ ip -d link show flannel.1
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 没有remote ip，非点对点</span>
</span></span></code></pre></div><p>Pod的流量如何转到vtep设备中</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ brctl show cni0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 每个Pod都会使用Veth pair来实现流量转到cni0网桥</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ route -n
</span></span><span style="display:flex;"><span>10.244.0.0      10.244.0.0      255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> flannel.1
</span></span><span style="display:flex;"><span>10.244.1.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> cni0
</span></span><span style="display:flex;"><span>10.244.2.0      10.244.2.0      255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> flannel.1
</span></span></code></pre></div><p>vtep封包的时候，如何拿到目的vetp端的IP及MAC信息</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># flanneld启动的时候会需要配置--iface=eth0,通过该配置可以将网卡的ip及Mac信息存储到ETCD中，</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 这样，flannel就知道所有的节点分配的IP段及vtep设备的IP和MAC信息，而且所有节点的flanneld都可以感知到节点的添加和删除操作，就可以动态的更新本机的转发配置</span>
</span></span></code></pre></div><p>演示跨主机Pod通信的流量详细过程：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl -n luffy get po -o wide
</span></span><span style="display:flex;"><span>myblog-5d9ff54d4b-4rftt   1/1     Running   <span style="color:#ae81ff">1</span>          25h    10.244.2.19   k8s-slave2
</span></span><span style="display:flex;"><span>myblog-5d9ff54d4b-n447p   1/1     Running   <span style="color:#ae81ff">1</span>          25h    10.244.1.32   k8s-slave1
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl -n luffy exec myblog-5d9ff54d4b-n447p -- ping 10.244.2.19 -c <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>PING 10.244.2.19 <span style="color:#f92672">(</span>10.244.2.19<span style="color:#f92672">)</span> 56<span style="color:#f92672">(</span>84<span style="color:#f92672">)</span> bytes of data.
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">64</span> bytes from 10.244.2.19: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">62</span> time<span style="color:#f92672">=</span>0.480 ms
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">64</span> bytes from 10.244.2.19: icmp_seq<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> ttl<span style="color:#f92672">=</span><span style="color:#ae81ff">62</span> time<span style="color:#f92672">=</span>1.44 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>--- 10.244.2.19 ping statistics ---
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">2</span> packets transmitted, <span style="color:#ae81ff">2</span> received, 0% packet loss, time 1001ms
</span></span><span style="display:flex;"><span>rtt min/avg/max/mdev <span style="color:#f92672">=</span> 0.480/0.961/1.443/0.482 ms
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看路由</span>
</span></span><span style="display:flex;"><span>$ kubectl -n luffy exec myblog-5d9ff54d4b-n447p -- route -n
</span></span><span style="display:flex;"><span>Kernel IP routing table
</span></span><span style="display:flex;"><span>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
</span></span><span style="display:flex;"><span>0.0.0.0         10.244.1.1      0.0.0.0         UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>10.244.0.0      10.244.1.1      255.255.0.0     UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>10.244.1.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看k8s-slave1 的veth pair 和网桥</span>
</span></span><span style="display:flex;"><span>$ brctl show
</span></span><span style="display:flex;"><span>bridge name     bridge id               STP enabled     interfaces
</span></span><span style="display:flex;"><span>cni0            8000.6a9a0b341d88       no              veth048cc253
</span></span><span style="display:flex;"><span>                                                        veth76f8e4ce
</span></span><span style="display:flex;"><span>                                                        vetha4c972e1
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 流量到了cni0后，查看slave1节点的route</span>
</span></span><span style="display:flex;"><span>$ route -n
</span></span><span style="display:flex;"><span>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
</span></span><span style="display:flex;"><span>0.0.0.0         192.168.136.2   0.0.0.0         UG    <span style="color:#ae81ff">100</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>10.0.136.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> vxlan20
</span></span><span style="display:flex;"><span>10.244.0.0      10.244.0.0      255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> flannel.1
</span></span><span style="display:flex;"><span>10.244.1.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> cni0
</span></span><span style="display:flex;"><span>10.244.2.0      10.244.2.0      255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> flannel.1
</span></span><span style="display:flex;"><span>172.17.0.0      0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> docker0
</span></span><span style="display:flex;"><span>192.168.136.0   0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">100</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 流量转发到了flannel.1网卡，查看该网卡，其实是vtep设备</span>
</span></span><span style="display:flex;"><span>$ ip -d link show flannel.1
</span></span><span style="display:flex;"><span>4: flannel.1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu <span style="color:#ae81ff">1450</span> qdisc noqueue state UNKNOWN mode DEFAULT group default
</span></span><span style="display:flex;"><span>    link/ether 8a:2a:89:4d:b0:31 brd ff:ff:ff:ff:ff:ff promiscuity <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>    vxlan id <span style="color:#ae81ff">1</span> local 172.21.51.68 dev eth0 srcport <span style="color:#ae81ff">0</span> <span style="color:#ae81ff">0</span> dstport <span style="color:#ae81ff">8472</span> nolearning ageing <span style="color:#ae81ff">300</span> noudpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues <span style="color:#ae81ff">1</span> numrxqueues <span style="color:#ae81ff">1</span> gso_max_size <span style="color:#ae81ff">65536</span> gso_max_segs <span style="color:#ae81ff">65535</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 该转发到哪里，通过etcd查询数据，然后本地缓存，流量不用走多播发送</span>
</span></span><span style="display:flex;"><span>$ bridge fdb show dev flannel.1
</span></span><span style="display:flex;"><span>a6:64:a0:a5:83:55 dst 192.168.136.10 self permanent
</span></span><span style="display:flex;"><span>86:c2:ad:4e:47:20 dst 172.21.51.69 self permanent
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 对端的vtep设备接收到请求后做解包，取出源payload内容，查看k8s-slave2的路由</span>
</span></span><span style="display:flex;"><span>$ route -n
</span></span><span style="display:flex;"><span>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
</span></span><span style="display:flex;"><span>0.0.0.0         192.168.136.2   0.0.0.0         UG    <span style="color:#ae81ff">100</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>10.0.136.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> vxlan20
</span></span><span style="display:flex;"><span>10.244.0.0      10.244.0.0      255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> flannel.1
</span></span><span style="display:flex;"><span>10.244.1.0      10.244.1.0      255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> flannel.1
</span></span><span style="display:flex;"><span>10.244.2.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> cni0
</span></span><span style="display:flex;"><span>172.17.0.0      0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> docker0
</span></span><span style="display:flex;"><span>192.168.136.0   0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">100</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#根据路由规则转发到cni0网桥,然后由网桥转到具体的Pod中</span>
</span></span></code></pre></div><p>实际的请求图：</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/flannel-actual.png" alt=""  />
</p>
<ul>
<li>k8s-slave1 节点中的 pod-a（10.244.2.19）当中的 IP 包通过 pod-a 内的路由表被发送到eth0，进一步通过veth pair转到宿主机中的网桥 <code>cni0</code></li>
<li>到达 <code>cni0</code> 当中的 IP 包通过匹配节点 k8s-slave1 的路由表发现通往 10.244.2.19 的 IP 包应该交给 <code>flannel.1</code> 接口</li>
<li><code>flannel.1</code> 作为一个 VTEP 设备，收到报文后将按照 <code>VTEP</code> 的配置进行封包，第一次会查询ETCD，知道10.244.2.19的vtep设备是k8s-slave2机器，IP地址是172.21.51.69，拿到MAC 地址进行 VXLAN 封包。</li>
<li>通过节点 k8s-slave2 跟 k8s-slave1之间的网络连接，VXLAN 包到达 k8s-slave2 的 eth0 接口</li>
<li>通过端口 8472，VXLAN 包被转发给 VTEP 设备 <code>flannel.1</code> 进行解包</li>
<li>解封装后的 IP 包匹配节点 k8s-slave2 当中的路由表（10.244.2.0），内核将 IP 包转发给<code>cni0</code></li>
<li><code>cni0</code>将 IP 包转发给连接在 <code>cni0</code> 上的 pod-b</li>
</ul>
<h6 id="利用host-gw模式提升集群网络性能">利用host-gw模式提升集群网络性能<a hidden class="anchor" aria-hidden="true" href="#利用host-gw模式提升集群网络性能">#</a></h6>
<p>vxlan模式适用于三层可达的网络环境，对集群的网络要求很宽松，但是同时由于会通过VTEP设备进行额外封包和解包，因此给性能带来了额外的开销。</p>
<p>网络插件的目的其实就是将本机的cni0网桥的流量送到目的主机的cni0网桥。实际上有很多集群是部署在同一二层网络环境下的，可以直接利用二层的主机当作流量转发的网关。这样的话，可以不用进行封包解包，直接通过路由表去转发流量。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/flannel-host-gw.png" alt=""  />
</p>
<p>为什么三层可达的网络不直接利用网关转发流量？</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>内核当中的路由规则，网关必须在跟主机当中至少一个 IP 处于同一网段。
</span></span><span style="display:flex;"><span>由于k8s集群内部各节点均需要实现Pod互通，因此，也就意味着host-gw模式需要整个集群节点都在同一二层网络内。
</span></span></code></pre></div><p>修改flannel的网络后端：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl edit cm kube-flannel-cfg -n kube-system
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>net-conf.json: |
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;Network&#34;</span>: <span style="color:#e6db74">&#34;10.244.0.0/16&#34;</span>,
</span></span><span style="display:flex;"><span>      <span style="color:#e6db74">&#34;Backend&#34;</span>: <span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>        <span style="color:#e6db74">&#34;Type&#34;</span>: <span style="color:#e6db74">&#34;host-gw&#34;</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>kind: ConfigMap
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>重建Flannel的Pod</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl -n kube-system get po |grep flannel
</span></span><span style="display:flex;"><span>kube-flannel-ds-amd64-5dgb8          1/1     Running   <span style="color:#ae81ff">0</span>          15m
</span></span><span style="display:flex;"><span>kube-flannel-ds-amd64-c2gdc          1/1     Running   <span style="color:#ae81ff">0</span>          14m
</span></span><span style="display:flex;"><span>kube-flannel-ds-amd64-t2jdd          1/1     Running   <span style="color:#ae81ff">0</span>          15m
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl -n kube-system delete po kube-flannel-ds-amd64-5dgb8 kube-flannel-ds-amd64-c2gdc kube-flannel-ds-amd64-t2jdd
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 等待Pod新启动后，查看日志，出现Backend type: host-gw字样</span>
</span></span><span style="display:flex;"><span>$  kubectl -n kube-system logs -f kube-flannel-ds-amd64-4hjdw
</span></span><span style="display:flex;"><span>I0704 01:18:11.916374       <span style="color:#ae81ff">1</span> kube.go:126<span style="color:#f92672">]</span> Waiting 10m0s <span style="color:#66d9ef">for</span> node controller to sync
</span></span><span style="display:flex;"><span>I0704 01:18:11.916579       <span style="color:#ae81ff">1</span> kube.go:309<span style="color:#f92672">]</span> Starting kube subnet manager
</span></span><span style="display:flex;"><span>I0704 01:18:12.917339       <span style="color:#ae81ff">1</span> kube.go:133<span style="color:#f92672">]</span> Node controller sync successful
</span></span><span style="display:flex;"><span>I0704 01:18:12.917848       <span style="color:#ae81ff">1</span> main.go:247<span style="color:#f92672">]</span> Installing signal handlers
</span></span><span style="display:flex;"><span>I0704 01:18:12.918569       <span style="color:#ae81ff">1</span> main.go:386<span style="color:#f92672">]</span> Found network config - Backend type: host-gw
</span></span><span style="display:flex;"><span>I0704 01:18:13.017841       <span style="color:#ae81ff">1</span> main.go:317<span style="color:#f92672">]</span> Wrote subnet file to /run/flannel/subnet.env
</span></span></code></pre></div><p>查看节点路由表：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ route -n 
</span></span><span style="display:flex;"><span>Destination     Gateway         Genmask         Flags Metric Ref    Use Iface
</span></span><span style="display:flex;"><span>0.0.0.0         192.168.136.2   0.0.0.0         UG    <span style="color:#ae81ff">100</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>10.244.0.0      0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> cni0
</span></span><span style="display:flex;"><span>10.244.1.0      172.21.51.68  255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>10.244.2.0      172.21.51.69  255.255.255.0   UG    <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span><span style="display:flex;"><span>172.17.0.0      0.0.0.0         255.255.0.0     U     <span style="color:#ae81ff">0</span>      <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> docker0
</span></span><span style="display:flex;"><span>192.168.136.0   0.0.0.0         255.255.255.0   U     <span style="color:#ae81ff">100</span>    <span style="color:#ae81ff">0</span>        <span style="color:#ae81ff">0</span> eth0
</span></span></code></pre></div><ul>
<li>k8s-slave1 节点中的 pod-a（10.244.2.19）当中的 IP 包通过 pod-a 内的路由表被发送到eth0，进一步通过veth pair转到宿主机中的网桥 <code>cni0</code></li>
<li>到达 <code>cni0</code> 当中的 IP 包通过匹配节点 k8s-slave1 的路由表发现通往 10.244.2.19 的 IP 包应该使用172.21.51.69这个网关进行转发</li>
<li>包到达k8s-slave2节点（172.21.51.69）节点的eth0网卡，根据该节点的路由规则，转发给cni0网卡</li>
<li><code>cni0</code>将 IP 包转发给连接在 <code>cni0</code> 上的 pod-b</li>
</ul>
<h4 id="kubernetes认证与授权">Kubernetes认证与授权<a hidden class="anchor" aria-hidden="true" href="#kubernetes认证与授权">#</a></h4>
<h6 id="apiserver安全控制">APIServer安全控制<a hidden class="anchor" aria-hidden="true" href="#apiserver安全控制">#</a></h6>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/k8s-apiserver-access-control-overview.svg" alt=""  />
</p>
<ul>
<li>
<p>Authentication：身份认证</p>
<ol>
<li>这个环节它面对的输入是整个<code>http request</code>，负责对来自client的请求进行身份校验，支持的方法包括:
<ul>
<li><code>basic auth</code></li>
<li>client证书验证（https双向验证）</li>
<li><code>jwt token</code>(用于serviceaccount)</li>
</ul>
</li>
<li>APIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐个使用这些方法对客户端请求进行验证， 只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功；</li>
<li>使用kubeadm引导启动的k8s集群，apiserver的初始配置中，默认支持<code>client证书</code>验证和<code>serviceaccount</code>两种身份验证方式。 证书认证通过设置<code>--client-ca-file</code>根证书以及<code>--tls-cert-file</code>和<code>--tls-private-key-file</code>来开启。</li>
<li>在这个环节，apiserver会通过client证书或 <code>http header</code>中的字段(比如serviceaccount的<code>jwt token</code>)来识别出请求的<code>用户身份</code>，包括”user”、”group”等，这些信息将在后面的<code>authorization</code>环节用到。</li>
</ol>
</li>
<li>
<p>Authorization：鉴权，你可以访问哪些资源</p>
<ol>
<li>这个环节面对的输入是<code>http request context</code>中的各种属性，包括：<code>user</code>、<code>group</code>、<code>request path</code>（比如：<code>/api/v1</code>、<code>/healthz</code>、<code>/version</code>等）、 <code>request verb</code>(比如：<code>get</code>、<code>list</code>、<code>create</code>等)。</li>
<li>APIServer会将这些属性值与事先配置好的访问策略(<code>access policy</code>）相比较。APIServer支持多种<code>authorization mode</code>，包括<code>Node、RBAC、Webhook</code>等。</li>
<li>APIServer启动时，可以指定一种<code>authorization mode</code>，也可以指定多种<code>authorization mode</code>，如果是后者，只要Request通过了其中一种mode的授权， 那么该环节的最终结果就是授权成功。在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，<code>authorization-mode</code>的默认配置是<code>”Node,RBAC”</code>。</li>
</ol>
</li>
<li>
<p>Admission Control：<a href="http://docs.kubernetes.org.cn/144.html">准入控制</a>，一个控制链(层层关卡)，用于拦截请求的一种方式。偏集群安全控制、管理方面。</p>
<ul>
<li>
<p>为什么需要？</p>
<p>认证与授权获取 http 请求 header 以及证书，无法通过body内容做校验。</p>
<p>Admission 运行在 API Server 的增删改查 handler 中，可以自然地操作 API resource</p>
</li>
<li>
<p>举个栗子</p>
<ul>
<li>
<p>以NamespaceLifecycle为例， 该插件确保处于Termination状态的Namespace不再接收新的对象创建请求，并拒绝请求不存在的Namespace。该插件还可以防止删除系统保留的Namespace:default，kube-system，kube-public。</p>
</li>
<li>
<p>LimitRanger，若集群的命名空间设置了LimitRange对象，若Pod声明时未设置资源值，则按照LimitRange的定义来未Pod添加默认值</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">LimitRange</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mem-limit-range</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">luffy</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">limits</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">default</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">512Mi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">defaultRequest</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">memory</span>: <span style="color:#ae81ff">256Mi</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Container</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default-mem-demo-2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">default-mem-demo-2-ctr</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nginx:alpin</span>
</span></span></code></pre></div></li>
<li>
<p>NodeRestriction， 此插件限制kubelet修改Node和Pod对象，这样的kubelets只允许修改绑定到Node的Pod API对象，以后版本可能会增加额外的限制 。开启Node授权策略后，默认会打开该项</p>
</li>
</ul>
</li>
<li>
<p>怎么用？</p>
<p>APIServer启动时通过 <code>--enable-admission-plugins --disable-admission-plugins</code> 指定需要打开或者关闭的 Admission Controller</p>
</li>
<li>
<p>场景</p>
<ul>
<li>自动注入sidecar容器或者initContainer容器</li>
<li>webhook admission，实现业务自定义的控制需求</li>
</ul>
</li>
</ul>
</li>
</ul>
<h6 id="kubectl的认证授权">kubectl的认证授权<a hidden class="anchor" aria-hidden="true" href="#kubectl的认证授权">#</a></h6>
<p>kubectl的日志调试级别：</p>
<table>
<thead>
<tr>
<th style="text-align:left">信息</th>
<th style="text-align:left">描述</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">v=0</td>
<td style="text-align:left">通常，这对操作者来说总是可见的。</td>
</tr>
<tr>
<td style="text-align:left">v=1</td>
<td style="text-align:left">当您不想要很详细的输出时，这个是一个合理的默认日志级别。</td>
</tr>
<tr>
<td style="text-align:left">v=2</td>
<td style="text-align:left">有关服务和重要日志消息的有用稳定状态信息，这些信息可能与系统中的重大更改相关。这是大多数系统推荐的默认日志级别。</td>
</tr>
<tr>
<td style="text-align:left">v=3</td>
<td style="text-align:left">关于更改的扩展信息。</td>
</tr>
<tr>
<td style="text-align:left">v=4</td>
<td style="text-align:left">调试级别信息。</td>
</tr>
<tr>
<td style="text-align:left">v=6</td>
<td style="text-align:left">显示请求资源。</td>
</tr>
<tr>
<td style="text-align:left">v=7</td>
<td style="text-align:left">显示 HTTP 请求头。</td>
</tr>
<tr>
<td style="text-align:left">v=8</td>
<td style="text-align:left">显示 HTTP 请求内容。</td>
</tr>
<tr>
<td style="text-align:left">v=9</td>
<td style="text-align:left">显示 HTTP 请求内容，并且不截断内容。</td>
</tr>
</tbody>
</table>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl get nodes -v<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>
</span></span><span style="display:flex;"><span>I0329 20:20:08.633065    <span style="color:#ae81ff">3979</span> loader.go:359<span style="color:#f92672">]</span> Config loaded from file /root/.kube/config
</span></span><span style="display:flex;"><span>I0329 20:20:08.633797    <span style="color:#ae81ff">3979</span> round_trippers.go:416<span style="color:#f92672">]</span> GET https://192.168.136.10:6443/api/v1/nodes?limit<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>
</span></span></code></pre></div><p><code>kubeadm init</code>启动完master节点后，会默认输出类似下面的提示内容：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>... ...
</span></span><span style="display:flex;"><span>Your Kubernetes master has initialized successfully!
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>To start using your cluster, you need to run the following as a regular user:
</span></span><span style="display:flex;"><span>  mkdir -p $HOME/.kube
</span></span><span style="display:flex;"><span>  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
</span></span><span style="display:flex;"><span>  sudo chown <span style="color:#66d9ef">$(</span>id -u<span style="color:#66d9ef">)</span>:<span style="color:#66d9ef">$(</span>id -g<span style="color:#66d9ef">)</span> $HOME/.kube/config
</span></span><span style="display:flex;"><span>... ...
</span></span></code></pre></div><p>这些信息是在告知我们如何配置<code>kubeconfig</code>文件。按照上述命令配置后，master节点上的<code>kubectl</code>就可以直接使用<code>$HOME/.kube/config</code>的信息访问<code>k8s cluster</code>了。 并且，通过这种配置方式，<code>kubectl</code>也拥有了整个集群的管理员(root)权限。</p>
<p>很多K8s初学者在这里都会有疑问：</p>
<ul>
<li>当<code>kubectl</code>使用这种<code>kubeconfig</code>方式访问集群时，<code>Kubernetes</code>的<code>kube-apiserver</code>是如何对来自<code>kubectl</code>的访问进行身份验证(<code>authentication</code>)和授权(<code>authorization</code>)的呢？</li>
<li>为什么来自<code>kubectl</code>的请求拥有最高的管理员权限呢？</li>
</ul>
<p>查看<code>/root/.kube/config</code>文件：</p>
<p>前面提到过apiserver的authentication支持通过<code>tls client certificate、basic auth、token</code>等方式对客户端发起的请求进行身份校验， 从kubeconfig信息来看，kubectl显然在请求中使用了<code>tls client certificate</code>的方式，即客户端的证书。</p>
<p>证书base64解码：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ echo xxxxxxxxxxxxxx |base64 -d &gt; kubectl.crt
</span></span></code></pre></div><p>说明在认证阶段，<code>apiserver</code>会首先使用<code>--client-ca-file</code>配置的CA证书去验证kubectl提供的证书的有效性,基本的方式 ：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$  openssl verify -CAfile /etc/kubernetes/pki/ca.crt kubectl.crt
</span></span><span style="display:flex;"><span>kubectl.crt: OK
</span></span></code></pre></div><p>除了认证身份，还会取出必要的信息供授权阶段使用，文本形式查看证书内容：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ openssl x509 -in kubectl.crt -text
</span></span><span style="display:flex;"><span>Certificate:
</span></span><span style="display:flex;"><span>    Data:
</span></span><span style="display:flex;"><span>        Version: <span style="color:#ae81ff">3</span> <span style="color:#f92672">(</span>0x2<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>        Serial Number: <span style="color:#ae81ff">4736260165981664452</span> <span style="color:#f92672">(</span>0x41ba9386f52b74c4<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Signature Algorithm: sha256WithRSAEncryption
</span></span><span style="display:flex;"><span>        Issuer: CN<span style="color:#f92672">=</span>kubernetes
</span></span><span style="display:flex;"><span>        Validity
</span></span><span style="display:flex;"><span>            Not Before: Feb <span style="color:#ae81ff">10</span> 07:33:39 <span style="color:#ae81ff">2020</span> GMT
</span></span><span style="display:flex;"><span>            Not After : Feb  <span style="color:#ae81ff">9</span> 07:33:40 <span style="color:#ae81ff">2021</span> GMT
</span></span><span style="display:flex;"><span>        Subject: O<span style="color:#f92672">=</span>system:masters, CN<span style="color:#f92672">=</span>kubernetes-admin
</span></span><span style="display:flex;"><span>        ...
</span></span></code></pre></div><p>认证通过后，提取出签发证书时指定的CN(Common Name),<code>kubernetes-admin</code>，作为请求的用户名 (User Name), 从证书中提取O(Organization)字段作为请求用户所属的组 (Group)，<code>group = system:masters</code>，然后传递给后面的授权模块。</p>
<p>kubeadm在init初始引导集群启动过程中，创建了许多默认的RBAC规则， 在k8s有关RBAC的官方文档中，我们看到下面一些<code>default clusterrole</code>列表:</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/kubeadm-default-clusterrole-list.png" alt=""  />
</p>
<p>其中第一个cluster-admin这个cluster role binding绑定了system:masters group，这和authentication环节传递过来的身份信息不谋而合。 沿着system:masters group对应的cluster-admin clusterrolebinding“追查”下去，真相就会浮出水面。</p>
<p>我们查看一下这一binding：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl describe clusterrolebinding cluster-admin
</span></span><span style="display:flex;"><span>Name:         cluster-admin
</span></span><span style="display:flex;"><span>Labels:       kubernetes.io/bootstrapping<span style="color:#f92672">=</span>rbac-defaults
</span></span><span style="display:flex;"><span>Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
</span></span><span style="display:flex;"><span>Role:
</span></span><span style="display:flex;"><span>  Kind:  ClusterRole
</span></span><span style="display:flex;"><span>  Name:  cluster-admin
</span></span><span style="display:flex;"><span>Subjects:
</span></span><span style="display:flex;"><span>  Kind   Name            Namespace
</span></span><span style="display:flex;"><span>  ----   ----            ---------
</span></span><span style="display:flex;"><span>  Group  system:masters
</span></span></code></pre></div><p>我们看到在kube-system名字空间中，一个名为cluster-admin的clusterrolebinding将cluster-admin cluster role与system:masters Group绑定到了一起， 赋予了所有归属于system:masters Group中用户cluster-admin角色所拥有的权限。</p>
<p>我们再来查看一下cluster-admin这个role的具体权限信息：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl describe clusterrole cluster-admin
</span></span><span style="display:flex;"><span>Name:         cluster-admin
</span></span><span style="display:flex;"><span>Labels:       kubernetes.io/bootstrapping<span style="color:#f92672">=</span>rbac-defaults
</span></span><span style="display:flex;"><span>Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
</span></span><span style="display:flex;"><span>PolicyRule:
</span></span><span style="display:flex;"><span>  Resources  Non-Resource URLs  Resource Names  Verbs
</span></span><span style="display:flex;"><span>  ---------  -----------------  --------------  -----
</span></span><span style="display:flex;"><span>  *.*        <span style="color:#f92672">[]</span>                 <span style="color:#f92672">[]</span>              <span style="color:#f92672">[</span>*<span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>             <span style="color:#f92672">[</span>*<span style="color:#f92672">]</span>                <span style="color:#f92672">[]</span>              <span style="color:#f92672">[</span>*<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>非资源类，如查看集群健康状态。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/how-kubectl-be-authorized.png" alt=""  />
</p>
<h6 id="rbac">RBAC<a hidden class="anchor" aria-hidden="true" href="#rbac">#</a></h6>
<p>Role-Based Access Control，基于角色的访问控制， apiserver启动参数添加&ndash;authorization-mode=RBAC 来启用RBAC认证模式，kubeadm安装的集群默认已开启。<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/">官方介绍</a></p>
<p>查看开启：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># master节点查看apiserver进程</span>
</span></span><span style="display:flex;"><span>$ ps aux |grep apiserver
</span></span></code></pre></div><p>RBAC模式引入了4个资源类型：</p>
<ul>
<li>
<p>Role，角色</p>
<p>一个Role只能授权访问单个namespace</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e">## 示例定义一个名为pod-reader的角色，该角色具有读取default这个命名空间下的pods的权限</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pod-reader</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>] <span style="color:#75715e"># &#34;&#34; indicates the core API group</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;pods&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## apiGroups: &#34;&#34;,&#34;apps&#34;, &#34;autoscaling&#34;, &#34;batch&#34;, kubectl api-versions</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## resources: &#34;services&#34;, &#34;pods&#34;,&#34;deployments&#34;... kubectl api-resources</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## verbs: &#34;get&#34;, &#34;list&#34;, &#34;watch&#34;, &#34;create&#34;, &#34;update&#34;, &#34;patch&#34;, &#34;delete&#34;, &#34;exec&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/</span>
</span></span></code></pre></div></li>
<li>
<p>ClusterRole</p>
<p>一个ClusterRole能够授予和Role一样的权限，但是它是集群范围内的。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e">## 定义一个集群角色，名为secret-reader，该角色可以读取所有的namespace中的secret资源</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># &#34;namespace&#34; omitted since ClusterRoles are not namespaced</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">secret-reader</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;secrets&#34;</span>]
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># User,Group,ServiceAccount</span>
</span></span></code></pre></div></li>
<li>
<p>Rolebinding</p>
<p>将role中定义的权限分配给用户和用户组。RoleBinding包含主题（users,groups,或service accounts）和授予角色的引用。对于namespace内的授权使用RoleBinding，集群范围内使用ClusterRoleBinding。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e">## 定义一个角色绑定，将pod-reader这个role的权限授予给jane这个User，使得jane可以在读取default这个命名空间下的所有的pod数据</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">read-pods</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User  </span> <span style="color:#75715e">#这里可以是User,Group,ServiceAccount</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">jane </span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span> <span style="color:#75715e">#这里可以是Role或者ClusterRole,若是ClusterRole，则权限也仅限于rolebinding的内部</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pod-reader</span> <span style="color:#75715e"># match the name of the Role or ClusterRole you wish to bind to</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span></code></pre></div><p><em>注意：rolebinding既可以绑定role，也可以绑定clusterrole，当绑定clusterrole的时候，subject的权限也会被限定于rolebinding定义的namespace内部，若想跨namespace，需要使用clusterrolebinding</em></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e">## 定义一个角色绑定，将dave这个用户和secret-reader这个集群角色绑定，虽然secret-reader是集群角色，但是因为是使用rolebinding绑定的，因此dave的权限也会被限制在development这个命名空间内</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This role binding allows &#34;dave&#34; to read secrets in the &#34;development&#34; namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># You need to already have a ClusterRole named &#34;secret-reader&#34;.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">read-secrets</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># The namespace of the RoleBinding determines where the permissions are granted.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># This only grants permissions within the &#34;development&#34; namespace.</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">development</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">User</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dave</span> <span style="color:#75715e"># Name is case sensitive</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dave</span> <span style="color:#75715e"># Name is case sensitive</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">luffy</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">secret-reader</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span></code></pre></div><p>考虑一个场景：  如果集群中有多个namespace分配给不同的管理员，每个namespace的权限是一样的，就可以只定义一个clusterrole，然后通过rolebinding将不同的namespace绑定到管理员身上，否则就需要每个namespace定义一个Role，然后做一次rolebinding。</p>
</li>
<li>
<p>ClusterRolebingding</p>
<p>允许跨namespace进行授权</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># This cluster role binding allows anyone in the &#34;manager&#34; group to read secrets in any namespace.</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">read-secrets-global</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Group</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">manager</span> <span style="color:#75715e"># Name is case sensitive</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">secret-reader</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span></code></pre></div></li>
</ul>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/rbac-2.jpg" alt=""  />
</p>
<h6 id="kubelet的认证授权">kubelet的认证授权<a hidden class="anchor" aria-hidden="true" href="#kubelet的认证授权">#</a></h6>
<p>查看kubelet进程</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ systemctl status kubelet
</span></span><span style="display:flex;"><span>● kubelet.service - kubelet: The Kubernetes Node Agent
</span></span><span style="display:flex;"><span>   Loaded: loaded <span style="color:#f92672">(</span>/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>  Drop-In: /usr/lib/systemd/system/kubelet.service.d
</span></span><span style="display:flex;"><span>           └─10-kubeadm.conf
</span></span><span style="display:flex;"><span>   Active: active <span style="color:#f92672">(</span>running<span style="color:#f92672">)</span> since Sun 2020-07-05 19:33:36 EDT; <span style="color:#ae81ff">1</span> day 12h ago
</span></span><span style="display:flex;"><span>     Docs: https://kubernetes.io/docs/
</span></span><span style="display:flex;"><span> Main PID: <span style="color:#ae81ff">10622</span> <span style="color:#f92672">(</span>kubelet<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Tasks: <span style="color:#ae81ff">24</span>
</span></span><span style="display:flex;"><span>   Memory: 60.5M
</span></span><span style="display:flex;"><span>   CGroup: /system.slice/kubelet.service
</span></span><span style="display:flex;"><span>           └─851 /usr/bin/kubelet --bootstrap-kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig<span style="color:#f92672">=</span>/etc/kubernetes/kubelet.conf
</span></span></code></pre></div><p>查看<code>/etc/kubernetes/kubelet.conf</code>，解析证书：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ echo xxxxx |base64 -d &gt;kubelet.crt
</span></span><span style="display:flex;"><span>$ openssl x509 -in kubelet.crt -text
</span></span><span style="display:flex;"><span>Certificate:
</span></span><span style="display:flex;"><span>    Data:
</span></span><span style="display:flex;"><span>        Version: <span style="color:#ae81ff">3</span> <span style="color:#f92672">(</span>0x2<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>        Serial Number: <span style="color:#ae81ff">9059794385454520113</span> <span style="color:#f92672">(</span>0x7dbadafe23185731<span style="color:#f92672">)</span>
</span></span><span style="display:flex;"><span>    Signature Algorithm: sha256WithRSAEncryption
</span></span><span style="display:flex;"><span>        Issuer: CN<span style="color:#f92672">=</span>kubernetes
</span></span><span style="display:flex;"><span>        Validity
</span></span><span style="display:flex;"><span>            Not Before: Feb <span style="color:#ae81ff">10</span> 07:33:39 <span style="color:#ae81ff">2020</span> GMT
</span></span><span style="display:flex;"><span>            Not After : Feb  <span style="color:#ae81ff">9</span> 07:33:40 <span style="color:#ae81ff">2021</span> GMT
</span></span><span style="display:flex;"><span>        Subject: O<span style="color:#f92672">=</span>system:nodes, CN<span style="color:#f92672">=</span>system:node:master-1
</span></span></code></pre></div><p>得到我们期望的内容：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>Subject: O<span style="color:#f92672">=</span>system:nodes, CN<span style="color:#f92672">=</span>system:node:k8s-master
</span></span></code></pre></div><p>我们知道，k8s会把O作为Group来进行请求，因此如果有权限绑定给这个组，肯定在clusterrolebinding的定义中可以找得到。因此尝试去找一下绑定了system:nodes组的clusterrolebinding</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl get clusterrolebinding|awk <span style="color:#e6db74">&#39;NR&gt;1{print $1}&#39;</span>|xargs kubectl get clusterrolebinding -oyaml|grep -n10 system:nodes
</span></span><span style="display:flex;"><span>98-  roleRef:
</span></span><span style="display:flex;"><span>99-    apiGroup: rbac.authorization.k8s.io
</span></span><span style="display:flex;"><span>100-    kind: ClusterRole
</span></span><span style="display:flex;"><span>101-    name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
</span></span><span style="display:flex;"><span>102-  subjects:
</span></span><span style="display:flex;"><span>103-  - apiGroup: rbac.authorization.k8s.io
</span></span><span style="display:flex;"><span>104-    kind: Group
</span></span><span style="display:flex;"><span>105:    name: system:nodes
</span></span><span style="display:flex;"><span>106-- apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style="display:flex;"><span>107-  kind: ClusterRoleBinding
</span></span><span style="display:flex;"><span>108-  metadata:
</span></span><span style="display:flex;"><span>109-    creationTimestamp: <span style="color:#e6db74">&#34;2020-02-10T07:34:02Z&#34;</span>
</span></span><span style="display:flex;"><span>110-    name: kubeadm:node-proxier
</span></span><span style="display:flex;"><span>111-    resourceVersion: <span style="color:#e6db74">&#34;213&#34;</span>
</span></span><span style="display:flex;"><span>112-    selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-proxier
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl describe clusterrole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
</span></span><span style="display:flex;"><span>Name:         system:certificates.k8s.io:certificatesigningrequests:selfnodeclient
</span></span><span style="display:flex;"><span>Labels:       kubernetes.io/bootstrapping<span style="color:#f92672">=</span>rbac-defaults
</span></span><span style="display:flex;"><span>Annotations:  rbac.authorization.kubernetes.io/autoupdate: true
</span></span><span style="display:flex;"><span>PolicyRule:
</span></span><span style="display:flex;"><span>  Resources                                                      Non-Resource URLs  Resource Names  Verbs
</span></span><span style="display:flex;"><span>  ---------                                                      -----------------  --------------  -----
</span></span><span style="display:flex;"><span>  certificatesigningrequests.certificates.k8s.io/selfnodeclient  <span style="color:#f92672">[]</span>                 <span style="color:#f92672">[]</span>              <span style="color:#f92672">[</span>create<span style="color:#f92672">]</span>
</span></span></code></pre></div><p>结局有点意外，除了<code>system:certificates.k8s.io:certificatesigningrequests:selfnodeclient</code>外，没有找到system相关的rolebindings，显然和我们的理解不一样。 尝试去找<a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#core-component-roles">资料</a>，发现了这么一段 :</p>
<table>
<thead>
<tr>
<th style="text-align:left">Default ClusterRole</th>
<th style="text-align:left">Default ClusterRoleBinding</th>
<th style="text-align:left">Description</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:left">system:kube-scheduler</td>
<td style="text-align:left">system:kube-scheduler user</td>
<td style="text-align:left">Allows access to the resources required by the <a href="https://kubernetes.io/docs/reference/generated/kube-scheduler/">scheduler</a>component.</td>
</tr>
<tr>
<td style="text-align:left">system:volume-scheduler</td>
<td style="text-align:left">system:kube-scheduler user</td>
<td style="text-align:left">Allows access to the volume resources required by the kube-scheduler component.</td>
</tr>
<tr>
<td style="text-align:left">system:kube-controller-manager</td>
<td style="text-align:left">system:kube-controller-manager user</td>
<td style="text-align:left">Allows access to the resources required by the <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-controller-manager/">controller manager</a> component. The permissions required by individual controllers are detailed in the <a href="https://kubernetes.io/docs/reference/access-authn-authz/rbac/#controller-roles">controller roles</a>.</td>
</tr>
<tr>
<td style="text-align:left">system:node</td>
<td style="text-align:left">None</td>
<td style="text-align:left">Allows access to resources required by the kubelet, <strong>including read access to all secrets, and write access to all pod status objects</strong>. You should use the <a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Node authorizer</a> and <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction admission plugin</a> instead of the <code>system:node</code> role, and allow granting API access to kubelets based on the Pods scheduled to run on them. The <code>system:node</code> role only exists for compatibility with Kubernetes clusters upgraded from versions prior to v1.8.</td>
</tr>
<tr>
<td style="text-align:left">system:node-proxier</td>
<td style="text-align:left">system:kube-proxy user</td>
<td style="text-align:left">Allows access to the resources required by the <a href="https://kubernetes.io/docs/reference/command-line-tools-reference/kube-proxy/">kube-proxy</a>component.</td>
</tr>
</tbody>
</table>
<p>大致意思是说：之前会定义system:node这个角色，目的是为了kubelet可以访问到必要的资源，包括所有secret的读权限及更新pod状态的写权限。如果1.8版本后，是建议使用 <a href="https://kubernetes.io/docs/reference/access-authn-authz/node/">Node authorizer</a> and <a href="https://kubernetes.io/docs/reference/access-authn-authz/admission-controllers/#noderestriction">NodeRestriction admission plugin</a> 来代替这个角色的。</p>
<p>我们目前使用1.16，查看一下授权策略：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ ps axu|grep apiserver
</span></span><span style="display:flex;"><span>kube-apiserver --authorization-mode<span style="color:#f92672">=</span>Node,RBAC  --enable-admission-plugins<span style="color:#f92672">=</span>NodeRestriction
</span></span></code></pre></div><p>查看一下官网对Node authorizer的介绍：</p>
<p><em>Node authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.</em></p>
<p><em>In future releases, the node authorizer may add or remove permissions to ensure kubelets have the minimal set of permissions required to operate correctly.</em></p>
<p><em>In order to be authorized by the Node authorizer, kubelets must use a credential that identifies them as being in the <code>system:nodes</code> group, with a username of <code>system:node:&lt;nodeName&gt;</code></em></p>
<h6 id="service-account及k8s-api调用">Service Account及K8S Api调用<a hidden class="anchor" aria-hidden="true" href="#service-account及k8s-api调用">#</a></h6>
<p>前面说，认证可以通过证书，也可以通过使用ServiceAccount（服务账户）的方式来做认证。大多数时候，我们在基于k8s做二次开发时都是选择通过ServiceAccount + RBAC 的方式。我们之前访问dashboard的时候，是如何做的？</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#75715e">## 新建一个名为admin的serviceaccount，并且把名为cluster-admin的这个集群角色的权限授予新建的</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#serviceaccount</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">admin</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kubernetes-dashboard</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1beta1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">admin</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">annotations</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">rbac.authorization.kubernetes.io/autoupdate</span>: <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cluster-admin</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>- <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">admin</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">kubernetes-dashboard</span>
</span></span></code></pre></div><p>我们查看一下：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl -n kubernetes-dashboard get sa admin -o yaml
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: ServiceAccount
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  creationTimestamp: <span style="color:#e6db74">&#34;2020-04-01T11:59:21Z&#34;</span>
</span></span><span style="display:flex;"><span>  name: admin
</span></span><span style="display:flex;"><span>  namespace: kubernetes-dashboard
</span></span><span style="display:flex;"><span>  resourceVersion: <span style="color:#e6db74">&#34;1988878&#34;</span>
</span></span><span style="display:flex;"><span>  selfLink: /api/v1/namespaces/kubernetes-dashboard/serviceaccounts/admin
</span></span><span style="display:flex;"><span>  uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f
</span></span><span style="display:flex;"><span>secrets:
</span></span><span style="display:flex;"><span>- name: admin-token-lfsrf
</span></span></code></pre></div><p>注意到serviceaccount上默认绑定了一个名为admin-token-lfsrf的secret，我们查看一下secret</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl -n kubernetes-dashboard describe secret admin-token-lfsrf
</span></span><span style="display:flex;"><span>Name:         admin-token-lfsrf
</span></span><span style="display:flex;"><span>Namespace:    kubernetes-dashboard
</span></span><span style="display:flex;"><span>Labels:       &lt;none&gt;
</span></span><span style="display:flex;"><span>Annotations:  kubernetes.io/service-account.name: admin
</span></span><span style="display:flex;"><span>              kubernetes.io/service-account.uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Type:  kubernetes.io/service-account-token
</span></span><span style="display:flex;"><span>Data
</span></span><span style="display:flex;"><span><span style="color:#f92672">====</span>
</span></span><span style="display:flex;"><span>ca.crt:     <span style="color:#ae81ff">1025</span> bytes
</span></span><span style="display:flex;"><span>namespace:  <span style="color:#ae81ff">4</span> bytes
</span></span><span style="display:flex;"><span>token:      eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLWxmc3JmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjM5ZWNjM2UtNzRkOS0xMWVhLWE1OWItMDAwYzI5ZGZkNzNmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlbW86YWRtaW4ifQ.ffGCU4L5LxTsMx3NcNixpjT6nLBi-pmstb4I-W61nLOzNaMmYSEIwAaugKMzNR-2VwM14WbuG04dOeO67niJeP6n8-ALkl-vineoYCsUjrzJ09qpM3TNUPatHFqyjcqJ87h4VKZEqk2qCCmLxB6AGbEHpVFkoge40vHs56cIymFGZLe53JZkhu3pwYuS4jpXytV30Ad-HwmQDUu_Xqcifni6tDYPCfKz2CZlcOfwqHeGIHJjDGVBKqhEeo8PhStoofBU6Y4OjObP7HGuTY-Foo4QindNnpp0QU6vSb7kiOiQ4twpayybH8PTf73dtdFt46UF6mGjskWgevgolvmO8A
</span></span></code></pre></div><p>演示role的权限：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat test-sa.yaml
</span></span><span style="display:flex;"><span>serviceaccount
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: ServiceAccount
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: test
</span></span><span style="display:flex;"><span>  namespace: kubernetes-dashboard
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>kind: ClusterRoleBinding
</span></span><span style="display:flex;"><span>apiVersion: rbac.authorization.k8s.io/v1beta1
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: test
</span></span><span style="display:flex;"><span>  annotations:
</span></span><span style="display:flex;"><span>    rbac.authorization.kubernetes.io/autoupdate: <span style="color:#e6db74">&#34;true&#34;</span>
</span></span><span style="display:flex;"><span>roleRef:
</span></span><span style="display:flex;"><span>  kind: ClusterRole
</span></span><span style="display:flex;"><span>  name: cluster-admin
</span></span><span style="display:flex;"><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style="display:flex;"><span>subjects:
</span></span><span style="display:flex;"><span>- kind: ServiceAccount
</span></span><span style="display:flex;"><span>  name: test
</span></span><span style="display:flex;"><span>  namespace: kubernetes-dashboard
</span></span></code></pre></div><p>curl演示</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ curl -k  -H <span style="color:#e6db74">&#34;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InhXcmtaSG5ZODF1TVJ6dUcycnRLT2c4U3ZncVdoVjlLaVRxNG1wZ0pqVmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1xNXBueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImViZDg2ODZjLWZkYzAtNDRlZC04NmZlLTY5ZmE0ZTE1YjBmMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbiJ9.iEIVMWg2mHPD88GQ2i4uc_60K4o17e39tN0VI_Q_s3TrRS8hmpi0pkEaN88igEKZm95Qf1qcN9J5W5eqOmcK2SN83Dd9dyGAGxuNAdEwi0i73weFHHsjDqokl9_4RGbHT5lRY46BbIGADIphcTeVbCggI6T_V9zBbtl8dcmsd-lD_6c6uC2INtPyIfz1FplynkjEVLapp_45aXZ9IMy76ljNSA8Uc061Uys6PD3IXsUD5JJfdm7lAt0F7rn9SdX1q10F2lIHYCMcCcfEpLr4Vkymxb4IU4RCR8BsMOPIO_yfRVeYZkG4gU2C47KwxpLsJRrTUcUXJktSEPdeYYXf9w&#34;</span> https://192.168.136.10:6443/api/v1/namespaces/luffy/pods?limit<span style="color:#f92672">=</span><span style="color:#ae81ff">500</span>
</span></span></code></pre></div><h4 id="通过hpa实现业务应用的动态扩缩容">通过HPA实现业务应用的动态扩缩容<a hidden class="anchor" aria-hidden="true" href="#通过hpa实现业务应用的动态扩缩容">#</a></h4>
<h5 id="hpa控制器介绍">HPA控制器介绍<a hidden class="anchor" aria-hidden="true" href="#hpa控制器介绍">#</a></h5>
<p>当系统资源过高的时候，我们可以使用如下命令来实现 Pod 的扩缩容功能</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl -n luffy scale deployment myblog --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>
</span></span></code></pre></div><p>但是这个过程是手动操作的。在实际项目中，我们需要做到是的是一个自动化感知并自动扩容的操作。Kubernetes 也为提供了这样的一个资源对象：Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称<a href="https://v1-14.docs.kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/">HPA</a></p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/hpa.png" alt=""  />
</p>
<p>基本原理：HPA 通过监控分析控制器控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量</p>
<p>HPA的实现有两个版本：</p>
<ul>
<li>autoscaling/v1，只包含了根据CPU指标的检测，稳定版本</li>
<li>autoscaling/v2beta1，支持根据memory或者用户自定义指标进行伸缩</li>
</ul>
<p>如何获取Pod的监控数据？</p>
<ul>
<li>k8s 1.8以下：使用heapster，1.11版本完全废弃</li>
<li>k8s 1.8以上：使用metric-server</li>
</ul>
<p>思考：为什么之前用 heapster ，现在废弃了项目，改用 metric-server ？</p>
<p>heapster时代，apiserver 会直接将metric请求通过apiserver proxy 的方式转发给集群内的 hepaster 服务，采用这种 proxy 方式是有问题的：</p>
<ul>
<li>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/service_name<span style="color:#f92672">[</span>:port_name<span style="color:#f92672">]</span>/proxy
</span></span></code></pre></div></li>
<li>
<p>proxy只是代理请求，一般用于问题排查，不够稳定，且版本不可控</p>
</li>
<li>
<p>heapster的接口不能像apiserver一样有完整的鉴权以及client集成</p>
</li>
<li>
<p>pod 的监控数据是核心指标（HPA调度），应该和 pod 本身拥有同等地位，即 metric应该作为一种资源存在，如metrics.k8s.io 的形式，称之为 Metric Api</p>
</li>
</ul>
<p>于是官方从 1.8 版本开始逐步废弃 heapster，并提出了上边 Metric api 的概念，而 metrics-server 就是这种概念下官方的一种实现，用于从 kubelet获取指标，替换掉之前的 heapster。</p>
<p><code>Metrics Server</code> 可以通过标准的 Kubernetes API 把监控数据暴露出来，比如获取某一Pod的监控数据：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>https://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># https://192.168.136.10:6443/api/v1/namespaces/luffy/pods?limit=500</span>
</span></span></code></pre></div><p>目前的采集流程：</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/k8s-hpa-ms.png" alt=""  />
</p>
<h5 id="metric-server">Metric Server<a hidden class="anchor" aria-hidden="true" href="#metric-server">#</a></h5>
<p><a href="https://v1-14.docs.kubernetes.io/docs/tasks/debug-application-cluster/resource-metrics-pipeline/#metrics-server">官方介绍</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>Metric server collects metrics from the Summary API, exposed by Kubelet on each node.
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>Metrics Server registered in the main API server through Kubernetes aggregator, which was introduced in Kubernetes 1.7
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><h6 id="安装">安装<a hidden class="anchor" aria-hidden="true" href="#安装">#</a></h6>
<p>官方代码仓库地址：https://github.com/kubernetes-sigs/metrics-server</p>
<p>Depending on your cluster setup, you may also need to change flags passed to the Metrics Server container. Most useful flags:</p>
<ul>
<li><code>--kubelet-preferred-address-types</code> - The priority of node address types used when determining an address for connecting to a particular node (default [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP])</li>
<li><code>--kubelet-insecure-tls</code> - Do not verify the CA of serving certificates presented by Kubelets. For testing purposes only.</li>
<li><code>--requestheader-client-ca-file</code> - Specify a root certificate bundle for verifying client certificates on incoming requests.</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml
</span></span></code></pre></div><p>修改args参数：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">84</span>       containers:
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">85</span>       - name: metrics-server
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">86</span>         image: registry.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">87</span>         imagePullPolicy: IfNotPresent
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">88</span>         args:
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">89</span>           - --cert-dir<span style="color:#f92672">=</span>/tmp
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">90</span>           - --secure-port<span style="color:#f92672">=</span><span style="color:#ae81ff">4443</span>
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">91</span>           - --kubelet-insecure-tls
</span></span><span style="display:flex;"><span> <span style="color:#ae81ff">92</span>           - --kubelet-preferred-address-types<span style="color:#f92672">=</span>InternalIP
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><p>执行安装：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl create -f components.yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl -n kube-system get pods
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl top nodes
</span></span></code></pre></div><h6 id="kubelet的指标采集">kubelet的指标采集<a hidden class="anchor" aria-hidden="true" href="#kubelet的指标采集">#</a></h6>
<p>无论是 heapster还是 metric-server，都只是数据的中转和聚合，两者都是调用的 kubelet 的 api 接口获取的数据，而 kubelet 代码中实际采集指标的是 cadvisor 模块，你可以在 node 节点访问 10250 端口获取监控数据：</p>
<ul>
<li>Kubelet Summary metrics:  https://127.0.0.1:10250/metrics，暴露 node、pod 汇总数据</li>
<li>Cadvisor metrics: https://127.0.0.1:10250/metrics/cadvisor，暴露 container 维度数据</li>
</ul>
<p>调用示例：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ curl -k  -H <span style="color:#e6db74">&#34;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InhXcmtaSG5ZODF1TVJ6dUcycnRLT2c4U3ZncVdoVjlLaVRxNG1wZ0pqVmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1xNXBueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImViZDg2ODZjLWZkYzAtNDRlZC04NmZlLTY5ZmE0ZTE1YjBmMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbiJ9.iEIVMWg2mHPD88GQ2i4uc_60K4o17e39tN0VI_Q_s3TrRS8hmpi0pkEaN88igEKZm95Qf1qcN9J5W5eqOmcK2SN83Dd9dyGAGxuNAdEwi0i73weFHHsjDqokl9_4RGbHT5lRY46BbIGADIphcTeVbCggI6T_V9zBbtl8dcmsd-lD_6c6uC2INtPyIfz1FplynkjEVLapp_45aXZ9IMy76ljNSA8Uc061Uys6PD3IXsUD5JJfdm7lAt0F7rn9SdX1q10F2lIHYCMcCcfEpLr4Vkymxb4IU4RCR8BsMOPIO_yfRVeYZkG4gU2C47KwxpLsJRrTUcUXJktSEPdeYYXf9w&#34;</span> https://localhost:10250/metrics
</span></span></code></pre></div><p>kubelet虽然提供了 metric 接口，但实际监控逻辑由内置的cAdvisor模块负责，早期的时候，cadvisor是单独的组件，从k8s 1.12开始，cadvisor 监听的端口在k8s中被删除，所有监控数据统一由Kubelet的API提供。</p>
<p>cadvisor获取指标时实际调用的是 runc/libcontainer库，而libcontainer是对 cgroup文件 的封装，即 cadvsior也只是个转发者，它的数据来自于cgroup文件。</p>
<p>cgroup文件中的值是监控数据的最终来源，如</p>
<ul>
<li>
<p>mem usage的值，</p>
<ul>
<li>
<p>对于docker容器来讲，来源于<code>/sys/fs/cgroup/memory/docker/[containerId]/memory.usage_in_bytes</code></p>
</li>
<li>
<p>对于pod来讲，<code>/sys/fs/cgroup/memory/kubepods/besteffort/pod[podId]/memory.usage_in_bytes</code>或者</p>
<p><code>/sys/fs/cgroup/memory/kubepods/burstable/pod[podId]/memory.usage_in_bytes</code></p>
</li>
</ul>
</li>
<li>
<p>如果没限制内存，Limit = machine_mem，否则来自于 <code>/sys/fs/cgroup/memory/docker/[id]/memory.limit_in_bytes</code></p>
</li>
<li>
<p>内存使用率 = memory.usage_in_bytes/memory.limit_in_bytes</p>
</li>
</ul>
<p>Metrics数据流：</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/hap-flow.webp" alt=""  />
</p>
<p>思考：</p>
<p>Metrics Server是独立的一个服务，只能服务内部实现自己的api，是如何做到通过标准的kubernetes 的API格式暴露出去的？</p>
<p><a href="https://github.com/kubernetes/kube-aggregator">kube-aggregator</a></p>
<h6 id="kube-aggregator聚合器及metric-server的实现">kube-aggregator聚合器及Metric-Server的实现<a hidden class="anchor" aria-hidden="true" href="#kube-aggregator聚合器及metric-server的实现">#</a></h6>
<p>kube-aggregator是对 apiserver 的api的一种拓展机制，它允许开发人员编写一个自己的服务，并把这个服务注册到k8s的api里面，即扩展 API 。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/kube-aggregation.webp" alt=""  />
</p>
<p>定义一个APIService对象：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apiregistration.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">APIService</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">v1beta1.luffy.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">group</span>: <span style="color:#ae81ff">luffy.k8s.io</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">groupPriorityMinimum</span>: <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">insecureSkipTLSVerify</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">service</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">service-A      </span> <span style="color:#75715e"># 必须https访问</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">luffy</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">port</span>: <span style="color:#ae81ff">443</span>   
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">version</span>: <span style="color:#ae81ff">v1beta1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">versionPriority</span>: <span style="color:#ae81ff">100</span>
</span></span></code></pre></div><p>k8s会自动帮我们代理如下url的请求：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>proxyPath :<span style="color:#f92672">=</span> <span style="color:#e6db74">&#34;/apis/&#34;</span> + apiService.Spec.Group + <span style="color:#e6db74">&#34;/&#34;</span> + apiService.Spec.Version
</span></span></code></pre></div><p>即：https://192.168.136.10:6443/apis/luffy.k8s.io/v1beta1/xxxx转到我们的service-A服务中，service-A中只需要实现 <code>https://service-A/luffy.k8s.io/v1beta1/xxxx</code> 即可。</p>
<p>看下metric-server的实现：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl get apiservice 
</span></span><span style="display:flex;"><span>NAME                       SERVICE                      AVAILABLE                      
</span></span><span style="display:flex;"><span>v1beta1.metrics.k8s.io   kube-system/metrics-server		True
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl get apiservice v1beta1.metrics.k8s.io -oyaml
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  group: metrics.k8s.io
</span></span><span style="display:flex;"><span>  groupPriorityMinimum: <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>  insecureSkipTLSVerify: true
</span></span><span style="display:flex;"><span>  service:
</span></span><span style="display:flex;"><span>    name: metrics-server
</span></span><span style="display:flex;"><span>    namespace: kube-system
</span></span><span style="display:flex;"><span>    port: <span style="color:#ae81ff">443</span>
</span></span><span style="display:flex;"><span>  version: v1beta1
</span></span><span style="display:flex;"><span>  versionPriority: <span style="color:#ae81ff">100</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl -n kube-system get svc metrics-server
</span></span><span style="display:flex;"><span>NAME             TYPE        CLUSTER-IP       EXTERNAL-IP   PORT<span style="color:#f92672">(</span>S<span style="color:#f92672">)</span>   AGE
</span></span><span style="display:flex;"><span>metrics-server   ClusterIP   10.110.111.146   &lt;none&gt;        443/TCP   11h
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ curl -k  -H <span style="color:#e6db74">&#34;Authorization: Bearer xxxx&#34;</span> https://10.110.111.146
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;paths&#34;</span>: <span style="color:#f92672">[</span>
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/apis&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/apis/metrics.k8s.io&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/apis/metrics.k8s.io/v1beta1&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/healthz&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/healthz/healthz&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/healthz/log&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/healthz/ping&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/healthz/poststarthook/generic-apiserver-start-informers&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/metrics&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/openapi/v2&#34;</span>,
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;/version&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># https://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces/&lt;namespace-name&gt;/pods/&lt;pod-name&gt;</span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># </span>
</span></span><span style="display:flex;"><span>$ curl -k  -H <span style="color:#e6db74">&#34;Authorization: Bearer xxxx&#34;</span> https://10.110.111.146/apis/metrics.k8s.io/v1beta1/namespaces/luffy/pods/myblog-5d9ff54d4b-4rftt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ curl -k  -H <span style="color:#e6db74">&#34;Authorization: Bearer xxxx&#34;</span> https://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces/luffy/pods/myblog-5d9ff54d4b-4rftt
</span></span></code></pre></div><h5 id="hpa实践">HPA实践<a hidden class="anchor" aria-hidden="true" href="#hpa实践">#</a></h5>
<h6 id="基于cpu的动态伸缩">基于CPU的动态伸缩<a hidden class="anchor" aria-hidden="true" href="#基于cpu的动态伸缩">#</a></h6>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/hpa.png" alt=""  />
</p>
<p>创建hpa对象：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 方式一</span>
</span></span><span style="display:flex;"><span>$ cat hpa-myblog.yaml
</span></span><span style="display:flex;"><span>apiVersion: autoscaling/v1
</span></span><span style="display:flex;"><span>kind: HorizontalPodAutoscaler
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: hpa-myblog-cpu
</span></span><span style="display:flex;"><span>  namespace: luffy
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  maxReplicas: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  minReplicas: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  scaleTargetRef:
</span></span><span style="display:flex;"><span>    apiVersion: apps/v1
</span></span><span style="display:flex;"><span>    kind: Deployment
</span></span><span style="display:flex;"><span>    name: myblog
</span></span><span style="display:flex;"><span>  targetCPUUtilizationPercentage: <span style="color:#ae81ff">10</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 方式二</span>
</span></span><span style="display:flex;"><span>$ kubectl -n luffy autoscale deployment myblog --cpu-percent<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span> --min<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span> --max<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>
</span></span></code></pre></div><blockquote>
<p>Deployment对象必须配置requests的参数，不然无法获取监控数据，也无法通过HPA进行动态伸缩</p>
</blockquote>
<p>验证：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ yum -y install httpd-tools
</span></span><span style="display:flex;"><span>$ kubectl -n luffy get svc myblog
</span></span><span style="display:flex;"><span>myblog   ClusterIP   10.104.245.225   &lt;none&gt;        80/TCP    6d18h
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 为了更快看到效果，先调整副本数为1</span>
</span></span><span style="display:flex;"><span>$ kubectl -n luffy scale deploy myblog --replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 模拟1000个用户并发访问页面10万次</span>
</span></span><span style="display:flex;"><span>$ ab -n <span style="color:#ae81ff">100000</span> -c <span style="color:#ae81ff">1000</span> http://10.104.245.225/blog/index/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl get hpa
</span></span><span style="display:flex;"><span>$ kubectl -n luffy get pods
</span></span></code></pre></div><p>压力降下来后，会有默认5分钟的scaledown的时间，可以通过controller-manager的如下参数设置：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>--horizontal-pod-autoscaler-downscale-stabilization
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>The value <span style="color:#66d9ef">for</span> this option is a duration that specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed. The default value is <span style="color:#ae81ff">5</span> minutes <span style="color:#f92672">(</span>5m0s<span style="color:#f92672">)</span>.
</span></span></code></pre></div><p>是一个逐步的过程，当前的缩放完成后，下次缩放的时间间隔，比如从3个副本降低到1个副本，中间大概会等待2*5min = 10分钟</p>
<h6 id="基于内存的动态伸缩">基于内存的动态伸缩<a hidden class="anchor" aria-hidden="true" href="#基于内存的动态伸缩">#</a></h6>
<p>创建hpa对象</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat hpa-demo-mem.yaml
</span></span><span style="display:flex;"><span>apiVersion: autoscaling/v2beta1
</span></span><span style="display:flex;"><span>kind: HorizontalPodAutoscaler
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: hpa-demo-mem
</span></span><span style="display:flex;"><span>  namespace: luffy
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  scaleTargetRef:
</span></span><span style="display:flex;"><span>    apiVersion: apps/v1
</span></span><span style="display:flex;"><span>    kind: Deployment
</span></span><span style="display:flex;"><span>    name: hpa-demo-mem
</span></span><span style="display:flex;"><span>  minReplicas: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  maxReplicas: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  metrics:
</span></span><span style="display:flex;"><span>  - type: Resource
</span></span><span style="display:flex;"><span>    resource:
</span></span><span style="display:flex;"><span>      name: memory
</span></span><span style="display:flex;"><span>      targetAverageUtilization: <span style="color:#ae81ff">30</span>
</span></span></code></pre></div><p>加压演示脚本：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat increase-mem-config.yaml
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: ConfigMap
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: increase-mem-config
</span></span><span style="display:flex;"><span>  namespace: luffy
</span></span><span style="display:flex;"><span>data:
</span></span><span style="display:flex;"><span>  increase-mem.sh: |
</span></span><span style="display:flex;"><span>    <span style="color:#75715e">#!/bin/bash  </span>
</span></span><span style="display:flex;"><span>    mkdir /tmp/memory  
</span></span><span style="display:flex;"><span>    mount -t tmpfs -o size<span style="color:#f92672">=</span>40M tmpfs /tmp/memory  
</span></span><span style="display:flex;"><span>    dd <span style="color:#66d9ef">if</span><span style="color:#f92672">=</span>/dev/zero of<span style="color:#f92672">=</span>/tmp/memory/block  
</span></span><span style="display:flex;"><span>    sleep <span style="color:#ae81ff">60</span> 
</span></span><span style="display:flex;"><span>    rm /tmp/memory/block  
</span></span><span style="display:flex;"><span>    umount /tmp/memory  
</span></span><span style="display:flex;"><span>    rmdir /tmp/memory
</span></span></code></pre></div><p>测试deployment：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat hpa-demo-mem-deploy.yaml
</span></span><span style="display:flex;"><span>apiVersion: apps/v1
</span></span><span style="display:flex;"><span>kind: Deployment
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: hpa-demo-mem
</span></span><span style="display:flex;"><span>  namespace: luffy
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  selector:
</span></span><span style="display:flex;"><span>    matchLabels:
</span></span><span style="display:flex;"><span>      app: nginx
</span></span><span style="display:flex;"><span>  template:
</span></span><span style="display:flex;"><span>    metadata:
</span></span><span style="display:flex;"><span>      labels:
</span></span><span style="display:flex;"><span>        app: nginx
</span></span><span style="display:flex;"><span>    spec:
</span></span><span style="display:flex;"><span>      volumes:
</span></span><span style="display:flex;"><span>      - name: increase-mem-script
</span></span><span style="display:flex;"><span>        configMap:
</span></span><span style="display:flex;"><span>          name: increase-mem-config
</span></span><span style="display:flex;"><span>      containers:
</span></span><span style="display:flex;"><span>      - name: nginx
</span></span><span style="display:flex;"><span>        image: nginx:alpine
</span></span><span style="display:flex;"><span>        ports:
</span></span><span style="display:flex;"><span>        - containerPort: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>        volumeMounts:
</span></span><span style="display:flex;"><span>        - name: increase-mem-script
</span></span><span style="display:flex;"><span>          mountPath: /etc/script
</span></span><span style="display:flex;"><span>        resources:
</span></span><span style="display:flex;"><span>          requests:
</span></span><span style="display:flex;"><span>            memory: 50Mi
</span></span><span style="display:flex;"><span>            cpu: 50m
</span></span><span style="display:flex;"><span>        securityContext:
</span></span><span style="display:flex;"><span>          privileged: true
</span></span></code></pre></div><p>测试：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl create -f increase-mem-config.yaml
</span></span><span style="display:flex;"><span>$ kubectl create -f hpa-demo-mem.yaml
</span></span><span style="display:flex;"><span>$ kubectl create -f hpa-demo-mem-deploy.yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl -n luffy exec -ti hpa-demo-mem-7fc75bf5c8-xx424 sh
</span></span><span style="display:flex;"><span><span style="color:#75715e">#/ sh /etc/script/increase-mem.sh</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 观察hpa及pod</span>
</span></span><span style="display:flex;"><span>$ kubectl -n luffy get hpa
</span></span><span style="display:flex;"><span>$ kubectl -n luffy get po
</span></span></code></pre></div><h6 id="基于自定义指标的动态伸缩">基于自定义指标的动态伸缩<a hidden class="anchor" aria-hidden="true" href="#基于自定义指标的动态伸缩">#</a></h6>
<p>除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使用 <code>Prometheus Adapter</code>，Prometheus 用于监控应用的负载和集群本身的各种指标，<code>Prometheus Adapter</code> 可以帮我们使用 Prometheus 收集的指标并使用它们来制定扩展策略，这些指标都是通过 APIServer 暴露的，而且 HPA 资源对象也可以很轻易的直接使用。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/custom-hpa.webp" alt=""  />
</p>
<p>架构图：</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/hpa-prometheus-custom.png" alt=""  />
</p>
<h4 id="kubernetes对接分部式存储">kubernetes对接分部式存储<a hidden class="anchor" aria-hidden="true" href="#kubernetes对接分部式存储">#</a></h4>
<h5 id="pv与pvc快速入门">PV与PVC快速入门<a hidden class="anchor" aria-hidden="true" href="#pv与pvc快速入门">#</a></h5>
<p>k8s存储的目的就是保证Pod重建后，数据不丢失。简单的数据持久化的下述方式：</p>
<ul>
<li>
<p>emptyDir</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-pd</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">image</span>: <span style="color:#ae81ff">k8s.gcr.io/test-webserver</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">webserver</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/cache</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cache-volume</span>
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">image</span>: <span style="color:#ae81ff">k8s.gcr.io/test-redis</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">redis</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/data</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cache-volume</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cache-volume</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">emptyDir</span>: {}
</span></span></code></pre></div><ul>
<li>Pod内的容器共享卷的数据</li>
<li>存在于Pod的生命周期，Pod销毁，数据丢失</li>
<li>Pod内的容器自动重建后，数据不会丢失</li>
</ul>
</li>
<li>
<p>hostPath</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Pod</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-pd</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">image</span>: <span style="color:#ae81ff">k8s.gcr.io/test-webserver</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-container</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/test-pd</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-volume</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-volume</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">hostPath</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># directory location on host</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/data</span>
</span></span><span style="display:flex;"><span>      <span style="color:#75715e"># this field is optional</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Directory</span>
</span></span></code></pre></div><p>通常配合nodeSelector使用</p>
</li>
<li>
<p>nfs存储</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">redisdata            </span> <span style="color:#75715e">#卷名称</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">nfs</span>:                        <span style="color:#75715e">#使用NFS网络存储卷</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">server</span>: <span style="color:#ae81ff">192.168.31.241</span>    <span style="color:#75715e">#NFS服务器地址</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/data/redis        </span> <span style="color:#75715e">#NFS服务器共享的目录</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">readOnly</span>: <span style="color:#66d9ef">false</span>           <span style="color:#75715e">#是否为只读</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div></li>
</ul>
<p>volume支持的种类众多（参考 <a href="https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes">https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes</a> ），每种对应不同的存储后端实现，因此为了屏蔽后端存储的细节，同时使得Pod在使用存储的时候更加简洁和规范，k8s引入了两个新的资源类型，PV和PVC。</p>
<p>PersistentVolume（持久化卷），是对底层的存储的一种抽象，它和具体的底层的共享存储技术的实现方式有关，比如 Ceph、GlusterFS、NFS 等，都是通过插件机制完成与共享存储的对接。如使用PV对接NFS存储：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolume</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-pv</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">capacity</span>: 
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">1Gi</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">persistentVolumeReclaimPolicy</span>: <span style="color:#ae81ff">Retain</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">nfs</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/data/k8s</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">server</span>: <span style="color:#ae81ff">121.204.157.52</span>
</span></span></code></pre></div><ul>
<li>capacity，存储能力， 目前只支持存储空间的设置， 就是我们这里的 storage=1Gi，不过未来可能会加入 IOPS、吞吐量等指标的配置。</li>
<li>accessModes，访问模式， 是用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：
<ul>
<li>ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载</li>
<li>ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载</li>
<li>ReadWriteMany（RWX）：读写权限，可以被多个节点挂载</li>
</ul>
</li>
</ul>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/pv-access-mode.webp" alt=""  />
</p>
<ul>
<li>persistentVolumeReclaimPolicy，pv的回收策略, 目前只有 NFS 和 HostPath 两种类型支持回收策略
<ul>
<li>Retain（保留）- 保留数据，需要管理员手工清理数据</li>
<li>Recycle（回收）- 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/*</li>
<li>Delete（删除）- 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务，比如 ASW EBS。</li>
</ul>
</li>
</ul>
<p>因为PV是直接对接底层存储的，就像集群中的Node可以为Pod提供计算资源（CPU和内存）一样，PV可以为Pod提供存储资源。因此PV不是namespaced的资源，属于集群层面可用的资源。Pod如果想使用该PV，需要通过创建PVC挂载到Pod中。</p>
<p>PVC全写是PersistentVolumeClaim（持久化卷声明），PVC 是用户存储的一种声明，创建完成后，可以和PV实现一对一绑定。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">pvc-nfs</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#ae81ff">ReadWriteOnce</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">1Gi</span>
</span></span></code></pre></div><p>然后Pod中通过如下方式去使用：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nginx</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: <span style="color:#ae81ff">nginx:alpine</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">imagePullPolicy</span>: <span style="color:#ae81ff">IfNotPresent</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">ports</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">containerPort</span>: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">name</span>: <span style="color:#ae81ff">web</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:                        <span style="color:#75715e">#挂载容器中的目录到pvc nfs中的目录</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">www</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/usr/share/nginx/html</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">www</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">persistentVolumeClaim</span>:              <span style="color:#75715e">#指定pvc</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">claimName</span>: <span style="color:#ae81ff">pvc-nfs</span>
</span></span><span style="display:flex;"><span>...
</span></span></code></pre></div><h5 id="pv与pvc管理nfs存储卷实践">PV与PVC管理NFS存储卷实践<a hidden class="anchor" aria-hidden="true" href="#pv与pvc管理nfs存储卷实践">#</a></h5>
<h6 id="环境准备">环境准备<a hidden class="anchor" aria-hidden="true" href="#环境准备">#</a></h6>
<p>服务端：121.204.157.52</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ yum -y install nfs-utils rpcbind
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 共享目录</span>
</span></span><span style="display:flex;"><span>$ mkdir -p /data/k8s <span style="color:#f92672">&amp;&amp;</span> chmod <span style="color:#ae81ff">755</span> /data/k8s
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ echo <span style="color:#e6db74">&#39;/data/k8s  *(insecure,rw,sync,no_root_squash)&#39;</span>&gt;&gt;/etc/exports
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ systemctl enable rpcbind <span style="color:#f92672">&amp;&amp;</span> systemctl start rpcbind
</span></span><span style="display:flex;"><span>$ systemctl enable nfs <span style="color:#f92672">&amp;&amp;</span> systemctl start nfs
</span></span></code></pre></div><p>客户端：k8s集群slave节点</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ yum -y install nfs-utils rpcbind
</span></span><span style="display:flex;"><span>$ mkdir /nfsdata
</span></span><span style="display:flex;"><span>$ mount -t nfs 121.204.157.52:/data/k8s /nfsdata
</span></span></code></pre></div><h6 id="pv与pvc演示">PV与PVC演示<a hidden class="anchor" aria-hidden="true" href="#pv与pvc演示">#</a></h6>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat pv-nfs.yaml
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: PersistentVolume
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: nfs-pv
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  capacity: 
</span></span><span style="display:flex;"><span>    storage: 1Gi
</span></span><span style="display:flex;"><span>  accessModes:
</span></span><span style="display:flex;"><span>  - ReadWriteOnce
</span></span><span style="display:flex;"><span>  persistentVolumeReclaimPolicy: Retain
</span></span><span style="display:flex;"><span>  nfs:
</span></span><span style="display:flex;"><span>    path: /data/k8s
</span></span><span style="display:flex;"><span>    server: 121.204.157.52
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl create -f pv-nfs.yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl get pv
</span></span><span style="display:flex;"><span>NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS      CLAIM   STORAGECLASS  
</span></span><span style="display:flex;"><span>nfs-pv   1Gi        RWO            Retain           Available
</span></span></code></pre></div><p>一个 PV 的生命周期中，可能会处于4中不同的阶段：</p>
<ul>
<li>Available（可用）：表示可用状态，还未被任何 PVC 绑定</li>
<li>Bound（已绑定）：表示 PV 已经被 PVC 绑定</li>
<li>Released（已释放）：PVC 被删除，但是资源还未被集群重新声明</li>
<li>Failed（失败）： 表示该 PV 的自动回收失败</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat pvc.yaml
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: PersistentVolumeClaim
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: pvc-nfs
</span></span><span style="display:flex;"><span>  namespace: default
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  accessModes:
</span></span><span style="display:flex;"><span>  - ReadWriteOnce
</span></span><span style="display:flex;"><span>  resources:
</span></span><span style="display:flex;"><span>    requests:
</span></span><span style="display:flex;"><span>      storage: 1Gi
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl create -f pvc.yaml
</span></span><span style="display:flex;"><span>$ kubectl get pvc
</span></span><span style="display:flex;"><span>NAME      STATUS   VOLUME   CAPACITY   ACCESS MODES   STORAGECLASS   AGE
</span></span><span style="display:flex;"><span>pvc-nfs   Bound    nfs-pv   1Gi        RWO                           3s
</span></span><span style="display:flex;"><span>$ kubectl get pv
</span></span><span style="display:flex;"><span>NAME     CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM             
</span></span><span style="display:flex;"><span>nfs-pv   1Gi        RWO            Retain           Bound    default/pvc-nfs             
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#访问模式，storage大小（pvc大小需要小于pv大小），以及 PV 和 PVC 的 storageClassName 字段必须一样，这样才能够进行绑定。</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#PersistentVolumeController会不断地循环去查看每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与未绑定的 PVC 进行绑定，这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态。而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看nfs数据目录</span>
</span></span><span style="display:flex;"><span>$ ls /nfsdata
</span></span></code></pre></div><p>创建Pod挂载pvc</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat deployment.yaml
</span></span><span style="display:flex;"><span>apiVersion: apps/v1
</span></span><span style="display:flex;"><span>kind: Deployment
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: nfs-pvc
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  replicas: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  selector:   <span style="color:#75715e">#指定Pod的选择器</span>
</span></span><span style="display:flex;"><span>    matchLabels:
</span></span><span style="display:flex;"><span>      app: nginx
</span></span><span style="display:flex;"><span>  template:
</span></span><span style="display:flex;"><span>    metadata:
</span></span><span style="display:flex;"><span>      labels:
</span></span><span style="display:flex;"><span>        app: nginx
</span></span><span style="display:flex;"><span>    spec:
</span></span><span style="display:flex;"><span>      containers:
</span></span><span style="display:flex;"><span>      - name: nginx
</span></span><span style="display:flex;"><span>        image: nginx:alpine
</span></span><span style="display:flex;"><span>        imagePullPolicy: IfNotPresent
</span></span><span style="display:flex;"><span>        ports:
</span></span><span style="display:flex;"><span>        - containerPort: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>          name: web
</span></span><span style="display:flex;"><span>        volumeMounts:                        <span style="color:#75715e">#挂载容器中的目录到pvc nfs中的目录</span>
</span></span><span style="display:flex;"><span>        - name: www
</span></span><span style="display:flex;"><span>          mountPath: /usr/share/nginx/html
</span></span><span style="display:flex;"><span>      volumes:
</span></span><span style="display:flex;"><span>      - name: www
</span></span><span style="display:flex;"><span>        persistentVolumeClaim:              <span style="color:#75715e">#指定pvc</span>
</span></span><span style="display:flex;"><span>          claimName: pvc-nfs
</span></span><span style="display:flex;"><span>          
</span></span><span style="display:flex;"><span>          
</span></span><span style="display:flex;"><span>$ kubectl create -f deployment.yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看容器/usr/share/nginx/html目录</span>
</span></span></code></pre></div><h6 id="storageclass实现动态挂载">storageClass实现动态挂载<a hidden class="anchor" aria-hidden="true" href="#storageclass实现动态挂载">#</a></h6>
<p>创建pv及pvc过程是手动，且pv与pvc一一对应，手动创建很繁琐。因此，通过storageClass  +  provisioner的方式来实现通过PVC自动创建并绑定PV。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/storage-class.png" alt=""  />
</p>
<p>部署： <a href="https://github.com/kubernetes-retired/external-storage">https://github.com/kubernetes-retired/external-storage</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#ae81ff">provisioner.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#75715e"># replace with namespace where provisioner is deployed</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">default</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">replicas</span>: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">strategy</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Recreate</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">selector</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">matchLabels</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">template</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">app</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">serviceAccountName</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">containers</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">image</span>: <span style="color:#ae81ff">quay.io/external_storage/nfs-client-provisioner:latest</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-root</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/persistentvolumes</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">env</span>:
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">PROVISIONER_NAME</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">value</span>: <span style="color:#ae81ff">luffy.com/nfs</span>
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NFS_SERVER</span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">value</span>: <span style="color:#ae81ff">172.21.51.55</span>
</span></span><span style="display:flex;"><span>            - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">NFS_PATH  </span>
</span></span><span style="display:flex;"><span>              <span style="color:#f92672">value</span>: <span style="color:#ae81ff">/data/k8s</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">volumes</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-root</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">nfs</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">server</span>: <span style="color:#ae81ff">172.21.51.55</span>
</span></span><span style="display:flex;"><span>            <span style="color:#f92672">path</span>: <span style="color:#ae81ff">/data/k8s</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">rbac.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">nfs-provisioner</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-provisioner-runner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">nfs-provisioner</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;persistentvolumes&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;delete&#34;</span>]
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;persistentvolumeclaims&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span>]
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;storage.k8s.io&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;storageclasses&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>]
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;events&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span>, <span style="color:#e6db74">&#34;patch&#34;</span>]
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">run-nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">nfs-provisioner</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">nfs-provisioner</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ClusterRole</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-provisioner-runner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">leader-locking-nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">nfs-provisioner</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">rules</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">apiGroups</span>: [<span style="color:#e6db74">&#34;&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">resources</span>: [<span style="color:#e6db74">&#34;endpoints&#34;</span>]
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">verbs</span>: [<span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span>, <span style="color:#e6db74">&#34;patch&#34;</span>]
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">RoleBinding</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">leader-locking-nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">nfs-provisioner</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">subjects</span>:
</span></span><span style="display:flex;"><span>  - <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ServiceAccount</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># replace with namespace where provisioner is deployed</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">namespace</span>: <span style="color:#ae81ff">nfs-provisioner</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">roleRef</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Role</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">leader-locking-nfs-client-provisioner</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">apiGroup</span>: <span style="color:#ae81ff">rbac.authorization.k8s.io</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">storage-class.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">storage.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">StorageClass</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">nfs</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">provisioner</span>: <span style="color:#ae81ff">luffy.com/nfs</span>
</span></span><span style="display:flex;"><span><span style="color:#ae81ff">pvc.yaml</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">PersistentVolumeClaim</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">test-claim2</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">accessModes</span>:
</span></span><span style="display:flex;"><span>    - <span style="color:#ae81ff">ReadWriteMany</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">resources</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">requests</span>:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">storage</span>: <span style="color:#ae81ff">1Mi</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">storageClassName</span>: <span style="color:#ae81ff">nfs</span>
</span></span></code></pre></div><h5 id="对接ceph存储实践">对接Ceph存储实践<a hidden class="anchor" aria-hidden="true" href="#对接ceph存储实践">#</a></h5>
<p>ceph的安装及使用参考 <a href="http://docs.ceph.org.cn/start/intro/">http://docs.ceph.org.cn/start/intro/</a></p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/ceph-art.png" alt=""  />
</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># CephFS需要使用两个Pool来分别存储数据和元数据</span>
</span></span><span style="display:flex;"><span>ceph osd pool create cephfs_data <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>ceph osd pool create cephfs_meta <span style="color:#ae81ff">128</span>
</span></span><span style="display:flex;"><span>ceph osd lspools
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 创建一个CephFS</span>
</span></span><span style="display:flex;"><span>ceph fs new cephfs cephfs_meta cephfs_data
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看</span>
</span></span><span style="display:flex;"><span>ceph fs ls
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>rados -p cephfs_meta ls
</span></span></code></pre></div><h6 id="storageclass实现动态挂载-1">storageClass实现动态挂载<a hidden class="anchor" aria-hidden="true" href="#storageclass实现动态挂载-1">#</a></h6>
<p>创建pv及pvc过程是手动，且pv与pvc一一对应，手动创建很繁琐。因此，通过storageClass  +  provisioner的方式来实现通过PVC自动创建并绑定PV。</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/storage-class.png" alt=""  />
</p>
<p>比如，针对cephfs，可以创建如下类型的storageclass：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">StorageClass</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">storage.k8s.io/v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">dynamic-cephfs</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">provisioner</span>: <span style="color:#ae81ff">ceph.com/cephfs</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">parameters</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">monitors</span>: <span style="color:#ae81ff">121.204.157.52</span>:<span style="color:#ae81ff">6789</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">adminId</span>: <span style="color:#ae81ff">admin</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">adminSecretName</span>: <span style="color:#ae81ff">ceph-admin-secret</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">adminSecretNamespace</span>: <span style="color:#e6db74">&#34;kube-system&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">claimRoot</span>: <span style="color:#ae81ff">/volumes/kubernetes</span>
</span></span></code></pre></div><p>NFS，ceph-rbd，cephfs均提供了对应的provisioner</p>
<p>部署cephfs-provisioner</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat external-storage-cephfs-provisioner.yaml
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: ServiceAccount
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>  namespace: kube-system
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>kind: ClusterRole
</span></span><span style="display:flex;"><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>rules:
</span></span><span style="display:flex;"><span>  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;persistentvolumes&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;delete&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;persistentvolumeclaims&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;storage.k8s.io&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;storageclasses&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;events&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span>, <span style="color:#e6db74">&#34;patch&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;endpoints&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;list&#34;</span>, <span style="color:#e6db74">&#34;watch&#34;</span>, <span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;update&#34;</span>, <span style="color:#e6db74">&#34;patch&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;secrets&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;delete&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>kind: ClusterRoleBinding
</span></span><span style="display:flex;"><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>subjects:
</span></span><span style="display:flex;"><span>  - kind: ServiceAccount
</span></span><span style="display:flex;"><span>    name: cephfs-provisioner
</span></span><span style="display:flex;"><span>    namespace: kube-system
</span></span><span style="display:flex;"><span>roleRef:
</span></span><span style="display:flex;"><span>  kind: ClusterRole
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style="display:flex;"><span>kind: Role
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>  namespace: kube-system
</span></span><span style="display:flex;"><span>rules:
</span></span><span style="display:flex;"><span>  - apiGroups: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    resources: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;secrets&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>    verbs: <span style="color:#f92672">[</span><span style="color:#e6db74">&#34;create&#34;</span>, <span style="color:#e6db74">&#34;get&#34;</span>, <span style="color:#e6db74">&#34;delete&#34;</span><span style="color:#f92672">]</span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>apiVersion: rbac.authorization.k8s.io/v1
</span></span><span style="display:flex;"><span>kind: RoleBinding
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>  namespace: kube-system
</span></span><span style="display:flex;"><span>roleRef:
</span></span><span style="display:flex;"><span>  apiGroup: rbac.authorization.k8s.io
</span></span><span style="display:flex;"><span>  kind: Role
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>subjects:
</span></span><span style="display:flex;"><span>- kind: ServiceAccount
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>  namespace: kube-system
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>---
</span></span><span style="display:flex;"><span>apiVersion: apps/v1
</span></span><span style="display:flex;"><span>kind: Deployment
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs-provisioner
</span></span><span style="display:flex;"><span>  namespace: kube-system
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  replicas: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  selector:
</span></span><span style="display:flex;"><span>    matchLabels:
</span></span><span style="display:flex;"><span>      app: cephfs-provisioner
</span></span><span style="display:flex;"><span>  strategy:
</span></span><span style="display:flex;"><span>    type: Recreate
</span></span><span style="display:flex;"><span>  template:
</span></span><span style="display:flex;"><span>    metadata:
</span></span><span style="display:flex;"><span>      labels:
</span></span><span style="display:flex;"><span>        app: cephfs-provisioner
</span></span><span style="display:flex;"><span>    spec:
</span></span><span style="display:flex;"><span>      containers:
</span></span><span style="display:flex;"><span>      - name: cephfs-provisioner
</span></span><span style="display:flex;"><span>        image: <span style="color:#e6db74">&#34;quay.io/external_storage/cephfs-provisioner:latest&#34;</span>
</span></span><span style="display:flex;"><span>        env:
</span></span><span style="display:flex;"><span>        - name: PROVISIONER_NAME
</span></span><span style="display:flex;"><span>          value: ceph.com/cephfs
</span></span><span style="display:flex;"><span>        imagePullPolicy: IfNotPresent
</span></span><span style="display:flex;"><span>        command:
</span></span><span style="display:flex;"><span>        - <span style="color:#e6db74">&#34;/usr/local/bin/cephfs-provisioner&#34;</span>
</span></span><span style="display:flex;"><span>        args:
</span></span><span style="display:flex;"><span>        - <span style="color:#e6db74">&#34;-id=cephfs-provisioner-1&#34;</span>
</span></span><span style="display:flex;"><span>        - <span style="color:#e6db74">&#34;-disable-ceph-namespace-isolation=true&#34;</span>
</span></span><span style="display:flex;"><span>      serviceAccount: cephfs-provisioner
</span></span></code></pre></div><p>在ceph monitor机器中查看admin账户的key</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ ceph auth ls
</span></span><span style="display:flex;"><span>$ ceph auth get-key client.admin
</span></span><span style="display:flex;"><span>AQAejeJbowvgMhAAsuloUOvepcj/TXEIoSrd7A<span style="color:#f92672">==</span>
</span></span></code></pre></div><p>创建secret</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ echo -n AQAejeJbowvgMhAAsuloUOvepcj/TXEIoSrd7A<span style="color:#f92672">==</span>|base64
</span></span><span style="display:flex;"><span>QVFBZWplSmJvd3ZnTWhBQXN1bG9VT3ZlcGNqL1RYRUlvU3JkN0E9PQ<span style="color:#f92672">==</span>
</span></span><span style="display:flex;"><span>$ cat ceph-admin-secret.yaml
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>data:
</span></span><span style="display:flex;"><span>  key: QVFBZWplSmJvd3ZnTWhBQXN1bG9VT3ZlcGNqL1RYRUlvU3JkN0E9PQ<span style="color:#f92672">==</span>
</span></span><span style="display:flex;"><span>kind: Secret
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: ceph-admin-secret
</span></span><span style="display:flex;"><span>  namespace: kube-system
</span></span><span style="display:flex;"><span>type: Opaque
</span></span></code></pre></div><p>创建storageclass</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat cephfs-storage-class.yaml
</span></span><span style="display:flex;"><span>kind: StorageClass
</span></span><span style="display:flex;"><span>apiVersion: storage.k8s.io/v1
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: dynamic-cephfs
</span></span><span style="display:flex;"><span>provisioner: ceph.com/cephfs
</span></span><span style="display:flex;"><span>parameters:
</span></span><span style="display:flex;"><span>    monitors: 36.111.140.31:6789
</span></span><span style="display:flex;"><span>    adminId: admin
</span></span><span style="display:flex;"><span>    adminSecretName: ceph-admin-secret
</span></span><span style="display:flex;"><span>    adminSecretNamespace: <span style="color:#e6db74">&#34;kube-system&#34;</span>
</span></span><span style="display:flex;"><span>    claimRoot: /volumes/kubernetes
</span></span></code></pre></div><h6 id="动态pvc验证及实现分析">动态pvc验证及实现分析<a hidden class="anchor" aria-hidden="true" href="#动态pvc验证及实现分析">#</a></h6>
<p>使用流程： 创建pvc，指定storageclass和存储大小，即可实现动态存储。</p>
<p>创建pvc测试自动生成pv</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat cephfs-pvc-test.yaml
</span></span><span style="display:flex;"><span>kind: PersistentVolumeClaim
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: cephfs-claim
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  accessModes:     
</span></span><span style="display:flex;"><span>    - ReadWriteOnce
</span></span><span style="display:flex;"><span>  storageClassName: dynamic-cephfs
</span></span><span style="display:flex;"><span>  resources:
</span></span><span style="display:flex;"><span>    requests:
</span></span><span style="display:flex;"><span>      storage: 2Gi
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl create -f cephfs-pvc-test.yaml
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ kubectl get pv
</span></span><span style="display:flex;"><span>pvc-2abe427e-7568-442d-939f-2c273695c3db   2Gi        RWO            Delete           Bound      default/cephfs-claim   dynamic-cephfs            1s
</span></span></code></pre></div><p>创建Pod使用pvc挂载cephfs数据盘</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat test-pvc-cephfs.yaml
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>kind: Pod
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: nginx-pod
</span></span><span style="display:flex;"><span>  labels:
</span></span><span style="display:flex;"><span>    name: nginx-pod
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  containers:
</span></span><span style="display:flex;"><span>  - name: nginx-pod
</span></span><span style="display:flex;"><span>    image: nginx:alpine
</span></span><span style="display:flex;"><span>    ports:
</span></span><span style="display:flex;"><span>    - name: web
</span></span><span style="display:flex;"><span>      containerPort: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>    volumeMounts:
</span></span><span style="display:flex;"><span>    - name: cephfs
</span></span><span style="display:flex;"><span>      mountPath: /usr/share/nginx/html
</span></span><span style="display:flex;"><span>  volumes:
</span></span><span style="display:flex;"><span>  - name: cephfs
</span></span><span style="display:flex;"><span>    persistentVolumeClaim:
</span></span><span style="display:flex;"><span>      claimName: cephfs-claim
</span></span><span style="display:flex;"><span>      
</span></span><span style="display:flex;"><span>$ kubectl create -f test-pvc-cephfs.yaml
</span></span></code></pre></div><p>我们所说的容器的持久化，实际上应该理解为宿主机中volume的持久化，因为Pod是支持销毁重建的，所以只能通过宿主机volume持久化，然后挂载到Pod内部来实现Pod的数据持久化。</p>
<p>宿主机上的volume持久化，因为要支持数据漂移，所以通常是数据存储在分布式存储中，宿主机本地挂载远程存储（NFS，Ceph，OSS），这样即使Pod漂移也不影响数据。</p>
<p>k8s的pod的挂载盘通常的格式为：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>/var/lib/kubelet/pods/&lt;Pod的ID&gt;/volumes/kubernetes.io~&lt;Volume类型&gt;/&lt;Volume名字&gt;
</span></span></code></pre></div><p>查看nginx-pod的挂载盘，</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ df -TH
</span></span><span style="display:flex;"><span>/var/lib/kubelet/pods/61ba43c5-d2e9-4274-ac8c-008854e4fa8e/volumes/kubernetes.io~cephfs/pvc-2abe427e-7568-442d-939f-2c273695c3db/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ findmnt /var/lib/kubelet/pods/61ba43c5-d2e9-4274-ac8c-008854e4fa8e/volumes/kubernetes.io~cephfs/pvc-2abe427e-7568-442d-939f-2c273695c3db/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>36.111.140.31:6789:/volumes/kubernetes/kubernetes/kubernetes-dynamic-pvc-ffe3d84d-c433-11ea-b347-6acc3cf3c15f
</span></span></code></pre></div><h4 id="使用helm3管理复杂应用的部署">使用Helm3管理复杂应用的部署<a hidden class="anchor" aria-hidden="true" href="#使用helm3管理复杂应用的部署">#</a></h4>
<h5 id="认识helm">认识Helm<a hidden class="anchor" aria-hidden="true" href="#认识helm">#</a></h5>
<ol>
<li>
<p>为什么有helm？</p>
</li>
<li>
<p>Helm是什么？</p>
<p>kubernetes的包管理器，“可以将Helm看作Linux系统下的apt-get/yum”。</p>
<ul>
<li>对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。</li>
<li>对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。</li>
</ul>
<p>除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。</p>
</li>
<li>
<p>Helm的版本</p>
<ul>
<li>
<p>helm2</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/helm2.jpg" alt=""  />
</p>
<p>C/S架构，helm通过Tiller与k8s交互</p>
</li>
<li>
<p>helm3</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/helm3.jpg" alt=""  />
</p>
<ul>
<li>
<p>从安全性和易用性方面考虑，移除了Tiller服务端，helm3直接使用kubeconfig文件鉴权访问APIServer服务器</p>
</li>
<li>
<p>由二路合并升级成为三路合并补丁策略（ 旧的配置，线上状态，新的配置 ）</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>helm install very_important_app ./very_important_app
</span></span></code></pre></div><p>这个应用的副本数量设置为 3 。现在，如果有人不小心执行了 <code>kubectl edit</code> 或：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>kubectl scale -replicas<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span> deployment/very_important_app
</span></span></code></pre></div><p>然后，团队中的某个人发现 very_important_app 莫名其妙宕机了，尝试执行命令：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>helm rollback very_important_app
</span></span></code></pre></div><p>在 Helm 2 中，这个操作将比较旧的配置与新的配置，然后生成一个更新补丁。由于，误操作的人仅修改了应用的线上状态（旧的配置并未更新）。Helm 在回滚时，什么事情也不会做。因为旧的配置与新的配置没有差别（都是 3 个副本）。然后，Helm 不执行回滚，副本数继续保持为 0</p>
</li>
<li>
<p>移除了helm serve本地repo仓库</p>
</li>
<li>
<p>创建应用时必须指定名字（或者&ndash;generate-name随机生成）</p>
</li>
</ul>
</li>
</ul>
</li>
<li>
<p>Helm的重要概念</p>
<ul>
<li>chart，应用的信息集合，包括各种对象的配置模板、参数定义、依赖关系、文档说明等</li>
<li>Repoistory，chart仓库，存储chart的地方，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。</li>
<li>release， 当 chart 被安装到 kubernetes 集群，就生成了一个 release ， 是 chart 的运行实例，代表了一个正在运行的应用</li>
</ul>
</li>
</ol>
<p>helm 是包管理工具，包就是指 chart，helm 能够：</p>
<ul>
<li>从零创建chart</li>
<li>与仓库交互，拉取、保存、更新 chart</li>
<li>在kubernetes集群中安装、卸载 release</li>
<li>更新、回滚、测试 release</li>
</ul>
<h5 id="安装与快速入门实践">安装与快速入门实践<a hidden class="anchor" aria-hidden="true" href="#安装与快速入门实践">#</a></h5>
<p>下载最新的稳定版本：https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz</p>
<p>更多版本可以参考： <a href="https://github.com/helm/helm/releases">https://github.com/helm/helm/releases</a></p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># k8s-master节点</span>
</span></span><span style="display:flex;"><span>$ wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz
</span></span><span style="display:flex;"><span>$ tar -zxf helm-v3.2.4-linux-amd64.tar.gz
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ cp linux-amd64/helm /usr/local/bin/
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 验证安装</span>
</span></span><span style="display:flex;"><span>$ helm version
</span></span><span style="display:flex;"><span>version.BuildInfo<span style="color:#f92672">{</span>Version:<span style="color:#e6db74">&#34;v3.2.4&#34;</span>, GitCommit:<span style="color:#e6db74">&#34;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688&#34;</span>, GitTreeState:<span style="color:#e6db74">&#34;clean&#34;</span>, GoVersion:<span style="color:#e6db74">&#34;go1.13.12&#34;</span><span style="color:#f92672">}</span>
</span></span><span style="display:flex;"><span>$ helm env
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 添加仓库</span>
</span></span><span style="display:flex;"><span>$ helm repo add stable http://mirror.azure.cn/kubernetes/charts/
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 同步最新charts信息到本地</span>
</span></span><span style="display:flex;"><span>$ helm repo update
</span></span></code></pre></div><p>快速入门实践：</p>
<p>示例一：使用helm安装mysql应用</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># helm 搜索chart包</span>
</span></span><span style="display:flex;"><span>$ helm search repo mysql
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 从仓库安装</span>
</span></span><span style="display:flex;"><span>$ helm install mysql stable/mysql
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ helm ls
</span></span><span style="display:flex;"><span>$ kubectl get all 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 从chart仓库中把chart包下载到本地</span>
</span></span><span style="display:flex;"><span>$ helm pull stable/mysql
</span></span><span style="display:flex;"><span>$ tree mysql
</span></span></code></pre></div><p>示例二：新建nginx的chart并安装</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ helm create nginx
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 从本地安装</span>
</span></span><span style="display:flex;"><span>$ helm install nginx ./nginx
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 安装到别的命名空间luffy</span>
</span></span><span style="display:flex;"><span>$ helm -n luffy install ./nginx
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 查看</span>
</span></span><span style="display:flex;"><span>$ helm ls
</span></span><span style="display:flex;"><span>$ helm -n luffy ls
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>$ kubectl get all 
</span></span><span style="display:flex;"><span>$ kubectl -n luffy get all
</span></span></code></pre></div><h5 id="chart的模板语法及开发">Chart的模板语法及开发<a hidden class="anchor" aria-hidden="true" href="#chart的模板语法及开发">#</a></h5>
<h6 id="nginx的chart实现分析">nginx的chart实现分析<a hidden class="anchor" aria-hidden="true" href="#nginx的chart实现分析">#</a></h6>
<p>格式：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ tree nginx/
</span></span><span style="display:flex;"><span>nginx/
</span></span><span style="display:flex;"><span>├── charts            <span style="color:#75715e"># 存放子chart</span>
</span></span><span style="display:flex;"><span>├── Chart.yaml          <span style="color:#75715e"># 该chart的全局定义信息</span>
</span></span><span style="display:flex;"><span>├── templates         <span style="color:#75715e"># chart运行所需的资源清单模板，用于和values做渲染</span>
</span></span><span style="display:flex;"><span>│   ├── deployment.yaml
</span></span><span style="display:flex;"><span>│   ├── _helpers.tpl      <span style="color:#75715e"># 定义全局的命名模板，方便在其他模板中引入使用</span>
</span></span><span style="display:flex;"><span>│   ├── hpa.yaml
</span></span><span style="display:flex;"><span>│   ├── ingress.yaml
</span></span><span style="display:flex;"><span>│   ├── NOTES.txt       <span style="color:#75715e"># helm安装完成后终端的提示信息</span>
</span></span><span style="display:flex;"><span>│   ├── serviceaccount.yaml
</span></span><span style="display:flex;"><span>│   ├── service.yaml
</span></span><span style="display:flex;"><span>│   └── tests
</span></span><span style="display:flex;"><span>│       └── test-connection.yaml
</span></span><span style="display:flex;"><span>└── values.yaml         <span style="color:#75715e"># 模板使用的默认值信息</span>
</span></span></code></pre></div><p>很明显，资源清单都在templates中，数据来源于values.yaml，安装的过程就是将模板与数据融合成k8s可识别的资源清单，然后部署到k8s环境中。</p>
<p>分析模板文件的实现：</p>
<ul>
<li>
<p>引用命名模板并传递作用域</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">{{</span> include <span style="color:#e6db74">&#34;nginx.fullname&#34;</span> . <span style="color:#f92672">}}</span>
</span></span></code></pre></div><p>include从_helpers.tpl中引用命名模板，并传递顶级作用域.</p>
</li>
<li>
<p>内置对象</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>.Values
</span></span><span style="display:flex;"><span>.Release.Name
</span></span></code></pre></div><ul>
<li><code>Release</code>：该对象描述了 release 本身的相关信息，它内部有几个对象：
<ul>
<li><code>Release.Name</code>：release 名称</li>
<li><code>Release.Namespace</code>：release 安装到的命名空间</li>
<li><code>Release.IsUpgrade</code>：如果当前操作是升级或回滚，则该值为 true</li>
<li><code>Release.IsInstall</code>：如果当前操作是安装，则将其设置为 true</li>
<li><code>Release.Revision</code>：release 的 revision 版本号，在安装的时候，值为1，每次升级或回滚都会增加</li>
<li><code>Reelase.Service</code>：渲染当前模板的服务，在 Helm 上，实际上该值始终为 Helm</li>
</ul>
</li>
<li><code>Values</code>：从 <code>values.yaml</code> 文件和用户提供的 values 文件传递到模板的 Values 值</li>
<li><code>Chart</code>：获取 <code>Chart.yaml</code> 文件的内容，该文件中的任何数据都可以访问，例如 <code>{{ .Chart.Name }}-{{ .Chart.Version}}</code> 可以渲染成 <code>mychart-0.1.0</code></li>
</ul>
</li>
<li>
<p>模板定义</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">{{</span>- define <span style="color:#e6db74">&#34;nginx.fullname&#34;</span> -<span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- <span style="color:#66d9ef">if</span> .Values.fullnameOverride <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- .Values.fullnameOverride | trunc <span style="color:#ae81ff">63</span> | trimSuffix <span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- <span style="color:#66d9ef">else</span> <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- $name :<span style="color:#f92672">=</span> default .Chart.Name .Values.nameOverride <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- <span style="color:#66d9ef">if</span> contains $name .Release.Name <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- .Release.Name | trunc <span style="color:#ae81ff">63</span> | trimSuffix <span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- <span style="color:#66d9ef">else</span> <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- printf <span style="color:#e6db74">&#34;%s-%s&#34;</span> .Release.Name $name | trunc <span style="color:#ae81ff">63</span> | trimSuffix <span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- end <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- end <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- end <span style="color:#f92672">}}</span>
</span></span></code></pre></div><ul>
<li>
<p>{{-  去掉左边的空格及换行，-}}  去掉右侧的空格及换行</p>
</li>
<li>
<p>示例</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: {{ <span style="color:#ae81ff">.Release.Name }}-configmap</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">myvalue</span>: <span style="color:#e6db74">&#34;Hello World&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">drink</span>: {{ <span style="color:#ae81ff">.Values.favorite.drink | default &#34;tea&#34; | quote }}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">food</span>: {{ <span style="color:#ae81ff">.Values.favorite.food | upper | quote }}</span>
</span></span><span style="display:flex;"><span>  {{ <span style="color:#ae81ff">if eq .Values.favorite.drink &#34;coffee&#34; }}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">mug</span>: <span style="color:#66d9ef">true</span>
</span></span><span style="display:flex;"><span>  {{ <span style="color:#ae81ff">end }}</span>
</span></span></code></pre></div><p>渲染完后是：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">v1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">ConfigMap</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: <span style="color:#ae81ff">mychart-1575971172-configmap</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">data</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">myvalue</span>: <span style="color:#e6db74">&#34;Hello World&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">drink</span>: <span style="color:#e6db74">&#34;coffee&#34;</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">food</span>: <span style="color:#e6db74">&#34;PIZZA&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">mug</span>: <span style="color:#66d9ef">true</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>管道及方法</p>
<ul>
<li>
<p>trunc表示字符串截取，63作为参数传递给trunc方法，trimSuffix表示去掉<code>-</code>后缀</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">{{</span>- .Values.fullnameOverride | trunc <span style="color:#ae81ff">63</span> | trimSuffix <span style="color:#e6db74">&#34;-&#34;</span> <span style="color:#f92672">}}</span>
</span></span></code></pre></div></li>
<li>
<p>nindent表示前面的空格数</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>  selector:
</span></span><span style="display:flex;"><span>    matchLabels:
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">{{</span>- include <span style="color:#e6db74">&#34;nginx.selectorLabels&#34;</span> . | nindent <span style="color:#ae81ff">6</span> <span style="color:#f92672">}}</span>
</span></span></code></pre></div></li>
<li>
<p>lower表示将内容小写，quote表示用双引号引起来</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>value: <span style="color:#f92672">{{</span> include <span style="color:#e6db74">&#34;mytpl&#34;</span> . | lower | quote <span style="color:#f92672">}}</span>
</span></span></code></pre></div></li>
</ul>
</li>
<li>
<p>条件判断语句每个if对应一个end</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">{{</span>- <span style="color:#66d9ef">if</span> .Values.fullnameOverride <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- <span style="color:#66d9ef">else</span> <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- end <span style="color:#f92672">}}</span>
</span></span></code></pre></div><p>通常用来根据values.yaml中定义的开关来控制模板中的显示：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">{{</span>- <span style="color:#66d9ef">if</span> not .Values.autoscaling.enabled <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span>  replicas: <span style="color:#f92672">{{</span> .Values.replicaCount <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">{{</span>- end <span style="color:#f92672">}}</span>
</span></span></code></pre></div></li>
<li>
<p>定义变量，模板中可以通过变量名字去引用</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#f92672">{{</span>- $name :<span style="color:#f92672">=</span> default .Chart.Name .Values.nameOverride <span style="color:#f92672">}}</span>
</span></span></code></pre></div></li>
<li>
<p>遍历values的数据</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>      <span style="color:#f92672">{{</span>- with .Values.nodeSelector <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span>      nodeSelector:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">{{</span>- toYaml . | nindent <span style="color:#ae81ff">8</span> <span style="color:#f92672">}}</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">{{</span>- end <span style="color:#f92672">}}</span>
</span></span></code></pre></div><p>toYaml处理值中的转义及特殊字符， &ldquo;kubernetes.io/role&rdquo;=master ， name=&ldquo;value1,value2&rdquo; 类似的情况</p>
</li>
<li>
<p>default设置默认值</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>image: <span style="color:#e6db74">&#34;{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}&#34;</span>
</span></span></code></pre></div></li>
</ul>
<p>Helm template</p>
<p>hpa.yaml</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>{{- <span style="color:#ae81ff">if .Values.autoscaling.enabled }}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">autoscaling/v2beta1</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">kind</span>: <span style="color:#ae81ff">HorizontalPodAutoscaler</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">metadata</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">name</span>: {{ <span style="color:#ae81ff">include &#34;nginx.fullname&#34; . }}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">labels</span>:
</span></span><span style="display:flex;"><span>    {{- <span style="color:#ae81ff">include &#34;nginx.labels&#34; . | nindent 4 }}</span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">spec</span>:
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">scaleTargetRef</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">apiVersion</span>: <span style="color:#ae81ff">apps/v1</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">kind</span>: <span style="color:#ae81ff">Deployment</span>
</span></span><span style="display:flex;"><span>    <span style="color:#f92672">name</span>: {{ <span style="color:#ae81ff">include &#34;nginx.fullname&#34; . }}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">minReplicas</span>: {{ <span style="color:#ae81ff">.Values.autoscaling.minReplicas }}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">maxReplicas</span>: {{ <span style="color:#ae81ff">.Values.autoscaling.maxReplicas }}</span>
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">metrics</span>:
</span></span><span style="display:flex;"><span>  {{- <span style="color:#ae81ff">if .Values.autoscaling.targetCPUUtilizationPercentage }}</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Resource</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">resource</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">cpu</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">targetAverageUtilization</span>: {{ <span style="color:#ae81ff">.Values.autoscaling.targetCPUUtilizationPercentage }}</span>
</span></span><span style="display:flex;"><span>  {{- <span style="color:#ae81ff">end }}</span>
</span></span><span style="display:flex;"><span>  {{- <span style="color:#ae81ff">if .Values.autoscaling.targetMemoryUtilizationPercentage }}</span>
</span></span><span style="display:flex;"><span>    - <span style="color:#f92672">type</span>: <span style="color:#ae81ff">Resource</span>
</span></span><span style="display:flex;"><span>      <span style="color:#f92672">resource</span>:
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">name</span>: <span style="color:#ae81ff">memory</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">targetAverageUtilization</span>: {{ <span style="color:#ae81ff">.Values.autoscaling.targetMemoryUtilizationPercentage }}</span>
</span></span><span style="display:flex;"><span>  {{- <span style="color:#ae81ff">end }}</span>
</span></span><span style="display:flex;"><span>{{- <span style="color:#ae81ff">end }}</span>
</span></span></code></pre></div><h6 id="创建应用的时候赋值">创建应用的时候赋值<a hidden class="anchor" aria-hidden="true" href="#创建应用的时候赋值">#</a></h6>
<ul>
<li>set的方式</li>
</ul>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 改变副本数和resource值</span>
</span></span><span style="display:flex;"><span>$ helm install nginx-2 ./nginx --set replicaCount<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span> --set resources.limits.cpu<span style="color:#f92672">=</span>200m --set resources.limits.memory<span style="color:#f92672">=</span>256Mi
</span></span></code></pre></div><ul>
<li>
<p>value文件的方式</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat nginx-values.yaml
</span></span><span style="display:flex;"><span>resources:
</span></span><span style="display:flex;"><span>  limits:
</span></span><span style="display:flex;"><span>    cpu: 100m
</span></span><span style="display:flex;"><span>    memory: 128Mi
</span></span><span style="display:flex;"><span>  requests:
</span></span><span style="display:flex;"><span>    cpu: 100m
</span></span><span style="display:flex;"><span>    memory: 128Mi
</span></span><span style="display:flex;"><span>autoscaling:
</span></span><span style="display:flex;"><span>  enabled: true
</span></span><span style="display:flex;"><span>  minReplicas: <span style="color:#ae81ff">1</span>
</span></span><span style="display:flex;"><span>  maxReplicas: <span style="color:#ae81ff">3</span>
</span></span><span style="display:flex;"><span>  targetCPUUtilizationPercentage: <span style="color:#ae81ff">80</span>
</span></span><span style="display:flex;"><span>ingress:
</span></span><span style="display:flex;"><span>  enabled: true
</span></span><span style="display:flex;"><span>  hosts:
</span></span><span style="display:flex;"><span>    - host: chart-example.luffy.com
</span></span><span style="display:flex;"><span>      paths:
</span></span><span style="display:flex;"><span>      - /
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ helm install -f nginx-values.yaml nginx-3 ./nginx
</span></span></code></pre></div></li>
</ul>
<p>更多语法参考：</p>
<p><a href="https://helm.sh/docs/topics/charts/">https://helm.sh/docs/topics/charts/</a></p>
<p>部署mysql失败的问题</p>
<h5 id="实战使用helm部署harbor镜像及chart仓库">实战：使用Helm部署Harbor镜像及chart仓库<a hidden class="anchor" aria-hidden="true" href="#实战使用helm部署harbor镜像及chart仓库">#</a></h5>
<h6 id="harbor踩坑部署">harbor踩坑部署<a hidden class="anchor" aria-hidden="true" href="#harbor踩坑部署">#</a></h6>
<p>架构 <a href="https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor">https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor</a></p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/harbor-architecture.png" alt=""  />
</p>
<ul>
<li>Core，核心组件
<ul>
<li>API Server，接收处理用户请求</li>
<li>Config Manager ：所有系统的配置，比如认证、邮件、证书配置等</li>
<li>Project Manager：项目管理</li>
<li>Quota Manager ：配额管理</li>
<li>Chart Controller：chart管理</li>
<li>Replication Controller ：镜像副本控制器，可以与不同类型的仓库实现镜像同步
<ul>
<li>Distribution (docker registry)</li>
<li>Docker Hub</li>
<li>&hellip;</li>
</ul>
</li>
<li>Scan Manager ：扫描管理，引入第三方组件，进行镜像安全扫描</li>
<li>Registry Driver ：镜像仓库驱动，目前使用docker registry</li>
</ul>
</li>
<li>Job Service，执行异步任务，如同步镜像信息</li>
<li>Log Collector，统一日志收集器，收集各模块日志</li>
<li>GC Controller</li>
<li>Chart Museum，chart仓库服务，第三方</li>
<li>Docker Registry，镜像仓库服务</li>
<li>kv-storage，redis缓存服务，job service使用，存储job metadata</li>
<li>local/remote storage，存储服务，比较镜像存储</li>
<li>SQL Database，postgresl，存储用户、项目等元数据</li>
</ul>
<p>通常用作企业级镜像仓库服务，实际功能强大很多。</p>
<p>组件众多，因此使用helm部署</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 添加harbor chart仓库</span>
</span></span><span style="display:flex;"><span>$ helm repo add harbor https://helm.goharbor.io
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 搜索harbor的chart</span>
</span></span><span style="display:flex;"><span>$ helm search repo harbor
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 不知道如何部署，因此拉到本地</span>
</span></span><span style="display:flex;"><span>$ helm pull harbor/harbor --version 1.4.1
</span></span></code></pre></div><p>创建pvc</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ kubectl create namespace harbor
</span></span><span style="display:flex;"><span>$ cat harbor-pvc.yaml
</span></span><span style="display:flex;"><span>kind: PersistentVolumeClaim
</span></span><span style="display:flex;"><span>apiVersion: v1
</span></span><span style="display:flex;"><span>metadata:
</span></span><span style="display:flex;"><span>  name: harbor-pvc
</span></span><span style="display:flex;"><span>  namespace: harbor
</span></span><span style="display:flex;"><span>spec:
</span></span><span style="display:flex;"><span>  accessModes:     
</span></span><span style="display:flex;"><span>    - ReadWriteOnce
</span></span><span style="display:flex;"><span>  storageClassName: dynamic-cephfs
</span></span><span style="display:flex;"><span>  resources:
</span></span><span style="display:flex;"><span>    requests:
</span></span><span style="display:flex;"><span>      storage: 20Gi
</span></span></code></pre></div><p>修改harbor配置：</p>
<ul>
<li>开启ingress访问</li>
<li>externalURL，web访问入口，和ingress的域名相同</li>
<li>持久化，使用PVC对接的cephfs</li>
<li>harborAdminPassword: &ldquo;Harbor12345&rdquo;，管理员默认账户 admin/Harbor12345</li>
<li>开启chartmuseum</li>
<li>clair和trivy漏洞扫描组件，暂不启用</li>
</ul>
<p>helm创建：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span><span style="color:#75715e"># 使用本地chart安装</span>
</span></span><span style="display:flex;"><span>$ helm install harbor ./harbor -n harbor 
</span></span></code></pre></div><p>踩坑一：redis持久化数据目录权限导致无法登录</p>
<p>redis数据目录，/var/lib/redis，需要设置redis的用户及用户组权限</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>      <span style="color:#f92672">initContainers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;change-permission-of-directory&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: {{ <span style="color:#ae81ff">.Values.redis.internal.image.repository }}:{{ .Values.redis.internal.image.tag }}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">imagePullPolicy</span>: {{ <span style="color:#ae81ff">.Values.imagePullPolicy }}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;/bin/sh&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;-c&#34;</span>, <span style="color:#e6db74">&#34;chown -R 999:999 /var/lib/redis&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">securityContext</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">runAsUser</span>: <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/var/lib/redis</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">subPath</span>: {{ <span style="color:#ae81ff">$redis.subPath }}</span>
</span></span></code></pre></div><p>踩坑二：registry组件的镜像存储目录权限导致镜像推送失败</p>
<p>registry的镜像存储目录，需要设置registry用户的用户及用户组，不然镜像推送失败</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>      <span style="color:#f92672">initContainers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;change-permission-of-directory&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">securityContext</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">runAsUser</span>: <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: {{ <span style="color:#ae81ff">.Values.registry.registry.image.repository }}:{{ .Values.registry.registry.image.tag }}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">imagePullPolicy</span>: {{ <span style="color:#ae81ff">.Values.imagePullPolicy }}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;/bin/sh&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;-c&#34;</span>, <span style="color:#e6db74">&#34;chown -R 10000:10000 {{ .Values.persistence.imageChartStorage.filesystem.rootdirectory }}&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">registry-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: {{ <span style="color:#ae81ff">.Values.persistence.imageChartStorage.filesystem.rootdirectory }}</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">subPath</span>: {{ <span style="color:#ae81ff">.Values.persistence.persistentVolumeClaim.registry.subPath }}</span>
</span></span></code></pre></div><p>踩坑三：chartmuseum存储目录权限，导致chart推送失败</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-yaml" data-lang="yaml"><span style="display:flex;"><span>      <span style="color:#f92672">initContainers</span>:
</span></span><span style="display:flex;"><span>      - <span style="color:#f92672">name</span>: <span style="color:#e6db74">&#34;change-permission-of-directory&#34;</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">image</span>: {{ <span style="color:#ae81ff">.Values.chartmuseum.image.repository }}:{{ .Values.chartmuseum.image.tag }}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">imagePullPolicy</span>: {{ <span style="color:#ae81ff">.Values.imagePullPolicy }}</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">command</span>: [<span style="color:#e6db74">&#34;/bin/sh&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">args</span>: [<span style="color:#e6db74">&#34;-c&#34;</span>, <span style="color:#e6db74">&#34;chown -R 10000:10000 /chart_storage&#34;</span>]
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">securityContext</span>:
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">runAsUser</span>: <span style="color:#ae81ff">0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#f92672">volumeMounts</span>:
</span></span><span style="display:flex;"><span>        - <span style="color:#f92672">name</span>: <span style="color:#ae81ff">chartmuseum-data</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">mountPath</span>: <span style="color:#ae81ff">/chart_storage</span>
</span></span><span style="display:flex;"><span>          <span style="color:#f92672">subPath</span>: {{ <span style="color:#ae81ff">.Values.persistence.persistentVolumeClaim.chartmuseum.subPath }}</span>
</span></span></code></pre></div><p>更新内容后，执行更新release</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ helm upgrade harbor -n harbor ./
</span></span></code></pre></div><h6 id="推送镜像到harbor仓库">推送镜像到Harbor仓库<a hidden class="anchor" aria-hidden="true" href="#推送镜像到harbor仓库">#</a></h6>
<p>配置hosts及docker非安全仓库：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ cat /etc/hosts
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>192.168.136.10 k8s-master core.harbor.domain
</span></span><span style="display:flex;"><span>...
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ cat /etc/docker/daemon.json
</span></span><span style="display:flex;"><span><span style="color:#f92672">{</span>                                            
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;insecure-registries&#34;</span>: <span style="color:#f92672">[</span>                   
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;192.168.136.10:5000&#34;</span>,                   
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;core.harbor.domain&#34;</span>                     
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">]</span>,                                         
</span></span><span style="display:flex;"><span>  <span style="color:#e6db74">&#34;registry-mirrors&#34;</span> : <span style="color:#f92672">[</span>                     
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;https://8xpk5wnt.mirror.aliyuncs.com&#34;</span>   
</span></span><span style="display:flex;"><span>  <span style="color:#f92672">]</span>                                          
</span></span><span style="display:flex;"><span><span style="color:#f92672">}</span>                           
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">#</span>
</span></span><span style="display:flex;"><span>$ systemctl restart docker
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 使用账户密码登录admin/Harbor12345</span>
</span></span><span style="display:flex;"><span>$ docker login core.harbor.domain
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ docker tag nginx:alpine core.harbor.domain/library/nginx:alpine
</span></span><span style="display:flex;"><span>$ docker push core.harbor.domain/library/nginx:alpine
</span></span></code></pre></div><h6 id="推送chart到harbor仓库">推送chart到Harbor仓库<a hidden class="anchor" aria-hidden="true" href="#推送chart到harbor仓库">#</a></h6>
<p>helm3默认没有安装helm push插件，需要手动安装。插件地址 <a href="https://github.com/chartmuseum/helm-push">https://github.com/chartmuseum/helm-push</a></p>
<p>安装插件：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ helm plugin install https://github.com/chartmuseum/helm-push
</span></span></code></pre></div><p>离线安装：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ helm plugin install ./helm-push
</span></span></code></pre></div><p>添加repo</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ helm repo add myharbor https://core.harbor.domain/chartrepo/library 
</span></span><span style="display:flex;"><span><span style="color:#75715e"># x509错误</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 添加证书信任，根证书为配置给ingress使用的证书</span>
</span></span><span style="display:flex;"><span>$ kubectl get secret harbor-harbor-ingress -n harbor -o jsonpath<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;{.data.ca\.crt}&#34;</span> | base64 -d &gt;harbor.ca.crt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ cp harbor.ca.crt /etc/pki/ca-trust/source/anchors
</span></span><span style="display:flex;"><span>$ update-ca-trust enable; update-ca-trust extract
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e"># 再次添加</span>
</span></span><span style="display:flex;"><span>$ helm repo add myharbor https://core.harbor.domain/chartrepo/library --ca-file<span style="color:#f92672">=</span>harbor.ca.crt
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>$ helm repo ls
</span></span></code></pre></div><p>推送chart到仓库：</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-shell" data-lang="shell"><span style="display:flex;"><span>$ helm push harbor myharbor --ca-file<span style="color:#f92672">=</span>harbor.ca.crt -u admin -p Harbor12345
</span></span></code></pre></div><p>查看harbor仓库的chart</p>
<h4 id="课程小结">课程小结<a hidden class="anchor" aria-hidden="true" href="#课程小结">#</a></h4>
<p>使用k8s的进阶内容。</p>
<ol>
<li>
<p>学习k8s在etcd中数据的存储，掌握etcd的基本操作命令</p>
</li>
<li>
<p>理解k8s调度的过程，预选及优先。影响调度策略的设置</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/kube-scheduler-process.png" alt=""  />
</p>
</li>
<li>
<p>Flannel网络的原理学习，了解网络的流向，帮助定位问题</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/flannel-actual.png" alt=""  />
</p>
</li>
<li>
<p>认证与授权，掌握kubectl、kubelet、rbac及二次开发如何调度API</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/rbac-2.jpg" alt=""  />
</p>
</li>
<li>
<p>利用HPA进行业务动态扩缩容，通过metrics-server了解整个k8s的监控体系</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/hpa-prometheus-custom.png" alt=""  />
</p>
</li>
<li>
<p>PV + PVC</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/storage-class.png" alt=""  />
</p>
</li>
<li>
<p>Helm</p>
<p><img loading="lazy" src="/images/Kubernetes%e8%bf%9b%e9%98%b6%e5%ae%9e%e8%b7%b5/helm3.jpg" alt=""  />
</p>
</li>
</ol>


  </div>

  <footer class="post-footer">
    <ul class="post-tags">
      <li><a href="https://iblog.zone/tags/docker/">docker</a></li>
      <li><a href="https://iblog.zone/tags/k8s/">k8s</a></li>
    </ul>
<nav class="paginav">
  <a class="prev" href="https://iblog.zone/archives/kubernetes%E9%9B%86%E7%BE%A4%E7%9A%84%E6%97%A5%E5%BF%97%E5%8F%8A%E7%9B%91%E6%8E%A7/">
    <span class="title">« 上一页</span>
    <br>
    <span>Kubernetes集群的日志及监控</span>
  </a>
  <a class="next" href="https://iblog.zone/archives/kubernetes%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5%E4%B9%8B%E6%97%85/">
    <span class="title">下一页 »</span>
    <br>
    <span>Kubernetes落地实践之旅</span>
  </a>
</nav>

  </footer>
</article>
    </main>
    
<footer class="footer">
    <span>&copy; 2022 <a href="https://iblog.zone">ylw&#39;s blog</a></span>
    <span>
	& <a href="https://beian.miit.gov.cn" rel="noopener" target="_blank">京ICP备2021039488号</a>
    </span>

    总访客：<span id="busuanzi_value_site_uv"></span>
    总浏览量：<span id="busuanzi_value_site_pv"></span>
    页面访问量：<span id="busuanzi_value_page_pv"></span>

</footer>
<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentColor">
        <path d="M12 6H0l6-6z" />
    </svg>
</a>

<script>
    let menu = document.getElementById('menu')
    if (menu) {
        menu.scrollLeft = localStorage.getItem("menu-scroll-position");
        menu.onscroll = function () {
            localStorage.setItem("menu-scroll-position", menu.scrollLeft);
        }
    }

    document.querySelectorAll('a[href^="#"]').forEach(anchor => {
        anchor.addEventListener("click", function (e) {
            e.preventDefault();
            var id = this.getAttribute("href").substr(1);
            if (!window.matchMedia('(prefers-reduced-motion: reduce)').matches) {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView({
                    behavior: "smooth"
                });
            } else {
                document.querySelector(`[id='${decodeURIComponent(id)}']`).scrollIntoView();
            }
            if (id === "top") {
                history.replaceState(null, null, " ");
            } else {
                history.pushState(null, null, `#${id}`);
            }
        });
    });

</script>
<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };

</script>
<script>
    document.getElementById("theme-toggle").addEventListener("click", () => {
        if (document.body.className.includes("dark")) {
            document.body.classList.remove('dark');
            localStorage.setItem("pref-theme", 'light');
        } else {
            document.body.classList.add('dark');
            localStorage.setItem("pref-theme", 'dark');
        }
    })

</script>
<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerText = '复制';

        function copyingDone() {
            copybutton.innerText = '已复制！';
            setTimeout(() => {
                copybutton.innerText = '复制';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });

        if (container.classList.contains("highlight")) {
            container.appendChild(copybutton);
        } else if (container.parentNode.firstChild == container) {
            
        } else if (codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.nodeName == "TABLE") {
            
            codeblock.parentNode.parentNode.parentNode.parentNode.parentNode.appendChild(copybutton);
        } else {
            
            codeblock.parentNode.appendChild(copybutton);
        }
    });
</script>
</body>

</html>
