[{"content":"1、查看日志，报错如下：\nMay 25 15:50:01 localhost crond[42423]: (root) FAILED to authorize user with PAM (Authentication token is no longer valid; new one required) May 25 15:51:01 localhost crond[43791]: (root) PAM ERROR (Authentication token is no longer valid; new one required) 2、查找原因，可能为密码过期\n查看密码过期时间\n[root@localhost log]# chage -l root Last password change : Feb 21, 2022 Password expires : May 22, 2022 Password inactive : never Account expires : never Minimum number of days between password change : 0 Maximum number of days between password change : 90 Number of days of warning before password expires : 7 今天25日，22日就过期了\n3、设置密码为永不过期\n[root@localhost log]# chage -M 99999 root [root@localhost log]# chage -l root Last password change : Feb 21, 2022 Password expires : never Password inactive : never Account expires : never Minimum number of days between password change : 0 Maximum number of days between password change : 99999 Number of days of warning before password expires : 7 4、查看日志，计划任务正常执行\n","permalink":"https://iblog.zone/archives/linux%E8%AE%A1%E5%88%92%E4%BB%BB%E5%8A%A1%E6%B2%A1%E6%9C%89%E6%AD%A3%E5%B8%B8%E6%89%A7%E8%A1%8C/","summary":"1、查看日志，报错如下：\nMay 25 15:50:01 localhost crond[42423]: (root) FAILED to authorize user with PAM (Authentication token is no longer valid; new one required) May 25 15:51:01 localhost crond[43791]: (root) PAM ERROR (Authentication token is no longer valid; new one required) 2、查找原因，可能为密码过期\n查看密码过期时间\n[root@localhost log]# chage -l root Last password change : Feb 21, 2022 Password expires : May 22, 2022 Password inactive : never Account expires : never Minimum number of days between password change : 0 Maximum number of days between password change : 90 Number of days of warning before password expires : 7 今天25日，22日就过期了","title":"Linux计划任务没有正常执行"},{"content":"1. Kali升级 apt-get update  apt-get dist-upgrade 2. 升级完成后，启动GVM（OpenVas）发现打不开 oot@Fkali:~# gvm-start [*] Please wait for the GVM / OpenVAS services to start. [*] [*] You might need to refresh your browser once it opens. [*] [*] Web UI (Greenbone Security Assistant): https://127.0.0.1:9392  Job for gvmd.service failed because a timeout was exceeded. See \u0026#34;systemctl status gvmd.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details. 3. 按照要求检测原因 oot@Fkali:~# systemctl status gvmd.service ● gvmd.service - Greenbone Vulnerability Manager daemon (gvmd)  Loaded: loaded (/lib/systemd/system/gvmd.service; disabled; vendor preset: disabled)  Active: activating (start) since Thu 2021-08-05 16:42:08 HKT; 31s ago  Docs: man:gvmd(8)  Process: 10055 ExecStart=/usr/sbin/gvmd --osp-vt-update=/run/ospd/ospd.sock (code=exited, status=0/SUCCESS)  Tasks: 0 (limit: 12492)  Memory: 4.0K  CPU: 14ms  CGroup: /system.slice/gvmd.service  Aug 05 16:42:08 Fkali systemd[1]: Starting Greenbone Vulnerability Manager daemon (gvmd)... Aug 05 16:42:08 Fkali systemd[1]: gvmd.service: Can\u0026#39;t open PID file /run/gvm/gvmd.pid (yet?) after start: Operation not permitted 4. 查看PID文件 查看PID文件，发现没有上面的路径：  root@Fkali:~# cd /run/gvm/ root@Fkali:/run/gvm# ls 5. 重新检测安装 oot@Fkali:/run/gvm# sudo gvm-check-setup shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory gvm-check-setup 21.4.1  Test completeness and readiness of GVM-21.4.1 chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory Step 1: Checking OpenVAS (Scanner)...  OK: OpenVAS Scanner is present in version 21.4.1.  OK: Server CA Certificate is present as /var/lib/gvm/CA/servercert.pem. Checking permissions of /var/lib/openvas/gnupg/*  OK: _gvm owns all files in /var/lib/openvas/gnupg  OK: redis-server is present.  OK: scanner (db_address setting) is configured properly using the redis-server socket: /var/run/redis-openvas/redis-server.sock  OK: redis-server is running and listening on socket: /var/run/redis-openvas/redis-server.sock.  OK: redis-server configuration is OK and redis-server is running.  OK: _gvm owns all files in /var/lib/openvas/plugins  OK: NVT collection in /var/lib/openvas/plugins contains 58045 NVTs. Checking that the obsolete redis database has been removed  OK: No old Redis DB  OK: ospd-OpenVAS is present in version 21.4.1. Step 2: Checking GVMD Manager ...  OK: GVM Manager (gvmd) is present in version 21.4.2. Step 3: Checking Certificates ...  OK: GVM client certificate is valid and present as /var/lib/gvm/CA/clientcert.pem.  OK: Your GVM certificate infrastructure passed validation. Step 4: Checking data ...  OK: SCAP data found in /var/lib/gvm/scap-data.  OK: CERT data found in /var/lib/gvm/cert-data. Step 5: Checking Postgresql DB and user ...  OK: Postgresql version and default port are OK.  gvmd | _gvm | UTF8 | en_HK.UTF-8 | en_HK.UTF-8 | Database is wrong version.  ERROR: Database is wrong version. You have installed a new gvmd version  FIX: Run \u0026#39;sudo runuser -u _gvm -- gvmd --migrate\u0026#39;   ERROR: Your GVM-21.4.1 installation is not yet complete!  Please follow the instructions marked with FIX above and run this script again. 6. 执行兼容处理后，再次检测安装 root@Fkali:/run/gvm# sudo gvm-check-setup shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory gvm-check-setup 21.4.1  Test completeness and readiness of GVM-21.4.1 chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory Step 1: Checking OpenVAS (Scanner)...  OK: OpenVAS Scanner is present in version 21.4.1.  OK: Server CA Certificate is present as /var/lib/gvm/CA/servercert.pem. Checking permissions of /var/lib/openvas/gnupg/*  OK: _gvm owns all files in /var/lib/openvas/gnupg  OK: redis-server is present.  OK: scanner (db_address setting) is configured properly using the redis-server socket: /var/run/redis-openvas/redis-server.sock  OK: redis-server is running and listening on socket: /var/run/redis-openvas/redis-server.sock.  OK: redis-server configuration is OK and redis-server is running.  OK: _gvm owns all files in /var/lib/openvas/plugins  OK: NVT collection in /var/lib/openvas/plugins contains 58045 NVTs. Checking that the obsolete redis database has been removed  OK: No old Redis DB  OK: ospd-OpenVAS is present in version 21.4.1. Step 2: Checking GVMD Manager ...  OK: GVM Manager (gvmd) is present in version 21.4.2. Step 3: Checking Certificates ...  OK: GVM client certificate is valid and present as /var/lib/gvm/CA/clientcert.pem.  OK: Your GVM certificate infrastructure passed validation. Step 4: Checking data ...  OK: SCAP data found in /var/lib/gvm/scap-data.  OK: CERT data found in /var/lib/gvm/cert-data. Step 5: Checking Postgresql DB and user ...  OK: Postgresql version and default port are OK.  gvmd | _gvm | UTF8 | en_HK.UTF-8 | en_HK.UTF-8 |  OK: At least one user exists. Step 6: Checking Greenbone Security Assistant (GSA) ... Oops, secure memory pool already initialized  ERROR: Greenbone Security Assistant too old or too new: 21.4.1~dev1  FIX: Please install Greenbone Security Assistant \u0026gt;= 21.04.   ERROR: Your GVM-21.4.1 installation is not yet complete!  Please follow the instructions marked with FIX above and run this script again. 7. 查找安装路径，修改配置文件 root@Fkali:/run/gvm# whereis gvm-check-setup  root@Fkali:/run/gvm# sed -i\u0026#34;\u0026#34; \u0026#39;s/GSA_MAJOR=\u0026#34;21.04\u0026#34;/GSA_MAJOR=\u0026#34;21.4\u0026#34;/g\u0026#39; $^Cusr/bin/gvm-check-setup 8. 重新安装 root@Fkali:/run/gvm# gvm-setup shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory Creating openvas-scanner\u0026#39;s certificate files sh: 0: getcwd() failed: No such file or directory  [\u0026gt;] Creating database shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory could not identify current directory: No such file or directory createuser: error: creation of new role failed: ERROR: role \u0026#34;_gvm\u0026#34; already exists could not identify current directory: No such file or directory createdb: error: database creation failed: ERROR: database \u0026#34;gvmd\u0026#34; already exists shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory could not identify current directory: No such file or directory could not identify current directory: No such file or directory psql: fatal: could not find own program executable shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory could not identify current directory: No such file or directory could not identify current directory: No such file or directory psql: fatal: could not find own program executable shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory could not identify current directory: No such file or directory could not identify current directory: No such file or directory psql: fatal: could not find own program executable shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory could not identify current directory: No such file or directory could not identify current directory: No such file or directory psql: fatal: could not find own program executable [\u0026gt;] Migrating database [\u0026gt;] Checking for admin user shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory could not identify current directory: No such file or directory could not identify current directory: No such file or directory psql: fatal: could not find own program executable [*] Define Feed Import Owner shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [\u0026gt;] Updating OpenVAS feeds [*] Updating: NVT sh: 0: getcwd() failed: No such file or directory sh: 0: getcwd() failed: No such file or directory rsync: [Receiver] getcwd(): No such file or directory (2) rsync error: errors selecting input/output files, dirs (code 3) at util.c(1088) [Receiver=3.2.3] [\u0026gt;] Uploading plugins in Redis [*] Updating: GVMD Data sh: 0: getcwd() failed: No such file or directory rsync: [Receiver] getcwd(): No such file or directory (2) rsync error: errors selecting input/output files, dirs (code 3) at util.c(1088) [Receiver=3.2.3] [*] Updating: Scap Data sh: 0: getcwd() failed: No such file or directory rsync: [Receiver] getcwd(): No such file or directory (2) rsync error: errors selecting input/output files, dirs (code 3) at util.c(1088) [Receiver=3.2.3] [*] Updating: Cert Data sh: 0: getcwd() failed: No such file or directory rsync: [Receiver] getcwd(): No such file or directory (2) rsync error: errors selecting input/output files, dirs (code 3) at util.c(1088) [Receiver=3.2.3] [*] Checking Default scanner 08b69003-5fc2-4037-a479-93b440211c73 OpenVAS /var/run/ospd/ospd.sock 0 OpenVAS Default  [+] Done 9. 再次检测安装 root@Fkali:/run/gvm# sudo gvm-check-setup shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory gvm-check-setup 21.4.1  Test completeness and readiness of GVM-21.4.1 chdir: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory Step 1: Checking OpenVAS (Scanner)...  OK: OpenVAS Scanner is present in version 21.4.1.  OK: Server CA Certificate is present as /var/lib/gvm/CA/servercert.pem. Checking permissions of /var/lib/openvas/gnupg/*  OK: _gvm owns all files in /var/lib/openvas/gnupg  OK: redis-server is present.  OK: scanner (db_address setting) is configured properly using the redis-server socket: /var/run/redis-openvas/redis-server.sock  OK: redis-server is running and listening on socket: /var/run/redis-openvas/redis-server.sock.  OK: redis-server configuration is OK and redis-server is running.  OK: _gvm owns all files in /var/lib/openvas/plugins  OK: NVT collection in /var/lib/openvas/plugins contains 58045 NVTs. Checking that the obsolete redis database has been removed  OK: No old Redis DB  OK: ospd-OpenVAS is present in version 21.4.1. Step 2: Checking GVMD Manager ...  OK: GVM Manager (gvmd) is present in version 21.4.2. Step 3: Checking Certificates ...  OK: GVM client certificate is valid and present as /var/lib/gvm/CA/clientcert.pem.  OK: Your GVM certificate infrastructure passed validation. Step 4: Checking data ...  OK: SCAP data found in /var/lib/gvm/scap-data.  OK: CERT data found in /var/lib/gvm/cert-data. Step 5: Checking Postgresql DB and user ...  OK: Postgresql version and default port are OK.  gvmd | _gvm | UTF8 | en_HK.UTF-8 | en_HK.UTF-8 |  OK: At least one user exists. Step 6: Checking Greenbone Security Assistant (GSA) ... Oops, secure memory pool already initialized  OK: Greenbone Security Assistant is present in version 21.4.1~dev1. Step 7: Checking if GVM services are up and running ...  OK: ospd-openvas service is active.  Starting gvmd service  Waiting for gvmd service  OK: gvmd service is active.  Starting greenbone-security-assistant service  Waiting for greenbone-security-assistant service  OK: greenbone-security-assistant service is active. Step 8: Checking few other requirements...  OK: nmap is present in version 21.4.1~dev1.  OK: ssh-keygen found, LSC credential generation for GNU/Linux targets is likely to work.  WARNING: Could not find makensis binary, LSC credential package generation for Microsoft Windows targets will not work.  SUGGEST: Install nsis.  OK: xsltproc found.  WARNING: Your password policy is empty.  SUGGEST: Edit the /etc/gvm/pwpolicy.conf file to set a password policy.  It seems like your GVM-21.4.1 installation is OK. 10. 启动成功 root@Fkali:/run/gvm# gvm-start shell-init: error retrieving current directory: getcwd: cannot access parent directories: No such file or directory [*] Please wait for the GVM / OpenVAS services to start. [*] [*] You might need to refresh your browser once it opens. [*] [*] Web UI (Greenbone Security Assistant): https://127.0.0.1:9392  ● greenbone-security-assistant.service - Greenbone Security Assistant (gsad)  Loaded: loaded (/lib/systemd/system/greenbone-security-assistant.service; disabled; vendor preset: disabled)  Active: active (running) since Thu 2021-08-05 17:30:51 HKT; 7ms ago  Docs: man:gsad(8)  https://www.greenbone.net  Process: 12680 ExecStart=/usr/sbin/gsad --listen=127.0.0.1 --port=9392 (code=exited, status=0/SUCCESS)  Main PID: 12681 (gsad)  Tasks: 3 (limit: 12492)  Memory: 2.1M  CPU: 10ms  CGroup: /system.slice/greenbone-security-assistant.service  ├─12681 /usr/sbin/gsad --listen=127.0.0.1 --port=9392  └─12682 /usr/sbin/gsad --listen=127.0.0.1 --port=9392  Aug 05 17:30:51 Fkali systemd[1]: Starting Greenbone Security Assistant (gsad)... Aug 05 17:30:51 Fkali gsad[12680]: Oops, secure memory pool already initialized Aug 05 17:30:51 Fkali systemd[1]: Started Greenbone Security Assistant (gsad).  ● gvmd.service - Greenbone Vulnerability Manager daemon (gvmd)  Loaded: loaded (/lib/systemd/system/gvmd.service; disabled; vendor preset: disabled)  Active: active (running) since Thu 2021-08-05 17:30:46 HKT; 5s ago  Docs: man:gvmd(8)  Process: 12653 ExecStart=/usr/sbin/gvmd --osp-vt-update=/run/ospd/ospd.sock (code=exited, status=0/SUCCESS)  Main PID: 12655 (gvmd)  Tasks: 1 (limit: 12492)  Memory: 73.2M  CPU: 296ms  CGroup: /system.slice/gvmd.service  └─12655 gvmd: Waiting for incoming connections  Aug 05 17:30:45 Fkali systemd[1]: Starting Greenbone Vulnerability Manager daemon (gvmd)... Aug 05 17:30:45 Fkali systemd[1]: gvmd.service: Can\u0026#39;t open PID file /run/gvm/gvmd.pid (yet?) after start: Operation not permitted Aug 05 17:30:46 Fkali systemd[1]: Started Greenbone Vulnerability Manager daemon (gvmd).  ● ospd-openvas.service - OpenVAS Wrapper of the Greenbone Vulnerability Management (ospd-openvas)  Loaded: loaded (/lib/systemd/system/ospd-openvas.service; disabled; vendor preset: disabled)  Active: active (running) since Thu 2021-08-05 17:30:45 HKT; 5s ago  Docs: man:ospd-openvas(8)  man:openvas(8)  Process: 12645 ExecStart=/usr/bin/ospd-openvas --unix-socket /run/ospd/ospd.sock --pid-file /run/ospd/ospd-openvas.pid --log-file /var/log/gvm/ospd-openvas.log --lock-file-dir /var/lib/openvas (code=exited, status=0/SUCCESS)  Main PID: 12647 (ospd-openvas)  Tasks: 4 (limit: 12492)  Memory: 20.2M  CPU: 194ms  CGroup: /system.slice/ospd-openvas.service  ├─12647 /usr/bin/python3 /usr/bin/ospd-openvas --unix-socket /run/ospd/ospd.sock --pid-file /run/ospd/ospd-openvas.pid --log-file /var/log/gvm/ospd-openvas.log --lock-file-dir /var/lib/openvas  └─12649 /usr/bin/python3 /usr/bin/ospd-openvas --unix-socket /run/ospd/ospd.sock --pid-file /run/ospd/ospd-openvas.pid --log-file /var/log/gvm/ospd-openvas.log --lock-file-dir /var/lib/openvas  Aug 05 17:30:45 Fkali systemd[1]: Starting OpenVAS Wrapper of the Greenbone Vulnerability Management (ospd-openvas)... Aug 05 17:30:45 Fkali systemd[1]: Started OpenVAS Wrapper of the Greenbone Vulnerability Management (ospd-openvas).  [*] Opening Web UI (https://127.0.0.1:9392) in: 5... 4... 3... 2... 1...   root@Fkali:~/Desktop# ospd-openvas ","permalink":"https://iblog.zone/archives/gvm%E5%8D%87%E7%BA%A7%E5%90%8E%E5%90%AF%E5%8A%A8%E5%BC%82%E5%B8%B8%E5%A4%84%E7%90%86/","summary":"1. Kali升级 apt-get update  apt-get dist-upgrade 2. 升级完成后，启动GVM（OpenVas）发现打不开 oot@Fkali:~# gvm-start [*] Please wait for the GVM / OpenVAS services to start. [*] [*] You might need to refresh your browser once it opens. [*] [*] Web UI (Greenbone Security Assistant): https://127.0.0.1:9392  Job for gvmd.service failed because a timeout was exceeded. See \u0026#34;systemctl status gvmd.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details. 3. 按照要求检测原因 oot@Fkali:~# systemctl status gvmd.service ● gvmd.service - Greenbone Vulnerability Manager daemon (gvmd)  Loaded: loaded (/lib/systemd/system/gvmd.","title":"GVM升级后启动异常处理"},{"content":"1. 问题 docker容器日志导致主机磁盘空间满了。docker logs -f container_name噼里啪啦一大堆，很占用空间，不用的日志可以清理掉了。\n2. 解决方法 2.1 找出Docker容器日志 在linux上，容器日志一般存放在/var/lib/docker/containers/container_id/下面， 以json.log结尾的文件（业务日志）很大，查看各个日志文件大小的脚本docker_log_size.sh，内容如下：\n#!/bin/sh  echo \u0026#34;======== docker containers logs file size ========\u0026#34;  logs=$(find /var/lib/docker/containers/ -name *-json.log)  for log in $logs  do  ls -lh $log  done # chmod +x docker_log_size.sh  # ./docker_log_size.sh 2.2 清理Docker容器日志（治标) 如果docker容器正在运行，那么使用rm -rf方式删除日志后，通过df -h会发现磁盘空间并没有释放。原因是在Linux或者Unix系统中，通过rm -rf或者文件管理器删除文件，将会从文件系统的目录结构上解除链接（unlink）。如果文件是被打开的（有一个进程正在使用），那么进程将仍然可以读取该文件，磁盘空间也一直被占用。正确姿势是cat /dev/null \u0026gt; *-json.log，当然你也可以通过rm -rf删除后重启docker。接下来，提供一个日志清理脚本clean_docker_log.sh，内容如下：\n#!/bin/sh  echo \u0026#34;======== start clean docker containers logs ========\u0026#34;  logs=$(find /var/lib/docker/containers/ -name *-json.log)  for log in $logs  do  echo \u0026#34;clean logs : $log\u0026#34;  cat /dev/null \u0026gt; $log  done  echo \u0026#34;======== end clean docker containers logs ========\u0026#34; # chmod +x clean_docker_log.sh  # ./clean_docker_log.sh 但是，这样清理之后，随着时间的推移，容器日志会像杂草一样，卷土重来。\n2.3 设置Docker容器日志大小（治本）  设置一个容器服务的日志大小上限  上述方法，日志文件迟早又会涨回来。要从根本上解决问题，需要限制容器服务的日志大小上限。这个通过配置容器docker-compose的max-size选项来实现\nnginx:  image: nginx:1.12.1  restart: always  logging:  driver: “json-file”  options:  max-size: “5g” 重启nginx容器之后，其日志文件的大小就被限制在5GB，再也不用担心了。\n 全局设置  新建/etc/docker/daemon.json，若有就不用新建了。添加log-dirver和log-opts参数，样例如下：\n# vim /etc/docker/daemon.json  {  \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;http://f613ce8f.m.daocloud.io\u0026#34;],  \u0026#34;log-driver\u0026#34;:\u0026#34;json-file\u0026#34;,  \u0026#34;log-opts\u0026#34;: {\u0026#34;max-size\u0026#34;:\u0026#34;500m\u0026#34;, \u0026#34;max-file\u0026#34;:\u0026#34;3\u0026#34;} } max-size=500m，意味着一个容器日志大小上限是500M， max-file=3，意味着一个容器有三个日志，分别是id+.json、id+1.json、id+2.json。\n// 重启docker守护进程  # systemctl daemon-reload  # systemctl restart docker  注意：设置的日志大小，只对新建的容器有效。\n 3. 参考文章 https://blog.csdn.net/xunzhaoyao/article/details/72959917 https://www.cnblogs.com/testzcy/p/7904829.html\n","permalink":"https://iblog.zone/archives/docker%E5%AE%B9%E5%99%A8%E6%97%A5%E5%BF%97%E6%9F%A5%E7%9C%8B%E4%B8%8E%E6%B8%85%E7%90%86/","summary":"1. 问题 docker容器日志导致主机磁盘空间满了。docker logs -f container_name噼里啪啦一大堆，很占用空间，不用的日志可以清理掉了。\n2. 解决方法 2.1 找出Docker容器日志 在linux上，容器日志一般存放在/var/lib/docker/containers/container_id/下面， 以json.log结尾的文件（业务日志）很大，查看各个日志文件大小的脚本docker_log_size.sh，内容如下：\n#!/bin/sh  echo \u0026#34;======== docker containers logs file size ========\u0026#34;  logs=$(find /var/lib/docker/containers/ -name *-json.log)  for log in $logs  do  ls -lh $log  done # chmod +x docker_log_size.sh  # ./docker_log_size.sh 2.2 清理Docker容器日志（治标) 如果docker容器正在运行，那么使用rm -rf方式删除日志后，通过df -h会发现磁盘空间并没有释放。原因是在Linux或者Unix系统中，通过rm -rf或者文件管理器删除文件，将会从文件系统的目录结构上解除链接（unlink）。如果文件是被打开的（有一个进程正在使用），那么进程将仍然可以读取该文件，磁盘空间也一直被占用。正确姿势是cat /dev/null \u0026gt; *-json.log，当然你也可以通过rm -rf删除后重启docker。接下来，提供一个日志清理脚本clean_docker_log.sh，内容如下：\n#!/bin/sh  echo \u0026#34;======== start clean docker containers logs ========\u0026#34;  logs=$(find /var/lib/docker/containers/ -name *-json.","title":"Docker容器日志查看与清理"},{"content":"nginx自1.9.0开始提供tcp/udp的反向代理功能，直到1.11.4才开始提供session日志功能。\n启用stream日志配置文件 主配置文件/etc/nginx/nginx.conf增加内容：\nstream { log_format proxy \u0026#39;$remote_addr [$time_local] \u0026#39; \u0026#39;$protocol $status $bytes_sent $bytes_received \u0026#39; \u0026#39;$session_time \u0026#34;$upstream_addr\u0026#34; \u0026#39; \u0026#39;\u0026#34;$upstream_bytes_sent\u0026#34; \u0026#34;$upstream_bytes_received\u0026#34; \u0026#34;$upstream_connect_time\u0026#34;\u0026#39;; access_log /var/log/nginx/tcp-access.log proxy ; open_log_file_cache off; include /etc/nginx/conf.d/*.stream; } 具体的tcp.stream配置文件\n upstream TCP59001 { hash $remote_addr consistent; server 192.168.1.176:59001; } server { listen 59001; proxy_connect_timeout 5s; proxy_timeout 30s; proxy_pass TCP59001; } nginx重读配置并检查tcp session日志的生成 nginx重读配置\nnginx -s reload 检查日志\ntail /var/log/nginx/tcp-access.log\n192.168.3.218 [25/Apr/2017:17:55:57 +0800] TCP 200 103 122 10.671 \u0026#34;192.168.1.176:59001\u0026#34; \u0026#34;122\u0026#34; \u0026#34;103\u0026#34; \u0026#34;0.000\u0026#34; 192.168.3.218 [25/Apr/2017:17:55:57 +0800] TCP 200 55 74 4.714 \u0026#34;192.168.1.176:59001\u0026#34; \u0026#34;74\u0026#34; \u0026#34;55\u0026#34; \u0026#34;0.000\u0026#34; 192.168.3.218 [25/Apr/2017:17:55:57 +0800] TCP 200 71 90 6.171 \u0026#34;192.168.1.176:59001\u0026#34; \u0026#34;90\u0026#34; \u0026#34;71\u0026#34; \u0026#34;0.000\u0026#34; 192.168.3.218 [25/Apr/2017:17:55:57 +0800] TCP 200 55 74 4.707 \u0026#34;192.168.1.176:59001\u0026#34; \u0026#34;74\u0026#34; \u0026#34;55\u0026#34; \u0026#34;0.000\u0026#34; 192.168.9.1 [25/Apr/2017:18:49:20 +0800] TCP 200 3423 3438 3375.851 \u0026#34;192.168.1.176:59003\u0026#34; \u0026#34;3438\u0026#34; \u0026#34;3423\u0026#34; \u0026#34;0.000\u0026#34; 192.168.9.1 [25/Apr/2017:18:54:55 +0800] TCP 200 359 374 334.827 \u0026#34;192.168.1.176:59001\u0026#34; \u0026#34;374\u0026#34; \u0026#34;359\u0026#34; \u0026#34;0.001\u0026#34; 至此配置已经完成，upstream的日志已经顺利记录到文件。\n配置经验  测试发现nginx会等待session结束才会记录到日志文件； session日志只是tcp层面的记录，包括session时间，发送接收字节数等等； session内部发送日志(比如一个socket连接建立起来以后，多次发送心跳数据）需要在应用层面才能记录；  参考 How nginx processes a TCP/UDP session\nModule ngxstreamcore_module\nModule ngxstreamlog_module\nhttp://nginx.org/en/docs/varindex.html log_format 能取到的变量可从此查询\n","permalink":"https://iblog.zone/archives/nginx-stream%E6%97%A5%E5%BF%97%E8%AE%BE%E7%BD%AE/","summary":"nginx自1.9.0开始提供tcp/udp的反向代理功能，直到1.11.4才开始提供session日志功能。\n启用stream日志配置文件 主配置文件/etc/nginx/nginx.conf增加内容：\nstream { log_format proxy \u0026#39;$remote_addr [$time_local] \u0026#39; \u0026#39;$protocol $status $bytes_sent $bytes_received \u0026#39; \u0026#39;$session_time \u0026#34;$upstream_addr\u0026#34; \u0026#39; \u0026#39;\u0026#34;$upstream_bytes_sent\u0026#34; \u0026#34;$upstream_bytes_received\u0026#34; \u0026#34;$upstream_connect_time\u0026#34;\u0026#39;; access_log /var/log/nginx/tcp-access.log proxy ; open_log_file_cache off; include /etc/nginx/conf.d/*.stream; } 具体的tcp.stream配置文件\n upstream TCP59001 { hash $remote_addr consistent; server 192.168.1.176:59001; } server { listen 59001; proxy_connect_timeout 5s; proxy_timeout 30s; proxy_pass TCP59001; } nginx重读配置并检查tcp session日志的生成 nginx重读配置\nnginx -s reload 检查日志\ntail /var/log/nginx/tcp-access.log\n192.168.3.218 [25/Apr/2017:17:55:57 +0800] TCP 200 103 122 10.671 \u0026#34;192.168.1.176:59001\u0026#34; \u0026#34;122\u0026#34; \u0026#34;103\u0026#34; \u0026#34;0.","title":"Nginx Stream日志设置"},{"content":"1.Compose介绍 Docker Compose是一个用来定义和运行复杂应用的Docker工具。一个使用Docker容器的应用，通常由多个容器组成。使用Docker Compose不再需要使用shell脚本来启动容器。\nCompose 通过一个配置文件来管理多个Docker容器，在配置文件中，所有的容器通过services来定义，然后使用docker-compose脚本来启动，停止和重启应用，和应用中的服务以及所有依赖服务的容器，非常适合组合使用多个容器进行开发的场景。\n2.Compose和Docker兼容性    compose文件格式版本 docker版本     3.4 17.09.0+   3.3 17.06.0+   3.2 17.04.0+   3.1 1.13.1+   3.0 1.13.0+   2.3 17.06.0+   2.2 1.13.0+   2.1 1.12.0+   2.0 1.10.0+   1.0 1.9.1.+    Docker版本变化说明：\nDocker从1.13.x版本开始，版本分为企业版EE和社区版CE，版本号也改为按照时间线来发布，比如17.03就是2017年3月。\nDocker的linux发行版的软件仓库从以前的https://apt.dockerproject.org和https://yum.dockerproject.org变更为目前的https://download.docker.com, 软件包名字改为docker-ce和docker-ee。\n3.安装docker Docker的社区版（Docker Community Edition）叫做docker-ce。老版本的Docker包叫做docker或者docker-engine，如果安装了老版本的docker得先卸载然后再安装新版本的docker。docker的发展非常迅速，apt源的更新往往比较滞后。所以docker官网推荐的安装方式都是下载docker安装脚本安装。\n卸载老旧的版本（若未安装过可省略此步）：\n$ sudo apt-get remove docker docker-engine docker.io 安装最新的docker：\n$ curl -fsSL get.docker.com -o get-docker.sh $ sudo sh get-docker.sh shell会提示你输入sudo的密码，然后开始执行最新的docker过程\n或者\n$ curl -sSL https://get.docker.com/ | sh 确认Docker成功最新的docker：\n$ sudo docker run hello-world 4.安装docker-compose 两种最新的docker安装方式\n1.从github上下载docker-compose二进制文件安装  下载最新版的docker-compose文件  sudo curl -L https://github.com/docker/compose/releases/download/1.16.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose 若是github访问太慢，可以用daocloud下载\nsudo curl -L https://get.daocloud.io/docker/compose/releases/download/1.25.1/docker-compose-`uname -s`-`uname -m` -o /usr/local/bin/docker-compose  添加可执行权限  sudo chmod +x /usr/local/bin/docker-compose  测试安装结果  $ docker-compose --version  docker-compose version 1.16.1, build 1719ceb 2.pip安装 sudo pip install docker-compose  5.docker-compose文件结构和示例 docker-compose文件结构 docker-compose.yml:\nversion: \u0026#34;3\u0026#34; services: redis: image: redis:alpine ports: - \u0026#34;6379\u0026#34; networks: - frontend deploy: replicas: 2 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure db: image: postgres:9.4 volumes: - db-data:/var/lib/postgresql/data networks: - backend deploy: placement: constraints: [node.role == manager] vote: image: dockersamples/examplevotingapp_vote:before ports: - 5000:80 networks: - frontend depends_on: - redis deploy: replicas: 2 update_config: parallelism: 2 restart_policy: condition: on-failure result: image: dockersamples/examplevotingapp_result:before ports: - 5001:80 networks: - backend depends_on: - db deploy: replicas: 1 update_config: parallelism: 2 delay: 10s restart_policy: condition: on-failure worker: image: dockersamples/examplevotingapp_worker networks: - frontend - backend deploy: mode: replicated replicas: 1 labels: [APP=VOTING] restart_policy: condition: on-failure delay: 10s max_attempts: 3 window: 120s placement: constraints: [node.role == manager] visualizer: image: dockersamples/visualizer:stable ports: - \u0026#34;8080:8080\u0026#34; stop_grace_period: 1m30s volumes: - \u0026#34;/var/run/docker.sock:/var/run/docker.sock\u0026#34; deploy: placement: constraints: [node.role == manager] networks: frontend: backend: volumes: db-data: docker-compose使用示例 通过docker-compose构建一个在docker中运行的基于python flask框架的web应用。\n **注意：**确保你已经安装了Docker Engine和Docker Compose。 您不需要安装Python或Redis，因为这两个都是由Docker镜像提供的。\n Step 1: 定义python应用\n 1 .创建工程目录  $ mkdir compose_test $ cd compose_test $ mkdir src # 源码文件夹 $ mkdir docker # docker配置文件夹 目录结构如下：\n└── compose_test  ├── docker  │ └── docker-compose.yml  ├── Dockerfile  └── src  ├── app.py  └── requirements.txt  2 .在compose_test/src/目录下创建python flask应用 compose_test/src/app.py文件。  from flask import Flask from redis import Redis  app = Flask(__name__) redis = Redis(host=\u0026#39;redis\u0026#39;, port=6379)  @app.route(\u0026#39;/\u0026#39;) def hello():  count = redis.incr(\u0026#39;hits\u0026#39;)  return \u0026#39;Hello World! I have been seen {}times.\\n\u0026#39;.format(count)  if __name__ == \u0026#34;__main__\u0026#34;:  app.run(host=\u0026#34;0.0.0.0\u0026#34;, debug=True)  3 .创建python 需求文件 compose_test/src/requirements.txt  flask redis Step 2: 创建容器的Dockerfile文件\n一个容器一个Dockerfile文件，在compose_test/目录中创建Dockerfile文件：\nFROM python:3.7  COPY src/ /opt/src WORKDIR /opt/src  RUN pip install -r requirements.txt CMD [\u0026#34;python\u0026#34;, \u0026#34;app.py\u0026#34;] Dockerfile文件告诉docker了如下信息：\n从Python 3.7的镜像开始构建一个容器镜像。\n复制src（即compose_test/src）目录到容器中的/opt/src目录。\n将容器的工作目录设置为/opt/src（通过docker exec -it your_docker_container_id bash 进入容器后的默认目录）。\n安装Python依赖关系。\n将容器的默认命令设置为python app.py。\nStep 3: 定义docker-compose脚本\n在compose_test/docker/目录下创建docker-compose.yml文件，并在里面定义服务，内容如下：\nversion: \u0026#39;3\u0026#39; services: web: build: ../ ports: - \u0026#34;5000:5000\u0026#34; redis: image: redis:3.0.7 这个compose文件定义了两个服务，即定义了web和redis两个容器。\nweb容器：\n 使用当前docker-compose.yml文件所在目录的上级目录（compose_test/Dockerfile）中的Dockerfile构建映像。 将容器上的暴露端口5000映射到主机上的端口5000。 我们使用Flask Web服务器的默认端口5000。  redis容器：\n redis服务使用从Docker Hub提取的官方redis镜像3.0.7版本。  Step 4: 使用Compose构建并运行您的应用程序\n在compose_test/docker/目录下执行docker-compose.yml文件：\n$ docker-compose up # 若是要后台运行： $ docker-compose up -d # 若不使用默认的docker-compose.yml 文件名： $ docker-compose -f server.yml up -d 然后在浏览器中输入http://0.0.0.0:5000/查看运行的应用程序。\nStep 5: 编辑compose文件以添加文件绑定挂载\n上面的代码是在构建时静态复制到容器中的，即通过Dockerfile文件中的COPY src /opt/src命令实现物理主机中的源码复制到容器中，这样在后续物理主机src目录中代码的更改不会反应到容器中。\n可以通过volumes 关键字实现物理主机目录挂载到容器中的功能（同时删除Dockerfile中的COPY指令，不需要创建镜像时将代码打包进镜像，而是通过volums动态挂载，容器和物理host共享数据卷）：\nversion: \u0026#39;3\u0026#39; services: web: build: ../ ports: - \u0026#34;5000:5000\u0026#34; volumes: - ../src:/opt/src redis: image: \u0026#34;redis:3.0.7\u0026#34; 通过volumes（卷）将主机上的项目目录（compose_test/src）挂载到容器中的/opt/src目录，允许您即时修改代码，而无需重新构建镜像。\nStep 6: 重新构建和运行应用程序\n使用更新的compose文件构建应用程序，然后运行它。\n$ docker-compose up -d 6.compose常用服务配置参考 Compose文件是一个定义服务，网络和卷的YAML文件。 Compose文件的默认文件名为docker-compose.yml。\n 提示：您可以对此文件使用.yml或.yaml扩展名。 他们都工作。\n 与docker运行一样，默认情况下，Dockerfile中指定的选项（例如，CMD，EXPOSE，VOLUME，ENV）都被遵守，你不需要在docker-compose.yml中再次指定它们。\n同时你可以使用类似Bash的$ {VARIABLE} 语法在配置值中使用环境变量，有关详细信息，请参阅变量替换。\n本节包含版本3中服务定义支持的所有配置选项。\nbuild build 可以指定包含构建上下文的路径：\nversion: \u0026#39;2\u0026#39; services: webapp: build: ./dir 或者，作为一个对象，该对象具有上下文路径和指定的Dockerfile文件以及args参数值：\nversion: \u0026#39;2\u0026#39; services: webapp: build: context: ./dir dockerfile: Dockerfile-alternate args: buildno: 1 webapp服务将会通过./dir目录下的Dockerfile-alternate文件构建容器镜像。\n如果你同时指定image和build，则compose会通过build指定的目录构建容器镜像，而构建的镜像名为image中指定的镜像名和标签。\nbuild: ./dir image: webapp:tag 这将由./dir构建的名为webapp和标记为tag的镜像。\ncontext\n包含Dockerfile文件的目录路径，或者是git仓库的URL。\n当提供的值是相对路径时，它被解释为相对于当前compose文件的位置。 该目录也是发送到Docker守护程序构建镜像的上下文。\ndockerfile\n备用Docker文件。Compose将使用备用文件来构建。 还必须指定构建路径。\nargs\n添加构建镜像的参数，环境变量只能在构建过程中访问。\n首先，在Dockerfile中指定要使用的参数：\nARG buildno ARG password RUN echo \u0026#34;Build number: $buildno\u0026#34; RUN script-requiring-password.sh \u0026#34;$password\u0026#34; 然后在args键下指定参数。 你可以传递映射或列表：\nbuild: context: . args: buildno: 1 password: secret build: context: . args: - buildno=1 - password=secret  ** 注意：YAML布尔值（true，false，yes，no，on，off）必须用引号括起来，以便解析器将它们解释为字符串。\n image 指定启动容器的镜像，可以是镜像仓库/标签或者镜像id（或者id的前一部分）\nimage: redis image: ubuntu:14.04 image: tutum/influxdb image: example-registry.com:4000/postgresql image: a4bc65fd 如果镜像不存在，Compose将尝试从官方镜像仓库将其pull下来，如果你还指定了build，在这种情况下，它将使用指定的build选项构建它，并使用image指定的名字和标记对其进行标记。\ncontainer_name 指定一个自定义容器名称，而不是生成的默认名称。\ncontainer_name: my-web-container 由于Docker容器名称必须是唯一的，因此如果指定了自定义名称，则无法将服务扩展到多个容器。\nvolumes 卷挂载路径设置。可以设置宿主机路径 （HOST:CONTAINER） 或加上访问模式 （HOST:CONTAINER:ro）,挂载数据卷的默认权限是读写（rw），可以通过ro指定为只读。\n你可以在主机上挂载相对路径，该路径将相对于当前正在使用的Compose配置文件的目录进行扩展。 相对路径应始终以 . 或者 .. 开始。\nvolumes: # 只需指定一个路径，让引擎创建一个卷 - /var/lib/mysql # 指定绝对路径映射 - /opt/data:/var/lib/mysql # 相对于当前compose文件的相对路径 - ./cache:/tmp/cache # 用户家目录相对路径 - ~/configs:/etc/configs/:ro # 命名卷 - datavolume:/var/lib/mysql 但是，如果要跨多个服务并重用挂载卷，请在顶级volumes关键字中命名挂在卷，但是并不强制，如下的示例亦有重用挂载卷的功能，但是不提倡。\nversion: \u0026#34;3\u0026#34; services: web1: build: ./web/ volumes: - ../code:/opt/web/code web2: build: ./web/ volumes: - ../code:/opt/web/code  ** 注意：通过顶级volumes定义一个挂载卷，并从每个服务的卷列表中引用它， 这会替换早期版本的Compose文件格式中volumes_from。\n version: \u0026#34;3\u0026#34; services: db: image: db volumes: - data-volume:/var/lib/db backup: image: backup-service volumes: - data-volume:/var/lib/backup/data volumes: data-volume: command 覆盖容器启动后默认执行的命令。\ncommand: bundle exec thin -p 3000 该命令也可以是一个类似于dockerfile的列表：\ncommand: [\u0026#34;bundle\u0026#34;, \u0026#34;exec\u0026#34;, \u0026#34;thin\u0026#34;, \u0026#34;-p\u0026#34;, \u0026#34;3000\u0026#34;] links 链接到另一个服务中的容器。 请指定服务名称和链接别名（SERVICE：ALIAS），或者仅指定服务名称。\nweb: links: - db - db:database - redis 在当前的web服务的容器中可以通过链接的db服务的别名database访问db容器中的数据库应用，如果没有指定别名，则可直接使用服务名访问。\n链接不需要启用服务进行通信 - 默认情况下，任何服务都可以以该服务的名称到达任何其他服务。 （实际是通过设置/etc/hosts的域名解析，从而实现容器间的通信。故可以像在应用中使用localhost一样使用服务的别名链接其他容器的服务，前提是多个服务容器在一个网络中可路由联通）\nlinks也可以起到和depends_on相似的功能，即定义服务之间的依赖关系，从而确定服务启动的顺序。\nexternal_links 链接到docker-compose.yml 外部的容器，甚至并非 Compose 管理的容器。参数格式跟 links 类似。\nexternal_links: - redis_1 - project_db_1:mysql - project_db_1:postgresql expose 暴露端口，但不映射到宿主机，只被连接的服务访问。\n仅可以指定内部端口为参数\nexpose: - \u0026#34;3000\u0026#34; - \u0026#34;8000\u0026#34; ports 暴露端口信息。\n常用的简单格式：使用宿主：容器 （HOST:CONTAINER）格式或者仅仅指定容器的端口（宿主将会随机选择端口）都可以。\n ** 注意：当使用 HOST:CONTAINER 格式来映射端口时，如果你使用的容器端口小于 60 你可能会得到错误得结果，因为 YAML 将会解析 xx:yy 这种数字格式为 60 进制。所以建议采用字符串格式。\n 简单的短格式：\nports: - \u0026#34;3000\u0026#34; - \u0026#34;3000-3005\u0026#34; - \u0026#34;8000:8000\u0026#34; - \u0026#34;9090-9091:8080-8081\u0026#34; - \u0026#34;49100:22\u0026#34; - \u0026#34;127.0.0.1:8001:8001\u0026#34; - \u0026#34;127.0.0.1:5000-5010:5000-5010\u0026#34; - \u0026#34;6060:6060/udp\u0026#34; 在v3.2中ports的长格式的语法允许配置不能用短格式表示的附加字段。\n长格式：\nports: - target: 80 published: 8080 protocol: tcp mode: host target：容器内的端口\npublished：物理主机的端口\nprotocol：端口协议（tcp或udp）\nmode：host 和ingress 两总模式，host用于在每个节点上发布主机端口，ingress 用于被负载平衡的swarm模式端口。\nrestart no是默认的重启策略，在任何情况下都不会重启容器。 指定为always时，容器总是重新启动。 如果退出代码指示出现故障错误，则on-failure将重新启动容器。\nrestart: \u0026#34;no\u0026#34; restart: always restart: on-failure restart: unless-stopped environment 添加环境变量。 你可以使用数组或字典两种形式。 任何布尔值; true，false，yes，no需要用引号括起来，以确保它们不被YML解析器转换为True或False。\n只给定名称的变量会自动获取它在 Compose 主机上的值，可以用来防止泄露不必要的数据。\nenvironment: RACK_ENV: development SHOW: \u0026#39;true\u0026#39; SESSION_SECRET: environment: - RACK_ENV=development - SHOW=true - SESSION_SECRET  ** 注意：如果你的服务指定了build选项，那么在构建过程中通过environment定义的环境变量将不会起作用。 将使用build的args子选项来定义构建时的环境变量。\n pid 将PID模式设置为主机PID模式。 这就打开了容器与主机操作系统之间的共享PID地址空间。 使用此标志启动的容器将能够访问和操作裸机的命名空间中的其他容器，反之亦然。即打开该选项的容器可以相互通过进程 ID 来访问和操作。\npid: \u0026#34;host\u0026#34; dns 配置 DNS 服务器。可以是一个值，也可以是一个列表。\ndns: 8.8.8.8 dns: - 8.8.8.8 - 9.9.9.9 ","permalink":"https://iblog.zone/archives/docker-compose%E6%95%99%E7%A8%8B/","summary":"1.Compose介绍 Docker Compose是一个用来定义和运行复杂应用的Docker工具。一个使用Docker容器的应用，通常由多个容器组成。使用Docker Compose不再需要使用shell脚本来启动容器。\nCompose 通过一个配置文件来管理多个Docker容器，在配置文件中，所有的容器通过services来定义，然后使用docker-compose脚本来启动，停止和重启应用，和应用中的服务以及所有依赖服务的容器，非常适合组合使用多个容器进行开发的场景。\n2.Compose和Docker兼容性    compose文件格式版本 docker版本     3.4 17.09.0+   3.3 17.06.0+   3.2 17.04.0+   3.1 1.13.1+   3.0 1.13.0+   2.3 17.06.0+   2.2 1.13.0+   2.1 1.12.0+   2.0 1.10.0+   1.0 1.9.1.+    Docker版本变化说明：\nDocker从1.13.x版本开始，版本分为企业版EE和社区版CE，版本号也改为按照时间线来发布，比如17.03就是2017年3月。\nDocker的linux发行版的软件仓库从以前的https://apt.dockerproject.org和https://yum.dockerproject.org变更为目前的https://download.docker.com, 软件包名字改为docker-ce和docker-ee。\n3.安装docker Docker的社区版（Docker Community Edition）叫做docker-ce。老版本的Docker包叫做docker或者docker-engine，如果安装了老版本的docker得先卸载然后再安装新版本的docker。docker的发展非常迅速，apt源的更新往往比较滞后。所以docker官网推荐的安装方式都是下载docker安装脚本安装。\n卸载老旧的版本（若未安装过可省略此步）：\n$ sudo apt-get remove docker docker-engine docker.","title":"Docker Compose教程"},{"content":"1.管理系统与服务器集成 1.1准备工作【应用】   需求\n对之前写过的黑马信息管理系统进行改进,实现可以通过浏览器进行访问的功能\n  准备工作\n  将资料中的黑马管理系统代码拷贝到当前模块下\n  导包的代码可能报错,因为之前的包路径可能和当前代码不一致,将导包的代码修改下\n    业务分析\n 解析URL封装到HttpReques对象 DynamicResourceProcess类（执行指定动态资源的service方法） 定义servlet类完成查询学生、添加学生、删除学生、修改学生的逻辑    项目结构\n  1.2HttpRequest类代码实现【应用】   实现步骤\n 提供一个存储url中用户信息的map集合 提供一个getParamter方法,用于根据请求参数的名称获取请求参数的值 提供一个parseParamter方法,用于解析请求参数把请求参数存储到map集合中    代码实现\n// 此处只给出了新增的代码,其他代码同之前没有变化 public class HttpRequest {   //用来存储请求URL中问号后面的那些数据  //id=1 name=itheima  private Map\u0026lt;String,String\u0026gt; paramterHashMap = new HashMap\u0026lt;\u0026gt;();   //parse --- 获取请求数据 并解析  public void parse(){  try {  SocketChannel socketChannel = (SocketChannel) selectionKey.channel();  StringBuilder sb = new StringBuilder();  //创建一个缓冲区  ByteBuffer byteBuffer = ByteBuffer.allocate(1024);  int len;  //循环读取  while((len = socketChannel.read(byteBuffer)) \u0026gt; 0){  byteBuffer.flip();  sb.append(new String(byteBuffer.array(),0,len));  //System.out.println(new String(byteBuffer.array(),0,len));  byteBuffer.clear();  }  //System.out.println(sb);  parseHttpRequest(sb);   //解析请求参数，把请求参数存储到paramterHashMap集合  parseParamter();   } catch (IOException e) {  e.printStackTrace();  }  }   //解析请求参数，把请求参数存储到paramterHashMap集合  private void parseParamter(){  //获取请求的uri  String requestURI = this.requestURI;  //按照问号进行切割，然后再获取到第二部分  String[] uriInfoArr = requestURI.split(\u0026#34;\\\\?\u0026#34;);  //判断数组的长度，如果长度为2，说明是存在请求参数。  if(uriInfoArr.length == 2){  //获取请求参数内容（问号后面的那些参数）  String paramterInfo = uriInfoArr[1];   //使用\u0026amp;进行切割  String[] paramterInfoArr = paramterInfo.split(\u0026#34;\u0026amp;\u0026#34;);   //遍历数组  //id=1 name=itheima age =23  for (String paramter : paramterInfoArr) {  String[] paramterArr = paramter.split(\u0026#34;=\u0026#34;);  //获取请求参数名称  String paramterName = paramterArr[0];  //获取请求参数的值  String paramterValue = paramterArr[1];  //添加到集合中  paramterHashMap.put(paramterName,paramterValue);  }  }  }   //id=1 name=itheima  //可以根据请求参数的名称来获取请求参数的值  public String getParamter(String name){  return paramterHashMap.get(name);  }  }   1.3DynamicResourceProcess类代码实现【应用】   实现步骤\n获取的uri是包含?后边的数据的,要进行切割,只要?号前边的内容\n  代码实现\n// 此处只给出了新增的代码,其他代码同之前没有变化 public class DynamicResourceProcess {   public void process(HttpRequest httpRequest,HttpResponse httpResponse){  //获取请求的uri  String requestURI = httpRequest.getRequestURI();  //对requestURI进行切割操作  String[] split = requestURI.split(\u0026#34;\\\\?\u0026#34;);  //根据请求的uri到map集合中直接找到对应的servlet的对象  HttpServlet httpServlet = ServletConcurrentHashMap.map.get(split[0]);  System.out.println(httpServlet);  // ...  } }   1.4StudentServlet类代码实现【应用】   实现步骤\n 在service方法中获取请求参数中的数据 判断是要添加学生还是修改学生等 调用对应的方法,执行对应的操作    代码实现\n@WebServlet(urlPatterns = \u0026#34;/servlet/studentservlet\u0026#34;) public class StudentServlet implements HttpServlet {   @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //获取method请求参数  String method = httpRequest.getParamter(\u0026#34;method\u0026#34;);  System.out.println(method);  //判断  if (\u0026#34;addStudent\u0026#34;.equals(method)) {  //添加学生  addStudent(httpRequest, httpResponse);  } else if (\u0026#34;delStudent\u0026#34;.equals(method)) {  //删除学生  delStudent(httpRequest, httpResponse);  } else if (\u0026#34;updateStudent\u0026#34;.equals(method)) {  //修改学生  updateStudent(httpRequest, httpResponse);  } else if (\u0026#34;findStudent\u0026#34;.equals(method)) {  //查询学生  findStudent(httpRequest, httpResponse);  }  }   //查询学生  private void findStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  }   //修改学生  private void updateStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  }   //删除学生  private void delStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  }   //添加学生  private void addStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  } }   1.5查询学生【应用】   实现步骤\n 创建StudentService对象 调用StudentService中的findAllStudent方法，完成学生数据的查询操作 遍历数组，拼接成一个字符串 将拼接的结果响应给浏览器    代码实现\n@WebServlet(urlPatterns = \u0026#34;/servlet/studentservlet\u0026#34;) public class StudentServlet implements HttpServlet {  //1.创建StudentService对象  private StudentService studentService = new StudentService();   @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //获取method请求参数   String method = httpRequest.getParamter(\u0026#34;method\u0026#34;);  System.out.println(method);  //判断  if (\u0026#34;addStudent\u0026#34;.equals(method)) {  //添加学生  addStudent(httpRequest, httpResponse);  } else if (\u0026#34;delStudent\u0026#34;.equals(method)) {  //删除学生  delStudent(httpRequest, httpResponse);  } else if (\u0026#34;updateStudent\u0026#34;.equals(method)) {  //修改学生  updateStudent(httpRequest, httpResponse);  } else if (\u0026#34;findStudent\u0026#34;.equals(method)) {  //查询学生  findStudent(httpRequest, httpResponse);  }  }   //查询学生  private void findStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  //2.调用StudentService中的findAllStudent方法，完成学生数据的查询操作  Student[] allStudent = studentService.findAllStudent();  //3.遍历数组，拼接成一个字符串  StringBuilder sb = new StringBuilder();  for (Student student : allStudent) {  sb.append(student.getId()).append(\u0026#34;, \u0026#34;).append(student.getName()).  append(\u0026#34;, \u0026#34;).append(student.getAge()).append(\u0026#34;, \u0026#34;).  append(student.getBirthday()).append(\u0026#34;\u0026lt;br\u0026gt;\u0026#34;);  }  String result = sb.toString();  //4.将拼接的结果响应给浏览器  //告诉浏览器响应的类型  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  if (result == null || \u0026#34;\u0026#34;.equals(result)) {  httpResponse.write(\u0026#34;暂无学生数据。。。。\u0026#34;);  } else {  httpResponse.write(result);  }  }  //修改学生  private void updateStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  }   //删除学生  private void delStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  }   //添加学生  private void addStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  } }   1.6添加学生【应用】   实现步骤\n 获取id的请求参数 判断id是否重复 如果重复。给浏览器响应，id已经重复 如果id不重复。添加学生。并给浏览器响应添加学生成功    代码实现\n@WebServlet(urlPatterns = \u0026#34;/servlet/studentservlet\u0026#34;) public class StudentServlet implements HttpServlet {  //1.创建StudentService对象  private StudentService studentService = new StudentService();   @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //获取method请求参数   String method = httpRequest.getParamter(\u0026#34;method\u0026#34;);  System.out.println(method);  //判断  if (\u0026#34;addStudent\u0026#34;.equals(method)) {  //添加学生  addStudent(httpRequest, httpResponse);  } else if (\u0026#34;delStudent\u0026#34;.equals(method)) {  //删除学生  delStudent(httpRequest, httpResponse);  } else if (\u0026#34;updateStudent\u0026#34;.equals(method)) {  //修改学生  updateStudent(httpRequest, httpResponse);  } else if (\u0026#34;findStudent\u0026#34;.equals(method)) {  //查询学生  findStudent(httpRequest, httpResponse);  }  }   //查询学生  private void findStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  //...  }  //修改学生  private void updateStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  }  //删除学生  private void delStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  }  //添加学生  private void addStudent(HttpRequest httpRequest, HttpResponse httpResponse) {  //1.获取id的请求参数  String id = httpRequest.getParamter(\u0026#34;id\u0026#34;);  //2.判断id是否重复  boolean exists = studentService.isExists(id);  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  if (exists) {  //3.如果重复。给浏览器响应，id已经重复  httpResponse.write(\u0026#34;id已经存在，请重新输入。。。\u0026#34;);  } else {  //4.如果id不重复。添加学生。并给浏览器响应添加学生成功  String name = httpRequest.getParamter(\u0026#34;name\u0026#34;);  String age = httpRequest.getParamter(\u0026#34;age\u0026#34;);  String birthday = httpRequest.getParamter(\u0026#34;birthday\u0026#34;);  //对数据进行处理  try {  int ageInt = Integer.parseInt(age);  SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;);  Date date = sdf.parse(birthday);  //创建一个学生对象  Student s = new Student();  s.setId(id);  s.setName(name);  s.setAge(age);  s.setBirthday(birthday);  //调用studentservice里面的方法  studentService.addStudent(s);  //给浏览器响应  httpResponse.write(\u0026#34;学生数据添加成功....\u0026#34;);  } catch (ParseException e) {  httpResponse.write(\u0026#34;日期格式不正确，正确的格式为:yyyy-MM-dd\u0026#34;);  e.printStackTrace();  } catch (NumberFormatException e) {  httpResponse.write(\u0026#34;年龄只能为整数\u0026#34;);  e.printStackTrace();  }  //birthday yyyy-MM-dd  }  } }   2.单元测试 2.1概述【理解】 JUnit是一个 Java 编程语言的单元测试工具。JUnit 是一个非常重要的测试工具\n2.2特点【理解】  JUnit是一个开放源代码的测试工具。 提供注解来识别测试方法。 JUnit测试可以让你编写代码更快，并能提高质量。 JUnit优雅简洁。没那么复杂，花费时间较少。 JUnit在一个条中显示进度。如果运行良好则是绿色；如果运行失败，则变成红色。  2.3使用步骤【应用】   使用步骤\n 将junit的jar包导入到工程中 junit-4.9.jar 编写测试方法该测试方法必须是公共的无参数无返回值的非静态方法 在测试方法上使用@Test注解标注该方法是一个测试方法 选中测试方法右键通过junit运行该方法    代码示例\npublic class JunitDemo1 {  @Test  public void add() {  System.out.println(2 / 0);  int a = 10;  int b = 20;  int sum = a + b;  System.out.println(sum);  } }   2.4相关注解【应用】   注解说明\n   注解 含义     @Test 表示测试该方法   @Before 在测试的方法前运行   @After 在测试的方法后运行      代码示例\npublic class JunitDemo2 {  @Before  public void before() {  // 在执行测试代码之前执行，一般用于初始化操作  System.out.println(\u0026#34;before\u0026#34;);  }  @Test  public void test() {  // 要执行的测试代码  System.out.println(\u0026#34;test\u0026#34;);  }  @After  public void after() {  // 在执行测试代码之后执行，一般用于释放资源  System.out.println(\u0026#34;after\u0026#34;);  } }   3.日志 3.1概述【理解】   概述\n程序中的日志可以用来记录程序在运行的时候点点滴滴。并可以进行永久存储。\n  日志与输出语句的区别\n    输出语句 日志技术     取消日志 需要修改代码，灵活性比较差 不需要修改代码，灵活性比较好   输出位置 只能是控制台 可以将日志信息写入到文件或者数据库中   多线程 和业务代码处于一个线程中 多线程方式记录日志，不影响业务代码的性能      3.2日志体系结构和Log4J【理解】   体系结构\n  Log4J\nLog4j是Apache的一个开源项目。\n通过使用Log4j，我们可以控制日志信息输送的目的地是控制台、文件等位置。\n我们也可以控制每一条日志的输出格式。\n通过定义每一条日志信息的级别，我们能够更加细致地控制日志的生成过程。\n最令人感兴趣的就是，这些可以通过一个配置文件来灵活地进行配置，而不需要修改应用的代码。\n  Apache基金会\nApache软件基金会（也就是Apache Software Foundation，简称为ASF），为支持开源软件项目而办的一个非盈利性组织。\n  3.3入门案例【应用】   使用步骤\n 导入log4j的相关jar包 编写log4j配置文件 在代码中获取日志的对象 按照级别设置记录日志信息    代码示例\n// log4j的配置文件,名字为log4j.properties, 放在src根目录下 log4j.rootLogger=debug,my,fileAppender  ### direct log messages to my ### log4j.appender.my=org.apache.log4j.ConsoleAppender log4j.appender.my.ImmediateFlush = true log4j.appender.my.Target=System.out log4j.appender.my.layout=org.apache.log4j.PatternLayout log4j.appender.my.layout.ConversionPattern=%d %t %5p %c{1}:%L - %m%n  # fileAppender��ʾ log4j.appender.fileAppender=org.apache.log4j.FileAppender log4j.appender.fileAppender.ImmediateFlush = true log4j.appender.fileAppender.Append=true log4j.appender.fileAppender.File=D:/log4j-log.log log4j.appender.fileAppender.layout=org.apache.log4j.PatternLayout log4j.appender.fileAppender.layout.ConversionPattern=%d %5p %c{1}:%L - %m%n  // 测试类 public class Log4JTest01 {   //使用log4j的api来获取日志的对象  //弊端：如果以后我们更换日志的实现类，那么下面的代码就需要跟着改  //不推荐使用  //private static final Logger LOGGER = Logger.getLogger(Log4JTest01.class);   //使用slf4j里面的api来获取日志的对象  //好处：如果以后我们更换日志的实现类，那么下面的代码不需要跟着修改  //推荐使用  private static final Logger LOGGER = LoggerFactory.getLogger(Log4JTest01.class);   public static void main(String[] args) {  //1.导入jar包  //2.编写配置文件  //3.在代码中获取日志的对象  //4.按照日志级别设置日志信息  LOGGER.debug(\u0026#34;debug级别的日志\u0026#34;);  LOGGER.info(\u0026#34;info级别的日志\u0026#34;);  LOGGER.warn(\u0026#34;warn级别的日志\u0026#34;);  LOGGER.error(\u0026#34;error级别的日志\u0026#34;);  } }   3.4配置文件详解【理解】   三个核心\n  Loggers(记录器) 日志的级别\nLoggers组件在此系统中常见的五个级别：DEBUG、INFO、WARN、ERROR 和 FATAL。\nDEBUG \u0026lt; INFO \u0026lt; WARN \u0026lt; ERROR \u0026lt; FATAL。\nLog4j有一个规则：只输出级别不低于设定级别的日志信息。\n  Appenders(输出源) 日志要输出的地方\n把日志输出到不同的地方，如控制台（Console）、文件（Files）等。\n org.apache.log4j.ConsoleAppender（控制台） org.apache.log4j.FileAppender（文件）    Layouts(布局) 日志输出的格式\n可以根据自己的喜好规定日志输出的格式\n常用的布局管理器：\n​\torg.apache.log4j.PatternLayout（可以灵活地指定布局模式）\n​ org.apache.log4j.SimpleLayout（包含日志信息的级别和信息字符串）\n​\torg.apache.log4j.TTCCLayout（包含日志产生的时间、线程、类别等信息）\n    配置根Logger\n  格式\nlog4j.rootLogger=日志级别，appenderName1，appenderName2，…\n  日志级别\nOFF、FATAL、ERROR、WARN、INFO、DEBUG、ALL或者自定义的级别。\n  appenderName1\n就是指定日志信息要输出到哪里。可以同时指定多个输出目的地，用逗号隔开。\n例如：log4j.rootLogger＝INFO，ca，fa\n    ConsoleAppender常用的选项\n  ImmediateFlush=true\n表示所有消息都会被立即输出，设为false则不输出，默认值是true。\n  Target=System.err\n默认值是System.out。\n    FileAppender常用的选项\n  ImmediateFlush=true\n表示所有消息都会被立即输出。设为false则不输出，默认值是true\n  Append=false\ntrue表示将消息添加到指定文件中，原来的消息不覆盖。\nfalse则将消息覆盖指定的文件内容，默认值是true。\n  File=D:/logs/logging.log4j\n指定消息输出到logging.log4j文件中\n    PatternLayout常用的选项\n  ConversionPattern=%m%n\n设定以怎样的格式显示消息\n    3.5在项目中的应用【应用】   步骤\n 导入相关的依赖 将资料中的properties配置文件复制到src目录下 在代码中获取日志的对象 按照级别设置记录日志信息    代码实现\n@WebServlet(urlPatterns = \u0026#34;/servlet/loginservlet\u0026#34;) public class LoginServlet implements HttpServlet{   //获取日志的对象  private static final Logger LOGGER = LoggerFactory.getLogger(LoginServlet.class);   @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //处理  System.out.println(\u0026#34;LoginServlet处理了登录请求\u0026#34;);   LOGGER.info(\u0026#34;现在已经处理了登录请求，准备给浏览器响应\u0026#34;);   //响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;登录成功\u0026#34;);  } }   ","permalink":"https://iblog.zone/archives/java%E5%9F%BA%E7%A1%80%E5%8A%A0%E5%BC%BA03/","summary":"1.管理系统与服务器集成 1.1准备工作【应用】   需求\n对之前写过的黑马信息管理系统进行改进,实现可以通过浏览器进行访问的功能\n  准备工作\n  将资料中的黑马管理系统代码拷贝到当前模块下\n  导包的代码可能报错,因为之前的包路径可能和当前代码不一致,将导包的代码修改下\n    业务分析\n 解析URL封装到HttpReques对象 DynamicResourceProcess类（执行指定动态资源的service方法） 定义servlet类完成查询学生、添加学生、删除学生、修改学生的逻辑    项目结构\n  1.2HttpRequest类代码实现【应用】   实现步骤\n 提供一个存储url中用户信息的map集合 提供一个getParamter方法,用于根据请求参数的名称获取请求参数的值 提供一个parseParamter方法,用于解析请求参数把请求参数存储到map集合中    代码实现\n// 此处只给出了新增的代码,其他代码同之前没有变化 public class HttpRequest {   //用来存储请求URL中问号后面的那些数据  //id=1 name=itheima  private Map\u0026lt;String,String\u0026gt; paramterHashMap = new HashMap\u0026lt;\u0026gt;();   //parse --- 获取请求数据 并解析  public void parse(){  try {  SocketChannel socketChannel = (SocketChannel) selectionKey.","title":"Java基础加强03"},{"content":"1.xml 1.1概述【理解】   万维网联盟(W3C)\n万维网联盟(W3C)创建于1994年，又称W3C理事会。1994年10月在麻省理工学院计算机科学实验室成立。 建立者： Tim Berners-Lee (蒂姆·伯纳斯·李)。 是Web技术领域最具权威和影响力的国际中立性技术标准机构。 到目前为止，W3C已发布了200多项影响深远的Web技术标准及实施指南，\n  如广为业界采用的超文本标记语言HTML（标准通用标记语言下的一个应用）、\n  可扩展标记语言XML（标准通用标记语言下的一个子集）\n  以及帮助残障人士有效获得Web信息的无障碍指南（WCAG）等\n    xml概述\nXML的全称为(EXtensible Markup Language)，是一种可扩展的标记语言 标记语言: 通过标签来描述数据的一门语言(标签有时我们也将其称之为元素) 可扩展：标签的名字是可以自定义的,XML文件是由很多标签组成的,而标签名是可以自定义的\n  作用\n 用于进行存储数据和传输数据 作为软件的配置文件    作为配置文件的优势\n 可读性好 可维护性高    1.2标签的规则【应用】   标签由一对尖括号和合法标识符组成\n\u0026lt;student\u0026gt;   标签必须成对出现\n\u0026lt;student\u0026gt; \u0026lt;/student\u0026gt; 前边的是开始标签，后边的是结束标签   特殊的标签可以不成对,但是必须有结束标记\n\u0026lt;address/\u0026gt;   标签中可以定义属性,属性和标签名空格隔开,属性值必须用引号引起来\n\u0026lt;student id=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;/student\u0026gt;   标签需要正确的嵌套\n这是正确的: \u0026lt;student id=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt; \u0026lt;/student\u0026gt; 这是错误的: \u0026lt;student id=\u0026#34;1\u0026#34;\u0026gt;\u0026lt;name\u0026gt;张三\u0026lt;/student\u0026gt;\u0026lt;/name\u0026gt;   1.3语法规则【应用】   语法规则\n  XML文件的后缀名为：xml\n  文档声明必须是第一行第一列\n\u0026lt;?xml version=\u0026ldquo;1.0\u0026rdquo; encoding=\u0026ldquo;UTF-8\u0026rdquo; standalone=\u0026ldquo;yes”?\u0026gt; version：该属性是必须存在的 encoding：该属性不是必须的\n​\t打开当前xml文件的时候应该是使用什么字符编码表(一般取值都是UTF-8)\nstandalone: 该属性不是必须的，描述XML文件是否依赖其他的xml文件，取值为yes/no\n  必须存在一个根标签，有且只能有一个\n  XML文件中可以定义注释信息\n  XML文件中可以存在以下特殊字符\n\u0026amp;lt; \u0026lt; 小于 \u0026amp;gt; \u0026gt; 大于 \u0026amp;amp; \u0026amp; 和号 \u0026amp;apos; \u0026#39; 单引号 \u0026amp;quot; \u0026#34; 引号   XML文件中可以存在CDATA区\n\u0026lt;![CDATA[ …内容… ]]\u0026gt;\n    示例代码\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!--注释的内容--\u0026gt; \u0026lt;!--本xml文件用来描述多个学生信息--\u0026gt; \u0026lt;students\u0026gt;   \u0026lt;!--第一个学生信息--\u0026gt;  \u0026lt;student id=\u0026#34;1\u0026#34;\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;info\u0026gt;学生\u0026amp;lt; \u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;\u0026amp;gt;的信息\u0026lt;/info\u0026gt;  \u0026lt;message\u0026gt; \u0026lt;![CDATA[内容 \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt; \u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ]]]\u0026gt;\u0026lt;/message\u0026gt;  \u0026lt;/student\u0026gt;   \u0026lt;!--第二个学生信息--\u0026gt;  \u0026lt;student id=\u0026#34;2\u0026#34;\u0026gt;  \u0026lt;name\u0026gt;李四\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;24\u0026lt;/age\u0026gt;  \u0026lt;/student\u0026gt;  \u0026lt;/students\u0026gt;   1.4xml解析【应用】   概述\nxml解析就是从xml中获取到数据\n  常见的解析思想\nDOM(Document Object Model)文档对象模型:就是把文档的各个组成部分看做成对应的对象。 会把xml文件全部加载到内存,在内存中形成一个树形结构,再获取对应的值\n  常见的解析工具\n JAXP: SUN公司提供的一套XML的解析的API JDOM: 开源组织提供了一套XML的解析的API-jdom DOM4J: 开源组织提供了一套XML的解析的API-dom4j,全称：Dom For Java pull: 主要应用在Android手机端解析XML    解析的准备工作\n  我们可以通过网站：https://dom4j.github.io/ 去下载dom4j\n今天的资料中已经提供,我们不用再单独下载了,直接使用即可\n  将提供好的dom4j-1.6.1.zip解压,找到里面的dom4j-1.6.1.jar\n  在idea中当前模块下新建一个libs文件夹,将jar包复制到文件夹中\n  选中jar包 -\u0026gt; 右键 -\u0026gt; 选择add as library即可\n    需求\n 解析提供好的xml文件 将解析到的数据封装到学生对象中 并将学生对象存储到ArrayList集合中 遍历集合    代码实现\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!--注释的内容--\u0026gt; \u0026lt;!--本xml文件用来描述多个学生信息--\u0026gt; \u0026lt;students\u0026gt;   \u0026lt;!--第一个学生信息--\u0026gt;  \u0026lt;student id=\u0026#34;1\u0026#34;\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/student\u0026gt;   \u0026lt;!--第二个学生信息--\u0026gt;  \u0026lt;student id=\u0026#34;2\u0026#34;\u0026gt;  \u0026lt;name\u0026gt;李四\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;24\u0026lt;/age\u0026gt;  \u0026lt;/student\u0026gt;  \u0026lt;/students\u0026gt;  // 上边是已经准备好的student.xml文件 public class Student {  private String id;  private String name;  private int age;   public Student() {  }   public Student(String id, String name, int age) {  this.id = id;  this.name = name;  this.age = age;  }   public String getId() {  return id;  }   public void setId(String id) {  this.id = id;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;id=\u0026#39;\u0026#34; + id + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } }  /** * 利用dom4j解析xml文件 */ public class XmlParse {  public static void main(String[] args) throws DocumentException {  //1.获取一个解析器对象  SAXReader saxReader = new SAXReader();  //2.利用解析器把xml文件加载到内存中,并返回一个文档对象  Document document = saxReader.read(new File(\u0026#34;myxml\\\\xml\\\\student.xml\u0026#34;));  //3.获取到根标签  Element rootElement = document.getRootElement();  //4.通过根标签来获取student标签  //elements():可以获取调用者所有的子标签.会把这些子标签放到一个集合中返回.  //elements(\u0026#34;标签名\u0026#34;):可以获取调用者所有的指定的子标签,会把这些子标签放到一个集合中并返回  //List list = rootElement.elements();  List\u0026lt;Element\u0026gt; studentElements = rootElement.elements(\u0026#34;student\u0026#34;);  //System.out.println(list.size());   //用来装学生对象  ArrayList\u0026lt;Student\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();   //5.遍历集合,得到每一个student标签  for (Element element : studentElements) {  //element依次表示每一个student标签   //获取id这个属性  Attribute attribute = element.attribute(\u0026#34;id\u0026#34;);  //获取id的属性值  String id = attribute.getValue();   //获取name标签  //element(\u0026#34;标签名\u0026#34;):获取调用者指定的子标签  Element nameElement = element.element(\u0026#34;name\u0026#34;);  //获取这个标签的标签体内容  String name = nameElement.getText();   //获取age标签  Element ageElement = element.element(\u0026#34;age\u0026#34;);  //获取age标签的标签体内容  String age = ageElement.getText();  // System.out.println(id); // System.out.println(name); // System.out.println(age);   Student s = new Student(id,name,Integer.parseInt(age));  list.add(s);  }  //遍历操作  for (Student student : list) {  System.out.println(student);  }  } }   1.5DTD约束【理解】   什么是约束\n用来限定xml文件中可使用的标签以及属性\n  约束的分类\n DTD schema    编写DTD约束\n  步骤\n  创建一个文件，这个文件的后缀名为.dtd\n  看xml文件中使用了哪些元素\n 可以定义元素   判断元素是简单元素还是复杂元素\n简单元素：没有子元素。 复杂元素：有子元素的元素；\n    代码实现\n\u0026lt;!ELEMENT persons (person)\u0026gt; \u0026lt;!ELEMENT person (name,age)\u0026gt; \u0026lt;!ELEMENT name (#PCDATA)\u0026gt; \u0026lt;!ELEMENT age (#PCDATA)\u0026gt;     引入DTD约束\n  引入DTD约束的三种方法\n  引入本地dtd\n  在xml文件内部引入\n  引入网络dtd\n    代码实现\n  引入本地DTD约束\n// 这是persondtd.dtd文件中的内容,已经提前写好 \u0026lt;!ELEMENT persons (person)\u0026gt; \u0026lt;!ELEMENT person (name,age)\u0026gt; \u0026lt;!ELEMENT name (#PCDATA)\u0026gt; \u0026lt;!ELEMENT age (#PCDATA)\u0026gt;  // 在person1.xml文件中引入persondtd.dtd约束 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE persons SYSTEM \u0026#39;persondtd.dtd\u0026#39;\u0026gt;  \u0026lt;persons\u0026gt;  \u0026lt;person\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/person\u0026gt;  \u0026lt;/persons\u0026gt;   在xml文件内部引入\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE persons [ \u0026lt;!ELEMENT persons (person)\u0026gt;  \u0026lt;!ELEMENT person (name,age)\u0026gt;  \u0026lt;!ELEMENT name (#PCDATA)\u0026gt;  \u0026lt;!ELEMENT age (#PCDATA)\u0026gt;  ]\u0026gt;  \u0026lt;persons\u0026gt;  \u0026lt;person\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/person\u0026gt;  \u0026lt;/persons\u0026gt;   引入网络dtd\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE persons PUBLIC \u0026#34;dtd文件的名称\u0026#34; \u0026#34;dtd文档的URL\u0026#34;\u0026gt;  \u0026lt;persons\u0026gt;  \u0026lt;person\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/person\u0026gt;  \u0026lt;/persons\u0026gt;       DTD语法\n  定义元素\n定义一个元素的格式为：\u0026lt;!ELEMENT 元素名 元素类型\u0026gt; 简单元素：\n​\tEMPTY: 表示标签体为空\n​\tANY: 表示标签体可以为空也可以不为空\n​\tPCDATA: 表示该元素的内容部分为字符串\n复杂元素： ​\t直接写子元素名称. 多个子元素可以使用\u0026rdquo;,\u0026ldquo;或者\u0026rdquo;|\u0026ldquo;隔开； ​\t\u0026ldquo;,\u0026ldquo;表示定义子元素的顺序 ; \u0026ldquo;|\u0026rdquo;: 表示子元素只能出现任意一个 ​\t\u0026ldquo;?\u0026ldquo;零次或一次, \u0026ldquo;+\u0026ldquo;一次或多次, \u0026ldquo;*\u0026ldquo;零次或多次;如果不写则表示出现一次\n  定义属性\n格式\n定义一个属性的格式为：\u0026lt;!ATTLIST 元素名称 属性名称 属性的类型 属性的约束\u0026gt; 属性的类型： ​\tCDATA类型：普通的字符串\n属性的约束:\n​\t// #REQUIRED： 必须的 ​\t// #IMPLIED： 属性不是必需的 ​\t// #FIXED value：属性值是固定的\n  代码实现\n\u0026lt;!ELEMENT persons (person+)\u0026gt; \u0026lt;!ELEMENT person (name,age)\u0026gt; \u0026lt;!ELEMENT name (#PCDATA)\u0026gt; \u0026lt;!ELEMENT age (#PCDATA)\u0026gt; \u0026lt;!ATTLIST person id CDATA #REQUIRED\u0026gt;  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;!DOCTYPE persons SYSTEM \u0026#39;persondtd.dtd\u0026#39;\u0026gt;  \u0026lt;persons\u0026gt;  \u0026lt;person id=\u0026#34;001\u0026#34;\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/person\u0026gt;   \u0026lt;person id = \u0026#34;002\u0026#34;\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/person\u0026gt;  \u0026lt;/persons\u0026gt; ​```     1.6schema约束【理解】   schema和dtd的区别\n  schema约束文件也是一个xml文件，符合xml的语法，这个文件的后缀名.xsd\n  一个xml中可以引用多个schema约束文件，多个schema使用名称空间区分（名称空间类似于java包名）\n  dtd里面元素类型的取值比较单一常见的是PCDATA类型，但是在schema里面可以支持很多个数据类型\n  schema 语法更加的复杂\n    编写schema约束\n  步骤\n1，创建一个文件，这个文件的后缀名为.xsd。 2，定义文档声明 3，schema文件的根标签为： \u0026lt;schema\u0026gt; 4，在\u0026lt;schema\u0026gt;中定义属性： ​\txmlns=http://www.w3.org/2001/XMLSchema 5，在\u0026lt;schema\u0026gt;中定义属性 ： ​\ttargetNamespace =唯一的url地址，指定当前这个schema文件的名称空间。 6，在\u0026lt;schema\u0026gt;中定义属性 ： ​\telementFormDefault=\u0026ldquo;qualified“，表示当前schema文件是一个质量良好的文件。 7，通过element定义元素 8，判断当前元素是简单元素还是复杂元素\n  代码实现\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;schema  xmlns=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34;  targetNamespace=\u0026#34;http://www.itheima.cn/javase\u0026#34;  elementFormDefault=\u0026#34;qualified\u0026#34; \u0026gt;   \u0026lt;!--定义persons复杂元素--\u0026gt;  \u0026lt;element name=\u0026#34;persons\u0026#34;\u0026gt;  \u0026lt;complexType\u0026gt;  \u0026lt;sequence\u0026gt;  \u0026lt;!--定义person复杂元素--\u0026gt;  \u0026lt;element name = \u0026#34;person\u0026#34;\u0026gt;  \u0026lt;complexType\u0026gt;  \u0026lt;sequence\u0026gt;  \u0026lt;!--定义name和age简单元素--\u0026gt;  \u0026lt;element name = \u0026#34;name\u0026#34; type = \u0026#34;string\u0026#34;\u0026gt;\u0026lt;/element\u0026gt;  \u0026lt;element name = \u0026#34;age\u0026#34; type = \u0026#34;string\u0026#34;\u0026gt;\u0026lt;/element\u0026gt;  \u0026lt;/sequence\u0026gt;   \u0026lt;/complexType\u0026gt;  \u0026lt;/element\u0026gt;  \u0026lt;/sequence\u0026gt;  \u0026lt;/complexType\u0026gt;   \u0026lt;/element\u0026gt;  \u0026lt;/schema\u0026gt;     引入schema约束\n  步骤\n1，在根标签上定义属性xmlns=\u0026ldquo;http://www.w3.org/2001/XMLSchema-instance\u0026quot; 2，通过xmlns引入约束文件的名称空间 3，给某一个xmlns属性添加一个标识，用于区分不同的名称空间 ​\t格式为: xmlns:标识=“名称空间地址” ,标识可以是任意的，但是一般取值都是xsi 4，通过xsi:schemaLocation指定名称空间所对应的约束文件路径 ​\t格式为：xsi:schemaLocation = \u0026ldquo;名称空间url 文件路径“\n  代码实现\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt;  \u0026lt;persons  xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xmlns=\u0026#34;http://www.itheima.cn/javase\u0026#34;  xsi:schemaLocation=\u0026#34;http://www.itheima.cn/javase person.xsd\u0026#34; \u0026gt;  \u0026lt;person\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/person\u0026gt;  \u0026lt;/persons\u0026gt; ​```     schema约束定义属性\n  代码示例\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;schema  xmlns=\u0026#34;http://www.w3.org/2001/XMLSchema\u0026#34;  targetNamespace=\u0026#34;http://www.itheima.cn/javase\u0026#34;  elementFormDefault=\u0026#34;qualified\u0026#34; \u0026gt;   \u0026lt;!--定义persons复杂元素--\u0026gt;  \u0026lt;element name=\u0026#34;persons\u0026#34;\u0026gt;  \u0026lt;complexType\u0026gt;  \u0026lt;sequence\u0026gt;  \u0026lt;!--定义person复杂元素--\u0026gt;  \u0026lt;element name = \u0026#34;person\u0026#34;\u0026gt;  \u0026lt;complexType\u0026gt;  \u0026lt;sequence\u0026gt;  \u0026lt;!--定义name和age简单元素--\u0026gt;  \u0026lt;element name = \u0026#34;name\u0026#34; type = \u0026#34;string\u0026#34;\u0026gt;\u0026lt;/element\u0026gt;  \u0026lt;element name = \u0026#34;age\u0026#34; type = \u0026#34;string\u0026#34;\u0026gt;\u0026lt;/element\u0026gt;  \u0026lt;/sequence\u0026gt;   \u0026lt;!--定义属性，required( 必须的)/optional( 可选的)--\u0026gt;  \u0026lt;attribute name=\u0026#34;id\u0026#34; type=\u0026#34;string\u0026#34; use=\u0026#34;required\u0026#34;\u0026gt;\u0026lt;/attribute\u0026gt;  \u0026lt;/complexType\u0026gt;   \u0026lt;/element\u0026gt;  \u0026lt;/sequence\u0026gt;  \u0026lt;/complexType\u0026gt;  \u0026lt;/element\u0026gt;  \u0026lt;/schema\u0026gt;  \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;persons  xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xmlns=\u0026#34;http://www.itheima.cn/javase\u0026#34;  xsi:schemaLocation=\u0026#34;http://www.itheima.cn/javase person.xsd\u0026#34; \u0026gt;  \u0026lt;person id=\u0026#34;001\u0026#34;\u0026gt;  \u0026lt;name\u0026gt;张三\u0026lt;/name\u0026gt;  \u0026lt;age\u0026gt;23\u0026lt;/age\u0026gt;  \u0026lt;/person\u0026gt;  \u0026lt;/persons\u0026gt; ​```     1.7服务器改进【应用】   准备xml文件\n  在当前模块下的webapp目录下新建一个文件夹，名字叫WEB-INF\n  新建一个xml文件，名字叫web.xml\n  将资料中的web.xml文件中引入约束的代码复制到新建的web.xml文件中\n  将要解析的数据配置到xml文件中\n    需求\n把uri和servlet信息放到一个concurrentHashMap集合当中 当浏览器请求一个动态资源时，我们会获取uri对应的servlet来处理当前业务\n  实现步骤\n 导入dom4j的jar包 定义一个XmlParseServletConfig类实现ParseServletConfig接口 在parse方法里面就可以解析xml文件了    代码实现\n// web.xml配置文件中配置的信息 \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34; ?\u0026gt; \u0026lt;web-app xmlns=\u0026#34;http://java.sun.com/xml/ns/javaee\u0026#34;  xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xsi:schemaLocation=\u0026#34;http://java.sun.com/xml/ns/javaee http://java.sun.com/xml/ns/javaee/web-app_2_5.xsd\u0026#34;  version=\u0026#34;2.5\u0026#34;\u0026gt;   \u0026lt;!--在以后需要配置servlet的时候，就直接在这里配置就可以了--\u0026gt;  \u0026lt;servlet\u0026gt;  \u0026lt;servlet-name\u0026gt;LoginServlet\u0026lt;/servlet-name\u0026gt;  \u0026lt;servlet-class\u0026gt;com.itheima.myservlet.LoginServlet\u0026lt;/servlet-class\u0026gt;  \u0026lt;/servlet\u0026gt;   \u0026lt;servlet-mapping\u0026gt;  \u0026lt;servlet-name\u0026gt;LoginServlet\u0026lt;/servlet-name\u0026gt;  \u0026lt;url-pattern\u0026gt;/servlet/loginservlet\u0026lt;/url-pattern\u0026gt;  \u0026lt;/servlet-mapping\u0026gt;  \u0026lt;/web-app\u0026gt;  // 定义一个XmlParseServletConfig类实现ParseServletConfig接口 public class XMLParseServletConfig implements ParseServletConfig {  //定义web.xml文件的路径  private static final String WEB_XML_PATH = \u0026#34;http-dynamic-server/webapp/WEB-INF/web.xml\u0026#34;;  \t//在parse方法里面就可以解析xml文件了  @Override  public void parse() {  try {  //1.创建一个解析器对象(注意:如果解析器对象等不能使用,请检查一下jar包是否导入)  SAXReader saxReader = new SAXReader();   //2.利用解析器把xml文件读取到内存中  Document document = saxReader.read(new File(WEB_XML_PATH));   //3.获取根节点元素对象  Element rootElement = document.getRootElement();   //创建一个Map集合，用来存储servlet的配置信息  HashMap\u0026lt;String,String\u0026gt; servletInfoHashMap = new HashMap\u0026lt;\u0026gt;();   //4.获取根元素对象下所有的servlet元素的对象  List\u0026lt;Element\u0026gt; servletInfos = rootElement.elements(\u0026#34;servlet\u0026#34;);   //5.遍历集合，依次获取到每一个servlet标签对象  for (Element servletInfo : servletInfos) {  //servletInfo依次表示每一个servlet标签对象   //获取到servlet下的servlet-name元素对象，并且获取标签体内容  String servletName = servletInfo.element(\u0026#34;servlet-name\u0026#34;).getText();  //获取到servlet下的servlet-class元素对象，并且获取标签体内容  String servletClass = servletInfo.element(\u0026#34;servlet-class\u0026#34;).getText();   servletInfoHashMap.put(servletName,servletClass);  }   //--------------------servlet-mapping--------------------------------------  //获取到所有的servlet-mapping标签  List\u0026lt;Element\u0026gt; servletMappingInfos = rootElement.elements(\u0026#34;servlet-mapping\u0026#34;);  //遍历集合依次得到每一个servlet-mapping标签  for (Element servletMappingInfo : servletMappingInfos) {  //servletMappingInfo依次表示每一个servlet-mapping标签   //获取servlet-mapping标签标签中的servlet-name标签的标签体内容  String servletName = servletMappingInfo.element(\u0026#34;servlet-name\u0026#34;).getText();   //获取servlet-mapping标签标签中的url-pattern标签的标签体内容  String urlPattern = servletMappingInfo.element(\u0026#34;url-pattern\u0026#34;).getText();   //通过servletName来获取到servlet的全类名  String servletClassName = servletInfoHashMap.get(servletName);   //通过反射来创建这个servlet对象  Class clazz = Class.forName(servletClassName);   //获取该类所实现的所有的接口信息,得到的是一个数组  Class[] interfaces = clazz.getInterfaces();   //定义一个boolean类型的变量  boolean flag = false;  //遍历数组  for (Class clazzInfo : interfaces) {  //判断当前所遍历的接口的字节码对象是否和HttpServlet的字节码文件对象相同  if(clazzInfo == HttpServlet.class){   //如果相同,就需要更改flag值.结束循环  flag = true;  break;  }  }   if(flag){  //true就表示当前的类已经实现了HttpServlet接口  HttpServlet httpServlet = (HttpServlet) clazz.newInstance();  //4.将uri和httpServlet添加到map集合中  ServletConcurrentHashMap.map.put(urlPattern,httpServlet);  }else{  //false就表示当前的类还没有实现HttpServlet接口  throw new NotImplementsHttpServletException(clazz.getName() + \u0026#34;Not Implements HttpServlet\u0026#34;);  }  }  } catch (NotImplementsHttpServletException e) {  e.printStackTrace();  }catch (Exception e) {  e.printStackTrace();  }  } }  public class LoaderResourceRunnable implements Runnable {  @Override  public void run() { // //执行parse方法 // ParseServletConfig parseServletConfig = new PropertiesParseServletConfig(); // parseServletConfig.parse();   ParseServletConfig parseServletConfig = new XMLParseServletConfig();  parseServletConfig.parse();   } }   2.枚举 2.1概述【理解】 为了间接的表示一些固定的值，Java就给我们提供了枚举 是指将变量的值一一列出来,变量的值只限于列举出来的值的范围内\n2.2定义格式【应用】   格式\npublic enum s { \t枚举项1,枚举项2,枚举项3; } 注意: 定义枚举类要用关键字enum   示例代码\n// 定义一个枚举类，用来表示春，夏，秋，冬这四个固定值 public enum Season {  SPRING,SUMMER,AUTUMN,WINTER; }   2.3枚举的特点【理解】   特点\n  所有枚举类都是Enum的子类\n  我们可以通过\u0026quot;枚举类名.枚举项名称\u0026quot;去访问指定的枚举项\n  每一个枚举项其实就是该枚举的一个对象\n  枚举也是一个类，也可以去定义成员变量\n  枚举类的第一行上必须是枚举项，最后一个枚举项后的分号是可以省略的，但是如果枚举类有其他的东西，这个分号就不能省略。建议不要省略\n  枚举类可以有构造器，但必须是private的，它默认的也是private的。\n枚举项的用法比较特殊：枚举(\u0026rdquo;\u0026rdquo;);\n  枚举类也可以有抽象方法，但是枚举项必须重写该方法\n    示例代码\npublic enum Season {   SPRING(\u0026#34;春\u0026#34;){   //如果枚举类中有抽象方法  //那么在枚举项中必须要全部重写  @Override  public void show() {  System.out.println(this.name);  }   },   SUMMER(\u0026#34;夏\u0026#34;){  @Override  public void show() {  System.out.println(this.name);  }  },   AUTUMN(\u0026#34;秋\u0026#34;){  @Override  public void show() {  System.out.println(this.name);  }  },   WINTER(\u0026#34;冬\u0026#34;){  @Override  public void show() {  System.out.println(this.name);  }  };   public String name;   //空参构造  //private Season(){}   //有参构造  private Season(String name){  this.name = name;  }   //抽象方法  public abstract void show(); }  public class EnumDemo {  public static void main(String[] args) {  /* 1.所有枚举类都是Enum的子类 2.我们可以通过\u0026#34;枚举类名.枚举项名称\u0026#34;去访问指定的枚举项 3.每一个枚举项其实就是该枚举的一个对象 4.枚举也是一个类，也可以去定义成员变量 5.枚举类的第一行上必须是枚举项，最后一个枚举项后的分号是可以省略的， 但是如果枚举类有其他的东西，这个分号就不能省略。建议不要省略 6.枚举类可以有构造器，但必须是private的，它默认的也是private的。 枚举项的用法比较特殊：枚举(\u0026#34;\u0026#34;); 7.枚举类也可以有抽象方法，但是枚举项必须重写该方法 */   //第二个特点的演示  //我们可以通过\u0026#34;枚举类名.枚举项名称\u0026#34;去访问指定的枚举项  System.out.println(Season.SPRING);  System.out.println(Season.SUMMER);  System.out.println(Season.AUTUMN);  System.out.println(Season.WINTER);   //第三个特点的演示  //每一个枚举项其实就是该枚举的一个对象  Season spring = Season.SPRING;  } }   2.4枚举的方法【应用】   方法介绍\n   方法名 说明     String name() 获取枚举项的名称   int ordinal() 返回枚举项在枚举类中的索引值   int compareTo(E o) 比较两个枚举项，返回的是索引值的差值   String toString() 返回枚举常量的名称   static \u0026lt;T\u0026gt; T valueOf(Class\u0026lt;T\u0026gt; type,String name) 获取指定枚举类中的指定名称的枚举值   values() 获得所有的枚举项      示例代码\npublic enum Season {  SPRING,SUMMER,AUTUMN,WINTER; }  public class EnumDemo {  public static void main(String[] args) { // String name() 获取枚举项的名称  String name = Season.SPRING.name();  System.out.println(name);  System.out.println(\u0026#34;-----------------------------\u0026#34;);  // int ordinal() 返回枚举项在枚举类中的索引值  int index1 = Season.SPRING.ordinal();  int index2 = Season.SUMMER.ordinal();  int index3 = Season.AUTUMN.ordinal();  int index4 = Season.WINTER.ordinal();  System.out.println(index1);  System.out.println(index2);  System.out.println(index3);  System.out.println(index4);  System.out.println(\u0026#34;-----------------------------\u0026#34;);  // int compareTo(E o) 比较两个枚举项，返回的是索引值的差值  int result = Season.SPRING.compareTo(Season.WINTER);  System.out.println(result);//-3  System.out.println(\u0026#34;-----------------------------\u0026#34;);  // String toString() 返回枚举常量的名称  String s = Season.SPRING.toString();  System.out.println(s);  System.out.println(\u0026#34;-----------------------------\u0026#34;);  // static \u0026lt;T\u0026gt; T valueOf(Class\u0026lt;T\u0026gt; type,String name) // 获取指定枚举类中的指定名称的枚举值  Season spring = Enum.valueOf(Season.class, \u0026#34;SPRING\u0026#34;);  System.out.println(spring);  System.out.println(Season.SPRING == spring);  System.out.println(\u0026#34;-----------------------------\u0026#34;);  // values() 获得所有的枚举项  Season[] values = Season.values();  for (Season value : values) {  System.out.println(value);  }  } }   3.注解 3.1概述【理解】   概述\n对我们的程序进行标注和解释\n  注解和注释的区别\n 注释: 给程序员看的 注解: 给编译器看的    使用注解进行配置配置的优势\n代码更加简洁,方便\n  3.2自定义注解【理解】   格式\npublic @interface 注解名称 {\n​\tpublic 属性类型 属性名() default 默认值 ;\n}\n  属性类型\n 基本数据类型 String Class 注解 枚举 以上类型的一维数组    代码演示\npublic @interface Anno2 { }  public enum Season {  SPRING,SUMMER,AUTUMN,WINTER; }  public @interface Anno1 {   //定义一个基本类型的属性  int a () default 23;   //定义一个String类型的属性  public String name() default \u0026#34;itheima\u0026#34;;   //定义一个Class类型的属性  public Class clazz() default Anno2.class;   //定义一个注解类型的属性  public Anno2 anno() default @Anno2;   //定义一个枚举类型的属性  public Season season() default Season.SPRING;   //以上类型的一维数组  //int数组  public int[] arr() default {1,2,3,4,5};   //枚举数组  public Season[] seasons() default {Season.SPRING,Season.SUMMER};   //value。后期我们在使用注解的时候，如果我们只需要给注解的value属性赋值。  //那么value就可以省略  public String value();  }  //在使用注解的时候如果注解里面的属性没有指定默认值。 //那么我们就需要手动给出注解属性的设置值。 //@Anno1(name = \u0026#34;itheima\u0026#34;) @Anno1(\u0026#34;abc\u0026#34;) public class AnnoDemo { }   注意\n如果只有一个属性需要赋值，并且属性的名称是value，则value可以省略，直接定义值即可\n  自定义注解案例\n  需求\n自定义一个注解@Test,用于指定类的方法上,如果某一个类的方法上使用了该注解,就执行该方法\n  实现步骤\n 自定义一个注解Test,并在类中的某几个方法上加上注解 在测试类中,获取注解所在的类的Class对象 获取类中所有的方法对象 遍历每一个方法对象,判断是否有对应的注解    代码实现\n//表示Test这个注解的存活时间 @Retention(value = RetentionPolicy.RUNTIME) public @interface Test { }  public class UseTest {   //没有使用Test注解  public void show(){  System.out.println(\u0026#34;UseTest....show....\u0026#34;);  }   //使用Test注解  @Test  public void method(){  System.out.println(\u0026#34;UseTest....method....\u0026#34;);  }   //没有使用Test注解  @Test  public void function(){  System.out.println(\u0026#34;UseTest....function....\u0026#34;);  } }  public class AnnoDemo {  public static void main(String[] args) throws ClassNotFoundException, IllegalAccessException, InstantiationException, InvocationTargetException {  //1.通过反射获取UseTest类的字节码文件对象  Class clazz = Class.forName(\u0026#34;com.itheima.myanno3.UseTest\u0026#34;);   //创建对象  UseTest useTest = (UseTest) clazz.newInstance();   //2.通过反射获取这个类里面所有的方法对象  Method[] methods = clazz.getDeclaredMethods();   //3.遍历数组，得到每一个方法对象  for (Method method : methods) {  //method依次表示每一个方法对象。  //isAnnotationPresent(Class\u0026lt;? extends Annotation\u0026gt; annotationClass)  //判断当前方法上是否有指定的注解。  //参数：注解的字节码文件对象  //返回值：布尔结果。 true 存在 false 不存在  if(method.isAnnotationPresent(Test.class)){  method.invoke(useTest);  }  }  } }     3.3元注解【理解】   概述\n元注解就是描述注解的注解\n  元注解介绍\n   元注解名 说明     @Target 指定了注解能在哪里使用   @Retention 可以理解为保留时间(生命周期)   @Inherited 表示修饰的自定义注解可以被子类继承   @Documented 表示该自定义注解，会出现在API文档里面。      示例代码\n@Target({ElementType.FIELD,ElementType.TYPE,ElementType.METHOD}) //指定注解使用的位置（成员变量，类，方法） @Retention(RetentionPolicy.RUNTIME) //指定该注解的存活时间 //@Inherited //指定该注解可以被继承 public @interface Anno { }  @Anno public class Person { }  public class Student extends Person {  public void show(){  System.out.println(\u0026#34;student.......show..........\u0026#34;);  } }  public class StudentDemo {  public static void main(String[] args) throws ClassNotFoundException {  //获取到Student类的字节码文件对象  Class clazz = Class.forName(\u0026#34;com.itheima.myanno4.Student\u0026#34;);   //获取注解。  boolean result = clazz.isAnnotationPresent(Anno.class);  System.out.println(result);  } }   3.4改写服务器【理解】   需求\n目前项目中Servlet和url对应关系,是配置在xml文件中的,将其改为在Servlet类上通过注解配置实现\n  实现步骤\n 定义一个注解(@WebServlet),注解内有一个属性urlPatterns 在servlet类上去使用该注解,来指定当前Servlet的访问路径 创建一个注解解析类(AnnoParseServletConfig),该类实现ParseServletConfig接口 实现parse方法    代码实现\n@Target(ElementType.TYPE) //指定该注解可以使用在类上 @Retention(RetentionPolicy.RUNTIME)//指定该注解的存活时间 --- 为运行期 public @interface WebServlet {   //让用户去指定某一个Servlet在进行访问的时候所对应的请求uri  public String urlPatterns(); }  // 这里只给出了LoginServlet的配置,其他Servlet同理 @WebServlet(urlPatterns = \u0026#34;/servlet/loginservlet\u0026#34;) public class LoginServlet implements HttpServlet{  @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //处理  System.out.println(\u0026#34;LoginServlet处理了登录请求\u0026#34;);   //响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;登录成功\u0026#34;);  } }  public class AnnoParseServletConfig implements ParseServletConfig {   //定义一个servlet路径所对应的常量  public static final String SERVLET_PATH = \u0026#34;http-dynamic-server\\\\src\\\\com\\\\itheima\\\\myservlet\u0026#34;;   //定义包名  public static final String SERVLET_PACKAGE_NAME = \u0026#34;com.itheima.myservlet.\u0026#34;;   @Override  public void parse() {  //获取类名 // 1.获得servlet所在文件夹的路径，并封装成File对象  File file = new File(SERVLET_PATH); // 2.调用listFiles方法，获取文件夹下所有的File对象  File[] servletFiles = file.listFiles(); // 3.遍历数组，获取每一个File对象  for (File servletFile : servletFiles) { // 4.获取File对象的名字（后缀名）  String servletFileName = servletFile.getName().replace(\u0026#34;.java\u0026#34;, \u0026#34;\u0026#34;); // 5.根据包名 + 类名 得到每一个类的全类名  String servletFullName = SERVLET_PACKAGE_NAME + servletFileName;  try { // 6.通过全类名获取字节码文件对象  Class servletClazz = Class.forName(servletFullName);  // 7.判断该类是否有WebServlet注解  if(servletClazz.isAnnotationPresent(WebServlet.class)){  // 8.判断该Servlet类是否实现HttpServlet接口  //获取该类所实现的所有的接口信息,得到的是一个数组  Class[] interfaces = servletClazz.getInterfaces();   //定义一个boolean类型的变量  boolean flag = false;  //遍历数组  for (Class clazzInfo : interfaces) {  //判断当前所遍历的接口的字节码对象是否和HttpServlet的字节码文件对象相同  if(clazzInfo == HttpServlet.class){  //如果相同,就需要更改flag值.结束循环  flag = true;  break;  }  }   if(flag){  // 9.如果满足，则获取注解中的urlPattrens的值，  WebServlet annotation = (WebServlet) servletClazz.getAnnotation(WebServlet.class);  String uri = annotation.urlPatterns();   // 10.创建当前Servlet类对象存入值位置  HttpServlet httpServlet = (HttpServlet) servletClazz.newInstance();  // 11.存入集合的键位置  ServletConcurrentHashMap.map.put(uri,httpServlet);  //  }else{  // 12.如果不满足，抛出异常  //false就表示当前的类还没有实现HttpServlet接口  throw new NotImplementsHttpServletException(servletClazz.getName() + \u0026#34;Not Implements HttpServlet\u0026#34;);  }  }  } catch (NotImplementsHttpServletException e) {  e.printStackTrace();  } catch (Exception e) {  e.printStackTrace();  }  }   }  }  public class LoaderResourceRunnable implements Runnable {  @Override  public void run() { // //执行parse方法 // ParseServletConfig parseServletConfig = new PropertiesParseServletConfig(); // parseServletConfig.parse();  // ParseServletConfig parseServletConfig = new XMLParseServletConfig(); // parseServletConfig.parse();   ParseServletConfig parseServletConfig = new AnnoParseServletConfig();  parseServletConfig.parse();   } }   ","permalink":"https://iblog.zone/archives/java%E5%9F%BA%E7%A1%80%E5%8A%A0%E5%BC%BA02/","summary":"1.xml 1.1概述【理解】   万维网联盟(W3C)\n万维网联盟(W3C)创建于1994年，又称W3C理事会。1994年10月在麻省理工学院计算机科学实验室成立。 建立者： Tim Berners-Lee (蒂姆·伯纳斯·李)。 是Web技术领域最具权威和影响力的国际中立性技术标准机构。 到目前为止，W3C已发布了200多项影响深远的Web技术标准及实施指南，\n  如广为业界采用的超文本标记语言HTML（标准通用标记语言下的一个应用）、\n  可扩展标记语言XML（标准通用标记语言下的一个子集）\n  以及帮助残障人士有效获得Web信息的无障碍指南（WCAG）等\n    xml概述\nXML的全称为(EXtensible Markup Language)，是一种可扩展的标记语言 标记语言: 通过标签来描述数据的一门语言(标签有时我们也将其称之为元素) 可扩展：标签的名字是可以自定义的,XML文件是由很多标签组成的,而标签名是可以自定义的\n  作用\n 用于进行存储数据和传输数据 作为软件的配置文件    作为配置文件的优势\n 可读性好 可维护性高    1.2标签的规则【应用】   标签由一对尖括号和合法标识符组成\n\u0026lt;student\u0026gt;   标签必须成对出现\n\u0026lt;student\u0026gt; \u0026lt;/student\u0026gt; 前边的是开始标签，后边的是结束标签   特殊的标签可以不成对,但是必须有结束标记\n\u0026lt;address/\u0026gt;   标签中可以定义属性,属性和标签名空格隔开,属性值必须用引号引起来\n\u0026lt;student id=\u0026#34;1\u0026#34;\u0026gt; \u0026lt;/student\u0026gt;   标签需要正确的嵌套","title":"Java基础加强02"},{"content":"1.类加载器 1.1类加载器【理解】   作用\n负责将.class文件（存储的物理文件）加载在到内存中\n  1.2类加载的过程【理解】   类加载时机\n 创建类的实例（对象） 调用类的类方法 访问类或者接口的类变量，或者为该类变量赋值 使用反射方式来强制创建某个类或接口对应的java.lang.Class对象 初始化某个类的子类 直接使用java.exe命令来运行某个主类    类加载过程\n  加载\n 通过包名 + 类名，获取这个类，准备用流进行传输 在这个类加载到内存中 加载完毕创建一个class对象    链接\n  验证\n确保Class文件字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身安全\n(文件中的信息是否符合虚拟机规范有没有安全隐患)\n    准备\n负责为类的类变量（被static修饰的变量）分配内存，并设置默认初始化值\n(初始化静态变量)\n    解析\n将类的二进制数据流中的符号引用替换为直接引用\n(本类中如果用到了其他类，此时就需要找到对应的类)\n    初始化\n根据程序员通过程序制定的主观计划去初始化类变量和其他资源\n(静态变量赋值以及初始化其他资源)\n    小结\n 当一个类被使用的时候，才会加载到内存 类加载的过程: 加载、验证、准备、解析、初始化    1.3类加载的分类【理解】   分类\n Bootstrap class loader：虚拟机的内置类加载器，通常表示为null ，并且没有父null Platform class loader：平台类加载器,负责加载JDK中一些特殊的模块 System class loader：系统类加载器,负责加载用户类路径上所指定的类库    类加载器的继承关系\n System的父加载器为Platform Platform的父加载器为Bootstrap    代码演示\npublic class ClassLoaderDemo1 {  public static void main(String[] args) {  //获取系统类加载器  ClassLoader systemClassLoader = ClassLoader.getSystemClassLoader();   //获取系统类加载器的父加载器 --- 平台类加载器  ClassLoader classLoader1 = systemClassLoader.getParent();   //获取平台类加载器的父加载器 --- 启动类加载器  ClassLoader classLoader2 = classLoader1.getParent();   System.out.println(\u0026#34;系统类加载器\u0026#34; + systemClassLoader);  System.out.println(\u0026#34;平台类加载器\u0026#34; + classLoader1);  System.out.println(\u0026#34;启动类加载器\u0026#34; + classLoader2);   } }   1.4双亲委派模型【理解】   介绍\n如果一个类加载器收到了类加载请求，它并不会自己先去加载，而是把这个请求委托给父类的加载器去执行，如果父类加载器还存在其父类加载器，则进一步向上委托，依次递归，请求最终将到达顶层的启动类加载器，如果父类加载器可以完成类加载任务，就成功返回，倘若父类加载器无法完成此加载任务，子加载器才会尝试自己去加载，这就是双亲委派模式\n  1.5ClassLoader 中的两个方法【应用】   方法介绍\n   方法名 说明     public static ClassLoader getSystemClassLoader() 获取系统类加载器   public InputStream getResourceAsStream(String name) 加载某一个资源文件      示例代码\npublic class ClassLoaderDemo2 {  public static void main(String[] args) throws IOException {  //static ClassLoader getSystemClassLoader() 获取系统类加载器  //InputStream getResourceAsStream(String name) 加载某一个资源文件   //获取系统类加载器  ClassLoader systemClassLoader = ClassLoader.getSystemClassLoader();   //利用加载器去加载一个指定的文件  //参数：文件的路径（放在src的根目录下，默认去那里加载）  //返回值：字节流。  InputStream is = systemClassLoader.getResourceAsStream(\u0026#34;prop.properties\u0026#34;);   Properties prop = new Properties();  prop.load(is);   System.out.println(prop);   is.close();  } }   2.反射 2.1反射的概述【理解】   反射机制\n是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法； 对于任意一个对象，都能够调用它的任意属性和方法； 这种动态获取信息以及动态调用对象方法的功能称为Java语言的反射机制。\n  2.2获取Class类对象的三种方式【应用】   三种方式分类\n 类名.class属性 对象名.getClass()方法 Class.forName(全类名)方法    示例代码\npublic class Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   public void study(){  System.out.println(\u0026#34;学生在学习\u0026#34;);  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } } public class ReflectDemo1 {  public static void main(String[] args) throws ClassNotFoundException {  //1.Class类中的静态方法forName(\u0026#34;全类名\u0026#34;)  //全类名:包名 + 类名  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect2.Student\u0026#34;);  System.out.println(clazz);   //2.通过class属性来获取  Class clazz2 = Student.class;  System.out.println(clazz2);   //3.利用对象的getClass方法来获取class对象  //getClass方法是定义在Object类中.  Student s = new Student();  Class clazz3 = s.getClass();  System.out.println(clazz3);   System.out.println(clazz == clazz2);  System.out.println(clazz2 == clazz3);  } }   2.3反射获取构造方法并使用【应用】 2.3.1Class类获取构造方法对象的方法   方法介绍\n   方法名 说明     Constructor[] getConstructors() 返回所有公共构造方法对象的数组   Constructor[] getDeclaredConstructors() 返回所有构造方法对象的数组   Constructor getConstructor(Class\u0026hellip; parameterTypes) 返回单个公共构造方法对象   Constructor getDeclaredConstructor(Class\u0026hellip; parameterTypes) 返回单个构造方法对象      示例代码\npublic class Student {  private String name;  private int age;   //私有的有参构造方法  private Student(String name) {  System.out.println(\u0026#34;name的值为:\u0026#34; + name);  System.out.println(\u0026#34;private...Student...有参构造方法\u0026#34;);  }   //公共的无参构造方法  public Student() {  System.out.println(\u0026#34;public...Student...无参构造方法\u0026#34;);  }   //公共的有参构造方法  public Student(String name, int age) {  System.out.println(\u0026#34;name的值为:\u0026#34; + name + \u0026#34;age的值为:\u0026#34; + age);  System.out.println(\u0026#34;public...Student...有参构造方法\u0026#34;);  } } public class ReflectDemo1 {  public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException {  //method1();  //method2();  //method3();  //method4();  }   private static void method4() throws ClassNotFoundException, NoSuchMethodException {  // Constructor\u0026lt;T\u0026gt; getDeclaredConstructor(Class\u0026lt;?\u0026gt;... parameterTypes)： // 返回单个构造方法对象  //1.获取Class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);  Constructor constructor = clazz.getDeclaredConstructor(String.class);  System.out.println(constructor);  }   private static void method3() throws ClassNotFoundException, NoSuchMethodException {  // Constructor\u0026lt;T\u0026gt; getConstructor(Class\u0026lt;?\u0026gt;... parameterTypes)： // 返回单个公共构造方法对象  //1.获取Class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);  //小括号中,一定要跟构造方法的形参保持一致.  Constructor constructor1 = clazz.getConstructor();  System.out.println(constructor1);   Constructor constructor2 = clazz.getConstructor(String.class, int.class);  System.out.println(constructor2);   //因为Student类中,没有只有一个int的构造,所以这里会报错.  Constructor constructor3 = clazz.getConstructor(int.class);  System.out.println(constructor3);  }   private static void method2() throws ClassNotFoundException {  // Constructor\u0026lt;?\u0026gt;[] getDeclaredConstructors()： // 返回所有构造方法对象的数组  //1.获取Class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);   Constructor[] constructors = clazz.getDeclaredConstructors();  for (Constructor constructor : constructors) {  System.out.println(constructor);  }  }   private static void method1() throws ClassNotFoundException {  // Constructor\u0026lt;?\u0026gt;[] getConstructors()： // 返回所有公共构造方法对象的数组  //1.获取Class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);  Constructor[] constructors = clazz.getConstructors();  for (Constructor constructor : constructors) {  System.out.println(constructor);  }  } }   2.3.2Constructor类用于创建对象的方法   方法介绍\n   方法名 说明     T newInstance(Object\u0026hellip;initargs) 根据指定的构造方法创建对象   setAccessible(boolean flag) 设置为true,表示取消访问检查      示例代码\n// Student类同上一个示例,这里就不在重复提供了 public class ReflectDemo2 {  public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InvocationTargetException, InstantiationException {  //T newInstance(Object... initargs)：根据指定的构造方法创建对象  //method1();  //method2();  //method3();  //method4();   }   private static void method4() throws ClassNotFoundException, NoSuchMethodException, InstantiationException, IllegalAccessException, InvocationTargetException {  //获取一个私有的构造方法并创建对象  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);   //2.获取一个私有化的构造方法.  Constructor constructor = clazz.getDeclaredConstructor(String.class);   //被private修饰的成员,不能直接使用的  //如果用反射强行获取并使用,需要临时取消访问检查  constructor.setAccessible(true);   //3.直接创建对象  Student student = (Student) constructor.newInstance(\u0026#34;zhangsan\u0026#34;);   System.out.println(student);  }   private static void method3() throws ClassNotFoundException, InstantiationException, IllegalAccessException {  //简写格式  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);   //2.在Class类中,有一个newInstance方法,可以利用空参直接创建一个对象  Student student = (Student) clazz.newInstance();//这个方法现在已经过时了,了解一下   System.out.println(student);  }   private static void method2() throws ClassNotFoundException, NoSuchMethodException, InstantiationException, IllegalAccessException, InvocationTargetException {  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);   //2.获取构造方法对象  Constructor constructor = clazz.getConstructor();   //3.利用空参来创建Student的对象  Student student = (Student) constructor.newInstance();   System.out.println(student);  }   private static void method1() throws ClassNotFoundException, NoSuchMethodException, InstantiationException, IllegalAccessException, InvocationTargetException {  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect3.Student\u0026#34;);   //2.获取构造方法对象  Constructor constructor = clazz.getConstructor(String.class, int.class);   //3.利用newInstance创建Student的对象  Student student = (Student) constructor.newInstance(\u0026#34;zhangsan\u0026#34;, 23);   System.out.println(student);  } }   2.3.3小结   获取class对象\n三种方式: Class.forName(“全类名”), 类名.class, 对象名.getClass()\n  获取里面的构造方法对象\ngetConstructor (Class\u0026hellip; parameterTypes) getDeclaredConstructor (Class\u0026hellip; parameterTypes)\n  如果是public的，直接创建对象\nnewInstance(Object\u0026hellip; initargs)\n  如果是非public的，需要临时取消检查，然后再创建对象\nsetAccessible(boolean) 暴力反射\n  2.4反射获取成员变量并使用【应用】 2.4.1Class类获取成员变量对象的方法   方法分类\n   方法名 说明     Field[] getFields() 返回所有公共成员变量对象的数组   Field[] getDeclaredFields() 返回所有成员变量对象的数组   Field getField(String name) 返回单个公共成员变量对象   Field getDeclaredField(String name) 返回单个成员变量对象      示例代码\npublic class Student {   public String name;   public int age;   public String gender;   private int money = 300;   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#34;, gender=\u0026#39;\u0026#34; + gender + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, money=\u0026#34; + money +  \u0026#39;}\u0026#39;;  } } public class ReflectDemo1 {  public static void main(String[] args) throws ClassNotFoundException, NoSuchFieldException {  // method1();  //method2();  //method3();  //method4();   }   private static void method4() throws ClassNotFoundException, NoSuchFieldException {  // Field getDeclaredField(String name)：返回单个成员变量对象  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect4.Student\u0026#34;);   //2.获取money成员变量  Field field = clazz.getDeclaredField(\u0026#34;money\u0026#34;);   //3.打印一下  System.out.println(field);  }   private static void method3() throws ClassNotFoundException, NoSuchFieldException {  // Field getField(String name)：返回单个公共成员变量对象  //想要获取的成员变量必须是真实存在的  //且必须是public修饰的.  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect4.Student\u0026#34;);   //2.获取name这个成员变量  //Field field = clazz.getField(\u0026#34;name\u0026#34;);  //Field field = clazz.getField(\u0026#34;name1\u0026#34;);  Field field = clazz.getField(\u0026#34;money\u0026#34;);   //3.打印一下  System.out.println(field);  }   private static void method2() throws ClassNotFoundException {  // Field[] getDeclaredFields()：返回所有成员变量对象的数组  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect4.Student\u0026#34;);   //2.获取所有的Field对象  Field[] fields = clazz.getDeclaredFields();   //3.遍历  for (Field field : fields) {  System.out.println(field);  }  }   private static void method1() throws ClassNotFoundException {  // Field[] getFields()：返回所有公共成员变量对象的数组   //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect4.Student\u0026#34;);   //2.获取Field对象.  Field[] fields = clazz.getFields();   //3.遍历  for (Field field : fields) {  System.out.println(field);  }  } }   2.4.2Field类用于给成员变量赋值的方法   方法介绍\n   方法名 说明     void set(Object obj, Object value) 赋值   Object get(Object obj) 获取值      示例代码\n// Student类同上一个示例,这里就不在重复提供了 public class ReflectDemo2 {  public static void main(String[] args) throws ClassNotFoundException, NoSuchFieldException, IllegalAccessException, InstantiationException { // Object get(Object obj) 返回由该 Field表示的字段在指定对象上的值。  //method1();  //method2();   }   private static void method2() throws ClassNotFoundException, NoSuchFieldException, InstantiationException, IllegalAccessException {  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect4.Student\u0026#34;);   //2.获取成员变量Field的对象  Field field = clazz.getDeclaredField(\u0026#34;money\u0026#34;);   //3.取消一下访问检查  field.setAccessible(true);   //4.调用get方法来获取值  //4.1创建一个对象  Student student = (Student) clazz.newInstance();  //4.2获取指定对象的money的值  Object o = field.get(student);   //5.打印一下  System.out.println(o);  }   private static void method1() throws ClassNotFoundException, NoSuchFieldException, InstantiationException, IllegalAccessException {  // void set(Object obj, Object value)：给obj对象的成员变量赋值为value  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect4.Student\u0026#34;);   //2.获取name这个Field对象  Field field = clazz.getField(\u0026#34;name\u0026#34;);   //3.利用set方法进行赋值.  //3.1先创建一个Student对象  Student student = (Student) clazz.newInstance();  //3.2有了对象才可以给指定对象进行赋值  field.set(student,\u0026#34;zhangsan\u0026#34;);   System.out.println(student);  } }   2.5反射获取成员方法并使用【应用】 2.5.1Class类获取成员方法对象的方法   方法分类\n   方法名 说明     Method[] getMethods() 返回所有公共成员方法对象的数组，包括继承的   Method[] getDeclaredMethods() 返回所有成员方法对象的数组，不包括继承的   Method getMethod(String name, Class\u0026hellip; parameterTypes) 返回单个公共成员方法对象   Method getDeclaredMethod(String name, Class\u0026hellip; parameterTypes) 返回单个成员方法对象      示例代码\npublic class Student {   //私有的，无参无返回值  private void show() {  System.out.println(\u0026#34;私有的show方法，无参无返回值\u0026#34;);  }   //公共的，无参无返回值  public void function1() {  System.out.println(\u0026#34;function1方法，无参无返回值\u0026#34;);  }   //公共的，有参无返回值  public void function2(String name) {  System.out.println(\u0026#34;function2方法，有参无返回值,参数为\u0026#34; + name);  }   //公共的，无参有返回值  public String function3() {  System.out.println(\u0026#34;function3方法，无参有返回值\u0026#34;);  return \u0026#34;aaa\u0026#34;;  }   //公共的，有参有返回值  public String function4(String name) {  System.out.println(\u0026#34;function4方法，有参有返回值,参数为\u0026#34; + name);  return \u0026#34;aaa\u0026#34;;  } } public class ReflectDemo1 {  public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException {  //method1();  //method2();  //method3();  //method4();  //method5();  }   private static void method5() throws ClassNotFoundException, NoSuchMethodException {  // Method getDeclaredMethod(String name, Class\u0026lt;?\u0026gt;... parameterTypes)： // 返回单个成员方法对象  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect5.Student\u0026#34;);  //2.获取一个成员方法show  Method method = clazz.getDeclaredMethod(\u0026#34;show\u0026#34;);  //3.打印一下  System.out.println(method);  }   private static void method4() throws ClassNotFoundException, NoSuchMethodException {  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect5.Student\u0026#34;);  //2.获取一个有形参的方法function2  Method method = clazz.getMethod(\u0026#34;function2\u0026#34;, String.class);  //3.打印一下  System.out.println(method);  }   private static void method3() throws ClassNotFoundException, NoSuchMethodException {  // Method getMethod(String name, Class\u0026lt;?\u0026gt;... parameterTypes) ： // 返回单个公共成员方法对象  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect5.Student\u0026#34;);  //2.获取成员方法function1  Method method1 = clazz.getMethod(\u0026#34;function1\u0026#34;);  //3.打印一下  System.out.println(method1);  }   private static void method2() throws ClassNotFoundException {  // Method[] getDeclaredMethods()： // 返回所有成员方法对象的数组，不包括继承的  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect5.Student\u0026#34;);   //2.获取Method对象  Method[] methods = clazz.getDeclaredMethods();  //3.遍历一下数组  for (Method method : methods) {  System.out.println(method);  }  }   private static void method1() throws ClassNotFoundException {  // Method[] getMethods()：返回所有公共成员方法对象的数组，包括继承的  //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect5.Student\u0026#34;);  //2.获取成员方法对象  Method[] methods = clazz.getMethods();  //3.遍历  for (Method method : methods) {  System.out.println(method);  }  } }   2.5.2Method类用于执行方法的方法   方法介绍\n   方法名 说明     Object invoke(Object obj, Object\u0026hellip; args) 运行方法    参数一: 用obj对象调用该方法\n参数二: 调用方法的传递的参数(如果没有就不写)\n返回值: 方法的返回值(如果没有就不写)\n  示例代码\npublic class ReflectDemo2 {  public static void main(String[] args) throws ClassNotFoundException, NoSuchMethodException, IllegalAccessException, InstantiationException, InvocationTargetException { // Object invoke(Object obj, Object... args)：运行方法 // 参数一：用obj对象调用该方法 // 参数二：调用方法的传递的参数（如果没有就不写） // 返回值：方法的返回值（如果没有就不写）   //1.获取class对象  Class clazz = Class.forName(\u0026#34;com.itheima.myreflect5.Student\u0026#34;);  //2.获取里面的Method对象 function4  Method method = clazz.getMethod(\u0026#34;function4\u0026#34;, String.class);  //3.运行function4方法就可以了  //3.1创建一个Student对象,当做方法的调用者  Student student = (Student) clazz.newInstance();  //3.2运行方法  Object result = method.invoke(student, \u0026#34;zhangsan\u0026#34;);  //4.打印一下返回值  System.out.println(result);  } }   3.http服务器改写 3.1静态资源和动态资源【理解】   静态资源\n在服务器提前准备好的文件。(图片，文本)\n  动态资源\n在图示的案例中，当用户点击了浏览器上的按钮。 本质上访问的就是服务端的某一个类中的某一个方法。 在方法中，可以写一些判断代码和逻辑代码，让响应的内容，有可能不一样了。 那么，服务端所对应的这个类我们常常将其称之为“动态资源”\n  3.2准备工作【理解】   修改四个地方\n HttpResponse -\u0026gt; 常量WEB_APP_PATH的值与当前模块一致 HttpServer -\u0026gt; main方法中端口改成80 HttpResponse -\u0026gt; 添加一个write方法，添加一个带参数的构造方法 HttpResponse -\u0026gt; 添加一个contentType成员变量，生成对应的set/get方法    示例代码\n// 1.HttpResponse -\u0026gt; 常量WEB_APP_PATH的值与当前模块一致 public class HttpResponse {  ...  public static final String WEB_APP_PATH = \u0026#34;http-dynamic-server\\\\webapp\u0026#34;;  ... }  // 2.HttpServer -\u0026gt; main方法中端口改成80 public class HttpServer {  public static void main(String[] args) throws IOException {  ...  //2.让这个通道绑定一个端口  serverSocketChannel.bind(new InetSocketAddress(80));  ...  } }  // 3.HttpResponse -\u0026gt; 添加一个write方法，添加一个带参数的构造方法 public class HttpResponse {  ...  // 已经提供了selectionKey，所以之前的方法接收这个参数的可以去掉了，直接使用这个即可  // HttpRequest也按照此方式进行优化，定义成员变量，在构造方法中赋值，其他方法直接使用即可  private SelectionKey selectionKey;   public HttpResponse(SelectionKey selectionKey) {  this.selectionKey = selectionKey;  }   //给浏览器响应数据的方法 ---- 浏览器在请求动态资源时,响应数据的方法.  //content:响应的内容  public void write(String content){  }  ... }  public class HttpServer {  public static void main(String[] args) throws IOException {  ...  //响应数据 //修改后的构造方法中要传入参数  HttpResponse httpResponse = new HttpResponse(selectionKey);  ...  } }  // 4.HttpResponse -\u0026gt; 添加一个contentType成员变量，生成对应的set/get方法 public class HttpResponse {  ...  private String contentType;//MIME类型   public String getContentType() {  return contentType;  }  public void setContentTpye(String contentType) {  this.contentType = contentType;  //添加到map集合中  hm.put(\u0026#34;Content-Type\u0026#34;,contentType);  }  ... }   3.3浏览器请求动态资源【理解】   两个小问题\n  服务器如何判断浏览器请求的是静态资源还是动态资源?\n我们可以规定：如果浏览器地址栏中的uri是以”/servlet”开始的，那么就表示请求动态资源\n  在一个项目中有很多类，很多方法。那么请求过来之后，执行哪个方法呢?\n写一个UserServlet类，在类中写service方法 我们可以规定：如果请求动态资源，就创建这个类对象，并调用service方法，表示服务器处理了当前请求\n    实现步骤\n  解析http请求\n  处理浏览器请求\n定义一个UserServlet 类，类中定义service方法，处理浏览器请求动态资源 解析完http请求之后，再判断uri是否以/servlet开头\n  响应\n    示例代码\npublic class UserServlet{  public void service(){  //模拟业务处理 ---- 就可以对这个手机号进行判断验证  System.out.println(\u0026#34;UserServlet处理了用户的请求...\u0026#34;);  } } public class HttpServer {  public static void main(String[] args) throws IOException {  ...  //响应数据  HttpResponse httpResponse = new HttpResponse(selectionKey);  httpResponse.setHttpRequest(httpRequest);   if(httpRequest.getRequestURI().startsWith(\u0026#34;/servlet\u0026#34;)){  //本次请求动态资源  //处理  UserServlet userServlet = new UserServlet();  userServlet.service();  //响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;ok,UserServlet处理了本次请求....\u0026#34;);  }else{  //本次请求静态资源  httpResponse.sendStaticResource();  }  ...  } }  public class HttpResponse {  ... \t//给浏览器响应数据的方法 ---- 浏览器在请求动态资源时,响应数据的方法.  //content:响应的内容  public void write(String content){  //准备响应行数据  this.version = \u0026#34;HTTP/1.1\u0026#34;;  this.status = \u0026#34;200\u0026#34;;  this.desc = \u0026#34;ok\u0026#34;;   //把响应行拼接在一起  String responseLine = this.version + \u0026#34; \u0026#34; + this.status + \u0026#34; \u0026#34; + this.desc + \u0026#34;\\r\\n\u0026#34;;   //准备响应头  StringBuilder sb = new StringBuilder();  Set\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; entries = hm.entrySet();  for (Map.Entry\u0026lt;String, String\u0026gt; entry : entries) {  //entry依次表示每一个键值对对象  //键 --- 响应头的名称  //值 --- 响应头的值  sb.append(entry.getKey()).append(\u0026#34;: \u0026#34;).append(entry.getValue()).append(\u0026#34;\\r\\n\u0026#34;);  }   //处理响应空行  String emptyLine = \u0026#34;\\r\\n\u0026#34;;   //拼接响应行,响应头,响应空行  String result = responseLine + sb.toString() + emptyLine;   try {  //给浏览器响应 响应行,响应头,响应空行  ByteBuffer byteBuffer1 = ByteBuffer.wrap(result.getBytes());  SocketChannel channel = (SocketChannel) selectionKey.channel();  channel.write(byteBuffer1);   //给浏览器响应 响应体  ByteBuffer byteBuffer2 = ByteBuffer.wrap(content.getBytes());  channel.write(byteBuffer2);   //释放资源  channel.close();   } catch (IOException e) {  System.out.println(\u0026#34;响应数据失败....\u0026#34;);  e.printStackTrace();  }   }  ... }   3.4main方法和Servlet优化【理解】   main方法优化\n  需求\n将请求动态资源的代码抽取到一个单独的类单独的方法中，简化main中的代码\n  代码实现\npublic class DynamicResourceProcess {   //执行指定动态资源的service方法  //参数一  //由于后期可能根据用户请求的uri做出相应的处理.  //参数二  //要给用户响应数据,那么就需要使用到httpResponse.  public void process(HttpRequest httpRequest,HttpResponse httpResponse) {  // 创建UserServlet对象,调用service方法,进行处理  UserServlet userServlet = new UserServlet();  userServlet.service();   //给浏览器响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;ok,UserServlet处理了本次请求....\u0026#34;);  } }  public class HttpServer {  public static void main(String[] args) throws IOException {  ...  //响应数据  HttpResponse httpResponse = new HttpResponse(selectionKey);  httpResponse.setHttpRequest(httpRequest);   if(httpRequest.getRequestURI().startsWith(\u0026#34;/servlet\u0026#34;)){  //本次请求动态资源  DynamicResourceProcess drp = new DynamicResourceProcess();  drp.process(httpRequest,httpResponse);  }else{  //本次请求静态资源  httpResponse.sendStaticResource();  }  ...  } }     Servlet优化\n  需求\n将给浏览器响应的代码写到Servlet中\n  代码实现\npublic class UserServlet implements HttpServlet{   //处理浏览器请求的方法  //参数一  //由于后期可能根据用户请求的uri做出相应的处理.  //参数二  //要给用户响应数据,那么就需要使用到httpResponse.  public void service(HttpRequest httpRequest, HttpResponse httpResponse){  //模拟业务处理 ---- 就可以对这个手机号进行判断验证  System.out.println(\u0026#34;UserServlet处理了用户的请求...\u0026#34;);  //给浏览器响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;ok,UserServlet处理了本次请求....\u0026#34;);  } }  public class DynamicResourceProcess {   //执行指定动态资源的service方法  //参数一  //由于后期可能根据用户请求的uri做出相应的处理.  //参数二  //要给用户响应数据,那么就需要使用到httpResponse.  public void process(HttpRequest httpRequest,HttpResponse httpResponse) {  // 创建UserServlet对象,调用service方法,进行处理  UserServlet userServlet = new UserServlet();  userServlet.service(httpRequest,httpResponse);  } }     3.5多个动态资源【理解】   多个动态资源\n针对每一个业务操作，我们都会去定义一个对应的Servlet来完成。 就会在服务端产生很多个Servlet\n  实现步骤\n 定义一个接口HttpServlet，接口中定义service方法。 针对于每一种业务，都定义一个servlet类与之对应，该类实现HttpServlet接口 获取请求的uri，进行判断，调用不同的servlet类中的service方法    代码实现\n// 1.定义一个接口HttpServlet，接口中定义service方法 public interface HttpServlet {   //定义业务处理的方法  public abstract void service(HttpRequest httpRequest, HttpResponse httpResponse); }  // 2.针对于每一种业务，都定义一个servlet类与之对应，该类实现HttpServlet接口 public class UserServlet implements HttpServlet{  //处理浏览器请求的方法  //参数一  //由于后期可能根据用户请求的uri做出相应的处理.  //参数二  //要给用户响应数据,那么就需要使用到httpResponse.  public void service(HttpRequest httpRequest, HttpResponse httpResponse){  //模拟业务处理 ---- 就可以对这个手机号进行判断验证  System.out.println(\u0026#34;UserServlet处理了用户的请求...\u0026#34;);  //给浏览器响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;ok,UserServlet处理了本次请求....\u0026#34;);  } }  public class LoginServlet implements HttpServlet{  @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //处理  System.out.println(\u0026#34;LoginServlet处理了登录请求\u0026#34;);  //响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;登录成功\u0026#34;);  } }  public class RegisterServlet implements HttpServlet{  @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //处理  System.out.println(\u0026#34;RegisterServlet处理了注册请求\u0026#34;);  //响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;注册成功\u0026#34;);  } }  public class SearchServlet implements HttpServlet{  @Override  public void service(HttpRequest httpRequest, HttpResponse httpResponse) {  //处理  System.out.println(\u0026#34;SearchServlet处理了搜索商品请求\u0026#34;);  //响应  httpResponse.setContentTpye(\u0026#34;text/html;charset=UTF-8\u0026#34;);  httpResponse.write(\u0026#34;响应了一些商品信息\u0026#34;);  } }  // 3.获取请求的uri，进行判断，调用不同的servlet类中的service方法 public class DynamicResourceProcess {   public void process(HttpRequest httpRequest,HttpResponse httpResponse){  //获取请求的uri  String requestURI = httpRequest.getRequestURI();   //根据请求的uri进行判断  if(\u0026#34;/servlet/loginservlet\u0026#34;.equals(requestURI)){  //登录请求  LoginServlet loginServlet = new LoginServlet();  loginServlet.service(httpRequest,httpResponse);  }else if(\u0026#34;/servlet/registerservlet\u0026#34;.equals(requestURI)){  //注册请求  RegisterServlet registerServlet = new RegisterServlet();  registerServlet.service(httpRequest,httpResponse);  }else if(\u0026#34;/servlet/searchservlet\u0026#34;.equals(requestURI)){  //搜索商品请求  SearchServlet searchServlet = new SearchServlet();  searchServlet.service(httpRequest,httpResponse);  }else{  //表示默认处理方法  //创建UserServlet对象,调用service方法,进行处理  UserServlet userServlet = new UserServlet();  userServlet.service(httpRequest,httpResponse);  }  } }   3.6通过反射和配置文件优化【理解】   优化步骤\n  把Servlet信息写到properties配置文件中\n格式为：servlet-info=/servlet/UserServlet，全类名；/servlet/loginServlet，全类名\n  定义一个接口ServletConcurrentHashMap，接口中定义ConcurrentHashMap，该集合存储所有的servlet信息\n  定义一个接口ParseServletConfig，该接口中定义一个方法（parse）\n  定义ParseServletConfig的实现类，解析配置文件，并把配置文件中Servlet信息存到map集合中\n  在main方法的第一行，开启一条线程执行解析配置文件的代码\n  修改处理DynamicResourceProcess中的process方法\n    代码实现\n// 1.把Servlet信息写到properties配置文件中 // 在webapp\\config\\servlet-info.properties文件中，写入如下内容 servlet-info=/servlet/loginservlet,com.itheima.myservlet.LoginServlet;/servlet/registerservlet,com.itheima.myservlet.RegisterServlet;/servlet/searchservlet,com.itheima.myservlet.SearchServlet;/servlet/lostpasswordservlet,com.itheima.myservlet.LostPasswordServlet  // 2.定义一个接口ServletConcurrentHashMap，接口中定义ConcurrentHashMap，该集合存储所有的servlet信息 public interface ServletConcurrentHashMap {  //存储请求路径和对应的servlet的map集合  //键: 请求的uri  //值: 对应的Servlet对象  public static final ConcurrentHashMap\u0026lt;String, HttpServlet\u0026gt; map = new ConcurrentHashMap\u0026lt;\u0026gt;(); }  // 3.定义一个接口ParseServletConfig，该接口中定义一个方法（parse） public interface ParseServletConfig {  //解析数据的方法  public abstract void parse(); }  // 4.定义ParseServletConfig的实现类，解析配置文件，并把配置文件中Servlet信息存到map集合中 public class PropertiesParseServletConfig implements ParseServletConfig {  @Override  public void parse() {   try {  //1.读取配置文件中的数据  Properties properties = new Properties();  FileReader fr = new FileReader(\u0026#34;http-dynamic-server/webapp/config/servlet-info.properties\u0026#34;);  properties.load(fr);  fr.close();   //2.获取集合中servlet-info的属性值  String properValue = (String) properties.get(\u0026#34;servlet-info\u0026#34;);  // uri,全类名;uri,全类名   //3.解析  String[] split = properValue.split(\u0026#34;;\u0026#34;);  for (String servletInfo : split) {  String[] servletInfoArr = servletInfo.split(\u0026#34;,\u0026#34;);  String uri = servletInfoArr[0];  String servletName = servletInfoArr[1];   //我们需要通过servletName(全类名)来创建他的对象  Class clazz = Class.forName(servletName);  HttpServlet httpServlet = (HttpServlet) clazz.newInstance();  //4.将uri和httpServlet添加到map集合中  ServletConcurrentHashMap.map.put(uri,httpServlet);  }  } catch (Exception e) {  System.out.println(\u0026#34;解析数据异常.....\u0026#34;);  e.printStackTrace();  }  } }  public class LoaderResourceRunnable implements Runnable {  @Override  public void run() {  //执行parse方法  ParseServletConfig parseServletConfig = new PropertiesParseServletConfig();  parseServletConfig.parse();   } }  // 5.在main方法的第一行，开启一条线程执行解析配置文件的代码 public class HttpServer {  public static void main(String[] args) throws IOException {  //开启一条线程去解析配置文件  new Thread(new LoaderResourceRunnable()).start();  ...  } }  // 6.修改处理DynamicResourceProcess中的process方法 public class DynamicResourceProcess {   public void process(HttpRequest httpRequest,HttpResponse httpResponse){  //获取请求的uri  String requestURI = httpRequest.getRequestURI();  //根据请求的uri到map集合中直接找到对应的servlet的对象  HttpServlet httpServlet = ServletConcurrentHashMap.map.get(requestURI);  //调用service方法对请求进行处理并响应  httpServlet.service(httpRequest,httpResponse);  } }   3.7Servlet忘记实现HttpServlet接口处理【理解】   出现情况\n在写Servlet时，忘记了实现HttpServlet接口\n  导致结果\n在反射创建对象后，强转成HttpServlet时，会报类型转换异常\n  解决方案\n在反射创建对象后，强转成HttpServlet前，进行判断\n如果有实现HttpServlet接口，就进行强转\n否则抛出一个异常\n  代码实现\npublic class PropertiesParseServletConfig implements ParseServletConfig {  @Override  public void parse() {   try {  //1.读取配置文件中的数据  Properties properties = new Properties();  FileReader fr = new FileReader(\u0026#34;http-dynamic-server/webapp/config/servlet-info.properties\u0026#34;);  properties.load(fr);  fr.close();   //2.获取集合中servlet-info的属性值  String properValue = (String) properties.get(\u0026#34;servlet-info\u0026#34;);  // uri,全类名;uri,全类名   //3.解析  String[] split = properValue.split(\u0026#34;;\u0026#34;);  for (String servletInfo : split) {  String[] servletInfoArr = servletInfo.split(\u0026#34;,\u0026#34;);  String uri = servletInfoArr[0];  String servletName = servletInfoArr[1];   //我们需要通过servletName(全类名)来创建他的对象  Class clazz = Class.forName(servletName);   //获取该类所实现的所有的接口信息,得到的是一个数组  Class[] interfaces = clazz.getInterfaces();   //定义一个boolean类型的变量  boolean flag = false;  //遍历数组  for (Class clazzInfo : interfaces) {  //判断当前所遍历的接口的字节码对象是否和HttpServlet的字节码文件对象相同  if(clazzInfo == HttpServlet.class){   //如果相同,就需要更改flag值.结束循环  flag = true;  break;  }  }   if(flag){  //true就表示当前的类已经实现了HttpServlet接口  HttpServlet httpServlet = (HttpServlet) clazz.newInstance();  //4.将uri和httpServlet添加到map集合中  ServletConcurrentHashMap.map.put(uri,httpServlet);  }else{  //false就表示当前的类还没有实现HttpServlet接口  throw new NotImplementsHttpServletException(clazz.getName() + \u0026#34;Not Implements HttpServlet\u0026#34;);  }  }  } catch (NotImplementsHttpServletException e) {  e.printStackTrace();  }catch (Exception e) {  System.out.println(\u0026#34;解析数据异常.....\u0026#34;);  e.printStackTrace();  }  } }   3.8响应404【理解】   出现情况\n客户端浏览器请求了一个服务器中不存在的动态资源\n  导致结果\n服务器中代码出现异常，程序停止\n  解决方案\n如果请求的动态资源不存在，服务器根据请求的uri找到对应的Servlet时为null，继续调用方法会出现异常\n增加一个非空的判断，如果不为null，则继续处理请求，调用方法\n如果为null，则响应404\n  代码实现\npublic class DynamicResourceProcess {  //执行指定动态资源的service方法  //参数一  //由于后期可能根据用户请求的uri做出相应的处理.  //参数二  //要给用户响应数据,那么就需要使用到httpResponse.  public void process(HttpRequest httpRequest,HttpResponse httpResponse){  //获取请求的uri  String requestURI = httpRequest.getRequestURI();  //根据请求的uri到map集合中直接找到对应的servlet的对象  HttpServlet httpServlet = ServletConcurrentHashMap.map.get(requestURI);  if(httpServlet != null){  //调用service方法对请求进行处理并响应  httpServlet.service(httpRequest,httpResponse);  }else{  //浏览器请求的动态资源不存在  //响应404  response404(httpResponse);  }  }  //浏览器请求动态资源不存在,响应404的方法  private void response404(HttpResponse httpResponse) {  try {  //准备响应行  String responseLine = \u0026#34;HTTP/1.1 404 NOT FOUND\\r\\n\u0026#34;;  //准备响应头  String responseHeader = \u0026#34;Content-Type: text/html;charset=UTF-8\\r\\n\u0026#34;;  //准备响应空行  String emptyLine = \u0026#34;\\r\\n\u0026#34;;  //拼接在一起  String result = responseLine + responseHeader + emptyLine;   //把响应行,响应头,响应空行去响应给浏览器  SelectionKey selectionKey = httpResponse.getSelectionKey();  SocketChannel channel = (SocketChannel) selectionKey.channel();   ByteBuffer byteBuffer1 = ByteBuffer.wrap(result.getBytes());  channel.write(byteBuffer1);   //给浏览器 响应 响应体内容  ByteBuffer byteBuffer2 = ByteBuffer.wrap(\u0026#34;404 NOT FOUND....\u0026#34;.getBytes());  channel.write(byteBuffer2);   //释放资源  channel.close();  } catch (IOException e) {  e.printStackTrace();  }  } }   ","permalink":"https://iblog.zone/archives/java%E5%9F%BA%E7%A1%80%E5%8A%A0%E5%BC%BA01/","summary":"1.类加载器 1.1类加载器【理解】   作用\n负责将.class文件（存储的物理文件）加载在到内存中\n  1.2类加载的过程【理解】   类加载时机\n 创建类的实例（对象） 调用类的类方法 访问类或者接口的类变量，或者为该类变量赋值 使用反射方式来强制创建某个类或接口对应的java.lang.Class对象 初始化某个类的子类 直接使用java.exe命令来运行某个主类    类加载过程\n  加载\n 通过包名 + 类名，获取这个类，准备用流进行传输 在这个类加载到内存中 加载完毕创建一个class对象    链接\n  验证\n确保Class文件字节流中包含的信息符合当前虚拟机的要求，并且不会危害虚拟机自身安全\n(文件中的信息是否符合虚拟机规范有没有安全隐患)\n    准备\n负责为类的类变量（被static修饰的变量）分配内存，并设置默认初始化值\n(初始化静态变量)\n    解析\n将类的二进制数据流中的符号引用替换为直接引用\n(本类中如果用到了其他类，此时就需要找到对应的类)\n    初始化\n根据程序员通过程序制定的主观计划去初始化类变量和其他资源\n(静态变量赋值以及初始化其他资源)\n    小结\n 当一个类被使用的时候，才会加载到内存 类加载的过程: 加载、验证、准备、解析、初始化    1.","title":"Java基础加强01"},{"content":"Docker 在不重建容器的情况下，日志文件默认会一直追加，时间一长会逐渐占满服务器的硬盘的空间，内存消耗也会一直增加，本篇来了解一些控制日志文件的方法。\n 清理单个文件 运行时控制 全局配置  Docker 的日志文件存在 /var/lib/docker/containers 目录中，通过下面的命令可以将日志文件夹根据升序的方式罗列出来。\n$ sudo du -d1 -h /var/lib/docker/containers | sort -h  28K /var/lib/docker/containers/0db860afe94df368335c2e96f290275f4c396b996b4e8d22770b01baafd9982c 36K /var/lib/docker/containers/6ee184044661c436b44769d56c203f1fc296dbfe08f6ed4cf79aa6fb8cae6659 44K /var/lib/docker/containers/66c44231981fcb5ecd33bf0fc3390e71c5cbbabb839d79441eb3317b8500d551 60K /var/lib/docker/containers/bc4136199037e73d712614ef57de0915d294cbe51045d213f0d822d71a86cf2c 344K /var/lib/docker/containers/7bd3a179cf67b1537e0965c1d1f518420ac5d4cd151ecb75c37ada8c2347ca6b 984K /var/lib/docker/containers/6bd1f79f16b8b06f2bd203dd84443004ba08c150ac51d23fa620e8b2cbf4b773 1.7M /var/lib/docker/containers/a93a4275571b0033367f9cab8213c467b21a03c600e2203195640b5a5bc7f523 4.4M /var/lib/docker/containers/082564c5bdb19b642491b09419a69061122483c0f959a36eb186dd1fec53c163 14M /var/lib/docker/containers/05fc24ef7a14e31e4557c9881482d350cfb05f2f1cb870638de344581154ca01 32M /var/lib/docker/containers/5d70c82942083d16593670058aefed339cfe874c9027205b1e6eb8e569894d65 129M /var/lib/docker/containers/a88d104d20e5ee58ffeaeecbb559b3231c5b8c73ad1443538928ebeae4ff705c 285M /var/lib/docker/containers/b623602a667c0b31068563f244610a548ed055ff9802197f372ff436a294ab5c 917M /var/lib/docker/containers/3d71c509ab6aea34400d37f6c006914eed2cb05e6e6cd07b3ee03eb783dc367b 1.4G /var/lib/docker/containers 有三种方式可以清理日志文件\n清理单个文件 感觉哪个容器的日志太大就清理哪个\n$ sudo sh -c \u0026#34;cat /dev/null \u0026gt; ${log_file}\u0026#34; ${log_file} 就是日志文件，可以通过 find 命令查找全部日志\n$ sudo find /var/lib/docker/containers -name *.log  /var/lib/docker/containers/3d71c509ab6aea34400d37f6c006914eed2cb05e6e6cd07b3ee03eb783dc367b/3d71c509ab6aea34400d37f6c006914eed2cb05e6e6cd07b3ee03eb783dc367b-json.log /var/lib/docker/containers/0db860afe94df368335c2e96f290275f4c396b996b4e8d22770b01baafd9982c/0db860afe94df368335c2e96f290275f4c396b996b4e8d22770b01baafd9982c-json.log /var/lib/docker/containers/bc4136199037e73d712614ef57de0915d294cbe51045d213f0d822d71a86cf2c/bc4136199037e73d712614ef57de0915d294cbe51045d213f0d822d71a86cf2c-json.log /var/lib/docker/containers/5d70c82942083d16593670058aefed339cfe874c9027205b1e6eb8e569894d65/5d70c82942083d16593670058aefed339cfe874c9027205b1e6eb8e569894d65-json.log /var/lib/docker/containers/6ee184044661c436b44769d56c203f1fc296dbfe08f6ed4cf79aa6fb8cae6659/6ee184044661c436b44769d56c203f1fc296dbfe08f6ed4cf79aa6fb8cae6659-json.log /var/lib/docker/containers/082564c5bdb19b642491b09419a69061122483c0f959a36eb186dd1fec53c163/082564c5bdb19b642491b09419a69061122483c0f959a36eb186dd1fec53c163-json.log /var/lib/docker/containers/b623602a667c0b31068563f244610a548ed055ff9802197f372ff436a294ab5c/b623602a667c0b31068563f244610a548ed055ff9802197f372ff436a294ab5c-json.log /var/lib/docker/containers/66c44231981fcb5ecd33bf0fc3390e71c5cbbabb839d79441eb3317b8500d551/66c44231981fcb5ecd33bf0fc3390e71c5cbbabb839d79441eb3317b8500d551-json.log /var/lib/docker/containers/a93a4275571b0033367f9cab8213c467b21a03c600e2203195640b5a5bc7f523/a93a4275571b0033367f9cab8213c467b21a03c600e2203195640b5a5bc7f523-json.log /var/lib/docker/containers/a88d104d20e5ee58ffeaeecbb559b3231c5b8c73ad1443538928ebeae4ff705c/a88d104d20e5ee58ffeaeecbb559b3231c5b8c73ad1443538928ebeae4ff705c-json.log /var/lib/docker/containers/6bd1f79f16b8b06f2bd203dd84443004ba08c150ac51d23fa620e8b2cbf4b773/6bd1f79f16b8b06f2bd203dd84443004ba08c150ac51d23fa620e8b2cbf4b773-json.log /var/lib/docker/containers/05fc24ef7a14e31e4557c9881482d350cfb05f2f1cb870638de344581154ca01/05fc24ef7a14e31e4557c9881482d350cfb05f2f1cb870638de344581154ca01-json.log /var/lib/docker/containers/7bd3a179cf67b1537e0965c1d1f518420ac5d4cd151ecb75c37ada8c2347ca6b/7bd3a179cf67b1537e0965c1d1f518420ac5d4cd151ecb75c37ada8c2347ca6b-json.log 或者查看具体容器名称的日志位置\n$ docker inspect --format=\u0026#39;{{.LogPath}}\u0026#39; redis /var/lib/docker/containers/6ee184044661c436b44769d56c203f1fc296dbfe08f6ed4cf79aa6fb8cae6659/6ee184044661c436b44769d56c203f1fc296dbfe08f6ed4cf79aa6fb8cae6659-json.log 这样只是解决燃眉之急，并不是长久之计，最好是创建容器时就控制日志的大小.\n运行时控制 启动容器时，我们可以通过参数来控制日志的文件个数和单个文件的大小\n# max-size 最大数值 # max-file 最大日志数 $ docker run -it --log-opt max-size=10m --log-opt max-file=3 redis 一两个容器还好，但是如果有很多容器需要管理，这样就很不方便了，最好还是可以统一管理。\n全局配置 创建或修改文件 /etc/docker/daemon.json，并增加以下配置\n{  \u0026#34;log-driver\u0026#34;:\u0026#34;json-file\u0026#34;,  \u0026#34;log-opts\u0026#34;:{  \u0026#34;max-size\u0026#34; :\u0026#34;50m\u0026#34;,\u0026#34;max-file\u0026#34;:\u0026#34;1\u0026#34;  } } 随后重启 Docker 服务\n$ sudo systemctl daemon-reload $ sudo systemctl restart docker 不过已存在的容器不会生效，需要重建才可以\n","permalink":"https://iblog.zone/archives/%E8%A7%A3%E5%86%B3docker%E6%97%A5%E5%BF%97%E6%96%87%E4%BB%B6%E5%A4%AA%E5%A4%A7%E7%9A%84%E9%97%AE%E9%A2%98/","summary":"Docker 在不重建容器的情况下，日志文件默认会一直追加，时间一长会逐渐占满服务器的硬盘的空间，内存消耗也会一直增加，本篇来了解一些控制日志文件的方法。\n 清理单个文件 运行时控制 全局配置  Docker 的日志文件存在 /var/lib/docker/containers 目录中，通过下面的命令可以将日志文件夹根据升序的方式罗列出来。\n$ sudo du -d1 -h /var/lib/docker/containers | sort -h  28K /var/lib/docker/containers/0db860afe94df368335c2e96f290275f4c396b996b4e8d22770b01baafd9982c 36K /var/lib/docker/containers/6ee184044661c436b44769d56c203f1fc296dbfe08f6ed4cf79aa6fb8cae6659 44K /var/lib/docker/containers/66c44231981fcb5ecd33bf0fc3390e71c5cbbabb839d79441eb3317b8500d551 60K /var/lib/docker/containers/bc4136199037e73d712614ef57de0915d294cbe51045d213f0d822d71a86cf2c 344K /var/lib/docker/containers/7bd3a179cf67b1537e0965c1d1f518420ac5d4cd151ecb75c37ada8c2347ca6b 984K /var/lib/docker/containers/6bd1f79f16b8b06f2bd203dd84443004ba08c150ac51d23fa620e8b2cbf4b773 1.7M /var/lib/docker/containers/a93a4275571b0033367f9cab8213c467b21a03c600e2203195640b5a5bc7f523 4.4M /var/lib/docker/containers/082564c5bdb19b642491b09419a69061122483c0f959a36eb186dd1fec53c163 14M /var/lib/docker/containers/05fc24ef7a14e31e4557c9881482d350cfb05f2f1cb870638de344581154ca01 32M /var/lib/docker/containers/5d70c82942083d16593670058aefed339cfe874c9027205b1e6eb8e569894d65 129M /var/lib/docker/containers/a88d104d20e5ee58ffeaeecbb559b3231c5b8c73ad1443538928ebeae4ff705c 285M /var/lib/docker/containers/b623602a667c0b31068563f244610a548ed055ff9802197f372ff436a294ab5c 917M /var/lib/docker/containers/3d71c509ab6aea34400d37f6c006914eed2cb05e6e6cd07b3ee03eb783dc367b 1.4G /var/lib/docker/containers 有三种方式可以清理日志文件\n清理单个文件 感觉哪个容器的日志太大就清理哪个\n$ sudo sh -c \u0026#34;cat /dev/null \u0026gt; ${log_file}\u0026#34; ${log_file} 就是日志文件，可以通过 find 命令查找全部日志\n$ sudo find /var/lib/docker/containers -name *.log  /var/lib/docker/containers/3d71c509ab6aea34400d37f6c006914eed2cb05e6e6cd07b3ee03eb783dc367b/3d71c509ab6aea34400d37f6c006914eed2cb05e6e6cd07b3ee03eb783dc367b-json.","title":"解决Docker日志文件太大的问题"},{"content":"#因在容器中排查故障需要，需要安装基础工具\n# 查看系统版本： cat /etc/os-release\nDebian基础镜像\n#先添加163源 tee /etc/apt/sources.list \u0026lt;\u0026lt; EOF deb http://mirrors.163.com/debian/ jessie main non-ffree contrib deb http://mirrirs.163.com/debian/ jessie-updates main non-free contrib EOF  #安装 curl telnet apt-get update \u0026amp;\u0026amp; apt-get install -y curl telnet vim Alpine基础镜像\n#先添加阿里源 cat \u0026gt; /etc/apk/repositories \u0026lt;\u0026lt; EOF http://mirrors.aliyun.com/alpine/v3.12/main/ http://mirrors.aliyun.com/alpine/v3.12/community EOF  #安装 curl scp telnet vim apk update \u0026amp;\u0026amp; apk add curl openssh-client busybox-extras vim ","permalink":"https://iblog.zone/archives/docker%E5%AE%B9%E5%99%A8%E4%B8%AD%E5%AE%89%E8%A3%85curltelnetvim%E5%9F%BA%E7%A1%80%E5%B7%A5%E5%85%B7/","summary":"#因在容器中排查故障需要，需要安装基础工具\n# 查看系统版本： cat /etc/os-release\nDebian基础镜像\n#先添加163源 tee /etc/apt/sources.list \u0026lt;\u0026lt; EOF deb http://mirrors.163.com/debian/ jessie main non-ffree contrib deb http://mirrirs.163.com/debian/ jessie-updates main non-free contrib EOF  #安装 curl telnet apt-get update \u0026amp;\u0026amp; apt-get install -y curl telnet vim Alpine基础镜像\n#先添加阿里源 cat \u0026gt; /etc/apk/repositories \u0026lt;\u0026lt; EOF http://mirrors.aliyun.com/alpine/v3.12/main/ http://mirrors.aliyun.com/alpine/v3.12/community EOF  #安装 curl scp telnet vim apk update \u0026amp;\u0026amp; apk add curl openssh-client busybox-extras vim ","title":"Docker容器中安装curl、telnet、vim基础工具"},{"content":"搜索镜像 docker search grafana/grafana 拉取镜像 版本号可以去官网查看：https://hub.docker.com/r/grafana/grafana\ndocker pull grafana/grafana:8.3.3 创建容器  --restart=always：容器退出后（kill后）自动重启。 --link prometheus：需要将prometheus容器（容器名）的hostname链接过来，否则无法连接到prometheus。 $PWD/grafana/config：映射配置文件位置 $PWD/grafana/data：映射数据存储位置。 /etc/localtime:/etc/localtime:ro：容器内部的时间格式化保持和宿主机一致。  docker run -d --restart=always \\ -u root \\ --name=grafana \\ --link prometheus \\ -p 3000:3000 \\ -v $PWD/grafana/config:/etc/grafana \\ -v $PWD/grafana/data:/var/lib/grafana \\ -v /etc/localtime:/etc/localtime:ro \\ grafana/grafana:8.3.3 可能出现的错误 使用-u root指定为root用户启动。\nmkdir: can\u0026#39;t create directory \u0026#39;/var/lib/grafana/plugins\u0026#39;: Permission denied 缺少配置文件\nmsg=\u0026#34;failed to parse \\\u0026#34;/etc/grafana/grafana.ini\\\u0026#34;: open /etc/grafana/grafana.ini: no such file or directory\u0026#34; 创建容器时需要先创建好grafana.ini配置文件。\ndocker cp grafana:/etc/grafana/grafana.ini ./grafana/config/ 进入容器 docker exec -it grafana bash 控制台 http://localhost:3000\nDashboard https://grafana.com/grafana/dashboards\n","permalink":"https://iblog.zone/archives/docker%E9%83%A8%E7%BD%B2grafana/","summary":"搜索镜像 docker search grafana/grafana 拉取镜像 版本号可以去官网查看：https://hub.docker.com/r/grafana/grafana\ndocker pull grafana/grafana:8.3.3 创建容器  --restart=always：容器退出后（kill后）自动重启。 --link prometheus：需要将prometheus容器（容器名）的hostname链接过来，否则无法连接到prometheus。 $PWD/grafana/config：映射配置文件位置 $PWD/grafana/data：映射数据存储位置。 /etc/localtime:/etc/localtime:ro：容器内部的时间格式化保持和宿主机一致。  docker run -d --restart=always \\ -u root \\ --name=grafana \\ --link prometheus \\ -p 3000:3000 \\ -v $PWD/grafana/config:/etc/grafana \\ -v $PWD/grafana/data:/var/lib/grafana \\ -v /etc/localtime:/etc/localtime:ro \\ grafana/grafana:8.3.3 可能出现的错误 使用-u root指定为root用户启动。\nmkdir: can\u0026#39;t create directory \u0026#39;/var/lib/grafana/plugins\u0026#39;: Permission denied 缺少配置文件\nmsg=\u0026#34;failed to parse \\\u0026#34;/etc/grafana/grafana.ini\\\u0026#34;: open /etc/grafana/grafana.ini: no such file or directory\u0026#34; 创建容器时需要先创建好grafana.ini配置文件。\ndocker cp grafana:/etc/grafana/grafana.ini .","title":"Docker部署Grafana"},{"content":"InfluxDB is the Time Series Database in the TICK stack\nhttps://www.influxdata.com/time-series-platform/\n摘要: Docker监控方案之数据存储工具Influxdb工具的介绍和安装。Influxdb也是和telegraf属于一家公司，用go开发的用来存储时间序列数据的数据库。可以将存储的数据进行时间序列化，是每个监控系统中最重要的一个环节。Docker监控方案(TIG)采用Influxdb来进行数据存储，当然可选的还有很多，比如Opentsdb，Graphite等。\n前言： Influxdb也是有influxdata公司(www.influxdata.com )开发的用于数据存储的时间序列数据库.可用于数据的时间排列。在整个TIG(Telegraf+influxdb+grafana)方案中，influxdb可算作一个中间件，主要负责原始数据的存储，并按照时间序列进行索引构建以提供时间序列查询接口。在整个TIG方案中，应该先构建的就是Influxdb。\nInfluxdb研究与实践： influxdb介绍：\n使用TSM(Time Structured Merge)存储引擎，允许高摄取速度和数据压缩； 使用go编写，无需其他依赖； 简单，高性能写查询httpAPI接口； 支持其他数据获取协议的插件，比如graphite,collected,OpenTSDB； 使用relay构建高可用https://docs.influxdata.com/influxdb/v1.0/high_availability/relay/； 扩展的类sql语言，很容易查询汇总数据； tag的支持，可用让查询变的更加高效和快速； 保留策略有效地自动淘汰过期的数据； 持续所产生的自动计算的数据会使得频繁的查询更加高效； web管理页面的支持\n下载安装：\ngithub：https://github.com/influxdata/influxdb 源码编译 官网下载： Centos系列：wgethttps://dl.influxdata.com/influxdb/releases/influxdb-1.0.0.x86_64.rpm \u0026amp;\u0026amp; sudo yum localinstall influxdb-1.0.0.x86_64.rpm 源码包系列：wgethttps://dl.influxdata.com/influxdb/releases/influxdb-1.0.0_linux_amd64.tar.gz \u0026amp;\u0026amp; tar xvfz influxdb-1.0.0_linux_amd64.tar.gz docker系列：docker pull influxdb 安装手册：https://docs.influxdata.com/influxdb/v0.9/introduction/installation/\n配置：\n#cat /etc/influxdb/influxdb.conf reporting-disabled = false [registration] [meta] dir = \u0026#34;/var/lib/influxdb/meta\u0026#34; hostname = \u0026#34;10.0.0.2\u0026#34; #此hostname必须写本机，否则无法连接到数据操作的API bind-address = \u0026#34;:8088\u0026#34; retention-autocreate = true election-timeout = \u0026#34;1s\u0026#34; heartbeat-timeout = \u0026#34;1s\u0026#34; leader-lease-timeout = \u0026#34;500ms\u0026#34; commit-timeout = \u0026#34;50ms\u0026#34; cluster-tracing = false [data] dir = \u0026#34;/var/lib/influxdb/data\u0026#34; max-wal-size = 104857600 # Maximum size the WAL can reach before a flush. Defaults to 100MB. wal-flush-interval = \u0026#34;10m\u0026#34; # Maximum time data can sit in WAL before a flush. wal-partition-flush-delay = \u0026#34;2s\u0026#34; # The delay time between each WAL partition being flushed. wal-dir = \u0026#34;/var/lib/influxdb/wal\u0026#34; wal-logging-enabled = true [hinted-handoff] enabled = true dir = \u0026#34;/var/lib/influxdb/hh\u0026#34; max-size = 1073741824 max-age = \u0026#34;168h\u0026#34; retry-rate-limit = 0 retry-interval = \u0026#34;1s\u0026#34; retry-max-interval = \u0026#34;1m\u0026#34; purge-interval = \u0026#34;1h\u0026#34; [admin] enabled = true bind-address = \u0026#34;:8083\u0026#34; https-enabled = false https-certificate = \u0026#34;/etc/ssl/influxdb.pem\u0026#34; [http] enabled = true bind-address = \u0026#34;:8086\u0026#34; auth-enabled = false log-enabled = true write-tracing = false pprof-enabled = false https-enabled = false https-certificate = \u0026#34;/etc/ssl/influxdb.pem\u0026#34; [opentsdb] enabled = false [collectd] enabled = false 注意：influxdb服务会启动三个端口：8086为服务的默认数据处理端口，主要用来influxdb数据库的相关操作，可提供相关的API；8083为管理员提供了一个可视化的web界面，用来为用户提供友好的可视化查询与数据管理；8088主要为了元数据的管理。需要注意的是，influxdb默认是需要influxdb用户启动，且数据存放在/var/lib/influxdb/下面，生产环境需要注意这个。\n启动：\n和telegraf启动方式一样，可以使用init.d或者systemd进行管理influxdb 注意，启动之后需要查看相关的端口是否正在监听，并检查日志确保服务正常启动\n使用：\n如果说使用telegraf最核心的部分在配置，那么influxdb最核心的就是SQL语言的使用了。influxdb默认支持三种操作方式： 登录influxdb的shell中操作:\n创建数据库： create database mydb 创建用户： create user \u0026#34;bigdata\u0026#34; with password \u0026#39;bigdata\u0026#39; with all privileges 查看数据库： show databases; 数据插入： insert bigdata,host=server001,regin=HC load=88 切换数据库：  use mydb 查看数据库中有哪些measurement(类似数据库中的表): show measurements 查询： select * from cpu limit 2 查询一小时前开始到现在结束的： #select load from cpu where time \u0026gt; now() - 1h 查询从历史纪元开始到1000天之间： #select load from cpu where time \u0026lt; now() + 1000d 查找一个时间区间： #select load from cpu where time \u0026gt; \u0026#39;2016-08-18\u0026#39; and time \u0026lt; \u0026#39;2016-09-19\u0026#39; 查询一个小时间区间的数据，比如在September 18, 2016 21:24:00:后的6分钟： #select load from cpu where time \u0026gt; \u0026#39;2016-09-18T21:24:00Z\u0026#39; +6m 使用正则查询所有measurement的数据： #select * from /.*/ limit 1 #select * from /^docker/ limit 3 #select * from /.*mem.*/ limit 3 正则匹配加指定tag：（=~ !~） #select * from cpu where \u0026#34;host\u0026#34; !~ /.*HC.*/ limit 4 #SELECT * FROM \u0026#34;h2o_feet\u0026#34; WHERE (\u0026#34;location\u0026#34; =~ /.*y.*/ OR \u0026#34;location\u0026#34; =~ /.*m.*/) AND \u0026#34;water_level\u0026#34; \u0026gt; 0 LIMIT 4 排序：group by的用法必须得是在复合函数中进行使用 #select count(type) from events group by time(10s) #select count(type) from events group by time(10s),type 给查询字段做tag： #select count(type) as number_of_types group by time(10m) #select count(type) from events group by time(1h) where time \u0026gt; now() - 3h 使用fill字段： #select count(type) from events group by time(1h) fill(0)/fill(-1)/fill(null) where time \u0026gt; now() - 3h 数据聚合： select count(type) from user_events merge admin_events group by time(10m) 使用API进行操作数据:\n创建数据库: curl -G \u0026#34;http://localhost:8086/query\u0026#34; --data-urlencode \u0026#34;q=create database mydb\u0026#34; 插入数据： curl -XPOST \u0026#39;http://localhost:8086/write?db=mydb\u0026#39; -d \u0026#39;biaoge,name=xxbandy,xingqu=coding age=2\u0026#39; curl -i -XPOST \u0026#39;http://localhost:8086/write?db=mydb\u0026#39; --data-binary \u0026#39;cpu_load_short,host=server01,region=us-west value=0.64 1434055562000000000\u0026#39; curl -i -XPOST \u0026#39;http://localhost:8086/write?db=mydb\u0026#39; --data-binary \u0026#39;cpu_load_short,host=server02 value=0.67 cpu_load_short,host=server02,region=us-west value=0.55 1422568543702900257 cpu_load_short,direction=in,host=server01,region=us-west value=2.0 1422568543702900257\u0026#39; 将sql语句写入文件，并通过api插入： #cat sql.txt cpu_load_short,host=server02 value=0.67 cpu_load_short,host=server02,region=us-west value=0.55 1422568543702900257 cpu_load_short,direction=in,host=server01,region=us-west value=2.0 1422568543702900257 #curl -i -XPOST \u0026#39;http://localhost:8086/write?db=mydb\u0026#39; --data-binary @cpu_data.txt  查询数据：（--data-urlencode \u0026#34;epoch=s\u0026#34; 指定时间序列 \u0026#34;chunk_size=20000\u0026#34; 指定查询块大小） # curl -G http://localhost:8086/query?pretty=true --data-urlencode \u0026#34;db=ydb\u0026#34; --data-urlencode \u0026#34;q=select * from biaoge where xingqu=\u0026#39;coding\u0026#39;\u0026#34; 数据分析： #curl -G http://localhost:8086/query?pretty=true --data-urlencode \u0026#34;db=mydb\u0026#34; --data-urlencode \u0026#34;q=select mean(load) from cpu\u0026#34; #curl -G http://localhost:8086/query?pretty=true --data-urlencode \u0026#34;db=mydb\u0026#34; --data-urlencode \u0026#34;q=select load from cpu\u0026#34; 可以看到load的值分别是42 78 15.4；用mean(load)求出来的值为45,13 curl -G http://localhost:8086/query?pretty=true --data-urlencode \u0026#34;db=ydb\u0026#34; --data-urlencode \u0026#34;q=select mean(load) from cpu where host=\u0026#39;server01\u0026#39;\u0026#34; 使用influxdb提供的web界面进行操作:\n这里只是简单的介绍了influxdb的使用，后期如果想在grafana中汇聚并完美地展示数据，可能需要熟悉influxdb的各种查询语法。(其实就是sql语句的一些使用技巧，聚合函数的使用，子查询等等)\n","permalink":"https://iblog.zone/archives/influxdb%E5%AD%A6%E4%B9%A0/","summary":"InfluxDB is the Time Series Database in the TICK stack\nhttps://www.influxdata.com/time-series-platform/\n摘要: Docker监控方案之数据存储工具Influxdb工具的介绍和安装。Influxdb也是和telegraf属于一家公司，用go开发的用来存储时间序列数据的数据库。可以将存储的数据进行时间序列化，是每个监控系统中最重要的一个环节。Docker监控方案(TIG)采用Influxdb来进行数据存储，当然可选的还有很多，比如Opentsdb，Graphite等。\n前言： Influxdb也是有influxdata公司(www.influxdata.com )开发的用于数据存储的时间序列数据库.可用于数据的时间排列。在整个TIG(Telegraf+influxdb+grafana)方案中，influxdb可算作一个中间件，主要负责原始数据的存储，并按照时间序列进行索引构建以提供时间序列查询接口。在整个TIG方案中，应该先构建的就是Influxdb。\nInfluxdb研究与实践： influxdb介绍：\n使用TSM(Time Structured Merge)存储引擎，允许高摄取速度和数据压缩； 使用go编写，无需其他依赖； 简单，高性能写查询httpAPI接口； 支持其他数据获取协议的插件，比如graphite,collected,OpenTSDB； 使用relay构建高可用https://docs.influxdata.com/influxdb/v1.0/high_availability/relay/； 扩展的类sql语言，很容易查询汇总数据； tag的支持，可用让查询变的更加高效和快速； 保留策略有效地自动淘汰过期的数据； 持续所产生的自动计算的数据会使得频繁的查询更加高效； web管理页面的支持\n下载安装：\ngithub：https://github.com/influxdata/influxdb 源码编译 官网下载： Centos系列：wgethttps://dl.influxdata.com/influxdb/releases/influxdb-1.0.0.x86_64.rpm \u0026amp;\u0026amp; sudo yum localinstall influxdb-1.0.0.x86_64.rpm 源码包系列：wgethttps://dl.influxdata.com/influxdb/releases/influxdb-1.0.0_linux_amd64.tar.gz \u0026amp;\u0026amp; tar xvfz influxdb-1.0.0_linux_amd64.tar.gz docker系列：docker pull influxdb 安装手册：https://docs.influxdata.com/influxdb/v0.9/introduction/installation/\n配置：\n#cat /etc/influxdb/influxdb.conf reporting-disabled = false [registration] [meta] dir = \u0026#34;/var/lib/influxdb/meta\u0026#34; hostname = \u0026#34;10.0.0.2\u0026#34; #此hostname必须写本机，否则无法连接到数据操作的API bind-address = \u0026#34;:8088\u0026#34; retention-autocreate = true election-timeout = \u0026#34;1s\u0026#34; heartbeat-timeout = \u0026#34;1s\u0026#34; leader-lease-timeout = \u0026#34;500ms\u0026#34; commit-timeout = \u0026#34;50ms\u0026#34; cluster-tracing = false [data] dir = \u0026#34;/var/lib/influxdb/data\u0026#34; max-wal-size = 104857600 # Maximum size the WAL can reach before a flush.","title":"Influxdb学习"},{"content":"拉取Consul镜像 $ docker pull consul # 默认拉取latest $ docker pull consul:1.9.3 # 拉取指定版本 安装并运行 docker run -d -p 8500:8500 --restart=always --name=consul consul:1.9.3 agent -server -bootstrap -ui -node=1 -client=\u0026#39;0.0.0.0\u0026#39;  agent: 表示启动 Agent 进程。 server：表示启动 Consul Server 模式 client：表示启动 Consul Cilent 模式。 bootstrap：表示这个节点是 Server-Leader ，每个数据中心只能运行一台服务器。技术角度上讲 Leader 是通过 Raft 算法选举的，但是集群第一次启动时需要一个引导 Leader，在引导群集后，建议不要使用此标志。 ui：表示启动 Web UI 管理器，默认开放端口 8500，所以上面使用 Docker 命令把 8500 端口对外开放。 node：节点的名称，集群中必须是唯一的，默认是该节点的主机名。 client：consul服务侦听地址，这个地址提供HTTP、DNS、RPC等服务，默认是127.0.0.1所以不对外提供服务，如果你要对外提供服务改成0.0.0.0 join：表示加入到某一个集群中去。 如：-json=192.168.0.11  ","permalink":"https://iblog.zone/archives/docker%E5%AE%89%E8%A3%85consul1.9.3/","summary":"拉取Consul镜像 $ docker pull consul # 默认拉取latest $ docker pull consul:1.9.3 # 拉取指定版本 安装并运行 docker run -d -p 8500:8500 --restart=always --name=consul consul:1.9.3 agent -server -bootstrap -ui -node=1 -client=\u0026#39;0.0.0.0\u0026#39;  agent: 表示启动 Agent 进程。 server：表示启动 Consul Server 模式 client：表示启动 Consul Cilent 模式。 bootstrap：表示这个节点是 Server-Leader ，每个数据中心只能运行一台服务器。技术角度上讲 Leader 是通过 Raft 算法选举的，但是集群第一次启动时需要一个引导 Leader，在引导群集后，建议不要使用此标志。 ui：表示启动 Web UI 管理器，默认开放端口 8500，所以上面使用 Docker 命令把 8500 端口对外开放。 node：节点的名称，集群中必须是唯一的，默认是该节点的主机名。 client：consul服务侦听地址，这个地址提供HTTP、DNS、RPC等服务，默认是127.0.0.1所以不对外提供服务，如果你要对外提供服务改成0.0.0.0 join：表示加入到某一个集群中去。 如：-json=192.168.0.11  ","title":"Docker安装Consul1.9.3"},{"content":"一、安装 1.1、配置INFLUXDB YUM源 [root@node ~]# cat /etc/yum.repos.d/influxdb.repo  [influxdb] name = InfluxDB Repository - RHEL \\$releasever baseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://repos.influxdata.com/influxdb.key 1.2、安装GRAFANA+INFLUXDB+TELEGRAF 安装influxdb yum install influxdb 安装telegraf yum install telegraf 安装grafana wget https://dl.grafana.com/oss/release/grafana-6.1.3-1.x86_64.rpm yum localinstall grafana-6.1.3-1.x86_64.rpm 1.3、软件版本 InfluxDB version: 1.7.4 Telegraf version: 1.10.2 Grafana version: 6.1.3 1.4、启动服务，添加开机启动 systemctl start influxdb.service systemctl start telegraf.service systemctl start grafana-server.service  systemctl enable influxdb.service systemctl enable telegraf.service systemctl enable grafana-server.service 1.5、查看GRAFANA界面 grafana默认监听在3000端口，默认用户名admin，密码admin\n二、数据采集之TELEGRAF Telegraf是用Go写的代理程序，可以用于收集系统和服务的统计数据，是TICK技术栈的一部分。它具备输入插件，可以直接从系统获取指标数据，从第三方API获取指标数据，甚至可以通过statsd和Kafka获取指标数据。它还具备输出插件，可以将采集的指标发送到各种数据存储，服务和消息队列。比如InfluxDB，Graphite，OpenTSDB，Datadog，Librato，Kafka，MQTT，NSQ等等，目前Telegraf尚不支持Oracle数据库统计数据的实时监控。\n来自于官网：https://www.influxdata.com/time-series-platform/telegraf/\n三、数据存储之INFLUXDB InfluxDB是一个时间序列数据库，旨在处理高写入和查询负载，主要用于存储系统的监控数据 InfluxDB有三大特性： • Time Series （时间序列）：可以使用与时间有关的相关函数（如最大，最小，求和等） • Metrics（度量）：你可以实时对大量数据进行计算 • Eevents（事件）：它支持任意的事件数据\n特点 • Schemaless（无结构），可以是任意数量的列 • Scalable（可扩展）：min, max, sum, count, mean, median 一系列函数，方便统计 • Native HTTP API, 内置http支持，使用http读写 • Powerful Query Language 类似sql • 自带压力测试工具等，功能强大\n四、数据展示之GRAFANA Grafana是一个开源指标分析和可视化套件，常用于可视化基础设施的性能数据和应用程序分析的时间序列数据。也可以应用于其他领域，包括工业传感器，家庭自动化，天气和过程控制。但请注意，我们使用Grafana最关心的是如何把数据进行聚合后进行展示。 Grafana支持多种不同的时序数据库数据源，Grafana对每种数据源提供不同的查询方法，而且能很好的支持每种数据源的特性。它支持下面几种数据源：Graphite、Elasticsearch、CloudWatch、InfluxDB、OpenTSDB、Prometheus、MySQL、Postgres、Microsoft SQL Server (MSSQL)。每种数据源都有相应的文档，可以将多个数据源的数据合并到一个单独的仪表板上。\n五、配置监控主机状态及MYSQL运行状态 5.1、创建数据库 [root@node ~]# influx \u0026gt; create user \u0026#34;mysql-server\u0026#34;with password \u0026#39;123456\u0026#39; \u0026gt; create database myserverDB 5.2、配置本机的INFLUXDB数据库为期望的输出源-OUTPUT [root@node ~]# vim /etc/telegraf/telegraf.conf  # 主配置文件，必须存在，如output保存的数据库不同，可在telegraf.d下的配置文件单独配置 [[outputs.influxdb]]  urls = [\u0026#34;http://127.0.0.1:8086\u0026#34;]  database = \u0026#34;myserverDB\u0026#34; 5.3、配置监控项-INPUT 配置基本监控项\n[root@node ~]# cat /etc/telegraf/telegraf.d/telegraf.conf  [global_tags] [agent]  interval = \u0026#34;10s\u0026#34;  round_interval = true  metric_batch_size = 1000  metric_buffer_limit = 10000  collection_jitter = \u0026#34;0s\u0026#34;  flush_interval = \u0026#34;10s\u0026#34;  flush_jitter = \u0026#34;0s\u0026#34;  precision = \u0026#34;\u0026#34;  debug = false  quiet = false  logfile = \u0026#34;\u0026#34;  hostname = \u0026#34;\u0026#34;  omit_hostname = false [[outputs.influxdb]]  urls = [\u0026#34;http://127.0.0.1:8086\u0026#34;]  database = \u0026#34;telegraf\u0026#34; [[inputs.cpu]]  percpu = true  totalcpu = true  collect_cpu_time = false  report_active = false [[inputs.disk]]  ignore_fs = [\u0026#34;tmpfs\u0026#34;, \u0026#34;devtmpfs\u0026#34;, \u0026#34;devfs\u0026#34;, \u0026#34;overlay\u0026#34;, \u0026#34;aufs\u0026#34;, \u0026#34;squashfs\u0026#34;] [[inputs.diskio]] [[inputs.kernel]] [[inputs.mem]] [[inputs.processes]] [[inputs.swap]] [[inputs.system]] [[inputs.net]]  interfaces = [\u0026#34;ens192\u0026#34;] [[inputs.netstat]] 配置mysql监控项\n[root@node ~]# cat /etc/telegraf/telegraf.d/telegraf_mysql.conf  [[inputs.mysql]]  interval = \u0026#34;5m\u0026#34;  servers = [\u0026#34;root:root@tcp(10.0.0.1:3306)/?tls=false\u0026#34;] # 如果密码有特殊字符且以@结尾，无需增加双引号或者单引号，直接填入即可  perf_events_statements_digest_text_limit = 120  perf_events_statements_limit = 250  perf_events_statements_time_limit = 86400  table_schema_databases = [\u0026#34;\u0026#34;]  gather_table_schema = false   gather_process_list = true   gather_info_schema_auto_inc = true   gather_slave_status = true  gather_binary_logs = false  gather_table_io_waits = false  gather_table_lock_waits = false  gather_index_io_waits = false  gather_event_waits = false  gather_file_events_stats = false  interval_slow = \u0026#34;30m\u0026#34; [[outputs.influxdb]]  urls = [\u0026#34;http://127.0.0.1:8086\u0026#34;]  database = \u0026#34;mysql\u0026#34; 更多配置，参考：https://docs.influxdata.com/telegraf/v1.13/administration/configuration/\n5.4、配置GRAFANA界面 选择Data Sources，添加需要的数据源\n创建主机仪表盘\n可以通过访问https://grafana.com/dashboards 来查看已经由其他用户共享的仪表盘，选取合适的使用，缩短上手时间 将合适的模版import导入，我这里用的是https://grafana.com/dashboards/1443， 注意模版与telegraf input配置一致。\n可以看到如下界面\n创建mysql仪表盘 import https://grafana.com/dashboards/1177\n六、通过自定义脚本采集监控数据 6.1、在INFLUXDB数据库创建MYSQL_RUN_STATUS库 create user \u0026#34;admin\u0026#34; with password \u0026#39;123456\u0026#39; create database mysql_run_status 6.2、在被监控主机上创建监控脚本，并开启MYSQL 通过influxDB数据库http接口上传数据，0为up，1为down\n[root@node2 local]# systemctl start mariadb [root@node2 local]# cat mysql_status.sh  #!/bin/bash systemctl status mariadb.service |grep running \u0026amp;\u0026gt;/dev/null if [ $? -eq 0 ];then  echo \u0026#34;mysql_status=0\u0026#34; \u0026gt; temp_mysql_run else  echo \u0026#34;mysql_status=1\u0026#34; \u0026gt; temp_mysql_run fi IP=192.168.143.131 test=`cat temp_mysql_run` curl -i -XPOST \u0026#39;http://192.168.143.130:8086/write?db=mysql_run_status\u0026#39; --data-binary \u0026#34;mysql_run_status,ip=$IP,$testcount=1\u0026#34; 6.3、执行监控脚本，查看入库情况 ./mysql_status.sh \u0026amp;\u0026gt; /dev/null \u0026gt; select * from mysql_run_status name: mysql_run_status time count ip mysql_status ---- ----- -- ------------ 1556267694277201332 1 192.168.143.131 0 6.4、GRAFANA展示 连接数据源\n创建仪表盘\nFAQ: 导入模板后没有图形显示\n1、检查软件版本是否跟上面的版本对应，高版本可能跟现有功能不兼容\n2、查看influxdb中是否有数据表和数据\n","permalink":"https://iblog.zone/archives/grafana-influxdb-telegraf-%E5%BF%AB%E9%80%9F%E7%9B%91%E6%8E%A7%E4%B8%BB%E6%9C%BA%E4%B8%8Emysql/","summary":"一、安装 1.1、配置INFLUXDB YUM源 [root@node ~]# cat /etc/yum.repos.d/influxdb.repo  [influxdb] name = InfluxDB Repository - RHEL \\$releasever baseurl = https://repos.influxdata.com/rhel/\\$releasever/\\$basearch/stable enabled = 1 gpgcheck = 1 gpgkey = https://repos.influxdata.com/influxdb.key 1.2、安装GRAFANA+INFLUXDB+TELEGRAF 安装influxdb yum install influxdb 安装telegraf yum install telegraf 安装grafana wget https://dl.grafana.com/oss/release/grafana-6.1.3-1.x86_64.rpm yum localinstall grafana-6.1.3-1.x86_64.rpm 1.3、软件版本 InfluxDB version: 1.7.4 Telegraf version: 1.10.2 Grafana version: 6.1.3 1.4、启动服务，添加开机启动 systemctl start influxdb.service systemctl start telegraf.service systemctl start grafana-server.service  systemctl enable influxdb.service systemctl enable telegraf.service systemctl enable grafana-server.","title":"GRAFANA+INFLUXDB+TELEGRAF 快速监控主机与MYSQL"},{"content":"一、环境信息 系统：CentOS Linux release 7.7.1908 磁盘：200G（系统盘）\n1、创建loop设备 mkdir -p /data/ceph-disk/ fallocate -l 40G /data/ceph-disk/sdb.img fallocate -l 40G /data/ceph-disk/sdc.img fallocate -l 40G /data/ceph-disk/sdd.img  losetup -l -P /dev/loop1 /data/ceph-disk/sdb.img losetup -l -P /dev/loop2 /data/ceph-disk/sdc.img losetup -l -P /dev/loop3 /data/ceph-disk/sdd.img  wipefs -a /dev/loop1 wipefs -a /dev/loop2 wipefs -a /dev/loop3 2、设置开机启动挂载loop设备 cat /etc/rc.local\nlosetup -l -P /dev/loop1 /data/ceph-disk/sdb.img losetup -l -P /dev/loop2 /data/ceph-disk/sdc.img losetup -l -P /dev/loop3 /data/ceph-disk/sdd.img  #卸载loop设备 #losetup --detach /dev/loop1 #losetup --detach /dev/loop2 #losetup --detach /dev/loop3 chmod a+x /etc/rc.local 二、安装ceph 1、配置ceph N版本yum源 mkdir -p /etc/yum.repos.d/backup-repo mv /etc/yum.repos.d/*.repo /etc/yum.repos.d/backup-repo \tcurl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo curl -o /etc/yum.repos.d/epel-7.repo http://mirrors.aliyun.com/repo/epel-7.repo  cat \u0026gt;\u0026gt;/etc/yum.repos.d/ceph.repo\u0026lt;\u0026lt;EOF [Ceph] name=Ceph packages for x86_64 baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/ gpgcheck=0 priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch gpgcheck=0 priority=1 [ceph-source] name=Ceph source packages baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS gpgcheck=0 priority=1 EOF  L版本源 cat \u0026gt;\u0026gt;/etc/yum.repos.d/ceph.repo\u0026lt;\u0026lt;EOF [Ceph] name=Ceph packages for x86_64 baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/x86_64/ gpgcheck=0 priority=1 [Ceph-noarch] name=Ceph noarch packages baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/noarch gpgcheck=0 priority=1 [ceph-source] name=Ceph source packages baseurl=http://mirrors.aliyun.com/ceph/rpm-nautilus/el7/SRPMS gpgcheck=0 priority=1 EOF  yum makecache yum -y install epel-release 2、系统初始化 systemctl stop firewalld systemctl disable firewalld sed -i \u0026#39;s/enforcing/disabled/\u0026#39; /etc/selinux/config setenforce 0 systemctl disable NetworkManager systemctl stop NetworkManager  timedatectl set-timezone Asia/Shanghai yum -y install ntpdate systemctl restart ntpd systemctl enable ntpd ntpdate ntp1.aliyun.com #生产环境建议使用多个内网yum源，时间不同步会导致集群故障  swapoff -a  echo \u0026#39;* soft nofile 65535\u0026#39; \u0026gt;\u0026gt;/etc/security/limits.conf echo \u0026#39;* hard nofile 65535\u0026#39; \u0026gt;\u0026gt;/etc/security/limits.conf echo \u0026#39;kernel.pid_max = 4194303\u0026#39; \u0026gt;\u0026gt;/etc/sysctl.conf echo \u0026#39;vm.swappiness = 0\u0026#39; \u0026gt;\u0026gt;/etc/sysctl.conf  sysctl -p  ceph_ip=`hostname -I` echo $ceph_ip `hostname` \u0026gt;\u0026gt;/etc/hosts ssh-keygen -t rsa -P \u0026#34;\u0026#34; -f ~/.ssh/id_rsa ssh-copy-id root@`hostname` 3、安装ceph mkdir -p ~/cephadmin cd ~/cephadmin yum install -y ceph-deploy python-setuptools ceph ceph-deploy new --public-network 192.168.86.0/24 --cluster-network 192.168.86.0/24 `hostname` ceph-deploy mon create-initial ceph-deploy admin `hostname` ceph-deploy mgr create `hostname`  cat \u0026gt;\u0026gt;~/cephadmin/ceph.conf\u0026lt;\u0026lt;EOF [osd] osd_op_thread_suicide_timeout = 600 osd_op_thread_timeout = 300 osd_recovery_thread_timeout = 300 osd_recovery_thread_suicide_timeout = 600 osd_memory_target = 2147483648 osd_scrub_begin_hour= 0 osd_scrub_end_hour= 8 osd_max_markdown_count = 10 osd_crush_chooseleaf_type = 0 osd crush update on start = false EOF  ceph-deploy --overwrite-conf config push `hostname` systemctl restart ceph-mon.target 4、修改ceph-volume代码支持loop设备 sed -i \u0026#34;s/return TYPE == \u0026#39;disk\u0026#39;/return TYPE == \u0026#39;disk\u0026#39; or TYPE == \u0026#39;loop\u0026#39;/g\u0026#34; /usr/lib/python2.7/site-packages/ceph_volume/util/disk.py 5、创建osd for i in {1..3}; do ceph-deploy osd create --bluestore --data \u0026#34;/dev/loop$i\u0026#34; `hostname`; done 6、配置逻辑节点支持多副本 ceph osd crush add-bucket virtual-node01 host ceph osd crush add-bucket virtual-node02 host ceph osd crush add-bucket virtual-node03 host ceph osd crush move virtual-node01 root=default ceph osd crush move virtual-node02 root=default ceph osd crush move virtual-node03 root=default  for i in 0 3 6 9 12 15 18 21; do ceph osd crush move osd.$i host=virtual-node01 ; done for i in 1 4 7 10 13 16 19 22; do ceph osd crush move osd.$i host=virtual-node02 ; done for i in 2 5 8 11 14 17 20 23; do ceph osd crush move osd.$i host=virtual-node03 ; done  ceph config set mon auth_allow_insecure_global_id_reclaim false 更多配置参考：\nhttps://www.kancloud.cn/willseecloud/ceph/1799256\nhttps://docs.ceph.com/en/pacific/cephadm/install/\n","permalink":"https://iblog.zone/archives/%E5%8D%95%E6%9C%BA%E5%8D%95%E7%9B%98%E5%9C%BA%E6%99%AF%E9%83%A8%E7%BD%B2ceph%E7%8E%AF%E5%A2%83/","summary":"一、环境信息 系统：CentOS Linux release 7.7.1908 磁盘：200G（系统盘）\n1、创建loop设备 mkdir -p /data/ceph-disk/ fallocate -l 40G /data/ceph-disk/sdb.img fallocate -l 40G /data/ceph-disk/sdc.img fallocate -l 40G /data/ceph-disk/sdd.img  losetup -l -P /dev/loop1 /data/ceph-disk/sdb.img losetup -l -P /dev/loop2 /data/ceph-disk/sdc.img losetup -l -P /dev/loop3 /data/ceph-disk/sdd.img  wipefs -a /dev/loop1 wipefs -a /dev/loop2 wipefs -a /dev/loop3 2、设置开机启动挂载loop设备 cat /etc/rc.local\nlosetup -l -P /dev/loop1 /data/ceph-disk/sdb.img losetup -l -P /dev/loop2 /data/ceph-disk/sdc.img losetup -l -P /dev/loop3 /data/ceph-disk/sdd.img  #卸载loop设备 #losetup --detach /dev/loop1 #losetup --detach /dev/loop2 #losetup --detach /dev/loop3 chmod a+x /etc/rc.","title":"单机单盘场景部署ceph环境"},{"content":"Nginx官方发布“避免10大NGINX配置错误”中，推荐nginx.conf配置为：\nhttp {   upstream node_backend {  zone upstreams 64K;  server 127.0.0.1:3000 max_fails=1 fail_timeout=2s;  keepalive 2;  }   server {  listen 80;  server_name example.com;   location / {  proxy_set_header Host $host;  proxy_pass http://node_backend/;  proxy_next_upstream error timeout http_500;   }  } } 即使没有负载平衡或在一台机器内，也要启用upstream{}块，它解锁了几个提高性能的功能：\n 该zone指令建立了一个共享内存区域，主机上的所有 NGINX 工作进程都可以访问有关上游服务器的配置和状态信息。几个上游组可以共享该区域。 该server指令有几个参数可用于调整服务器行为。在这个例子中，我们改变了 NGINX 用来确定服务器不健康并因此没有资格接受请求的条件。在这里，如果通信尝试在每 2 秒内失败一次（而不是默认的每10 秒一次），它就会认为服务器不健康。 我们把这个设置和proxy_next_upstream指令结合起来，以配置NGINX认为的失败的通信尝试，在这种情况下，它把请求传递给上游组的下一个服务器。在默认的错误和超时条件中，我们添加了http_500，以便NGINX认为来自上游服务器的HTTP 500（内部服务器错误）代码代表一个失败的尝试。 keepalive指令设置每个工作进程的缓存中保存的与上游服务器的空闲keepalive连接的数量。默认情况下，NGINX 会为每个新的传入请求打开一个到上游（后端）服务器的新连接。这是安全但低效的，因为 NGINX 和服务器必须交换三个数据包来建立连接，并交换三个或四个数据包来终止它。在高流量时，为每个请求打开一个新连接会耗尽系统资源，并且根本无法打开连接。修复是在 NGINX 和上游服务器之间启用keepalive 连接——而不是在请求完成时关闭，连接保持打开状态以用于其他请求。这既减少了源端口用完的可能性，又提高了性能。该参数设置为块中列出的服务器数量的两倍。  ","permalink":"https://iblog.zone/archives/nginx%E5%AE%98%E6%96%B9%E6%8E%A8%E8%8D%90%E7%9A%84nginx.conf%E6%A0%87%E5%87%86%E9%85%8D%E7%BD%AE/","summary":"Nginx官方发布“避免10大NGINX配置错误”中，推荐nginx.conf配置为：\nhttp {   upstream node_backend {  zone upstreams 64K;  server 127.0.0.1:3000 max_fails=1 fail_timeout=2s;  keepalive 2;  }   server {  listen 80;  server_name example.com;   location / {  proxy_set_header Host $host;  proxy_pass http://node_backend/;  proxy_next_upstream error timeout http_500;   }  } } 即使没有负载平衡或在一台机器内，也要启用upstream{}块，它解锁了几个提高性能的功能：\n 该zone指令建立了一个共享内存区域，主机上的所有 NGINX 工作进程都可以访问有关上游服务器的配置和状态信息。几个上游组可以共享该区域。 该server指令有几个参数可用于调整服务器行为。在这个例子中，我们改变了 NGINX 用来确定服务器不健康并因此没有资格接受请求的条件。在这里，如果通信尝试在每 2 秒内失败一次（而不是默认的每10 秒一次），它就会认为服务器不健康。 我们把这个设置和proxy_next_upstream指令结合起来，以配置NGINX认为的失败的通信尝试，在这种情况下，它把请求传递给上游组的下一个服务器。在默认的错误和超时条件中，我们添加了http_500，以便NGINX认为来自上游服务器的HTTP 500（内部服务器错误）代码代表一个失败的尝试。 keepalive指令设置每个工作进程的缓存中保存的与上游服务器的空闲keepalive连接的数量。默认情况下，NGINX 会为每个新的传入请求打开一个到上游（后端）服务器的新连接。这是安全但低效的，因为 NGINX 和服务器必须交换三个数据包来建立连接，并交换三个或四个数据包来终止它。在高流量时，为每个请求打开一个新连接会耗尽系统资源，并且根本无法打开连接。修复是在 NGINX 和上游服务器之间启用keepalive 连接——而不是在请求完成时关闭，连接保持打开状态以用于其他请求。这既减少了源端口用完的可能性，又提高了性能。该参数设置为块中列出的服务器数量的两倍。  ","title":"Nginx官方推荐的nginx.conf标准配置"},{"content":"连接数据库 $ influx -precision rfc3339 Connected to http://localhost:8086 version 1.2.x InfluxDB shell 1.2.x \u0026gt; exit # 退出命令行 说明:\n InfluxDB的HTTP接口默认起在8086上，所以influx默认也是连的本地的8086端口，你可以通过influx --help来看怎么修改默认值。 -precision参数表明了任何返回的时间戳的格式和精度，rfc3339`是让InfluxDB返回RFC339格式(YYYY-MM-DDTHH:MM:SS.nnnnnnnnnZ)的时间戳。  创建数据库 \u0026gt; CREATE DATABASE mydb 查看数据库 \u0026gt; SHOW DATABASES name: databases --------------- name _internal mydb 说明：_internal数据库是用来存储InfluxDB内部的实时监控数据的\n使用数据库 \u0026gt; USE mydb Using database mydb 数据存储格式介绍 首先对数据存储的格式来个入门介绍。InfluxDB里存储的数据被称为时间序列数据，其包含一个数值，就像CPU的load值或是温度值类似的。时序数据有零个或多个数据点，每一个都是一个指标值。数据点包括time(一个时间戳)，measurement(例如cpu_load)，至少一个k-v格式的field(也即指标的数值例如 “value=0.64”或者“temperature=21.2”)，零个或多个tag，其一般是对于这个指标值的元数据(例如“host=server01”, “region=EMEA”, “dc=Frankfurt)。\n在概念上，你可以将measurement类比于SQL里面的table，其主键索引总是时间戳。tag和field是在table里的其他列，tag是被索引起来的，field没有。不同之处在于，在InfluxDB里，你可以有几百万的measurements，你不用事先定义数据的scheme，并且null值不会被存储。\n将数据点写入InfluxDB，只需要遵守如下的行协议：\n\u0026lt;measurement\u0026gt;[,\u0026lt;tag-key\u0026gt;=\u0026lt;tag-value\u0026gt;...] \u0026lt;field-key\u0026gt;=\u0026lt;field-value\u0026gt;[,\u0026lt;field2-key\u0026gt;=\u0026lt;field2-value\u0026gt;...] [unix-nano-timestamp] 下面是数据写入InfluxDB的格式示例：\ncpu,host=serverA,region=us_west value=0.64 payment,device=mobile,product=Notepad,method=credit billed=33,licenses=3i 1434067467100293230 stock,symbol=AAPL bid=127.46,ask=127.48 temperature,machine=unit42,type=assembly external=25,internal=37 1434067467000000000 写入数据 \u0026gt; INSERT cpu,host=serverA,region=us_west value=0.64 这样一个measurement为cpu，tag是host和region，value值为0.64的数据点被写入了InfluxDB中\n查询measurement（table） \u0026gt; show measurements 查询数据 \u0026gt; SELECT \u0026#34;host\u0026#34;, \u0026#34;region\u0026#34;, \u0026#34;value\u0026#34; FROM \u0026#34;cpu\u0026#34; name: cpu --------- time host region value 2015-10-21T19:28:07.580664347Z serverA us_west 0.64 \u0026gt; SELECT * FROM /.*/ LIMIT 1 -- \u0026gt; SELECT * FROM \u0026#34;cpu_load_short\u0026#34; -- \u0026gt; SELECT * FROM \u0026#34;cpu_load_short\u0026#34; WHERE \u0026#34;value\u0026#34; \u0026gt; 0.9 用户操作 # 查看所有用户 \u0026gt; show users user admin ---- ----- \u0026gt; # 创建一个root用户，设置密码为newpwd，主要不要使用双引号\u0026#34; 括起来，不然会报错 \u0026gt; create user \u0026#34;root\u0026#34; with password \u0026#39;newpwd\u0026#39; \u0026gt; # 再次查看用户信息，发现admin为false，说明还要设置一下权限。 \u0026gt; show users user admin ---- ----- root false \u0026gt; # 删除root用户 \u0026gt; drop user root \u0026gt; \u0026gt; show users user admin ---- ----- \u0026gt; # 重新设置root用户，并设置带上所有权限 \u0026gt; create user \u0026#34;root\u0026#34; with password \u0026#39;newpwd\u0026#39; with all privileges \u0026gt; # 发现admin权限为true了，那么admin的用户就创建好了。 \u0026gt; show users user admin ---- ----- root true \u0026gt;  开启认证需要修改influxdb.conf配置文件，把 [http] 下的 auth-enabled 选项设置为 true\n 更详细的使用方法，请参考：\nhttps://jasper-zhang1.gitbooks.io/influxdb/content/Guide/writing_data.html\n","permalink":"https://iblog.zone/archives/influxdb%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4/","summary":"连接数据库 $ influx -precision rfc3339 Connected to http://localhost:8086 version 1.2.x InfluxDB shell 1.2.x \u0026gt; exit # 退出命令行 说明:\n InfluxDB的HTTP接口默认起在8086上，所以influx默认也是连的本地的8086端口，你可以通过influx --help来看怎么修改默认值。 -precision参数表明了任何返回的时间戳的格式和精度，rfc3339`是让InfluxDB返回RFC339格式(YYYY-MM-DDTHH:MM:SS.nnnnnnnnnZ)的时间戳。  创建数据库 \u0026gt; CREATE DATABASE mydb 查看数据库 \u0026gt; SHOW DATABASES name: databases --------------- name _internal mydb 说明：_internal数据库是用来存储InfluxDB内部的实时监控数据的\n使用数据库 \u0026gt; USE mydb Using database mydb 数据存储格式介绍 首先对数据存储的格式来个入门介绍。InfluxDB里存储的数据被称为时间序列数据，其包含一个数值，就像CPU的load值或是温度值类似的。时序数据有零个或多个数据点，每一个都是一个指标值。数据点包括time(一个时间戳)，measurement(例如cpu_load)，至少一个k-v格式的field(也即指标的数值例如 “value=0.64”或者“temperature=21.2”)，零个或多个tag，其一般是对于这个指标值的元数据(例如“host=server01”, “region=EMEA”, “dc=Frankfurt)。\n在概念上，你可以将measurement类比于SQL里面的table，其主键索引总是时间戳。tag和field是在table里的其他列，tag是被索引起来的，field没有。不同之处在于，在InfluxDB里，你可以有几百万的measurements，你不用事先定义数据的scheme，并且null值不会被存储。\n将数据点写入InfluxDB，只需要遵守如下的行协议：\n\u0026lt;measurement\u0026gt;[,\u0026lt;tag-key\u0026gt;=\u0026lt;tag-value\u0026gt;...] \u0026lt;field-key\u0026gt;=\u0026lt;field-value\u0026gt;[,\u0026lt;field2-key\u0026gt;=\u0026lt;field2-value\u0026gt;...] [unix-nano-timestamp] 下面是数据写入InfluxDB的格式示例：\ncpu,host=serverA,region=us_west value=0.64 payment,device=mobile,product=Notepad,method=credit billed=33,licenses=3i 1434067467100293230 stock,symbol=AAPL bid=127.46,ask=127.48 temperature,machine=unit42,type=assembly external=25,internal=37 1434067467000000000 写入数据 \u0026gt; INSERT cpu,host=serverA,region=us_west value=0.","title":"InfluxDB常用命令"},{"content":"docker swarm监控方案有很多，主流的有cAdvisor+InfluxDB+Grafana和cAdvisor+Prometheus+Grafana，本文介绍cAdvisor+InfluxDB+Grafana方案\n组件说明  cAdvisor：数据收集模块，需要部署在集群中的每一个节点上，当然前提条件是节点接受task。 InfluxDB：数据存储模块。 Grafana：数据展示模块  创建docker compose文件 在manager结点上创建文件，并输入如下内容：\nversion: \u0026#39;3\u0026#39;  services:  influx:  image: influxdb:1.8\t# 保证模板兼容性  volumes:  - influx:/var/lib/influxdb  deploy:  replicas: 1  placement:  constraints:  - node.role == manager   grafana:  image: grafana/grafana:4.2.0  # 保证模板兼容性  ports:  - 0.0.0.0:80:3000  volumes:  - grafana:/var/lib/grafana  depends_on:  - influx  deploy:  replicas: 1  placement:  constraints:  - node.role == manager   cadvisor:  image: google/cadvisor  hostname: \u0026#39;{{.Node.Hostname}}\u0026#39;  command: -logtostderr -docker_only -storage_driver=influxdb -storage_driver_db=cadvisor -storage_driver_host=influx:8086  volumes:  - /:/rootfs:ro  - /var/run:/var/run:rw  - /sys:/sys:ro  - /var/lib/docker/:/var/lib/docker:ro  depends_on:  - influx  deploy:  mode: global  volumes:  influx:  driver: local  grafana:  driver: local 部署容器栈 在manager节点上执行如下命令：\ndocker stack deploy -c docker-stack.yml monitor 命令返回以后并不代表task已经完成部署，需要花一些时间，运行如下命令监控容器栈的部署状态：\ndocker stack services monitor 如下图所示：\n当红框中的数字前后匹配时，代表容器栈完成部署，再执行后序步骤。\n创建名称为cadvisor数据库存储数据 执行如下命令，确认monitor_cadvisor服务运行的node:\ndocker service ps monitor_influx 结果如下图红框所示：\n登录worker2结点，执行事下指令创建数据库：\ndocker exec `docker ps | grep -i influx | awk \u0026#39;{print $1}\u0026#39;` influx -execute \u0026#39;CREATE DATABASE cadvisor\u0026#39; 设置Grafana 因为Grafana被部署在了worker1节点上，在浏览器中访问http://192.168.56.104:80，使用默认的用户名\\密码：admin\\admin，如下图：\n点击\u0026quot;Add data source\u0026quot;添加数据源。如下图所示，按图中红框填写，其它项忽略：\n点击\u0026quot;Save \u0026amp;Test\u0026quot;，如果出现\u0026quot;Data source is working\u0026quot;表示数据源添加成功\n增加Dashboard配置。首先从https://github.com/botleg/swarm-monitoring/blob/master/dashboard.json下载示例配置文件，如下图所示，按提示上传配置文件：\n操作完成以后，结果如下图：\n至此，docker swarm集群监控系统部署完成，可以实现对宿主机及其上运行的容器的监控。\n","permalink":"https://iblog.zone/archives/docker-swarm%E9%9B%86%E7%BE%A4%E7%9B%91%E6%8E%A7%E6%96%B9%E6%A1%88/","summary":"docker swarm监控方案有很多，主流的有cAdvisor+InfluxDB+Grafana和cAdvisor+Prometheus+Grafana，本文介绍cAdvisor+InfluxDB+Grafana方案\n组件说明  cAdvisor：数据收集模块，需要部署在集群中的每一个节点上，当然前提条件是节点接受task。 InfluxDB：数据存储模块。 Grafana：数据展示模块  创建docker compose文件 在manager结点上创建文件，并输入如下内容：\nversion: \u0026#39;3\u0026#39;  services:  influx:  image: influxdb:1.8\t# 保证模板兼容性  volumes:  - influx:/var/lib/influxdb  deploy:  replicas: 1  placement:  constraints:  - node.role == manager   grafana:  image: grafana/grafana:4.2.0  # 保证模板兼容性  ports:  - 0.0.0.0:80:3000  volumes:  - grafana:/var/lib/grafana  depends_on:  - influx  deploy:  replicas: 1  placement:  constraints:  - node.","title":"Docker Swarm集群监控方案"},{"content":"安装 yum -y install https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.6.0-x86_64.rpm 配置 filebeat.inputs: - type: log  enabled: true  paths:  - /data/logs/server.ser  tags: [\u0026#34;online_server\u0026#34;] - type: log  enabled: true  paths:  - /data/logs/pay.ser  tags: [\u0026#34;online_pay\u0026#34;] - type: log  enabled: true  paths:  - /data/logs/log.*  tags: [\u0026#34;online_admin\u0026#34;]  output.elasticsearch:  hosts: [\u0026#34;10.0.0.1:9200\u0026#34;]  indices:  - index: \u0026#34;online_server-%{+yyyy.MM.dd}\u0026#34;  when.contains:  tags: \u0026#34;online_server\u0026#34;  - index: \u0026#34;online_pay-%{+yyyy.MM.dd}\u0026#34;  when.contains:  tags: \u0026#34;online_pay\u0026#34;  - index: \u0026#34;online_admin-%{+yyyy.MM.dd}\u0026#34;  when.contains:  tags: \u0026#34;online_admin\u0026#34; 查看ES是否有索引 curl -XGET \u0026#34;http://10.0.0.1:9200/_cat/indices?v\u0026#34; ","permalink":"https://iblog.zone/archives/filebeat7.6%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","summary":"安装 yum -y install https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-7.6.0-x86_64.rpm 配置 filebeat.inputs: - type: log  enabled: true  paths:  - /data/logs/server.ser  tags: [\u0026#34;online_server\u0026#34;] - type: log  enabled: true  paths:  - /data/logs/pay.ser  tags: [\u0026#34;online_pay\u0026#34;] - type: log  enabled: true  paths:  - /data/logs/log.*  tags: [\u0026#34;online_admin\u0026#34;]  output.elasticsearch:  hosts: [\u0026#34;10.0.0.1:9200\u0026#34;]  indices:  - index: \u0026#34;online_server-%{+yyyy.MM.dd}\u0026#34;  when.contains:  tags: \u0026#34;online_server\u0026#34;  - index: \u0026#34;online_pay-%{+yyyy.MM.dd}\u0026#34;  when.","title":"Filebeat7.6安装配置"},{"content":"规则：\n$remote_addr ~* ^(.*)\\.(.*)\\.(.*)\\.*[0268]$ 匹配末尾为0268这样的偶数ip，跳转到指定域名\n$remote_addr ~* ^(112)\\.(.*)\\.(.*)\\.(.*)$ 开头为 112 的 IP 跳转到指定的域名；\n$http_x_forwarded_for ~* ^(112)\\.(.*)\\.(.*)\\.(.*)$ 根据 forward 地址段来分流，开头为 112 的跳转到指定域名\n例如：\nserver {  listen 8080; # 监听端口  server_name 10.0.0.1; # 监听地址   access_log /data/logs/nginx_logs/10.0.0.1.log main;   location / {  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  if ( $remote_addr ~* 10.0.0.2) { ## 请求ip为10.0.0.2的流量转发到下列地址  proxy_pass http://172.16.0.2:10000;  break;  }  if ( $remote_addr ~* ^(.*)\\.(.*)\\.(.*)\\.*[0268]$) { ## 请求ip尾数为0268的ip转发到下列地址  proxy_pass http://172.16.0.3:10000;  break;  }  proxy_pass http://172.16.0.4:8080; ## 默认转发地址  } } if 指令条件判断的含义：\n正则表达式匹配，其中：  ~ 为区分大小写匹配  ~* 为不区分大小写匹配  !~ 和 !~* 分别为区分大小写不匹配及不区分大小写不匹配  文件及目录匹配，其中：  -f 和 !-f 用来判断是否存在文件  -d 和 !-d 用来判断是否存在目录  -e 和 !-e 用来判断是否存在文件或目录  -x 和 !-x 用来判断文件是否可执行  rewrite指令的最后一项参数为flag标记，flag标记有：  last 相当于 apache 里面的 [L] 标记，表示 rewrite。  break 本条规则匹配完成后，终止匹配，不再匹配后面的规则。  redirect 返回 302 临时重定向，浏览器地址会显示跳转后的 URL 地址。  permanent 返回 301 永久重定向，浏览器地址会显示跳转后的 URL 地址。 ","permalink":"https://iblog.zone/archives/nginx%E6%A0%B9%E6%8D%AE%E8%AF%B7%E6%B1%82ip%E8%BD%AC%E5%8F%91%E8%AF%B7%E6%B1%82/","summary":"规则：\n$remote_addr ~* ^(.*)\\.(.*)\\.(.*)\\.*[0268]$ 匹配末尾为0268这样的偶数ip，跳转到指定域名\n$remote_addr ~* ^(112)\\.(.*)\\.(.*)\\.(.*)$ 开头为 112 的 IP 跳转到指定的域名；\n$http_x_forwarded_for ~* ^(112)\\.(.*)\\.(.*)\\.(.*)$ 根据 forward 地址段来分流，开头为 112 的跳转到指定域名\n例如：\nserver {  listen 8080; # 监听端口  server_name 10.0.0.1; # 监听地址   access_log /data/logs/nginx_logs/10.0.0.1.log main;   location / {  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  if ( $remote_addr ~* 10.0.0.2) { ## 请求ip为10.0.0.2的流量转发到下列地址  proxy_pass http://172.16.0.2:10000;  break;  }  if ( $remote_addr ~* ^(.*)\\.(.*)\\.(.*)\\.*[0268]$) { ## 请求ip尾数为0268的ip转发到下列地址  proxy_pass http://172.","title":"Nginx根据请求IP转发请求"},{"content":"脚本内容如下：Xtrabackup.sh\n#!/bin/bash ##备份策略: ##周日(7)： 全备 ##周一 ~ 周六(1-6)： 增量备份  source /etc/profile ulimit -HSn 102400  ## ##========== global var ============ ##  #如果一台服务器上有多个MySQL，可以使用 BAK_DIR_ROOT进行备份路径的区别 #可增加端口作为区分，例如 /opt/backup/mysqk/3306 BAK_DIR_ROOT=\u0026#34;/data/mysql_backup\u0026#34; #默认周日进行全备 (1 - 7), 1 是周一，7是周日 FULL_BAK_DAY_OF_WEEK=7 #备份文件保留周期，默认保留35天 (4-5周) HOLD_DAYS=35  MYSQL_USERNAME=\u0026#34;root\u0026#34; MYSQL_PASSWORD=\u0026#34;root\u0026#34; MYSQL_HOST=\u0026#34;10.0.0.1\u0026#34;  MYSQL_CNF=\u0026#34;/etc/my.cnf\u0026#34; MYSQL_MULTI_GROUP=\u0026#34;--socket=/data/mysql/mysql.sock\u0026#34; #如果使用多实例，比如通过ecloud的方式下发安装，默认使用多实例 #MYSQL_MULTI_GROUP=\u0026#34;--defaults-group=mysqld3307 --socket=/tmp/mysql3307.sock\u0026#34;  CURRENT_WEEK_OF_YEAR=$(date +%U) CURRENT_DAY_OF_WEEK=$(date +%u) CURRENT_DATE=$(date +%F) CURRENT_TIME=$(date +%H-%M-%S) CURRENT_DATETIME=\u0026#34;${CURRENT_DATE}_${CURRENT_TIME}\u0026#34;  BAK_WEEK_DIR=\u0026#34;${BAK_DIR_ROOT}/WEEK_${CURRENT_WEEK_OF_YEAR}\u0026#34;  BAK_FULL_DIR=\u0026#34;${BAK_WEEK_DIR}/FULL\u0026#34;  BAK_LOG=\u0026#34;${BAK_WEEK_DIR}/backup.log\u0026#34;    ## ##========== function ============= ##  function clean_backup() {  find ${BAK_DIR_ROOT} -mtime +${HOLD_DAYS} -prune -exec rm -rf {} \\; }   function write_start_log() {   if [[ ! -d ${BAK_WEEK_DIR} ]];then  mkdir -p ${BAK_WEEK_DIR}  fi   echo \u0026#34;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt;\u0026gt; ${CURRENT_DATETIME}Begin Backup \u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026lt;\u0026#34; \u0026gt;\u0026gt; ${BAK_LOG}  }  function full_backup() {   if [[ ! -d ${BAK_FULL_DIR} ]]; then  mkdir -p ${BAK_FULL_DIR}  fi   echo \u0026#34;*** FULL BACKUP Date : ${CURRENT_DATETIME}\u0026#34; \u0026gt;\u0026gt; ${BAK_FULL_DIR}/full_backup.date   innobackupex --defaults-file=${MYSQL_CNF} ${MYSQL_MULTI_GROUP} --no-timestamp --user=${MYSQL_USERNAME} --password=${MYSQL_PASSWORD} --host=${MYSQL_HOST} --compress --compress-threads=4 --stream=xbstream --parallel=4 --extra-lsndir=\u0026#34;${BAK_FULL_DIR}/LSN_INFO\u0026#34; ${BAK_FULL_DIR} \u0026gt; \u0026#34;${BAK_FULL_DIR}/mysql_backup_full.xbstream\u0026#34; 2\u0026gt;\u0026gt; ${BAK_LOG}  }   #每周周一到周六进行增量备份 function incr_backup() {   CURRENT_INCR_DIR=\u0026#34;${BAK_WEEK_DIR}/INCR_${CURRENT_DAY_OF_WEEK}\u0026#34;  PREV_DAY_OF_WEEK=$((${CURRENT_DAY_OF_WEEK} - 1))  BASE_DIR=\u0026#34;${BAK_WEEK_DIR}/INCR_${PREV_DAY_OF_WEEK}\u0026#34;   #如果不存在之前的增量，则使用全量路径作为增量的BASE  #比如周一的时候  if [[ ! -d ${BASE_DIR} ]];then  BASE_DIR=${BAK_FULL_DIR}  fi   #如果在此函数中，还没有BASE，则认为可能是在项目第一周执行  #进行一次全量备份  if [[ ! -d ${BASE_DIR} ]];then  echo \u0026#34;*** ${BASE_DIR}as BASE_DIR is not exists!\u0026#34; \u0026gt;\u0026gt; ${BAK_LOG}  echo \u0026#34;*** So Backup Processor into FULL BACKUP \u0026#34; \u0026gt;\u0026gt; ${BAK_LOG}  full_backup  exit $?  fi   #如果存放增量数据的目录已经存在，这里进行添加时间戳处理（一天备份多次）  if [[ -d ${CURRENT_INCR_DIR} ]];then  CURRENT_INCR_DIR=\u0026#34;${CURRENT_INCR_DIR}_${CURRENT_DATETIME}\u0026#34;  fi   #如果BASE_DIR 存在，则进行增量备份  if [[ ! -d ${CURRENT_INCR_DIR} ]];then  mkdir -p ${CURRENT_INCR_DIR}  fi   echo \u0026#34;*** INCR BACKUP Date : ${CURRENT_DATETIME}\u0026#34; \u0026gt;\u0026gt; ${CURRENT_INCR_DIR}/incr_backup.date   innobackupex --defaults-file=${MYSQL_CNF} ${MYSQL_MULTI_GROUP} --no-timestamp --user=${MYSQL_USERNAME} --password=${MYSQL_PASSWORD} --host=${MYSQL_HOST} --compress --compress-threads=4 --stream=xbstream --parallel=4 --incremental --incremental-basedir=\u0026#34;${BASE_DIR}/LSN_INFO\u0026#34; --extra-lsndir=\u0026#34;${CURRENT_INCR_DIR}/LSN_INFO\u0026#34; ${CURRENT_INCR_DIR} \u0026gt; \u0026#34;${CURRENT_INCR_DIR}/mysql_backup_incr_${CURRENT_DAY_OF_WEEK}.xbstream\u0026#34; 2\u0026gt;\u0026gt; ${BAK_LOG}  }  # 同步至远程备份机器 function async_remote() {   echo \u0026#34;*** Start Backup \u0026#34; \u0026gt;\u0026gt; ${BAK_LOG}   rsync -av --delete /data/mysql_backup/* back@10.0.0.2:/data/mysql_backup/ \u0026gt;\u0026gt; ${BAK_LOG} }  # 修改同步目录权限 function modify_Dir_Permissions() {   chown -R back.back /data/mysql_backup }  #################main #################  clean_backup write_start_log   #如果指定的全备时间 == 当前的时间，则执行全备 if [[ ${FULL_BAK_DAY_OF_WEEK} -eq ${CURRENT_DAY_OF_WEEK} ]];then  full_backup  modify_Dir_Permissions  async_remote  exit $? else  incr_backup  modify_Dir_Permissions  async_remote  exit $? fi ","permalink":"https://iblog.zone/archives/mysql%E7%94%9F%E4%BA%A7%E7%BA%A7%E5%85%A8%E5%A4%87-%E5%A2%9E%E5%A4%87%E5%A4%87%E4%BB%BD%E8%84%9A%E6%9C%AC/","summary":"脚本内容如下：Xtrabackup.sh\n#!/bin/bash ##备份策略: ##周日(7)： 全备 ##周一 ~ 周六(1-6)： 增量备份  source /etc/profile ulimit -HSn 102400  ## ##========== global var ============ ##  #如果一台服务器上有多个MySQL，可以使用 BAK_DIR_ROOT进行备份路径的区别 #可增加端口作为区分，例如 /opt/backup/mysqk/3306 BAK_DIR_ROOT=\u0026#34;/data/mysql_backup\u0026#34; #默认周日进行全备 (1 - 7), 1 是周一，7是周日 FULL_BAK_DAY_OF_WEEK=7 #备份文件保留周期，默认保留35天 (4-5周) HOLD_DAYS=35  MYSQL_USERNAME=\u0026#34;root\u0026#34; MYSQL_PASSWORD=\u0026#34;root\u0026#34; MYSQL_HOST=\u0026#34;10.0.0.1\u0026#34;  MYSQL_CNF=\u0026#34;/etc/my.cnf\u0026#34; MYSQL_MULTI_GROUP=\u0026#34;--socket=/data/mysql/mysql.sock\u0026#34; #如果使用多实例，比如通过ecloud的方式下发安装，默认使用多实例 #MYSQL_MULTI_GROUP=\u0026#34;--defaults-group=mysqld3307 --socket=/tmp/mysql3307.sock\u0026#34;  CURRENT_WEEK_OF_YEAR=$(date +%U) CURRENT_DAY_OF_WEEK=$(date +%u) CURRENT_DATE=$(date +%F) CURRENT_TIME=$(date +%H-%M-%S) CURRENT_DATETIME=\u0026#34;${CURRENT_DATE}_${CURRENT_TIME}\u0026#34;  BAK_WEEK_DIR=\u0026#34;${BAK_DIR_ROOT}/WEEK_${CURRENT_WEEK_OF_YEAR}\u0026#34;  BAK_FULL_DIR=\u0026#34;${BAK_WEEK_DIR}/FULL\u0026#34;  BAK_LOG=\u0026#34;${BAK_WEEK_DIR}/backup.log\u0026#34;    ## ##========== function ============= ##  function clean_backup() {  find ${BAK_DIR_ROOT} -mtime +${HOLD_DAYS} -prune -exec rm -rf {} \\; }   function write_start_log() {   if [[ !","title":"MySQL生产级全备+增备备份脚本"},{"content":"1、RocketMQ-Console介绍 RocketMQ-Console现在已更名为Rocketmq-Dashboard，为了方便，下面还是使用RocketMQ-Console\nGit地址: https://github.com/apache/rocketmq-dashboard\n包含了多个功能：运维、驾驶舱、集群、主题、消费者、生产者、消息、消息轨迹、connector 等\n2、环境信息  服务器     服务器 操作系统 硬件配置 版本 说明     Linux CentOS 7 4C16G 64位 生产环境建议Linux/Unix     软件     工具/环境 版本 说明     JDK 1.8 RocketMQ Console要求版本在1.7以上   Git 1.8.3.1 源码方式安装需要，版本无要求   Maven 3.6.3 源码方式安装需求，版本无要求    3、安装 安装RocketMQ-Console，可以通过两种方式:\n 通过Docker镜像安装； 通过GitHub拉取源代码，进行编译，然后启动安装；  具体如下：\n通过Docker方式 # 拉取镜像 # 还可以自己通过源码的方式打包镜像，需要有镜像仓库。打镜像命令：mvn clean package -Dmaven.test.skip=true docker:build $ docker pull styletang/rocketmq-console-ng # 启动,默认端口号是8080端口，映射到服务器里面可以按照需求进行更改 $ docker run -e \u0026#34;JAVA_OPTS=-Drocketmq.namesrv.addr=127.0.0.1:9876 -Dcom.rocketmq.sendMessageWithVIPChannel=false\u0026#34; -p 8080:8080 -t styletang/rocketmq-console-ng 通过源码方式（推荐） # 从GitHub上面拉取代码 $ git clone https://github.com/apache/rocketmq-dashboard.git $ cd rocketmq-dashboard 默认端口号是8080，可以到rocketmq-dashboard/src/main/resources/application.properties进行修改。\n#这个填写自己的nameserver的地址，默认是localhost:9876 rocketmq.config.namesrvAddr=192.168.113.11:9876 #rocketmq-console的数据目录，默认为 /tmp/rocketmq-console/data rocketmq.config.dataPath=/usr/local/rocketmq-console/data #开启认证登录功能，默认为false rocketmq.config.loginRequired=true 修改了以上参数之后，使用maven进行编译\n# maven打包 $ mvn clean package -Dmaven.test.skip=true $ nohup java -jar target/rocketmq-dashboard-1.0.0.jar \u0026amp; 4、开放防火墙对应端口号 如果你的服务器开通了防火墙，需要对端口号进行开放\n#查看默认防火墙状态（关闭后显示notrunning，开启后显示running） $ firewall-cmd --state # 开启一个指定端口号 # --permanent 永久生效，没有此参数重启后失效  $ firewall-cmd --zone=public --add-port=8080/tcp --permanent #重载防火墙规则 firewall-cmd --reload 5、访问 地址：http://localhost:8080 端口如果更改过就用更改过的\n6、设置登录验证的账号密码 前面我们虽然开启了登录验证的功能，但是并没有设置账户密码，这里默认的账户密码为 admin/admin。我们修改下这个密码。 在 rocketmq-console的数据目录下新建账户文件 users.properties 。\nvim /usr/local/rocketmq-console/data/users.properties 下面是文件的内容\n# 该文件支持热修改，即添加和修改用户时，不需要重新启动console # 格式， 每行定义一个用户， username=password[,N] #N是可选项，可以为0 (普通用户)； 1 （管理员）   #定义管理员  admin=test123456,1  #定义普通用户 #user1=user1 #user2=user2 然后我们杀进程，重启这个jar包即可。\n","permalink":"https://iblog.zone/archives/rocketmq-console%E5%AE%89%E8%A3%85%E5%8F%8A%E9%85%8D%E7%BD%AE/","summary":"1、RocketMQ-Console介绍 RocketMQ-Console现在已更名为Rocketmq-Dashboard，为了方便，下面还是使用RocketMQ-Console\nGit地址: https://github.com/apache/rocketmq-dashboard\n包含了多个功能：运维、驾驶舱、集群、主题、消费者、生产者、消息、消息轨迹、connector 等\n2、环境信息  服务器     服务器 操作系统 硬件配置 版本 说明     Linux CentOS 7 4C16G 64位 生产环境建议Linux/Unix     软件     工具/环境 版本 说明     JDK 1.8 RocketMQ Console要求版本在1.7以上   Git 1.8.3.1 源码方式安装需要，版本无要求   Maven 3.6.3 源码方式安装需求，版本无要求    3、安装 安装RocketMQ-Console，可以通过两种方式:\n 通过Docker镜像安装； 通过GitHub拉取源代码，进行编译，然后启动安装；  具体如下：\n通过Docker方式 # 拉取镜像 # 还可以自己通过源码的方式打包镜像，需要有镜像仓库。打镜像命令：mvn clean package -Dmaven.","title":"RocketMQ Console安装及配置"},{"content":"下载Microsoft SQL Server 2008 R2，打开安装文件夹，以管理员身份运行setup.exe。（百度网盘链接: https://pan.baidu.com/s/11b_YAsNm-zO_BDbX8QnpSA?pwd=arxn 提取码: arxn ）\n1、安装 弹出安装程序界面，选择安装，“全新安装或向现有安装添加功能”。\n进行检测，全部通过即可，一般都是全部通过的（只要不存在失败和警告，即不存在问题，可继续安装），有时可能需要重启计算机，按照要求操作即可。\n输入产品秘钥，此安装程序默认输入（软件自动默认输入），直接点击下一步。\n接受许可条款，点击下一步。\n安装支持文件。\n安装程序支持规则，通过即可点击下一步。\n2、设置角色 设置角色，选择默认的“SQL Server功能安装”，点击下一步。\n3、功能选择 选择功能，“全选”，安装目录可自定义（自定义安装在C/D盘），点击下一步。\n4、安装规则 安装规则，全部为“通过/跳过”就没问题（与前面类似，只要不出现警告和失败即不存在问题，可继续下面的安装），点击下一步。\n5、实例配置 实例配置，选择：默认实例，实例根目录安装位置可自定义的即可，点击下一步。\n磁盘空间需求，满足要求即可点击下一步。\n服务器配置，按照如图配置一样的账户名，点击下一步。（下图中启动类型，可根据需要设置成手动或者自动，对于本系统，全部设置成手动即可）。\n点击下三角，选择“***\\SYSTEM”，***表示本机电脑账户名称，不同电脑账户名称不同。\n数据库引擎配置，选择“混合模式”，这里设置密码为xxxxxxx，指定SQL Server 管理员“添加当前用户”。\nAnalysis Services 配置，添加当前用户，然后点击下一步。\nReporting Services 配置，按照本机模式默认配置。\n错误报告，可不勾选发送错误报告，点击下一步。\n6、安装配置规则 安装配置规则，通过/跳过，没问题，点击下一步。\n开始安装，耐心等待即可。（根据电脑配置不同安装时间在20min-40min不等。）\n7、安装完成 安装完成，至此安装完毕。\n8、启动数据库 启动SQL Server 2008，选择开始菜单中的Microsoft SQL Server 2008下的“SQL Server Management Studio”（也可将该选项拖到桌面作为快捷方式，便于以后使用），启动SQL Server服务，如图所示：\n点击启动后看到如下界面：\n","permalink":"https://iblog.zone/archives/microsoft-sql-server-2008-r2%E5%AE%89%E8%A3%85/","summary":"下载Microsoft SQL Server 2008 R2，打开安装文件夹，以管理员身份运行setup.exe。（百度网盘链接: https://pan.baidu.com/s/11b_YAsNm-zO_BDbX8QnpSA?pwd=arxn 提取码: arxn ）\n1、安装 弹出安装程序界面，选择安装，“全新安装或向现有安装添加功能”。\n进行检测，全部通过即可，一般都是全部通过的（只要不存在失败和警告，即不存在问题，可继续安装），有时可能需要重启计算机，按照要求操作即可。\n输入产品秘钥，此安装程序默认输入（软件自动默认输入），直接点击下一步。\n接受许可条款，点击下一步。\n安装支持文件。\n安装程序支持规则，通过即可点击下一步。\n2、设置角色 设置角色，选择默认的“SQL Server功能安装”，点击下一步。\n3、功能选择 选择功能，“全选”，安装目录可自定义（自定义安装在C/D盘），点击下一步。\n4、安装规则 安装规则，全部为“通过/跳过”就没问题（与前面类似，只要不出现警告和失败即不存在问题，可继续下面的安装），点击下一步。\n5、实例配置 实例配置，选择：默认实例，实例根目录安装位置可自定义的即可，点击下一步。\n磁盘空间需求，满足要求即可点击下一步。\n服务器配置，按照如图配置一样的账户名，点击下一步。（下图中启动类型，可根据需要设置成手动或者自动，对于本系统，全部设置成手动即可）。\n点击下三角，选择“***\\SYSTEM”，***表示本机电脑账户名称，不同电脑账户名称不同。\n数据库引擎配置，选择“混合模式”，这里设置密码为xxxxxxx，指定SQL Server 管理员“添加当前用户”。\nAnalysis Services 配置，添加当前用户，然后点击下一步。\nReporting Services 配置，按照本机模式默认配置。\n错误报告，可不勾选发送错误报告，点击下一步。\n6、安装配置规则 安装配置规则，通过/跳过，没问题，点击下一步。\n开始安装，耐心等待即可。（根据电脑配置不同安装时间在20min-40min不等。）\n7、安装完成 安装完成，至此安装完毕。\n8、启动数据库 启动SQL Server 2008，选择开始菜单中的Microsoft SQL Server 2008下的“SQL Server Management Studio”（也可将该选项拖到桌面作为快捷方式，便于以后使用），启动SQL Server服务，如图所示：\n点击启动后看到如下界面：","title":"Microsoft SQL Server 2008 R2安装"},{"content":" MySQL权限级别 全局性的管理权限，作用于整个MySQL实例级别 数据库级别的权限，作用于某个指定的数据库上或者所有的数据库上 数据库对象级别的权限，作用于指定的数据库对象上(表、视图等)或 者所有的数据库对象上 权限存储在mysql库的user, db, tables_priv, columns_priv, and procs_priv这几个系统表中，待MySQL实例启动后就加载到内存中  MySQL权限级别介绍 对比root用户在几个权限系统表中的数据\nmysql\u0026gt; select * from user where user=‘root’ and host=‘localhost’; ##都是’Y’ mysql\u0026gt; select * from db where user=‘root’ and host=‘localhost’; ##无记录 mysql\u0026gt; select * from tables_priv where host=‘localhost’ and user=‘root’; ##无记录 mysql\u0026gt; select * from columns_priv where user=‘root’ and host=‘localhost’; ##无记录 mysql\u0026gt; select * from procs_priv where user=‘root’ and host=‘localhost’; ##无记录 MySQL权限详解   All/All Privileges权限代表全局或者全数据库对象级别的所有权限\n  Alter权限代表允许修改表结构的权限，但必须要求有create和insert权限配合。如果是rename表名，则要求有alter和drop原表，create和 insert新表的权限\n  Alter routine权限代表允许修改或者删除存储过程、函数的权限\n  Create权限代表允许创建新的数据库和表的权限\n  Create routine权限代表允许创建存储过程、函数的权限\n  Create tablespace权限代表允许创建、修改、删除表空间和日志组的权限\n  Create temporary tables权限代表允许创建临时表的权限\n  Create user权限代表允许创建、修改、删除、重命名user的权限\n  Create view权限代表允许创建视图的权限\n  Delete权限代表允许删除行数据的权限\n  Drop权限代表允许删除数据库、表、视图的权限，包括truncate table命令\n  Event权限代表允许查询，创建，修改，删除MySQL事件\n  Execute权限代表允许执行存储过程和函数的权限\n  File权限代表允许在MySQL可以访问的目录进行读写磁盘文件操作，可使用的命令包括load data infile,select \u0026hellip; into outfile,load file()函数\n  Grant option权限代表是否允许此用户授权或者收回给其他用户你给予的权限\n  Index权限代表是否允许创建和删除索引\n  Insert权限代表是否允许在表里插入数据，同时在执行analyze table,optimize table,repair table语句的时候也需要insert权限\n  Lock权限代表允许对拥有select权限的表进行锁定，以防止其他链接对此表的读或写\n  Process权限代表允许查看MySQL中的进程信息，比如执行show processlist,mysqladmin processlist, show engine等命令\n  Reference权限是在5.7.6版本之后引入，代表是否允许创建外键\n  Reload权限代表允许执行flush命令，指明重新加载权限表到系统内存中， refresh命令代表关闭和重新开启日志文件并刷新所有的表\n  Replication client权限代表允许执行show master status,show slave status,show binary logs命令\n  Replication slave权限代表允许slave主机通过此用户连接master以便建立主从复制关系\n  Select权限代表允许从表中查看数据，某些不查询表数据的select执行则不需要此权限，如Select 1+1，Select PI()+2;而且select权限在执行update/delete 语句中含有where条件的情况下也是需要的\n  Show databases权限代表通过执行show databases命令查看所有的数据库名\n  Show view权限代表通过执行show create view命令查看视图创建的语句\n  Shutdown权限代表允许关闭数据库实例，执行语句包括mysqladmin shutdown\n  Super权限代表允许执行一系列数据库管理命令，包括kill强制关闭某个连接命令，change master to创建复制关系命令，以及create/alter/drop server等命令\n  Trigger权限代表允许创建，删除，执行，显示触发器的权限\n  Update权限代表允许修改表中的数据的权限\n  Usage权限是创建一个用户之后的默认权限，其本身代表连接登录权限\n  系统权限表 权限存储在mysql库的user,db, tables_priv, columns_priv, and procs_priv这几个系统表中，待MySQL实例启动后就加载到内存中\n User表:存放用户账户信息以及全局级别(所有数据库)权限，决定了来自哪些主机的哪些用户可以访问数据库实例，如果有全局权限则意味着对所有数据库都有此权限 Db表:存放数据库级别的权限，决定了来自哪些主机的哪些用户可以访 问此数据库 Tables_priv表:存放表级别的权限，决定了来自哪些主机的哪些用户可以 访问数据库的这个表 Columns_priv表:存放列级别的权限，决定了来自哪些主机的哪些用户可 以访问数据库表的这个字段 Procs_priv表:存放存储过程和函数级别的权限  User和db权限表结构\n User权限表结构中的特殊字段  Plugin,password,authentication_string三个字段存放用户认证信息 Password_expired设置成’Y’则表明允许DBA将此用户的密码设置成过期而且过期后要求用户的使用者重置密码(alter user/set password重置密码) Password_last_changed作为一个时间戳字段代表密码上次修改时间，执行create user/alter user/set password/grant等命令创建用户或修改用户密码时此数值自动更新 Password_lifetime代表从password_last_changed时间开始此密码过期的天数 Account_locked代表此用户被锁住，无法使用   Tables_priv和columns_priv权限表结构  Timestamp和grantor两个字段暂时没用 Tables_priv和columns_priv权限值   procs_priv权限表结构  Routine_type是枚举类型，代表是存储过程还是函数 Timestamp和grantor两个字段暂时没用 系统权限表字段长度限制表 权限认证中的大小写敏感问题 字段user,password,authencation_string,db,table_name大小写敏感 字段host,column_name,routine_name大小写不敏感 User用户大小写敏感    mysql\u0026gt; create user abc@localhost; ERROR 1396 (HY000): Operation CREATE USER failed for \u0026#39;abc\u0026#39;@\u0026#39;localhost\u0026#39; mysql\u0026gt; create user Abc@localhost; Query OK, 0 rows affected (0.01 sec)  ​\tHost主机名大小写不敏感  mysql\u0026gt; create user abc@Localhost; ERROR 1396 (HY000): Operation CREATE USER failed for \u0026#39;abc\u0026#39;@\u0026#39;localhost\u0026#39; MySQL授权用户  MySQL的授权用户由两部分组成:用户名和登录主机名 表达用户的语法为‘user_name’@‘host_name’ 单引号不是必须，但如果其中包含特殊字符则是必须的 ‘’@‘localhost’代表匿名登录的用户 Host_name可以使主机名或者ipv4/ipv6的地址。Localhost代表本机，127.0.0.1代表ipv4的本机地址，::1代表ipv6的本机地址 Host_name字段允许使用%和_两个匹配字符，比如’%’代表所有主机，’%.mysql.com’代表来自mysql.com这个域名下的所有主机，‘192.168.1.%’代表所有来自192.168.1网段的主机  MySQL修改权限的生效  执行Grant,revoke,setpassword,renameuser命令修改权限之后，MySQL会自动将修改后的权限信息同步加载到系统内存中 如果执行insert/update/delete操作上述的系统权限表之后，则必须再执行刷新权限命令才能同步到系统内存中，刷新权限命令包括:flush privileges/mysqladmin flush-privileges/mysqladmin reload 如果是修改tables和columns级别的权限，则客户端的下次操作新权限就会生效 如果是修改database级别的权限，则新权限在客户端执行use database命令后生效 如果是修改global级别的权限，则需要重新创建连接新权限才能生效 \u0026ndash;skip-grant-tables可以跳过所有系统权限表而允许所有用户登录，只在特殊情况下暂时使用  MySQL用户连接 mysql --user=finley --password db_name mysql -u finley -p db_name mysql --user=finley --password=password db_name shell\u0026gt; mysql -u finley -ppassword db_name 创建MySQL用户  有两种方式创建MySQL授权用户  执行createuser/grant命令(推荐方式) 通过insert语句直接操作MySQL系统权限表    mysql\u0026gt; CREATE USER \u0026#39;finley\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;some_pass\u0026#39;; mysql\u0026gt;GRANTALLPRIVILEGESON*.*TO\u0026#39;finley\u0026#39;@\u0026#39;localhost\u0026#39; WITH GRANT OPTION; mysql\u0026gt; CREATE USER \u0026#39;finley\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;some_pass\u0026#39;; mysql\u0026gt; GRANT ALL PRIVILEGES ON *.* TO \u0026#39;finley\u0026#39;@\u0026#39;%‘ WITH GRANT OPTION; mysql\u0026gt; CREATE USER \u0026#39;admin\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;admin_pass\u0026#39;; mysql\u0026gt; GRANT RELOAD,PROCESS ON *.* TO \u0026#39;admin\u0026#39;@\u0026#39;localhost\u0026#39;; mysql\u0026gt; grant select(id) on test.temp to cdq@localhost; 创建MySQL用户\nmysql\u0026gt; CREATE USER \u0026#39;custom\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;obscure\u0026#39;; mysql\u0026gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP -\u0026gt; ON bankaccount.* -\u0026gt; TO \u0026#39;custom\u0026#39;@\u0026#39;localhost\u0026#39;;  mysql\u0026gt; CREATE USER \u0026#39;custom\u0026#39;@\u0026#39;host47.example.com\u0026#39; IDENTIFIED BY \u0026#39;obscure\u0026#39;; mysql\u0026gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP -\u0026gt; ON expenses.* -\u0026gt; TO \u0026#39;custom\u0026#39;@\u0026#39;host47.example.com\u0026#39;;  mysql\u0026gt; CREATE USER \u0026#39;custom\u0026#39;@\u0026#39;%.example.com\u0026#39; IDENTIFIED BY \u0026#39;obscure\u0026#39;; mysql\u0026gt; GRANT SELECT,INSERT,UPDATE,DELETE,CREATE,DROP 回收MySQL用户权限  通过revoke命令收回用户权限  mysql\u0026gt; revoke select on `sys`.`sys_config` from \u0026#39;mysql.sys\u0026#39;@localhost; 删除MySQL用户  通过执行drop user命令删除MySQL用户  mysql\u0026gt; DROP USER \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39;; 设置MySQL用户资源限制  通过设置全局变量max_user_connections可以限制所有用户在同一时间连接MySQL实例的数量，但此参数无法对每个用户区别对待，所以 MySQL提供了对每个用户的资源限制管理 MAX_QUERIES_PER_HOUR:一个用户在一个小时内可以执行查询的次数(基本包含所有语句) MAX_UPDATES_PER_HOUR:一个用户在一个小时内可以执行修改的次数(仅包含修改数据库或表的语句) MAX_CONNECTIONS_PER_HOUR:一个用户在一个小时内可以连接MySQL的时间 MAX_USER_CONNECTIONS:一个用户可以在同一时间连接MySQL实例的数量 从5.0.3版本开始，对用户‘user’@‘%.example.com’的资源限制是指所有通过example.com域名主机连接user用户的连接，而不是分别指从 host1.example.com和host2.example.com主机过来的连接  设置MySQL用户资源限制  通过执行createuser/alteruser设置/修改用户的资源限制  mysql\u0026gt; CREATE USER \u0026#39;francis\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;frank\u0026#39; -\u0026gt; -\u0026gt; -\u0026gt; -\u0026gt; WITH MAX_QUERIES_PER_HOUR 20 MAX_UPDATES_PER_HOUR 10 MAX_CONNECTIONS_PER_HOUR 5 MAX_USER_CONNECTIONS 2; mysql\u0026gt; ALTER USER \u0026#39;francis\u0026#39;@\u0026#39;localhost\u0026#39; WITH MAX_QUERIES_PER_HOUR 100;  取消某项资源限制既是把原先的值修改成0  mysql\u0026gt; ALTER USER \u0026#39;francis\u0026#39;@\u0026#39;localhost\u0026#39; WITH MAX_CONNECTIONS_PER_HOUR 0;  当针对某个用户的max_user_connections非0时，则忽略全局系统参数max_user_connections，反之则全局系统参数生效 设置MySQL用户的密码 执行create user创建用户和密码  mysql\u0026gt; CREATE USER \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;mypass\u0026#39;;  修改用户密码的方式包括:  mysql\u0026gt; ALTER USER \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;mypass\u0026#39;; mysql\u0026gt; SET PASSWORD FOR \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39; = PASSWORD(\u0026#39;mypass\u0026#39;); mysql\u0026gt; GRANT USAGE ON *.* TO \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;mypass\u0026#39;; shell\u0026gt; mysqladmin -u user_name -h host_name password \u0026#34;new_password\u0026#34;;  修改本身用户密码的方式包括:  mysql\u0026gt; ALTER USER USER() IDENTIFIED BY \u0026#39;mypass\u0026#39;; mysql\u0026gt; SET PASSWORD = PASSWORD(\u0026#39;mypass\u0026#39;); 设置MySQL用户密码过期策略  设置系统参数default_password_lifetime作用于所有的用户账户 default_password_lifetime=180 设置180天过期 default_password_lifetime=0 设置密码不过期 如果为每个用户设置了密码过期策略，则会覆盖上述系统参数  ALTER USER \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39; PASSWORD EXPIRE INTERVAL90DAY; ALTER USER \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39; PASSWORD EXPIRE INEVER;密码不过期 ALTER USER‘jeffrey’@‘localhost’ PASSWORD EXPIRE DEFAULT;默认过期策略  手动强制某个用户密码过期  ALTER USER \u0026#39;jeffrey\u0026#39;@\u0026#39;localhost\u0026#39; PASSWORD EXPIRE; mysql\u0026gt; SELECT 1; ERROR 1820 (HY000): You must SET PASSWORD before executing this statement mysql\u0026gt; ALTER USER USER() IDENTIFIED BY \u0026#39;new_password\u0026#39;; Query OK, 0 rows affected (0.01 sec) mysql\u0026gt; SELECT 1; |1| MySQL用户lock 通过执行create user/alter user命令中带account lock/unlock子句设置用户的lock状态 Createuser语句默认的用户是unlock状态\nmysql\u0026gt;create user abc2@localhost identified by \u0026#39;mysql\u0026#39; account lock; QueryOK,0rowsaffected(0.01sec) Alter user语句默认不会修改用户的lock/unlock状态 mysql\u0026gt;alter user \u0026#39;mysql.sys\u0026#39;@localhost account lock; Query OK,0 row saffected(0.00sec) mysql\u0026gt;alter user \u0026#39;mysql.sys\u0026#39;@localhost account unlock; Query OK,0 row saffected(0.00sec) -- 当客户端使用lock状态的用户登录MySQL时，会收到如此报错 Access denied for user \u0026#39;user_name\u0026#39;@\u0026#39;host_name\u0026#39;. Account is locked. 企业应用中的常规MySQL用户  企业生产系统中MySQL用户的创建通常由DBA统一协调创建，而且按需创建 DBA通常直接使用root用户来管理数据库 通常会创建指定业务数据库上的增删改查、临时表、执行存储过程的权限给应用程序来连接数据库  Create user app_full identified by ‘mysql’; Grant select,update,insert,delete,create temporary tables,execute on esn.* to app_full@’10.0.0.%’; mysql\u0026gt;show grants for app_full@\u0026#39;10.0.0.%\u0026#39;; +------------------------------------------------------------------------------------------------------------+ |Grantsforapp_full@10.0.0.% | +------------------------------------------------------------------------------------------------------------+ |GRANTUSAGEON*.*TO\u0026#39;app_full\u0026#39;@\u0026#39;10.0.0.%\u0026#39; | | GRANT SELECT, INSERT, UPDATE, DELETE, CREATE TEMPORARY TABLES, EXECUTE ON `esn`.* TO \u0026#39;app_full\u0026#39;@\u0026#39;10.0.0.%\u0026#39; |  通常也会创建指定业务数据库上的只读权限给特定应用程序或某些高级别人员来查询数据，防止数据被修改  Create user app_readonly identified by ‘mysql’; Grant select on esn.* to app_readonly identified by ‘mysq’; ","permalink":"https://iblog.zone/archives/mysql%E6%9D%83%E9%99%90%E7%BA%A7%E5%88%AB%E4%BB%8B%E7%BB%8D/","summary":"MySQL权限级别 全局性的管理权限，作用于整个MySQL实例级别 数据库级别的权限，作用于某个指定的数据库上或者所有的数据库上 数据库对象级别的权限，作用于指定的数据库对象上(表、视图等)或 者所有的数据库对象上 权限存储在mysql库的user, db, tables_priv, columns_priv, and procs_priv这几个系统表中，待MySQL实例启动后就加载到内存中  MySQL权限级别介绍 对比root用户在几个权限系统表中的数据\nmysql\u0026gt; select * from user where user=‘root’ and host=‘localhost’; ##都是’Y’ mysql\u0026gt; select * from db where user=‘root’ and host=‘localhost’; ##无记录 mysql\u0026gt; select * from tables_priv where host=‘localhost’ and user=‘root’; ##无记录 mysql\u0026gt; select * from columns_priv where user=‘root’ and host=‘localhost’; ##无记录 mysql\u0026gt; select * from procs_priv where user=‘root’ and host=‘localhost’; ##无记录 MySQL权限详解   All/All Privileges权限代表全局或者全数据库对象级别的所有权限","title":"MySQL权限级别介绍"},{"content":"一、nginx获取客户端真实IP、域名、协议、端口 需要在Nginx的配置文件nginx.conf中添加如下配置\nproxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; 各参数的含义如下所示。\n Host包含客户端真实的域名和端口号； X-Forwarded-Proto表示客户端真实的协议（http还是https）； X-Real-IP表示客户端真实的IP； X-Forwarded-For这个Header和X-Real-IP类似，但它在多层代理时会包含真实客户端及中间每个代理服务器的IP  二、nginx负载均衡配置 http {  ……  upstream real_server {  server 192.168.103.100:2001 weight=1; #轮询服务器和访问权重  server 192.168.103.100:2002 weight=2;  }   server {  listen 80;   location / {  proxy_pass http://real_server;  }  } } nginx负载均衡失败重试配置\nupstream real_server {  server 192.168.103.100:2001 weight=1 max_fails=2 fail_timeout=60s;  server 192.168.103.100:2002 weight=2 max_fails=2 fail_timeout=60s; } 意思是在fail_timeout时间内失败了max_fails次请求后，则认为该上游服务器不可用，然后将该服务地址踢除掉。fail_timeout时间后会再次将该服务器加入存活列表，进行重试\n三、nginx限流配置 1.配置参数 limit_req_zone指令设置参数\nlimit_req_zone $binary_remote_addr zone=mylimit:10m rate=10r/s;  limit_req_zone定义在http块中，$binary_remote_addr表示保存客户端IP地址的二进制形式。 Zone定义IP状态及URL访问频率的共享内存区域。zone=keyword标识区域的名字，以及冒号后面跟区域大小。16000个IP地址的状态信息约1MB，所以示例中区域可以存储160000个IP地址。 Rate定义最大请求速率。示例中速率不能超过每秒10个请求。  2.设置限流 location / {  limit_req zone=mylimit burst=20 nodelay;  proxy_pass http://real_server; } burst排队大小，nodelay不限制单个请求间的时间。\n3.不限流白名单 geo $limit { default 1; 192.168.2.0/24 0; }  map $limit $limit_key { 1 $binary_remote_addr; 0 \u0026#34;\u0026#34;; }  limit_req_zone $limit_key zone=mylimit:10m rate=1r/s;  location / {  limit_req zone=mylimit burst=1 nodelay;  proxy_pass http://real_server; } 上述配置中，192.168.2.0/24网段的IP访问是不限流的，其他限流。\nIP后面的数字含义：\n 24表示子网掩码:255.255.255.0 16表示子网掩码:255.255.0.0 8表示子网掩码:255.0.0.0  四、nginx缓存配置 1.浏览器缓存 静态资源缓存用expire\nlocation ~* .(jpg|jpeg|png|gif|ico|css|js)$ {  expires 2d; } Response Header中添加了Expires和Cache-Control,\n静态资源包括（一般缓存）\n 普通不变的图像，如logo，图标等 js、css静态文件 可下载的内容，媒体文件  协商缓存（add_header ETag/Last-Modified value）\n HTML文件 经常替换的图片 经常修改的js、css文件 基本不变的API接口  不需要缓存\n 用户隐私等敏感数据 经常改变的api数据接口  2.代理层缓存 //缓存路径，inactive表示缓存的时间，到期之后将会把缓存清理 proxy_cache_path /data/cache/nginx/ levels=1:2 keys_zone=cache:512m inactive = 1d max_size=8g;  location / {  location ~ \\.(htm|html)?$ {  proxy_cache cache;  proxy_cache_key $uri$is_args$args; //以此变量值做HASH，作为KEY  //HTTP响应首部可以看到X-Cache字段，内容可以有HIT,MISS,EXPIRES等等  add_header X-Cache $upstream_cache_status;  proxy_cache_valid 200 10m;  proxy_cache_valid any 1m;  proxy_pass http://real_server;  proxy_redirect off;  }  location ~ .*\\.(gif|jpg|jpeg|bmp|png|ico|txt|js|css)$ {  root /data/webapps/edc;  expires 3d;  add_header Static Nginx-Proxy;  } } 在本地磁盘创建一个文件目录，根据设置，将请求的资源以K-V形式缓存在此目录当中，KEY需要自己定义（这里用的是url的hash值），同时可以根据需要指定某内容的缓存时长，比如状态码为200缓存10分钟，状态码为301，302的缓存5分钟，其他所有内容缓存1分钟等等。 可以通过purger的功能清理缓存。\nAB测试/个性化需求时应禁用掉浏览器缓存\n五、nginx黑名单配置 1.一般配置 location / {  deny 192.168.1.1;  deny 192.168.1.0/24;  allow 10.1.1.0/16;  allow 2001:0db8::/32;  deny all; } 2. Lua+Redis动态黑名单(OpenResty) 安装运行\nyum install yum-utils yum-config-manager --add-repo https://openresty.org/package/centos/openresty.repo yum install openresty yum install openresty-resty 查看 yum --disablerepo=\u0026#34;*\u0026#34; --enablerepo=\u0026#34;openresty\u0026#34; list available 运行 service openresty start 配置(/usr/local/openresty/nginx/conf/nginx.conf)\nlua_shared_dict ip_blacklist 1m;  server {  listen 80;   location / {  access_by_lua_file lua/ip_blacklist.lua;  proxy_pass http://real_server;  } } lua脚本（ip_blacklist.lua）\nlocal redis_host = \u0026#34;192.168.1.132\u0026#34; local redis_port = 6379 local redis_pwd = 123456 local redis_db = 2  -- connection timeout for redis in ms. local redis_connection_timeout = 100  -- a set key for blacklist entries local redis_key = \u0026#34;ip_blacklist\u0026#34;  -- cache lookups for this many seconds local cache_ttl = 60  -- end configuration  local ip = ngx.var.remote_addr local ip_blacklist = ngx.shared.ip_blacklist local last_update_time = ip_blacklist:get(\u0026#34;last_update_time\u0026#34;);  -- update ip_blacklist from Redis every cache_ttl seconds: if last_update_time == nil or last_update_time \u0026lt; ( ngx.now() - cache_ttl ) then   local redis = require \u0026#34;resty.redis\u0026#34;;  local red = redis:new();   red:set_timeout(redis_connect_timeout);   local ok, err = red:connect(redis_host, redis_port);  if not ok then  ngx.log(ngx.ERR, \u0026#34;Redis connection error while connect: \u0026#34; .. err);  else  local ok, err = red:auth(redis_pwd)  if not ok then  ngx.log(ngx.ERR, \u0026#34;Redis password error while auth: \u0026#34; .. err);  else  local new_ip_blacklist, err = red:smembers(redis_key);  if err then  ngx.log(ngx.ERR, \u0026#34;Redis read error while retrieving ip_blacklist: \u0026#34; .. err);  else  ngx.log(ngx.ERR, \u0026#34;Get data success:\u0026#34; .. new_ip_blacklist)  -- replace the locally stored ip_blacklist with the updated values:  ip_blacklist:flush_all();  for index, banned_ip in ipairs(new_ip_blacklist) do  ip_blacklist:set(banned_ip, true);  end  -- update time  ip_blacklist:set(\u0026#34;last_update_time\u0026#34;, ngx.now());  end  end  end end  if ip_blacklist:get(ip) then  ngx.log(ngx.ERR, \u0026#34;Banned IP detected and refused access: \u0026#34; .. ip);  return ngx.exit(ngx.HTTP_FORBIDDEN); end 六、nginx灰度发布配置 1.根据Cookie实现灰度发布 根据Cookie查询version值，如果该version值为v1转发到host1，为v2转发到host2，都不匹配的情况下转发到默认配置。\nupstream host1 {  server 192.168.2.46:2001 weight=1; #轮询服务器和访问权重  server 192.168.2.46:2002 weight=2; }  upstream host2 {  server 192.168.1.155:1111 max_fails=1 fail_timeout=60; }  upstream default {  server 192.168.1.153:1111 max_fails=1 fail_timeout=60; }  map $COOKIE_version $group {  ~*v1$ host1;  ~*v2$ host2;  default default; }  lua_shared_dict ip_blacklist 1m;  server {  listen 80;   #set $group \u0026#34;default\u0026#34;;  #if ($http_cookie ~* \u0026#34;version=v1\u0026#34;){  # set $group host1;  #}  #if ($http_cookie ~* \u0026#34;version=v2\u0026#34;){  # set $group host2;  #}   location / {  access_by_lua_file lua/ip_blacklist.lua;  proxy_pass http://$group;  } } 2.根据来路IP实现灰度发布 server {  ……………  set $group default;  if ($remote_addr ~ \u0026#34;192.168.119.1\u0026#34;) {  set $group host1;  }  if ($remote_addr ~ \u0026#34;192.168.119.2\u0026#34;) {  set $group host2;  } 3.更细粒度灰度发布 参考：\nhttps://github.com/sunshinelyz/ABTestingGateway\n七、nginx生成缩略图 配置Nginx 使用 Nginx 自带模块生成缩略图，模块：\u0026ndash;with-http_image_filter_module，例如，我们可以使用如下参数安装Nginx：\n./configure --prefix=/usr/local/nginx-1.19.1 --with-http_stub_status_module --with-http_realip_module --with-http_image_filter_module --with-debug 接下来，修改 nginx.conf 配置文件，或者将下面的配置放到nginx.conf文件相应的 server 块中。\nlocation ~* /(\\d+)\\.(jpg)$ {  set $h $arg_h; # 获取参数ｈ的值  set $w $arg_w; # 获取参数 w 的值  #image_filter crop $h $w;  image_filter resize $h $w;# 根据给定的长宽生成缩略图 } location ~* /(\\d+)_(\\d+)x(\\d+)\\.(jpg)$ {  if ( -e $document_root/$1.$4 ) { # 判断原图是否存在  rewrite /(\\d+)_(\\d+)x(\\d+)\\.(jpg)$ /$1.$4?h=$2\u0026amp;w=$3 last;  }  return 404; } 访问图片 配置完成后，我们就可以使用类似如下的方式来访问图片。\nhttp://www.binghe.com/123_100x10.jpg\n当我们在浏览器地址栏中输入上面的链接时，Nginx会作出如下的逻辑处理。\n 首先判断是否存在原图 123.jpg,不存在直接返回 404（如果原图都不存在，那就没必要生成缩略图了） 跳转到 http://www.binghe.com/123.jpg?h=100\u0026amp;w=10，将参数高 h=100 和宽 w=10 带到 url 中。 Image_filter resize 指令根据 h 和 w 参数生成相应缩略图。  注意：使用Nginx生成等比例缩略图时有一个长宽取小的原则，例如原图是 100\\*10,你传入的是 10\\*2，那么Nginx会给你生成 10\\*1 的图片。生成缩略图只是 image_filter 功能中的一个，它一共支持 4 种参数：\n test：返回是否真的是图片 size：返回图片长短尺寸，返回 json 格式数据 corp：截取图片的一部分，从左上角开始截取，尺寸写小了，图片会被剪切 resize：缩放图片，等比例缩放  Nginx 生成缩略图优缺点 优点：\n 根据传入参数即可生成各种比例图片 不占用任何硬盘空间  缺点：\n 消耗 CPU 访问量大将会给服务器带来比较大的负担  建议：\n生成缩略是个消耗 CPU 的操作，如果访问量比较大的站点，最好考虑使用程序生成缩略图到硬盘上，或者在前端加上 Cache缓存或者使用 CDN\n八、nginx禁用ip和ip段 Nginx的ngx_http_access_module 模块可以封配置内的ip或者ip段，语法如下：\ndeny IP; deny subnet; allow IP; allow subnet; # block all ips deny all; # allow all ips allow all; 如果规则之间有冲突，会以最前面匹配的规则为准。\n配置禁用ip和ip段 下面说明假定nginx的目录在/usr/local/nginx/。\n首先要建一个封ip的配置文件blockips.conf，然后vi blockips.conf编辑此文件，在文件中输入要封的ip。\ndeny 1.2.3.4; deny 91.212.45.0/24; deny 91.212.65.0/24; 然后保存此文件，并且打开nginx.conf文件，在http配置节内添加下面一行配置：\ninclude blockips.conf; 保存nginx.conf文件，然后测试现在的nginx配置文件是否是合法的：\n/usr/local/nginx/sbin/nginx -t 如果配置没有问题，就会输出：\nthe configuration file /usr/local/nginx/conf/nginx.conf syntax is ok configuration file /usr/local/nginx/conf/nginx.conf test is successful 如果配置有问题就需要检查下哪儿有语法问题，如果没有问题，需要执行下面命令，让nginx重新载入配置文件。\n/usr/local/nginx/sbin/nginx -s reload 仅允许内网ip 如何禁止所有外网ip，仅允许内网ip呢？\n如下配置文件\nlocation / {  # block one workstation  deny 192.168.1.1;  # allow anyone in 192.168.1.0/24  allow 192.168.1.0/24;  # drop rest of the world  deny all; } 上面配置中禁止了192.168.1.1，允许其他内网网段，然后deny all禁止其他所有ip。\n格式化nginx的403页面 如何格式化nginx的403页面呢？\n首先执行下面的命令：\ncd /usr/local/nginx/html vi error403.html 然后输入403的文件内容，例如：\n\u0026lt;html\u0026gt; \u0026lt;head\u0026gt;\u0026lt;title\u0026gt;Error 403 - IP Address Blocked\u0026lt;/title\u0026gt;\u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; Your IP Address is blocked. If you this an error, please contact binghe with your IP at test@binghe.com \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 如果启用了SSI，可以在403中显示被封的客户端ip，如下：\nYour IP Address is \u0026lt;!--#echo var=\u0026#34;REMOTE_ADDR\u0026#34; --\u0026gt; blocked. 保存error403文件，然后打开nginx的配置文件vi nginx.conf,在server配置节内添加下面内容。\n# redirect server error pages to the static page  error_page 403 /error403.html;  location = /error403.html {  root html;  } 然后保存配置文件，通过nginx -t命令测试配置文件是否正确，若正确通过nginx -s reload载入配置\n九、nginx日志分割 首先，我们要创建一个脚本文件，用来分割Nginx日志，具体脚本如下：\nvim /usr/local/nginx-1.19.1/cutnginxlog.sh 脚本内容如下：\n#!/bin/sh # Program: # Auto cut nginx log script.  # nginx日志路径  LOGS_PATH=/usr/local/nginx-1.19.1/logs TODAY=$(date -d \u0026#39;today\u0026#39; +%Y-%m-%d)  # 移动日志并改名 mv ${LOGS_PATH}/error.log ${LOGS_PATH}/error_${TODAY}.log mv ${LOGS_PATH}/access.log ${LOGS_PATH}/access_${TODAY}.log  # 向nginx主进程发送重新打开日志文件的信号 kill -USR1 $(cat /usr/local/nginx-1.19.1/logs/nginx.pid) 接下来就是给cutnginxlog.sh文件授权。\nchmod a+x cutnginxlog.sh 接下来添加计划任务，定时执行cutnginxlog.sh脚本，以root用户执行如下命令：\necho \u0026#39;59 23 * * * root /usr/local/nginx-1.19.1/cutnginxlog.sh \u0026gt;\u0026gt; /usr/local/nginx-1.19.1/cutnginxlog.log 2\u0026gt;\u0026amp;1\u0026#39; \u0026gt;\u0026gt; /etc/crontab 意思就是在每天的23点59分执行脚本。将自动任务的执行日志（错误和正确的日志）自动写入cutnginxlog.log，“命令 \u0026raquo; 2\u0026gt;\u0026amp;1” 表示以追加方式将正确输出和错误输出都保存到同一个文件中\n十、为已安装的nginx动态添加模块 这里以安装第三方ngx_http_google_filter_module模块为例。\nNginx的模块是需要重新编译Nginx，而不是像Apache一样配置文件引用.so\n下载第三方扩展模块ngx_http_google_filter_module # cd /data/software/ # git clone https://github.com/cuber/ngx_http_google_filter_module 查看nginx编译安装时安装了哪些模块 将命令行切换到Nginx执行程序所在的目录并输入./nginx -V，具体如下：\n[root@binghe sbin]# ./nginx -V nginx version: nginx/1.19.1 built by gcc 4.4.7 20120313 (Red Hat 4.4.7-17) (GCC) built with OpenSSL 1.0.2 22 Jan 2015 TLS SNI support enabled configure arguments: --prefix=/usr/local/nginx-1.19.1 --with-openssl=/usr/local/src/openssl-1.0.2 --with-pcre=/usr/local/src/pcre-8.37 --with-zlib=/usr/local/src/zlib-1.2.8 --with-http_ssl_module [root@binghe sbin]#  可以看出编译安装Nginx使用的参数如下：\n--prefix=/usr/local/nginx-1.19.1 --with-openssl=/usr/local/src/openssl-1.0.2 --with-pcre=/usr/local/src/pcre-8.37 --with-zlib=/usr/local/src/zlib-1.2.8 --with-http_ssl_module 加入需要安装的模块，重新编译 这里添加 \u0026ndash;add-module=/data/software/ngx_http_google_filter_module\n具体如下：\n./configure --prefix=/usr/local/nginx-1.19.1 --with-openssl=/usr/local/src/openssl-1.0.2 --with-pcre=/usr/local/src/pcre-8.37 --with-zlib=/usr/local/src/zlib-1.2.8 --with-http_ssl_module -–add-module=/data/software/ngx_http_google_filter_module 如上，将之前安装Nginx的参数全部加上，最后添加 \u0026ndash;add-module=/data/software/ngx_http_google_filter_module\n之后，我们要进行编译操作，如下：\n# make //千万不要make install，不然就真的覆盖 这里，需要注意的是：不要执行make install命令。\n替换nginx二进制文件 # 备份原来的nginx执行程序 # mv /usr/local/nginx-1.19.1/sbin/nginx /usr/local/nginx-1.19.1/sbin/nginx.bak  # 将新编译的nginx执行程序复制到/usr/local/nginx-1.19.1/sbin/目录下 # cp /opt/nginx/sbin/nginx /usr/local/nginx-1.19.1/sbin/ 十一、nginx格式化日志并推送到远程服务器统一收集维护 格式化Nginx日志并推送到远程服务器，其实很简单，我们只需要在Nginx服务器的配置文件nginx.conf中进行简单的配置即可。例如，我们可以在nginx.conf文件中添加如下配置。\nlog_format common \u0026#34;$remote_addr,$http_ip,$http_mac,$time_local,$status,$request_length,$bytes_sent,$body_bytes_sent,$http_user_agent,$http_referer,$request_method,$request_time,$request_uri,$server_protocol,$request_body,$http_token\u0026#34;;  log_format main \u0026#34;$remote_addr,$http_ip,$http_mac,$time_local,$status,$request_length,$bytes_sent,$body_bytes_sent,$http_user_agent,$http_referer,$request_method,$request_time,$request_uri,$server_protocol,$request_body,$http_token\u0026#34;;  access_log logs/access.log common;  access_log syslog:server=192.168.1.100:9999,facility=local7,tag=nginx,severity=info main;  map $http_upgrade $connection_upgrade {  default upgrade;  \u0026#39;\u0026#39; close;  } 上述配置是将Nginx的日志各项参数以逗号分隔的形式进行输出，同时将Nginx日志实时推送到192.168.1.100:9999上。\n此时，我们只需要在192.168.1.100服务器上部署一个TCP或UDP服务，监听端口为9999，并在192.168.1.100服务器的防火墙开放9999端口。我们写的TCP或UDP服务就会实时接收到Nginx服务器发送过来的日志。\n通过这种方式，我们就可以将Nginx日志实时收集到某个存储集群中，对Nginx日志进行统一存储、维护和分析\n十二、nginx配置WebSocket Nginx配置WebSocket也比较简单，只需要在nginx.conf文件中进行相应的配置。这种方式很简单，但是很有效，能够横向扩展WebSocket服务端的服务能力。\n先直接展示配置文件，如下所示(使用的话直接复制，然后改改ip和port即可)\nmap $http_upgrade $connection_upgrade {  default upgrade;  \u0026#39;\u0026#39; close; } upstream wsbackend{  server ip1:port1;  server ip2:port2;  keepalive 1000; }  server {  listen 20038;  location /{  proxy_http_version 1.1;  proxy_pass http://wsbackend;  proxy_redirect off;  proxy_set_header Host $host;  proxy_set_header X-Real-IP $remote_addr;  proxy_read_timeout 3600s;  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  proxy_set_header Upgrade $http_upgrade;  proxy_set_header Connection $connection_upgrade;  } } 接下来，我们就分别分析上述配置的具体含义。\n首先：\nmap $http_upgrade $connection_upgrade {  default upgrade;  \u0026#39;\u0026#39; close; } 表示的是：\n 如果 $http_upgrade 不为 '' (空)，则 $connection_upgrade 为 upgrade 。 如果 $http_upgrade 为 '' (空)，则 $connection_upgrade 为 close。  其次：\nupstream wsbackend{  server ip1:port1;  server ip2:port2;  keepalive 1000; } 表示的是 nginx负载均衡：\n 两台服务器 (ip1:port1)和(ip2:port2) 。 keepalive 1000 表示的是每个nginx进程中上游服务器保持的空闲连接，当空闲连接过多时，会关闭最少使用的空闲连接.当然，这不是限制连接总数的，可以想象成空闲连接池的大小，设置的值应该是上游服务器能够承受的。  最后：\nserver {  listen 20038;  location /{  proxy_http_version 1.1;  proxy_pass http://wsbackend;  proxy_redirect off;  proxy_set_header Host $host;  proxy_set_header X-Real-IP $remote_addr;  proxy_read_timeout 3600s;  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  proxy_set_header Upgrade $http_upgrade;  proxy_set_header Connection $connection_upgrade;  } } 表示的是监听的服务器的配置\n listen 20038 表示 nginx 监听的端口 locations / 表示监听的路径(/表示所有路径，通用匹配，相当于default) proxt_http_version 1.1 表示反向代理发送的HTTP协议的版本是1.1，HTTP1.1支持长连接 proxy_pass http://wsbackend; 表示反向代理的uri，这里可以使用负载均衡变量 proxy_redirect off; 表示不要替换路径，其实这里如果是/则有没有都没关系，因为default也是将路径替换到proxy_pass的后边 proxy_set_header Host $host; 表示传递时请求头不变， $host是nginx内置变量，表示的是当前的请求头，proxy_set_header表示设置请求头 proxy_set_header X-Real-IP $remote_addr; 表示传递时来源的ip还是现在的客户端的ip proxy_read_timeout 3600s；表的两次请求之间的间隔超过 3600s 后才关闭这个连接，默认的60s，自动关闭的元凶 proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; 表示X-Forwarded-For头不发生改变 proxy_set_header Upgrade $http_upgrade; 表示设置Upgrade不变 proxy_set_header Connection $connection_upgrade; 表示如果 $http_upgrade为upgrade，则请求为upgrade(websocket)，如果不是，就关闭连接  十三、nginx实现MySQL负载均衡 user nginx; #user root; worker_processes 1; error_log /var/log/nginx/error.log warn; pid /var/run/nginx.pid; events {  worker_connections 1024; } http {  include /etc/nginx/mime.types;  default_type application/octet-stream;  log_format main \u0026#39;$remote_addr - $remote_user [$time_local] \u0026#34;$request\u0026#34; \u0026#39;  \u0026#39;$status $body_bytes_sent \u0026#34;$http_referer\u0026#34; \u0026#39;  \u0026#39;\u0026#34;$http_user_agent\u0026#34; \u0026#34;$http_x_forwarded_for\u0026#34;\u0026#39;;  access_log /var/log/nginx/access.log main;  sendfile on;  #tcp_nopush on;  keepalive_timeout 65;  #gzip on;  include /etc/nginx/conf.d/*.conf; }  stream{  upstream mysql{  server 192.168.1.101:3306 weight=1;  server 192.168.1.102:3306 weight=1;  }   server{  listen 3306;  server_name 192.168.1.100;  proxy_pass mysql;  } } 配置完成后，我们就可以通过如下方式来访问MySQL数据库。\njdbc:mysql://192.168.1.100:3306/数据库名称 此时，Nginx会将访问MySQL的请求路由到IP地址为192.168.1.101和192.168.1.102的MySQL上\n十四、nginx解决跨域问题 server {  location / {  root html;  index index.html index.htm;  //允许cros跨域访问  add_header \u0026#39;Access-Control-Allow-Origin\u0026#39; \u0026#39;*\u0026#39;;   }  //自定义本地路径  location /apis {  rewrite ^.+apis/?(.*)$ /$1 break;  include uwsgi_params;  proxy_pass http://www.binghe.com;  } } 十五、nginx图片显示过慢，文件下载不完全问题处理 问题定位 经过一系列的排查（中间过程我就省略了，直接写重点了！），最终定位到是Nginx的问题。当我打开这位读者的网站后台管理系统，发现图片显示非常慢，在Nginx前端代理上查出如下错误信息。\n[error] 28423#0: *5 connect() failed (111: Connection refused) while connecting to upstream 直接在后台服务器上用后台服务器的IP地址去访问，发现速度相当快，于是怀疑是Nginx的配置问题。\n注意：当下载大的附件，或是页面中有大图片时，就会下载中断或是图片无法显示，也许你会说我用的Nginx缺省的配置也从来没有碰到过这种问题呀！我想说的是：那是因为你的网站没有大文件，至少没有大到使用Nginx的默认配置加载不出来。\n这里，我给出一段Nginx的配置，如下所示。\nlocation /file {  root /home/file;  index index.html index.htm;  proxy_set_header X-Real-IP $remote_addr;  proxy_set_header Host $host;  proxy_pass http://127.0.0.1:8080 ;  client_max_body_size 100m;  client_body_buffer_size 128k;  proxy_connect_timeout 600;  proxy_read_timeout 600;  proxy_send_timeout 600;  proxy_buffer_size 32k;  proxy_buffers 4 64k;  proxy_busy_buffers_size 64k;  proxy_temp_file_write_size 64k; } 其中几个重要的参数如下所示。\n proxy_connect_timeout 600; #nginx跟后端服务器连接超时时间(代理连接超时) proxy_read_timeout 600; #连接成功后，后端服务器响应时间(代理接收超时) proxy_send_timeout 600; #后端服务器数据回传时间(代理发送超时) proxy_buffer_size 32k; #设置代理服务器（nginx）保存用户头信息的缓冲区大小 proxy_buffers 4 32k; #proxy_buffers缓冲区，网页平均在32k以下的话，这样设置 proxy_busy_buffers_size 64k; #高负荷下缓冲大小（proxy_buffers*2） proxy_temp_file_write_size 16k; #设定缓存文件夹大小，大于这个值，将从upstream服务器传  看到这里，发现问题了，这位读者的Nginx有下面一行配置。\nproxy_temp_file_write_size 16k; 而他服务器上的图片基本都在100K~5M之间。\n问题就出在proxy_temp_file_write_size上，当服务器上的文件超过该参数设置的大小时，Nginx会先将文件写入临时目录(缺省为Nginx安装目下/proxy_temp目录)，缺省Nginx是以nobody身份启动的，用ls -al 命令查看proxy_temp目录 nobody是proxy_temp目录的所有者，怪了那为什么没权限呢？接下来查看proxy_temp的父目录既Nginx安装目录。发现nobody竞然没权限，怪不得会出现上面的问题。\n解决问题 定位到问题，接下来解决问题就比较简单了。可以使用两种方式解决这个问题，如下所示。\n 设置任何人都可以写 proxy_temp目录，重启 Nginx 即可解决。 直接更改proxy_temp_file_write_size的值，将其修改为大于图片和文件的大小，重启Nginx。  如果是以第一种方式解决问题的话，比如我的proxy_temp目录是/usr/local/nginx/proxy_temp，用如下命令将/usr/local/nginx/proxy_temp目录设置为任何人都可以写，问题解决。\nchmod -R 777 /usr/local/nginx/proxy_temp/ 如果是使用第二种方式解决问题的话，就可以直接修改nginx.conf文件，如下所示。\nlocation /file {  root /home/file;  index index.html index.htm;  proxy_set_header X-Real-IP $remote_addr;  proxy_set_header Host $host;  proxy_pass http://127.0.0.1:8080 ;  client_max_body_size 100m;  client_body_buffer_size 256k;  proxy_connect_timeout 1200;  proxy_read_timeout 1200;  proxy_send_timeout 6000;  proxy_buffer_size 32k;  proxy_buffers 4 64k;  proxy_busy_buffers_size 128k;  proxy_temp_file_write_size 10m; } 十六、使用Nginx搭建流媒体服务器实现直播 安装Nginx 注意：这里以CentOS 6.8服务器为例，以root用户身份来安装Nginx。\n1.安装依赖环境 yum -y install wget gcc-c++ ncurses ncurses-devel cmake make perl bison openssl openssl-devel gcc* libxml2 libxml2-devel curl-devel libjpeg* libpng* freetype* autoconf automake zlib* fiex* libxml* libmcrypt* libtool-ltdl-devel* libaio libaio-devel bzr libtool 2.安装openssl wget https://www.openssl.org/source/openssl-1.0.2s.tar.gz tar -zxvf openssl-1.0.2s.tar.gz cd /usr/local/src/openssl-1.0.2s ./config --prefix=/usr/local/openssl-1.0.2s make make install 3.安装pcre wget https://ftp.pcre.org/pub/pcre/pcre-8.43.tar.gz tar -zxvf pcre-8.43.tar.gz cd /usr/local/src/pcre-8.43 ./configure --prefix=/usr/local/pcre-8.43 make make install 4.安装zlib wget https://sourceforge.net/projects/libpng/files/zlib/1.2.11/zlib-1.2.11.tar.gz tar -zxvf zlib-1.2.11.tar.gz cd /usr/local/src/zlib-1.2.11 ./configure --prefix=/usr/local/zlib-1.2.11 make make 5.下载nginx-rtmp-module nginx-rtmp-module的官方github地址：https://github.com/arut/nginx-rtmp-module\n使用命令：\ngit clone https://github.com/arut/nginx-rtmp-module.git 6.安装Nginx wget http://nginx.org/download/nginx-1.19.1.tar.gz tar -zxvf nginx-1.19.1.tar.gz cd /usr/local/src/nginx-1.19.1 ./configure --prefix=/usr/local/nginx-1.19.1 --with-openssl=/usr/local/src/openssl-1.0.2s --with-pcre=/usr/local/src/pcre-8.43 --with-zlib=/usr/local/src/zlib-1.2.11 --add-module=/usr/local/src/nginx-rtmp-module --with-http_ssl_module make make install 这里需要注意的是：安装Nginx时，指定的是openssl、pcre和zlib的源码解压目录，安装完成后Nginx配置文件的完整路径为：/usr/local/nginx-1.19.1/conf/nginx.conf。\n配置Nginx 配置Nginx主要是对Nginx的nginx.conf文件进行配置，我们可以在命令行输入如下命令编辑nginx.conf文件。\nvim /usr/local/nginx-1.19.1/conf/nginx.conf 在文件中添加如下内容。\nrtmp {  server {  listen 1935; #监听的端口  chunk_size 4096;  application hls { #rtmp推流请求路径   live on;  hls on;  hls_path /usr/share/nginx/html/hls;  hls_fragment 5s;  }  } } 其中，hls_path需要可读可写的权限。接下来，我们创建/usr/share/nginx/html/hls 目录。\nmkdir -p /usr/share/nginx/html/hls chmod -R 777 /usr/share/nginx/html/hls 接下来，修改http中的server模块：\nserver {  listen 81;  server_name localhost;   #charset koi8-r;    #access_log logs/host.access.log main;    location / {  root /usr/share/nginx/html;  index index.html index.htm;  }   #error_page 404 /404.html;    # redirect server error pages to the static page /50x.html   #   error_page 500 502 503 504 /50x.html;  location = /50x.html {  root html;  } } 然后启动Nginx：\n/usr/local/nginx-1.19.1/sbin/nginx -c /usr/local/nginx-1.19.1/conf/nginx.conf 使用OBS推流 OBS（Open Broadcaster Software） 是以互联网流媒体直播内容为目的免费和开放源码软件。需要下载这个软件，借助这个软件进行推流（电脑没有摄像头的貌似安装不了。。。）\nOBS的下载链接为：https://obsproject.com/zh-cn/download。\n安装后，桌面上会有一个如下所示的图标。\n打开后我们需要有一个场景，并且在这个场景下有一个流的来源(可以是窗口，如果选的是视频则会自动识别摄像头)，接下来就是设置了。\n在配置中最需要关注的就是流的配置，由于是自建的流媒体服务器所以我们按照如下所示的方式进行配置。\nrtmp://你的服务器ip:端口(1935)/live #URL填写流的地址 设置完成我们就可以 开始推流了。\n拉流测试地址 推荐一个拉流的测试地址，里面针对各种协议都能测试拉流测试，需要注意图中几个地方，由于我们使用的rtmp协议，我们选择这一栏，底下填写我们推流的地址和我们在上面obs的设置里面配置的流的名称，start， ok搞定！！！\n十七、nginx的转发规则 Nginx的location语法 location [=|~|~*|^~] /uri/ { … }  = 严格匹配。如果请求匹配这个location，那么将停止搜索并立即处理此请求 ~ 区分大小写匹配(可用正则表达式) ~* 不区分大小写匹配(可用正则表达式) !~ 区分大小写不匹配 !~* 不区分大小写不匹配 ^~ 如果把这个前缀用于一个常规字符串,那么告诉nginx 如果路径匹配那么不测试正则表达式  示例1：\nlocation / { } 匹配任意请求\n示例2：\nlocation ~* .(gif|jpg|jpeg)$ ｛  rewrite .(gif|jpg|jpeg)$ /logo.png; ｝ 不区分大小写匹配任何以gif、jpg、jpeg结尾的请求，并将该请求重定向到 /logo.png请求\n示例3：\nlocation ~ ^.+\\.txt$ {  root /usr/local/nginx/html/; } 区分大小写匹配以.txt结尾的请求，并设置此location的路径是/usr/local/nginx/html/。也就是以.txt结尾的请求将访问/usr/local/nginx/html/ 路径下的txt文件\nalias与root的区别  root 实际访问文件路径会拼接URL中的路径 alias 实际访问文件路径不会拼接URL中的路径  示例如下：\nlocation ^~ /binghe/ {  alias /usr/local/nginx/html/binghetic/; }  请求：http://test.com/binghe/binghe1.html 实际访问：/usr/local/nginx/html/binghetic/binghe1.html 文件  location ^~ /binghe/ {  root /usr/local/nginx/html/; }  请求：http://test.com/binghe/binghe1.html 实际访问：/usr/local/nginx/html/binghe/binghe1.html 文件  last 和 break关键字的区别 （1）last 和 break 当出现在location 之外时，两者的作用是一致的没有任何差异\n（2）last 和 break 当出现在location 内部时：\n last 使用了last 指令，rewrite 后会跳出location 作用域，重新开始再走一次刚才的行为 break 使用了break 指令，rewrite后不会跳出location 作用域，其整个生命周期都在当前location中。  permanent 和 redirect关键字的区别  rewrite … permanent 永久性重定向，请求日志中的状态码为301 rewrite … redirect 临时重定向，请求日志中的状态码为302  综合实例 将符合某个正则表达式的URL重定向到一个固定页面\n比如：我们需要将符合“/test/(\\d+)/[\\w-.]+” 这个正则表达式的URL重定向到一个固定的页面。符合这个正则表达式的页面可能是：http://test.com/test/12345/abc122.html、http://test.com/test/456/11111cccc.js等\n从上面的介绍可以看出，这里可以使用rewrite重定向或者alias关键字来达到我们的目的。因此，这里可以这样做：\n（1）使用rewrite关键字\nlocation ~ ^.+\\.txt$ {  root /usr/local/nginx/html/; } location ~* ^/test/(\\d+)/[\\w-\\.]+$ {  rewrite ^/test/(\\d+)/[\\w-\\.]+$ /testpage.txt last; } 这里将所有符合条件的URL（PS：不区分大小写）都重定向到/testpage.txt请求，也就是 /usr/local/nginx/html/testpage.txt 文件\n（2）使用alias关键字\nlocation ~* ^/test/(\\d+)/[\\w-\\.]+$ {  alias /usr/local/nginx/html/binghetic/binghe1.html; } 这里将所有符合条件的URL（不区分大小写）都重定向到/usr/local/nginx/html/binghetic/binghe1.html 文件\n十八、nginx.conf常用配置 user nginx; worker_processes auto; worker_rlimit_nofile 65535;    pid /var/run/nginx.pid; events {  worker_connections 65535;  multi_accept on;  use epoll; } http {  include /etc/nginx/mime.types;  default_type application/octet-stream;  log_format main \u0026#39;[$time_local] RemoteAddr:\u0026#34;$remote_addr\u0026#34; RemoteUser:\u0026#34;$remote_user\u0026#34; Host:\u0026#34;$host\u0026#34; \u0026#39;  \u0026#39;RequestUil:\u0026#34;$request\u0026#34; HttpStatus:\u0026#34;$status\u0026#34; BodyBytesSent:\u0026#34;$body_bytes_sent\u0026#34; \u0026#39;  \u0026#39;HttpReferer:\u0026#34;$http_referer\u0026#34; HttpUserAgent:\u0026#34;$http_user_agent\u0026#34; \u0026#39;  \u0026#39;Http_X_ForwardedFor:\u0026#34;$http_x_forwarded_for\u0026#34; UpstreamResponseTime:\u0026#34;$upstream_response_time\u0026#34; \u0026#39;  \u0026#39;UpstreamAddr:\u0026#34;$upstream_addr\u0026#34; RequestTime:\u0026#34;$request_time\u0026#34; --- $server_port\u0026#39;;  log_format json \u0026#39;{\u0026#39;  \u0026#39;\u0026#34;RemoteAddr\u0026#34;:\u0026#34;$remote_addr\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;RemoteUser\u0026#34;:\u0026#34;$remote_user\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;TimeLocal\u0026#34;:\u0026#34;$time_local\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;RequestUil\u0026#34;:\u0026#34;$request\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;HttpHost\u0026#34;:\u0026#34;$http_host\u0026#34;\u0026#39;  \u0026#39;\u0026#34;HttpStatus\u0026#34;:\u0026#34;$status\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;BodyBytesSent\u0026#34;:\u0026#34;$body_bytes_sent\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;HttpReferer\u0026#34;:\u0026#34;$http_referer\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;HttpUserAgent\u0026#34;:\u0026#34;$http_user_agent\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;Http_X_ForwardedFor\u0026#34;:\u0026#34;$http_x_forwarded_for\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;SslProtocol\u0026#34;:\u0026#34;$ssl_protocol\u0026#34;\u0026#39;  \u0026#39;\u0026#34;SslCipher\u0026#34;:\u0026#34;$ssl_cipher\u0026#34;\u0026#39;  \u0026#39;\u0026#34;UpstreamResponseTime\u0026#34;:\u0026#34;$upstream_response_time\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;UpstreamAddr\u0026#34;:\u0026#34;$upstream_addr\u0026#34;,\u0026#39;  \u0026#39;\u0026#34;RequestTime\u0026#34;:\u0026#34;$request_time\u0026#34;,\u0026#39;  \u0026#39;}\u0026#39;;   access_log /data/logs/nginx_logs/access.log main;  error_log /data/logs/nginx_logs/error.log error;    access_log off;  server_tokens off;  sendfile on;  tcp_nopush on;  tcp_nodelay on;  send_timeout 300;  keepalive_timeout 300;  resolver_timeout 60;  server_names_hash_max_size 512;  server_names_hash_bucket_size 128;   client_body_timeout 300;  client_header_timeout 300;  client_header_buffer_size 512k;  client_max_body_size 300m;  large_client_header_buffers 8 32k;  client_body_buffer_size 256k;   fastcgi_connect_timeout 300;  fastcgi_send_timeout 300;  fastcgi_read_timeout 300;  fastcgi_buffer_size 128k;  fastcgi_buffers 8 256k;  fastcgi_busy_buffers_size 256k;  fastcgi_temp_file_write_size 256k;  fastcgi_temp_path /tmp/ngx_fcgi_tmp;  fastcgi_cache_path /tmp/fcgi_cache_path levels=1:2 keys_zone=ngx_fcgi_cache:512m inactive=1d max_size=10g;   gzip on;  gzip_http_version 1.1;  gzip_min_length 1k;  gzip_buffers 4 16k;  gzip_comp_level 9;  gzip_types text/plain application/json application/x-javascript text/css application/xml text/javascript application/x-httpd-php image/jpeg image/gif image/png;  gzip_vary on;  gzip_disable \u0026#34;MSIE [1-6]\\.\u0026#34;;   proxy_http_version 1.1;  proxy_set_header Connection \u0026#34;\u0026#34;;  proxy_set_header Host $host;  proxy_connect_timeout 300;  proxy_read_timeout 300;  proxy_send_timeout 300;  proxy_buffering on;  proxy_buffer_size 128k;  proxy_buffers 8 128k;  proxy_busy_buffers_size 256k;  proxy_temp_file_write_size 256k;  proxy_temp_path /tmp/proxy_temp_path;  proxy_cache_path /tmp/proxy_cache_path levels=1:2 keys_zone=ngx_proxy_cache:512m inactive=1d max_size=10g;  include /etc/nginx/conf.d/*.conf; } include /etc/nginx/stream.d/*.conf; 十九、stream常用配置 stream {  upstream redis {  server 192.168.1.1:6379;  }  server {  listen 6372;  proxy_pass redis;  proxy_connect_timeout 1h;  proxy_timeout 1h;  } } 二十、server常用配置 server {  listen 80;  listen 81;  server_name 127.0.0.1 test.example.com;  index index.php index.html index.htm;  root /data/web/test/www;  charset utf-8;  access_log /data/logs/nginx_logs/test.example.com.log main;  location / {  if (!-e $request_filename) {  rewrite ^/(.*) /index.php?$1 last;  }  }   location /business/ {  if ($arg_icpid = \u0026#34;4pd1mtsDhfe\u0026#34; ) {  proxy_pass http://127.0.0.1:38888/test$request_uri\u0026amp;tbid=aldIthSBg04;  }  proxy_pass http://127.0.0.1:9302;  }    location /gateway/ {  proxy_pass http://127.0.0.1:38888/;  }  location ~ \\.php$ {  fastcgi_param REMOTE_ADDR $http_x_real_ip;  fastcgi_param LY_ADDRESS $remote_addr;  fastcgi_pass unix:/dev/shm/php-cgi.sock;  fastcgi_index index.php;  fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name;  fastcgi_param SERVERNAME $hostname;  include fastcgi_params;  }  location = /favicon.ico {  log_not_found off;  access_log off;  }  location = /robots.txt {  allow all;  log_not_found off;  access_log off;  } }  server {  listen 80;  listen 443 ssl;  server_name test.example.com;  charset utf-8;  access_log /data/logs/nginx_logs/test.example.com.log main;   ssl_certificate /etc/nginx/cert/example.com.pem;  ssl_certificate_key /etc/nginx/cert/example.com.key;  ssl_session_timeout 5m;  ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:ECDHE:ECDH:AES:HIGH:!NULL:!aNULL:!MD5:!ADH:!RC4;  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;  ssl_prefer_server_ciphers on;   location / {  proxy_pass http://127.0.0.1:38888;  }  location = /favicon.ico {  log_not_found off;  access_log off;  }  location = /robots.txt {  allow all;  log_not_found off;  access_log off;  } } ","permalink":"https://iblog.zone/archives/nginx%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C%E5%8F%8A%E9%85%8D%E7%BD%AE/","summary":"一、nginx获取客户端真实IP、域名、协议、端口 需要在Nginx的配置文件nginx.conf中添加如下配置\nproxy_set_header Host $http_host; proxy_set_header X-Real-IP $remote_addr; proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for; proxy_set_header X-Forwarded-Proto $scheme; 各参数的含义如下所示。\n Host包含客户端真实的域名和端口号； X-Forwarded-Proto表示客户端真实的协议（http还是https）； X-Real-IP表示客户端真实的IP； X-Forwarded-For这个Header和X-Real-IP类似，但它在多层代理时会包含真实客户端及中间每个代理服务器的IP  二、nginx负载均衡配置 http {  ……  upstream real_server {  server 192.168.103.100:2001 weight=1; #轮询服务器和访问权重  server 192.168.103.100:2002 weight=2;  }   server {  listen 80;   location / {  proxy_pass http://real_server;  }  } } nginx负载均衡失败重试配置\nupstream real_server {  server 192.168.103.100:2001 weight=1 max_fails=2 fail_timeout=60s;  server 192.","title":"Nginx常用操作及配置"},{"content":"前言 在工作中，公司有很多内部的包并不希望发布到npm官网仓库，因为可能涉及到一些私有代码不能暴露。对于前端来讲，这时就可以选择在公司内网搭建npm私有仓库。当前比较主流的几种解决方案：verdaccio、nexus、cnpm。大家可以按照自己的需求选择。本文中采用的是cnpm私服搭建。\ncnpm私服搭建流程 1. 安装node,新的node版本会自带npm 官网地址：nodejs.org/zh-cn/\n2. 拉取代码，对应公司需求做相应更改 git clone https://github.com/cnpm/cnpmjs.org.git 3. 修改配置文件 ./config/index.js  // 仓库站点访问端口  registryPort: 7001,  // 页面访问端口  webPort: 7002,  // 外网可以访问的话则注释，否则只能内网访问  bindingHost: \u0026#39;127.0.0.1\u0026#39;,  // 数据库配置  database: {  db: \u0026#39;cnpmjs\u0026#39;, // 数据库  username: \u0026#39;root\u0026#39;, // 数据库用户名  password: \u0026#39;\u0026#39;, // 数据库密码  dialect: \u0026#39;mysql\u0026#39;, // 数据库类型 \u0026#39;mysql\u0026#39;, \u0026#39;sqlite\u0026#39;, \u0026#39;postgres\u0026#39;, \u0026#39;mariadb\u0026#39;  host: \u0026#39;\u0026#39;, // 数据库服务地址  port: 3306 // 端口  }  // 用户配置 key 为用户名和密码，value为邮箱  admins: {  harlie: \u0026#39;yanghui3021@163.com\u0026#39;,  fengmk2: \u0026#39;fengmk2@gmail.com\u0026#39;,  admin: \u0026#39;admin@cnpmjs.org\u0026#39;,  dead_horse: \u0026#39;dead_horse@qq.com\u0026#39;,  },   // true为只有管理员可以发布，false是任何人发布都必须带有私有标识  enablePrivate: false,   //私有标识前缀  scopes: [ \u0026#39;@harlie\u0026#39;,\u0026#39;@cnpm\u0026#39;, \u0026#39;@cnpmtest\u0026#39; ],   //同步模块上游registry地址  sourceNpmRegistry: \u0026#39;https://registry.npm.taobao.org\u0026#39;,   // 同步模式 \u0026#39;none\u0026#39; 不进行同步 , \u0026#39;all\u0026#39; 定时同步所有源 registry 的模块, \u0026#39;exist\u0026#39; 只同步已经存在于数据库的模块  syncModel: \u0026#39;exist\u0026#39;,   // 同步间隔时间  syncInterval: \u0026#39;10m\u0026#39;, 4. 数据库创建 （MySQL）  create database cnpmjs; // 创建数据库  use cnpmjs;  source cnpmjs.org/docs/db.sql; // 拉取项目的 docs/db.sql 5. 命令启动执行 （到这里私有库的搭建已经完成，是不是并不复杂）  node dispatch.js 启动成功后就可以通过 http://x.x.x.x:7002 查看网址页面。\n私有库使用 1. 使用nrm镜像源管理工具添加源  npm install nrm -g  nrm add cnpmorg http://42.192.37.59:7001 // 添加源  nrm use cnpmorg // 使用cnpmorg源，名字自己定义  nrm ls 2. 内部源使用  ···NPM···  npm config set registry http://x.x.x.x:7001   ···CNPM···  cnpm config set registry http://x.x.x.x:7001   ···YARN···  yarn config set registry http://x.x.x.x:7001 3. 本地项目包发布  npm login 如下图所示则登陆成功   npm publish 包发布   npm view @harlie/cnpm-test  可以看到所发布的包的详情，通过网址页面搜索cnpm-test ,可以在页面查看相应版本信息。\n","permalink":"https://iblog.zone/archives/%E5%89%8D%E7%AB%AFnpm%E7%A7%81%E6%9C%8D%E6%90%AD%E5%BB%BA/","summary":"前言 在工作中，公司有很多内部的包并不希望发布到npm官网仓库，因为可能涉及到一些私有代码不能暴露。对于前端来讲，这时就可以选择在公司内网搭建npm私有仓库。当前比较主流的几种解决方案：verdaccio、nexus、cnpm。大家可以按照自己的需求选择。本文中采用的是cnpm私服搭建。\ncnpm私服搭建流程 1. 安装node,新的node版本会自带npm 官网地址：nodejs.org/zh-cn/\n2. 拉取代码，对应公司需求做相应更改 git clone https://github.com/cnpm/cnpmjs.org.git 3. 修改配置文件 ./config/index.js  // 仓库站点访问端口  registryPort: 7001,  // 页面访问端口  webPort: 7002,  // 外网可以访问的话则注释，否则只能内网访问  bindingHost: \u0026#39;127.0.0.1\u0026#39;,  // 数据库配置  database: {  db: \u0026#39;cnpmjs\u0026#39;, // 数据库  username: \u0026#39;root\u0026#39;, // 数据库用户名  password: \u0026#39;\u0026#39;, // 数据库密码  dialect: \u0026#39;mysql\u0026#39;, // 数据库类型 \u0026#39;mysql\u0026#39;, \u0026#39;sqlite\u0026#39;, \u0026#39;postgres\u0026#39;, \u0026#39;mariadb\u0026#39;  host: \u0026#39;\u0026#39;, // 数据库服务地址  port: 3306 // 端口  }  // 用户配置 key 为用户名和密码，value为邮箱  admins: {  harlie: \u0026#39;yanghui3021@163.","title":"前端npm私服搭建"},{"content":"一、基本介绍 1、如果没有搭建私服会有什么问题？  如果没有私服，我们所需的所有构件都需要通过 Maven 的中央仓库或者第三方的 Maven 仓库下载到本地，而一个团队中的所有人都重复的从 Maven 仓库下载构件无疑加大了仓库的负载和浪费了外网带宽，如果网速慢的话，还会影响项目的进程。 另外，很多情况下项目的开发都是在内网进行的，可能根本连接不了 Maven 的中央仓库和第三方的 Maven 仓库。 我们开发的公共构件如果需要提供给其它项目使用，也需要搭建私服。  2、搭建私服的优点 Maven 私服的概念就是在本地架设一个 Maven 仓库服务器，在代理远程仓库的同时维护本地仓库。当我们需要下载一些构件（artifact）时，如果本地仓库没有，再去私服下载，私服没有，再去中央仓库下载。这样做会有如下一些优点：\n 减少网络带宽流量 加速 Maven 构建 部署第三方构件 提高稳定性、增强控制 降低中央仓库的负载  3、Nexus 介绍 Nexus 是一个专门的 Maven 仓库管理软件，它不仅能搭建 Maven 私服，还具备如下一些优点使其日趋成为最流行的 Maven 仓库管理器：\n 提供了强大的仓库管理功能，构件搜索功能 它基于 REST，友好的 UI 是一个 ext.js 的 REST 客户端 它占用较少的内存 基于简单文件系统而非数据库  二、Nexus 服务的安装 使用 Docker 镜像进行安装 （1）首先执行如下命令下载 Nexus3 镜像：\ndocker pull sonatype/nexus3 （2）接着执行如下命令，创建宿主机挂载目录：\nmkdir -p /data/nexus-data （3）最后执行如下命令运行 Nexus3 容器即可：\nmkdir /data/nexus-data \u0026amp;\u0026amp; chmod -R 777 /data/nexus-data docker run -d --restart=always -p 8081:8081 --name nexus -v /data/nexus-data:/nexus-data sonatype/nexus3 （4）同样不要忘记执行如下命令开放 8081 端口：\nfirewall-cmd --permanent --add-port=8081/tcp firewall-cmd --reload 三、Nexus 服务的配置 （1）Nexus 服务启动以后，我们使用浏览器访问 [http://服务器ip:8081/，点击右上角登录按钮：]\n（2）首次登录会提示密码保存在 /usr/local/sonatype-work/nexus3/admin.password 文件中，我们查看服务器上这个文件内容，然后作为密码登录：（我们的密码保存在/data/nexus-data/admin.password）\n（3）登录后会让我们设置新的密码：\n（4）登录后的界面如下：\n （1）默认仓库说明\n maven-central：maven 中央库，默认从 [repo1.maven.org/maven2/ 拉取] maven-releases：私库发行版 jar，初次安装请将 Deployment policy 设置为 Allow redeploy maven-snapshots：私库快照（调试版本）jar maven-public：仓库分组，把上面三个仓库组合在一起对外提供服务，在本地 maven 基础配置 settings.xml 或项目 pom.xml 中使用  （2）仓库类型说明：\n group：这是一个仓库聚合的概念，用户仓库地址选择 Group 的地址，即可访问 Group 中配置的，用于方便开发人员自己设定的仓库。maven-public 就是一个 Group 类型的仓库，内部设置了多个仓库，访问顺序取决于配置顺序，3.x 默认为 Releases、Snapshots、Central，当然你也可以自己设置。 hosted：私有仓库，内部项目的发布仓库，专门用来存储我们自己生成的 jar 文件 snapshots：本地项目的快照仓库 releases： 本地项目发布的正式版本 proxy：代理类型，从远程中央仓库中寻找数据的仓库（可以点击对应的仓库的 Configuration 页签下 Remote Storage 属性的值即被代理的远程仓库的路径），如可配置阿里云 maven 仓库 central：中央仓库   四、配置阿里云公共仓库 Nexus的maven-group的默认查找方式为：maven-releases \u0026ndash;\u0026gt; maven-snapshots \u0026ndash;\u0026gt; maven-central，我们在中间再加一个阿里云仓库，加快访问速度。\n选择proxy类型\n信息填写如下\n仓库名字：maven-aliyun 阿里云远程maven仓库地址：http://maven.aliyun.com/nexus/content/groups/public/ 其他选项：默认即可 修改maven-public中的仓库引用及顺序：\n至此，配置完毕！我们在maven使用maven-public仓库地址的时候，会按照如下顺序访问：本地仓库 \u0026ndash;\u0026gt; 私服maven-releases \u0026ndash;\u0026gt; 私服maven-snapshots \u0026ndash;\u0026gt; 远程阿里云maven仓库 \u0026ndash;\u0026gt; 远程中央仓库。\n五、在项目中使用Nexus私服 1. 概述 让 Maven 项目使用 Nexus 作为远程仓库有两种方式。\n 单个项目：在项目的 pom.xml 中进行更改，让单个项目使用 Nexus 仓库； 所有项目：通过修改 Maven的 配置文件 settings.xm l进行更改，让所有项目都使用 Nexus仓库；  2. 单个项目配置 在项目的 pom.xml 文件的 dependencies 标签之前添加\n\u0026lt;repositories\u0026gt;  \u0026lt;repository\u0026gt;  \u0026lt;id\u0026gt;nexus3\u0026lt;/id\u0026gt;  \u0026lt;name\u0026gt;nexus3\u0026lt;/name\u0026gt;  \u0026lt;!-- 注意：这里是 Nexus 服务上的仓库地址 --\u0026gt;  \u0026lt;url\u0026gt;http://127.0.0.1:8081/repository/maven-public/\u0026lt;/url\u0026gt;  \u0026lt;!-- Release版本则代表稳定的版本 --\u0026gt;  \u0026lt;releases\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/releases\u0026gt;  \u0026lt;!-- Snapshot版本代表不稳定、尚处于开发中的版本，默认关闭，需要手动启动 --\u0026gt;  \u0026lt;snapshots\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/snapshots\u0026gt;  \u0026lt;/repository\u0026gt;  \u0026lt;/repositories\u0026gt;   \u0026lt;!-- 指定插件仓库 --\u0026gt;  \u0026lt;pluginRepositories\u0026gt;  \u0026lt;pluginRepository\u0026gt;  \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt;  \u0026lt;name\u0026gt;nexus\u0026lt;/name\u0026gt;  \u0026lt;url\u0026gt;http://127.0.0.1:8081/repository/maven-public/\u0026lt;/url\u0026gt;  \u0026lt;releases\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/releases\u0026gt;  \u0026lt;snapshots\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/snapshots\u0026gt;  \u0026lt;/pluginRepository\u0026gt;  \u0026lt;/pluginRepositories\u0026gt; 仓库地址的获取方式如下\nhttp://127.0.0.1:8081/repository/maven-public/ 点开上图的仓库\n比如在项目中添加 junit 依赖，可以看到下载的连接是本地的 Nexus 服务的地址\n同样，可以在 Nexus 服务看到 Junit 已经下载下来。\n3. 所有项目 在 Maven 安装目录找到如下文件\nC:\\devtools\\maven-3.5.0\\conf\\settings.xml 编辑settings.xml文件\n 在profiles节点添加如下内容  \u0026lt;profile\u0026gt;  \u0026lt;!--profile 的 id--\u0026gt;  \u0026lt;id\u0026gt;dev\u0026lt;/id\u0026gt;  \u0026lt;repositories\u0026gt;  \u0026lt;repository\u0026gt;  \u0026lt;!--仓库 id，repositories 可以配置多个仓库，保证 id 不重复--\u0026gt;  \u0026lt;id\u0026gt;nexus\u0026lt;/id\u0026gt;  \u0026lt;!--仓库地址，即 nexus 仓库组的地址--\u0026gt;  \u0026lt;url\u0026gt;http://127.0.0.1:8081/repository/maven-public/\u0026lt;/url\u0026gt;  \u0026lt;!--是否下载 releases 构件--\u0026gt;  \u0026lt;releases\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/releases\u0026gt;  \u0026lt;!--是否下载 snapshots 构件--\u0026gt;  \u0026lt;snapshots\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/snapshots\u0026gt;  \u0026lt;/repository\u0026gt;  \u0026lt;/repositories\u0026gt;  \u0026lt;pluginRepositories\u0026gt;  \u0026lt;!-- 插件仓库，maven 的运行依赖插件，也需要从私服下载插件 --\u0026gt;  \u0026lt;pluginRepository\u0026gt;  \u0026lt;!-- 插件仓库的 id 不允许重复，如果重复后边配置会覆盖前边 --\u0026gt;  \u0026lt;id\u0026gt;public\u0026lt;/id\u0026gt;  \u0026lt;name\u0026gt;Public Repositories\u0026lt;/name\u0026gt;  \u0026lt;url\u0026gt;http://127.0.0.1:8081/repository/maven-public/\u0026lt;/url\u0026gt;  \u0026lt;/pluginRepository\u0026gt;  \u0026lt;/pluginRepositories\u0026gt; \u0026lt;/profile\u0026gt;  打开activeProfiles节点注释，添加如下内容  \u0026lt;activeProfiles\u0026gt;  \u0026lt;activeProfile\u0026gt;dev\u0026lt;/activeProfile\u0026gt; \u0026lt;/activeProfiles\u0026gt; 比如在项目中添加 log4j 依赖，可以看到下载的连接是本地的 Nexus 服务的地址\n同样，可以在 Nexus 服务看到 log4j 已经下载下来。\n","permalink":"https://iblog.zone/archives/maven%E7%A7%81%E6%9C%8Dnexus%E7%9A%84%E6%90%AD%E5%BB%BA%E4%B8%8E%E4%BD%BF%E7%94%A8/","summary":"一、基本介绍 1、如果没有搭建私服会有什么问题？  如果没有私服，我们所需的所有构件都需要通过 Maven 的中央仓库或者第三方的 Maven 仓库下载到本地，而一个团队中的所有人都重复的从 Maven 仓库下载构件无疑加大了仓库的负载和浪费了外网带宽，如果网速慢的话，还会影响项目的进程。 另外，很多情况下项目的开发都是在内网进行的，可能根本连接不了 Maven 的中央仓库和第三方的 Maven 仓库。 我们开发的公共构件如果需要提供给其它项目使用，也需要搭建私服。  2、搭建私服的优点 Maven 私服的概念就是在本地架设一个 Maven 仓库服务器，在代理远程仓库的同时维护本地仓库。当我们需要下载一些构件（artifact）时，如果本地仓库没有，再去私服下载，私服没有，再去中央仓库下载。这样做会有如下一些优点：\n 减少网络带宽流量 加速 Maven 构建 部署第三方构件 提高稳定性、增强控制 降低中央仓库的负载  3、Nexus 介绍 Nexus 是一个专门的 Maven 仓库管理软件，它不仅能搭建 Maven 私服，还具备如下一些优点使其日趋成为最流行的 Maven 仓库管理器：\n 提供了强大的仓库管理功能，构件搜索功能 它基于 REST，友好的 UI 是一个 ext.js 的 REST 客户端 它占用较少的内存 基于简单文件系统而非数据库  二、Nexus 服务的安装 使用 Docker 镜像进行安装 （1）首先执行如下命令下载 Nexus3 镜像：\ndocker pull sonatype/nexus3 （2）接着执行如下命令，创建宿主机挂载目录：\nmkdir -p /data/nexus-data （3）最后执行如下命令运行 Nexus3 容器即可：","title":"Maven私服Nexus的搭建与使用"},{"content":"多节点 Swarm 集群下，可能节点的配置不同（比如 CPU、内存等），部署着不同类型的服务（比如 Web服务、Job服务等），当这些服务以 Service 或者 Stack 的形式部署到集群，默认情况下会随机分配到各个节点。不同类型的服务对服务器需求的资源是不同的，为了更合理的利用服务器资源，我们可能希望某些服务能够部署到指定的服务器上。另外一种场景，Swarm 集群中的节点跨机房，为了内部服务间通信更快，我们可能希望关联比较密切的服务能够部署到同一机房的节点上。那么，如何做到呢？\n很简单，先给节点添加标签，然后服务发布时添加限制条件即可！\nNode Label 管理 示例集群信息：\ndocker@node1:~$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION axr4zun8u1es8ytizjpt3zlnw * node1 Ready Active Leader 18.03.0-ce vdip2js7tfflxv0smj6wdw0bv node2 Ready Active 18.03.0-ce vi17ametnwd58297z6nlcl2o0 node3 Ready Active 18.03.0-ce ※ 添加标签\ndocker node update --label-add role=web node1 ※ 查看标签\ndocker node inspect node1 [  {  \u0026#34;ID\u0026#34;: \u0026#34;axr4zun8u1es8ytizjpt3zlnw\u0026#34;,  \u0026#34;Version\u0026#34;: {  \u0026#34;Index\u0026#34;: 476  },  \u0026#34;CreatedAt\u0026#34;: \u0026#34;2018-07-19T03:50:02.734603631Z\u0026#34;,  \u0026#34;UpdatedAt\u0026#34;: \u0026#34;2018-07-30T06:37:04.465194614Z\u0026#34;,  \u0026#34;Spec\u0026#34;: {  \u0026#34;Labels\u0026#34;: {  \u0026#34;role\u0026#34;: \u0026#34;web\u0026#34; # 人工添加的标签  },  \u0026#34;Role\u0026#34;: \u0026#34;manager\u0026#34;,  \u0026#34;Availability\u0026#34;: \u0026#34;active\u0026#34;  }  # 省略  } ] ※ 删除标签\ndocker node update --label-rm role node1 服务部署条件约束 ※ Service 方式\ndocker service create \\  --name nginx_2 \\  --constraint \u0026#39;node.labels.role == web\u0026#39; \\  nginx ※ Stack 方式\nversion: \u0026#39;3.6\u0026#39; services:  mycat:  image: nginx  ports:  - target: 8080  published: 8080  protocol: tcp  mode: ingress  deploy:  mode: global  placement:  constraints: # 添加条件约束  - node.labels.role==web  restart_policy:  condition: on-failure  max_attempts: 3 constraints 为数组，填写多个约束时，它们之间的关系是 AND。\n条件约束补充 constraints 可以匹配 node 标签和 engine 标签，engine.labels 适用于 Docker Engine 标签，如操作系统，驱动程序等，node.labels 适用于上述人为添加到节点的。\n   node attribute matches example     node.id Node ID node.id==2ivku8v2gvtg4   node.hostname Node hostname node.hostname!=node-2   node.role Node role node.role==manager   node.labels user defined node labels node.labels.security==high   engine.labels Docker Engine\u0026rsquo;s labels engine.labels.operatingsystem==ubuntu 14.04    ","permalink":"https://iblog.zone/archives/docker-swarm-%E8%8A%82%E7%82%B9%E6%A0%87%E7%AD%BE%E4%B8%8E%E6%9C%8D%E5%8A%A1%E7%BA%A6%E6%9D%9F/","summary":"多节点 Swarm 集群下，可能节点的配置不同（比如 CPU、内存等），部署着不同类型的服务（比如 Web服务、Job服务等），当这些服务以 Service 或者 Stack 的形式部署到集群，默认情况下会随机分配到各个节点。不同类型的服务对服务器需求的资源是不同的，为了更合理的利用服务器资源，我们可能希望某些服务能够部署到指定的服务器上。另外一种场景，Swarm 集群中的节点跨机房，为了内部服务间通信更快，我们可能希望关联比较密切的服务能够部署到同一机房的节点上。那么，如何做到呢？\n很简单，先给节点添加标签，然后服务发布时添加限制条件即可！\nNode Label 管理 示例集群信息：\ndocker@node1:~$ docker node ls ID HOSTNAME STATUS AVAILABILITY MANAGER STATUS ENGINE VERSION axr4zun8u1es8ytizjpt3zlnw * node1 Ready Active Leader 18.03.0-ce vdip2js7tfflxv0smj6wdw0bv node2 Ready Active 18.03.0-ce vi17ametnwd58297z6nlcl2o0 node3 Ready Active 18.03.0-ce ※ 添加标签\ndocker node update --label-add role=web node1 ※ 查看标签\ndocker node inspect node1 [  {  \u0026#34;ID\u0026#34;: \u0026#34;axr4zun8u1es8ytizjpt3zlnw\u0026#34;,  \u0026#34;Version\u0026#34;: {  \u0026#34;Index\u0026#34;: 476  },  \u0026#34;CreatedAt\u0026#34;: \u0026#34;2018-07-19T03:50:02.","title":"Docker Swarm 节点标签与服务约束"},{"content":"一、集群搭建 1、环境准备  五台安装了 Docker 的 CentOS 机器，版本为：CentOS 7.8.2003 Docker Engine 1.12+（最低要求 1.12，本文使用 19.03.12） 防火墙开启以下端口或者关闭防火墙：  TCP 端口 2377，用于集群管理通信； TCP 和 UDP 端口 7946，用于节点之间通信； UDP 端口 4789，用于覆盖网络。　    2、机器分布    角色 IP HOSTNAME Docker 版本     Manager 192.168.10.101 manager1 19.03.12   Manager 192.168.10.102 manager2 19.03.12   Manager 192.168.10.103 manager3 19.03.12   Worker 192.168.10.10 worker1 19.03.12   Worker 192.168.10.11 worker2 19.03.12     可以通过 hostname 主机名 修改机器的主机名（立即生效，重启后失效）； 或者 hostnamectl set-hostname 主机名 修改机器的主机名（立即生效，重启也生效）； 或者 vi /etc/hosts 编辑 hosts 文件，如下所示， 给 127.0.0.1 添加主机名（重启生效）。  127.0.0.1 localhost localhost.localdomain localhost4 localhost4.localdomain4 manager1 ::1 localhost localhost.localdomain localhost6 localhost6.localdomain6 3、创建集群 　在任意节点下通过 docker swarm init 命令创建一个新的 Swarm 集群并加入，且该节点会默认成为 Manager 节点。根据我们预先定义的角色，在 101 ~ 103 的任意一台机器上运行该命令即可。\n　通常，第一个加入集群的管理节点将成为 Leader，后来加入的管理节点都是 Reachable。当前的 Leader 如果挂掉，所有的 Reachable 将重新选举一个新的 Leader。\n[root@localhost ~]# docker swarm init --advertise-addr 192.168.10.101 Swarm initialized: current node (clumstpieg0qzzxt1caeazg8g) is now a manager.  To add a worker to this swarm, run the following command:   docker swarm join --token SWMTKN-1-5ob7jlej85qsygxubqypjuftiwruvew8e2cr4u3iuo4thxyrhg-3hbf2u3i1iagurdprl3n3yra1 192.168.10.101:2377  To add a manager to this swarm, run \u0026#39;docker swarm join-token manager\u0026#39; and follow the instructions. 4、加入集群 　Docker 中内置的集群模式自带了公钥基础设施(PKI)系统，使得安全部署容器变得简单。集群中的节点使用传输层安全协议(TLS)对集群中其他节点的通信进行身份验证、授权和加密。\n　默认情况下，通过 docker swarm init 命令创建一个新的 Swarm 集群时，Manager 节点会生成新的根证书颁发机构（CA）和密钥对，用于保护与加入群集的其他节点之间的通信安全。\n　Manager 节点会生成两个令牌，供其他节点加入集群时使用：一个 Worker 令牌，一个 Manager 令牌。每个令牌都包括根 CA 证书的摘要和随机生成的密钥。当节点加入群集时，加入的节点使用摘要来验证来自远程管理节点的根 CA 证书。远程管理节点使用密钥来确保加入的节点是批准的节点。\nManager\n　若要向该集群添加 Manager 节点，管理节点先运行 docker swarm join-token manager 命令查看管理节点的令牌信息。\ndocker swarm join-token manager 　然后在其他节点上运行 docker swarm join 并携带令牌参数加入 Swarm 集群，该节点角色为 Manager。\nWorker\n　通过创建集群时返回的结果可以得知，要向这个集群添加一个 Worker 节点，运行下图中的命令即可。或者管理节点先运行 docker swarm join-token worker 命令查看工作节点的令牌信息。\n　然后在其他节点上运行 docker swarm join 并携带令牌参数加入 Swarm 集群，该节点角色为 Worker。\n5、查看集群信息 　在任意 Manager 节点中运行 docker info 可以查看当前集群的信息。\n6、查看集群节点 　在任意 Manager 节点中运行 docker node ls 可以查看当前集群节点信息。\ndocker node ls  * 代表当前节点，现在的环境为 3 个管理节点构成 1 主 2 从，以及 2 个工作节点。\n 　节点 MANAGER STATUS 说明：表示节点是属于 Manager 还是 Worker，没有值则属于 Worker 节点。\n Leader：该节点是管理节点中的主节点，负责该集群的集群管理和编排决策； Reachable：该节点是管理节点中的从节点，如果 Leader 节点不可用，该节点有资格被选为新的 Leader； Unavailable：该管理节点已不能与其他管理节点通信。如果管理节点不可用，应该将新的管理节点加入群集，或者将工作节点升级为管理节点。　  　节点 AVAILABILITY 说明：表示调度程序是否可以将任务分配给该节点。\n Active：调度程序可以将任务分配给该节点； Pause：调度程序不会将新任务分配给该节点，但现有任务仍可以运行； Drain：调度程序不会将新任务分配给该节点，并且会关闭该节点所有现有任务，并将它们调度在可用的节点上。  7、删除节点　 Manager\n　删除节点之前需要先将该节点的 AVAILABILITY 改为 Drain。其目的是为了将该节点的服务迁移到其他可用节点上，确保服务正常。最好检查一下容器迁移情况，确保这一步已经处理完成再继续往下。\ndocker node update --availability drain 节点名称|节点ID 　然后，将该 Manager 节点进行降级处理，降级为 Worker 节点。\ndocker node demote 节点名称|节点ID 　然后，在已经降级为 Worker 的节点中运行以下命令，离开集群。\ndocker swarm leave 　最后，在管理节点中对刚才离开的节点进行删除。\ndocker node rm 节点名称|节点ID Worker　　删除节点之前需要先将该节点的 AVAILABILITY 改为 Drain。其目的是为了将该节点的服务迁移到其他可用节点上，确保服务正常。最好检查一下容器迁移情况，确保这一步已经处理完成再继续往下。\ndocker node update --availability drain 节点名称|节点ID 　然后，在准备删除的 Worker 节点中运行以下命令，离开集群。\ndocker swarm leave 　最后，在管理节点中对刚才离开的节点进行删除。\ndocker node rm 节点名称|节点ID 二、服务部署  注意：跟集群管理有关的任何操作，都是在 Manager 节点上操作的。\n 1、创建服务 　下面这个案例，使用 nginx 镜像创建了一个名为 mynginx 的服务，该服务会被随机指派给一个工作节点运行。\ndocker service create --replicas 1 --name mynginx -p 80:80 nginx  docker service create：创建服务； --replicas：指定一个服务有几个实例运行； --name：服务名称。  2、查看服务 　可以通过 docker service ls 查看运行的服务。\n[root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE PORTS hepx06k5ik5n mynginx replicated 1/1 nginx:latest *:80-\u0026gt;80/tcp 　可以通过 docker service inspect 服务名称|服务ID 查看服务的详细信息。\n[root@manager1 ~]# docker service inspect mynginx [  {  \u0026#34;ID\u0026#34;: \u0026#34;k0dbjg1zzy3l3g71kdwa56ect\u0026#34;,  \u0026#34;Version\u0026#34;: {  \u0026#34;Index\u0026#34;: 127  },  \u0026#34;CreatedAt\u0026#34;: \u0026#34;2020-09-16T10:05:55.627974095Z\u0026#34;,  \u0026#34;UpdatedAt\u0026#34;: \u0026#34;2020-09-16T10:05:55.629507771Z\u0026#34;,  \u0026#34;Spec\u0026#34;: {  \u0026#34;Name\u0026#34;: \u0026#34;mynginx\u0026#34;,  \u0026#34;Labels\u0026#34;: {},  \u0026#34;TaskTemplate\u0026#34;: {  \u0026#34;ContainerSpec\u0026#34;: {  \u0026#34;Image\u0026#34;: \u0026#34;nginx:latest@sha256:c628b67d21744fce822d22fdcc0389f6bd763daac23a6b77147d0712ea7102d0\u0026#34;,  \u0026#34;Init\u0026#34;: false,  \u0026#34;StopGracePeriod\u0026#34;: 10000000000,  \u0026#34;DNSConfig\u0026#34;: {},  \u0026#34;Isolation\u0026#34;: \u0026#34;default\u0026#34;  },  \u0026#34;Resources\u0026#34;: {  \u0026#34;Limits\u0026#34;: {},  \u0026#34;Reservations\u0026#34;: {}  },  \u0026#34;RestartPolicy\u0026#34;: {  \u0026#34;Condition\u0026#34;: \u0026#34;any\u0026#34;,  \u0026#34;Delay\u0026#34;: 5000000000,  \u0026#34;MaxAttempts\u0026#34;: 0  },  \u0026#34;Placement\u0026#34;: {  \u0026#34;Platforms\u0026#34;: [  {  \u0026#34;Architecture\u0026#34;: \u0026#34;amd64\u0026#34;,  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  },  {  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  },  {  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  },  {  \u0026#34;Architecture\u0026#34;: \u0026#34;arm64\u0026#34;,  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  },  {  \u0026#34;Architecture\u0026#34;: \u0026#34;386\u0026#34;,  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  },  {  \u0026#34;Architecture\u0026#34;: \u0026#34;mips64le\u0026#34;,  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  },  {  \u0026#34;Architecture\u0026#34;: \u0026#34;ppc64le\u0026#34;,  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  },  {  \u0026#34;Architecture\u0026#34;: \u0026#34;s390x\u0026#34;,  \u0026#34;OS\u0026#34;: \u0026#34;linux\u0026#34;  }  ]  },  \u0026#34;ForceUpdate\u0026#34;: 0,  \u0026#34;Runtime\u0026#34;: \u0026#34;container\u0026#34;  },  \u0026#34;Mode\u0026#34;: {  \u0026#34;Replicated\u0026#34;: {  \u0026#34;Replicas\u0026#34;: 1  }  },  \u0026#34;UpdateConfig\u0026#34;: {  \u0026#34;Parallelism\u0026#34;: 1,  \u0026#34;FailureAction\u0026#34;: \u0026#34;pause\u0026#34;,  \u0026#34;Monitor\u0026#34;: 5000000000,  \u0026#34;MaxFailureRatio\u0026#34;: 0,  \u0026#34;Order\u0026#34;: \u0026#34;stop-first\u0026#34;  },  \u0026#34;RollbackConfig\u0026#34;: {  \u0026#34;Parallelism\u0026#34;: 1,  \u0026#34;FailureAction\u0026#34;: \u0026#34;pause\u0026#34;,  \u0026#34;Monitor\u0026#34;: 5000000000,  \u0026#34;MaxFailureRatio\u0026#34;: 0,  \u0026#34;Order\u0026#34;: \u0026#34;stop-first\u0026#34;  },  \u0026#34;EndpointSpec\u0026#34;: {  \u0026#34;Mode\u0026#34;: \u0026#34;vip\u0026#34;,  \u0026#34;Ports\u0026#34;: [  {  \u0026#34;Protocol\u0026#34;: \u0026#34;tcp\u0026#34;,  \u0026#34;TargetPort\u0026#34;: 80,  \u0026#34;PublishedPort\u0026#34;: 80,  \u0026#34;PublishMode\u0026#34;: \u0026#34;ingress\u0026#34;  }  ]  }  },  \u0026#34;Endpoint\u0026#34;: {  \u0026#34;Spec\u0026#34;: {  \u0026#34;Mode\u0026#34;: \u0026#34;vip\u0026#34;,  \u0026#34;Ports\u0026#34;: [  {  \u0026#34;Protocol\u0026#34;: \u0026#34;tcp\u0026#34;,  \u0026#34;TargetPort\u0026#34;: 80,  \u0026#34;PublishedPort\u0026#34;: 80,  \u0026#34;PublishMode\u0026#34;: \u0026#34;ingress\u0026#34;  }  ]  },  \u0026#34;Ports\u0026#34;: [  {  \u0026#34;Protocol\u0026#34;: \u0026#34;tcp\u0026#34;,  \u0026#34;TargetPort\u0026#34;: 80,  \u0026#34;PublishedPort\u0026#34;: 80,  \u0026#34;PublishMode\u0026#34;: \u0026#34;ingress\u0026#34;  }  ],  \u0026#34;VirtualIPs\u0026#34;: [  {  \u0026#34;NetworkID\u0026#34;: \u0026#34;st2xiy7pjzap093wz4w4u6nbs\u0026#34;,  \u0026#34;Addr\u0026#34;: \u0026#34;10.0.0.15/24\u0026#34;  }  ]  }  } ]　　可以通过 docker service ps 服务名称|服务ID 查看服务运行在哪些节点上。\n　在对应的任务节点上运行 docker ps 可以查看该服务对应容器的相关信息。\n3、调用服务 　接下来我们测试一下服务是否能被正常访问，并且该集群下任意节点的 IP 地址都要能访问到该服务才行。\n　测试结果：5 台机器均可正常访问到该服务。\n4、弹性服务 　将 service 部署到集群以后，可以通过命令弹性扩缩容 service 中的容器数量。在 service 中运行的容器被称为 task（任务）。\n　通过 docker service scale 服务名称|服务ID=n 可以将 service 运行的任务扩缩容为 n 个。\n　通过 docker service update --replicas n 服务名称|服务ID 也可以达到扩缩容的效果。\n　将 mynginx service 运行的任务扩展为 5 个：\n[root@manager1 ~]# docker service scale mynginx=5 mynginx scaled to 5 overall progress: 5 out of 5 tasks 1/5: running [==================================================\u0026gt;] 2/5: running [==================================================\u0026gt;] 3/5: running [==================================================\u0026gt;] 4/5: running [==================================================\u0026gt;] 5/5: running [==================================================\u0026gt;] verify: Service converged 　通过 docker service ps 服务名称|服务ID 查看服务运行在哪些节点上。\n　我们再来一波缩容的操作，命令如下：\n[root@manager1 ~]# docker service update --replicas 3 mynginx mynginx overall progress: 3 out of 3 tasks 1/3: running [==================================================\u0026gt;] 2/3: running [==================================================\u0026gt;] 3/3: running [==================================================\u0026gt;] verify: Service converged 　通过 docker service ps 服务名称|服务ID 查看服务运行在哪些节点上。\n　在 Swarm 集群模式下真正意义实现了所谓的弹性服务，动态扩缩容一行命令搞定，简单、便捷、强大。\n5、删除服务 　通过 docker service rm 服务名称|服务ID 即可删除服务。\n[root@manager1 ~]# docker service rm mynginx mynginx [root@manager1 ~]# docker service ls ID NAME MODE REPLICAS IMAGE PORTS 三、滚动更新及回滚 　以下案例将演示 Redis 版本如何滚动升级至更高版本再回滚至上一次的操作。\n　首先，创建 5 个 Redis 服务副本，版本为 5，详细命令如下：\n# 创建 5 个副本，每次更新 2 个，更新间隔 10s，20% 任务失败继续执行，超出 20% 执行回滚，每次回滚 2 个 docker service create --replicas 5 --name redis \\ --update-delay 10s \\ --update-parallelism 2 \\ --update-failure-action continue \\ --rollback-monitor 20s \\ --rollback-parallelism 2 \\ --rollback-max-failure-ratio 0.2 \\ redis:5  --update-delay：定义滚动更新的时间间隔； --update-parallelism：定义并行更新的副本数量，默认为 1； --update-failure-action：定义容器启动失败之后所执行的动作； --rollback-monitor：定义回滚的监控时间； --rollback-parallelism：定义并行回滚的副本数量； --rollback-max-failure-ratio：任务失败回滚比率，超过该比率执行回滚操作，0.2 表示 20%。  　然后通过以下命令实现服务的滚动更新。\ndocker service update --image redis:6 redis 　回滚服务，只能回滚到上一次操作的状态，并不能连续回滚到指定操作。\ndocker service update --rollback redis 四、常用命令 1、docker swarm    命令 说明     docker swarm init 初始化集群   docker swarm join-token worker 查看工作节点的 token   docker swarm join-token manager 查看管理节点的 token   docker swarm join 加入集群    2、docker node    命令 说明     docker node ls 查看集群所有节点   docker node ps 查看当前节点所有任务   docker node rm 节点名称|节点ID 删除节点（-f强制删除）   docker node inspect 节点名称|节点ID 查看节点详情   docker node demote 节点名称|节点ID 节点降级，由管理节点降级为工作节点   docker node promote 节点名称|节点ID 节点升级，由工作节点升级为管理节点   docker node update 节点名称|节点ID 更新节点    3、docker service    命令 说明     docker service create 创建服务   docker service ls 查看所有服务   docker service inspect 服务名称|服务ID 查看服务详情   docker service logs 服务名称|服务ID 查看服务日志   docker service rm 服务名称|服务ID 删除服务（-f强制删除）   docker service scale 服务名称|服务ID=n 设置服务数量   docker service update 服务名称|服务ID 更新服务    五、参考资料  https://docs.docker.com/engine/swarm/swarm-tutorial/ https://docs.docker.com/engine/swarm/swarm-mode/ https://docs.docker.com/engine/swarm/how-swarm-mode-works/pki/ https://docs.docker.com/engine/swarm/join-nodes/ https://docs.docker.com/engine/swarm/swarm-tutorial/rolling-update/  ","permalink":"https://iblog.zone/archives/docker-swarm%E9%9B%86%E7%BE%A4%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%BC%B9%E6%80%A7%E6%9C%8D%E5%8A%A1%E9%83%A8%E7%BD%B2/","summary":"一、集群搭建 1、环境准备  五台安装了 Docker 的 CentOS 机器，版本为：CentOS 7.8.2003 Docker Engine 1.12+（最低要求 1.12，本文使用 19.03.12） 防火墙开启以下端口或者关闭防火墙：  TCP 端口 2377，用于集群管理通信； TCP 和 UDP 端口 7946，用于节点之间通信； UDP 端口 4789，用于覆盖网络。　    2、机器分布    角色 IP HOSTNAME Docker 版本     Manager 192.168.10.101 manager1 19.03.12   Manager 192.168.10.102 manager2 19.03.12   Manager 192.168.10.103 manager3 19.03.12   Worker 192.168.10.10 worker1 19.03.12   Worker 192.168.10.11 worker2 19.","title":"Docker Swarm集群环境搭建及弹性服务部署"},{"content":"一、Swarm 简介 　Docker Swarm 是 Docker 官方推出的容器集群管理工具，基于 Go 语言实现。代码开源在：https://github.com/docker/swarm 使用它可以将多个 Docker 主机封装为单个大型的虚拟 Docker 主机，快速打造一套容器云平台。\n　Docker Swarm 是生产环境中运行 Docker 应用程序最简单的方法。作为容器集群管理器，Swarm 最大的优势之一就是 100% 支持标准的 Docker API。各种基于标准 API 的工具比如 Compose、docker-py、各种管理软件，甚至 Docker 本身等都可以很容易的与 Swarm 进行集成。大大方便了用户将原先基于单节点的系统移植到 Swarm 上，同时 Swarm 内置了对 Docker 网络插件的支持，用户可以很容易地部署跨主机的容器集群服务。\n　Docker Swarm 和 Docker Compose 一样，都是 Docker 官方容器编排工具，但不同的是，Docker Compose 是一个在单个服务器或主机上创建多个容器的工具，而 Docker Swarm 则可以在多个服务器或主机上创建容器集群服务，对于微服务的部署，显然 Docker Swarm 会更加适合。\n　二、Swarm 核心概念 1、Swarm 　Docker Engine 1.12 引入了 Swarm 模式，一个 Swarm 由多个 Docker 主机组成，它们以 Swarm 集群模式运行。Swarm 集群由 Manager 节点（管理者角色，管理成员和委托任务）和 Worker 节点（工作者角色，运行 Swarm 服务）组成。这些 Docker 主机有些是 Manager 节点，有些是 Worker 节点，或者同时扮演这两种角色。\n　Swarm 创建服务时，需要指定要使用的镜像、在运行的容器中执行的命令、定义其副本的数量、可用的网络和数据卷、将服务公开给外部的端口等等。与独立容器相比，群集服务的主要优势之一是，你可以修改服务的配置，包括它所连接的网络和数据卷等，而不需要手动重启服务。还有就是，如果一个 Worker Node 不可用了，Docker 会调度不可用 Node 的 Task 任务到其他 Nodes 上。\n2、Nodes　 　Swarm 集群由 Manager 节点（管理者角色，管理成员和委托任务）和 Worker 节点（工作者角色，运行 Swarm 服务）组成。一个节点就是 Swarm 集群中的一个实例，也就是一个 Docker 主机。你可以运行一个或多个节点在单台物理机或云服务器上，但是生产环境上，典型的部署方式是：Docker 节点交叉分布式部署在多台物理机或云主机上。节点名称默认为机器的 hostname。\n Manager：负责整个集群的管理工作包括集群配置、服务管理、容器编排等所有跟集群有关的工作，它会选举出一个 leader 来指挥编排任务； Worker：工作节点接收和执行从管理节点分派的任务（Tasks）运行在相应的服务（Services）上。  3、Services and Tasks 　服务（Service）是一个抽象的概念，是对要在管理节点或工作节点上执行的任务的定义。它是集群系统的中心结构，是用户与集群交互的主要根源。Swarm 创建服务时，可以为服务定义以下信息：\n 服务名称； 使用哪个镜像来创建容器； 要运行多少个副本； 服务的容器要连接到哪个网络上； 要映射哪些端口。  　任务（Task）包括一个 Docker 容器和在容器中运行的命令。任务是一个集群的最小单元，任务与容器是一对一的关系。管理节点根据服务规模中设置的副本数量将任务分配给工作节点。一旦任务被分配到一个节点，便无法移动到另一个节点。它只能在分配的节点上运行或失败。\n4、Replicated and global services 　Swarm 不只是提供了优秀的高可用性，同时也提供了节点的弹性扩容和缩容的功能。可以通过以下两种类型的 Services 部署实现：\n Replicated Services：当服务需要动态扩缩容时，只需通过 scale 参数或者 --replicas n 参数指定运行相同任务的数量，即可复制出新的副本，将一系列复制任务分发至各节点当中，这种操作便称之为副本服务（Replicate）。 Global Services：我们也可以通过 --mode global 参数将服务分发至全部节点之上，这种操作我们称之为全局服务（Global）。在每个节点上运行一个相同的任务，不需要预先指定任务的数量，每增加一个节点到 Swarm 中，协调器就会创建一个任务，然后调度器把任务分配给新节点。  　下图用黄色表示拥有三个副本服务 Replicated Service，用灰色表示拥有一个全局服务 Global Service。\n三、Swarm 工作流程 Swarm Manager：\n API：接受命令并创建 service 对象（创建对象） orchestrator：为 service 对象创建的 task 进行编排工作（服务编排） allocater：为各个 task 分配 IP 地址（分配 IP） dispatcher：将 task 分发到 nodes（分发任务） scheduler：安排一个 worker 节点运行 task（运行任务）  Worker Node：\n worker：连接到调度器，检查分配的 task（检查任务） executor：执行分配给 worker 节点的 task（执行任务）　  四、Overlay 网络 　关于 Docker 的网络我们在《Docker 网络模式详解及容器间网络通信》中已经给大家详细讲解过。不过，Docker Swarm 集群模式下却默认使用的是 Overlay 网络（覆盖网络），这里简单介绍一下什么是 Overlay 网络。\n　Overlay 网络其实并不是一门新技术，它是指构建在另一个网络上的计算机网络，这是一种网络虚拟化技术的形式，近年来云计算虚拟化技术的演进促进了网络虚拟化技术的应用。所以 Overlay 网络就是建立在另一个计算机网络之上的虚拟网络，它是不能独立出现的，Overlay 底层依赖的网络就是 Underlay 网络。\n　Underlay 网络是专门用来承载用户 IP 流量的基础架构层，它与 Overlay 网络之间的关系有点类似物理机和虚拟机。Underlay 网络和物理机都是真正存在的实体，它们分别对应着真实存在的网络设备和计算设备，而 Overlay 网络和虚拟机都是依托在下层实体的基础之上，使用软件虚拟出来的层级。\n　在 Docker 版本 1.12 以后 Swarm 模式原生已支持覆盖网络（Overlay Network），只要是这个覆盖网络内的容器，不管在不在同一个宿主机上都能相互通信，即跨主机通信。不同覆盖网络内的容器之间是相互隔离的（相互 ping 不通）。\n　Overlay 网络是目前主流的容器跨节点数据传输和路由方案。当然，容器在跨主机进行通信的时候，除了可以使用 overlay 网络模式进行通信之外，还可以使用 host 网络模式，直接使用物理机的 IP 地址就可以进行通信。\n五、参考资料  https://docs.docker.com/engine/swarm/ https://docs.docker.com/engine/swarm/key-concepts/ https://docs.docker.com/engine/swarm/swarm-tutorial/ https://docs.docker.com/engine/swarm/how-swarm-mode-works/nodes/ https://docs.docker.com/engine/swarm/how-swarm-mode-works/services/  ","permalink":"https://iblog.zone/archives/docker-swarm%E9%9B%86%E7%BE%A4%E7%AE%A1%E7%90%86%E5%88%A9%E5%99%A8%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5%E6%89%AB%E7%9B%B2/","summary":"一、Swarm 简介 　Docker Swarm 是 Docker 官方推出的容器集群管理工具，基于 Go 语言实现。代码开源在：https://github.com/docker/swarm 使用它可以将多个 Docker 主机封装为单个大型的虚拟 Docker 主机，快速打造一套容器云平台。\n　Docker Swarm 是生产环境中运行 Docker 应用程序最简单的方法。作为容器集群管理器，Swarm 最大的优势之一就是 100% 支持标准的 Docker API。各种基于标准 API 的工具比如 Compose、docker-py、各种管理软件，甚至 Docker 本身等都可以很容易的与 Swarm 进行集成。大大方便了用户将原先基于单节点的系统移植到 Swarm 上，同时 Swarm 内置了对 Docker 网络插件的支持，用户可以很容易地部署跨主机的容器集群服务。\n　Docker Swarm 和 Docker Compose 一样，都是 Docker 官方容器编排工具，但不同的是，Docker Compose 是一个在单个服务器或主机上创建多个容器的工具，而 Docker Swarm 则可以在多个服务器或主机上创建容器集群服务，对于微服务的部署，显然 Docker Swarm 会更加适合。\n　二、Swarm 核心概念 1、Swarm 　Docker Engine 1.12 引入了 Swarm 模式，一个 Swarm 由多个 Docker 主机组成，它们以 Swarm 集群模式运行。Swarm 集群由 Manager 节点（管理者角色，管理成员和委托任务）和 Worker 节点（工作者角色，运行 Swarm 服务）组成。这些 Docker 主机有些是 Manager 节点，有些是 Worker 节点，或者同时扮演这两种角色。","title":"Docker Swarm集群管理利器核心概念扫盲"},{"content":"Apache Maven主要用于Java项目的自由开源项目管理。 Maven使用项目对象模型（POM），该对象本质上是一个XML文件，其中包含关于项目，配置，依赖关系等信息。\n在本教程中，我们将向您展示两种在CentOS 7上安装Apache Maven的两种不同方法。它们分别：1.yum安装Apache Maven。2.从官方站点下载最新版本Apache Maven并配置环境变量PATH与JAVA_HOME环境变量。\nCentOS 默认储存库包含可以通过yum软件包管理器安装的Maven软件包。这是在CentOS上安装Maven的最简单方法，但是存储库中包含的版本可能落后于最新版本的Maven。\n要安装最新版本的Maven，请按照本文第二部分提供的说明进行操作，我们将从其官方网站上下载Maven。具体的取决于你的喜好，选择最适合您的设置和环境的安装方法。\n先决条件 您所登录的用户必须具有sudo权限，才能安装软件包。\n使用Yum在CentOS上安装Apache Maven 使用yum在CentOS 7上安装Maven是一个简单，直接的过程。通过在终端中键入以下命令来安装Maven：\nsudo yum install maven 验证安装，通过键入mvn -version命令：\nmvn -version 输出应如下所示：\nApache Maven 3.0.5 (Red Hat 3.0.5-17) Maven home: /usr/share/maven Java version: 1.8.0_191, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.191.b12-0.el7_5.x86_64/jre Default locale: en_US, platform encoding: UTF-8 OS name: \u0026#34;linux\u0026#34;, version: \u0026#34;3.10.0-862.3.2.el7.x86_64\u0026#34;, arch: \u0026#34;amd64\u0026#34;, family: \u0026#34;unix\u0026#34; 现在，Maven已安装在CentOS系统上。\n安装Apache Maven的最新版本 以下提供了有关如何在CentOS 7上安装最新版本的Apache Maven的逐步说明。我们将从官方网站上下载最新版本的Apache Maven。\n1.安装OpenJDK Maven 3.3+需要安装JDK 1.7或更高版本。我们将安装OpenJDK ，这是CentOS 7中默认的Java开发和运行时。通过输入以下命令安装OpenJDK软件包：\nsudo yum install java-1.8.0-openjdk-devel 通过运行以下命令验证Java是否已成功安装：\njava -version 输出应如下所示：\nopenjdk version \u0026#34;1.8.0_191\u0026#34; OpenJDK Runtime Environment (build 1.8.0_191-b12) OpenJDK 64-Bit Server VM (build 25.191-b12, mixed mode) 2.下载Apache Maven 在撰写本文时，Apache Maven的最新版本为3.6.0。在继续下一步之前，您应该检查Maven下载页面，查看是否有较新的版本。首先使用wget命令下载Apache Maven到/tmp目录中：\nwget https://www-us.apache.org/dist/maven/maven-3/3.6.0/binaries/apache-maven-3.6.0-bin.tar.gz -P /tmp 下载完成后，解压缩到/opt目录：\nsudo tar xf /tmp/apache-maven-3.6.0.tar.gz -C /opt 要拥有对Maven版本和更新的更多控制权，我们将创建一个maven的符号链接 ，链接将指向Maven安装目录：\nsudo ln -s /opt/apache-maven-3.6.0 /opt/maven 要升级您的Maven安装，只需解压缩较新的版本并更改符号链接以指向它即可。\n3.设置环境变量 接下来，我们需要设置环境变量。打开您的文本编辑器，然后在/etc/profile.d/目录中创建一个名为mavenenv.sh的新文件。\nsudo vim /etc/profile.d/maven.sh 复制粘贴以下几行：\nexport JAVA_HOME=/usr/lib/jvm/jre-openjdk export M2_HOME=/opt/maven export MAVEN_HOME=/opt/maven export PATH=${M2_HOME}/bin:${PATH} /etc/profile.d/maven.sh\n保存并关闭文件。脚本将在shell启动时运行。要让脚本可以在启动运行，我们为脚本添加可执行命令。通过运行以下chmod命令使脚本具有可执行权限：\nsudo chmod +x /etc/profile.d/maven.sh 使用source命令加载到环境变量：\nsource /etc/profile.d/maven.sh 4.验证安装 要验证已安装的Maven，请使用mvn -version命令，它将打印Maven版本：\nmvn -version 您应该会看到类似以下的内容：\nApache Maven 3.6.0 (97c98ec64a1fdfee7767ce5ffb20918da4f719f3; 2018-10-24T18:41:47Z) Maven home: /opt/maven Java version: 1.8.0_191, vendor: Oracle Corporation, runtime: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.191.b12-0.el7_5.x86_64/jre Default locale: en_US, platform encoding: UTF-8 OS name: \u0026#34;linux\u0026#34;, version: \u0026#34;3.10.0-862.3.2.el7.x86_64\u0026#34;, arch: \u0026#34;amd64\u0026#34;, family: \u0026#34;unix\u0026#34; 现在，您的CentOS系统上已安装了最新版本的Maven\n","permalink":"https://iblog.zone/archives/centos7%E4%B8%8A%E5%AE%89%E8%A3%85apache-maven/","summary":"Apache Maven主要用于Java项目的自由开源项目管理。 Maven使用项目对象模型（POM），该对象本质上是一个XML文件，其中包含关于项目，配置，依赖关系等信息。\n在本教程中，我们将向您展示两种在CentOS 7上安装Apache Maven的两种不同方法。它们分别：1.yum安装Apache Maven。2.从官方站点下载最新版本Apache Maven并配置环境变量PATH与JAVA_HOME环境变量。\nCentOS 默认储存库包含可以通过yum软件包管理器安装的Maven软件包。这是在CentOS上安装Maven的最简单方法，但是存储库中包含的版本可能落后于最新版本的Maven。\n要安装最新版本的Maven，请按照本文第二部分提供的说明进行操作，我们将从其官方网站上下载Maven。具体的取决于你的喜好，选择最适合您的设置和环境的安装方法。\n先决条件 您所登录的用户必须具有sudo权限，才能安装软件包。\n使用Yum在CentOS上安装Apache Maven 使用yum在CentOS 7上安装Maven是一个简单，直接的过程。通过在终端中键入以下命令来安装Maven：\nsudo yum install maven 验证安装，通过键入mvn -version命令：\nmvn -version 输出应如下所示：\nApache Maven 3.0.5 (Red Hat 3.0.5-17) Maven home: /usr/share/maven Java version: 1.8.0_191, vendor: Oracle Corporation Java home: /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.191.b12-0.el7_5.x86_64/jre Default locale: en_US, platform encoding: UTF-8 OS name: \u0026#34;linux\u0026#34;, version: \u0026#34;3.10.0-862.3.2.el7.x86_64\u0026#34;, arch: \u0026#34;amd64\u0026#34;, family: \u0026#34;unix\u0026#34; 现在，Maven已安装在CentOS系统上。\n安装Apache Maven的最新版本 以下提供了有关如何在CentOS 7上安装最新版本的Apache Maven的逐步说明。我们将从官方网站上下载最新版本的Apache Maven。\n1.安装OpenJDK Maven 3.3+需要安装JDK 1.7或更高版本。我们将安装OpenJDK ，这是CentOS 7中默认的Java开发和运行时。通过输入以下命令安装OpenJDK软件包：","title":"CentOS7上安装Apache Maven"},{"content":"一、什么是consul 1、Consul 是 HashiCorp 公司推出的开源软件，用于实现分布式系统的服务发现与配置。\nConsul 是分布式的、高可用的、 可横向扩展的\n2、官方网站:\nhttps://www.consul.io/ 3、Consul 集群间使用了 Gossip 协议通信和 raft 一致性算法\n二、下载consul软件 1，下载地址:\nhttps://www.consul.io/downloads 选择64位linux版本下载\n2,把consul的安装文件解压:\n[root@localhost consul]# cd /usr/local/source/consul [root@localhost consul]# ls consul_1.8.4_linux_amd64.zip [root@localhost consul]# unzip consul_1.8.4_linux_amd64.zip Archive: consul_1.8.4_linux_amd64.zip  inflating: consul [root@localhost consul]# ls consul consul_1.8.4_linux_amd64.zip 3,复制到到各服务器的/usr/local/soft目录下\n三、在第一台consul服务器上运行: 1,生成数据目录:\n[root@consul1 /]# mkdir /data/ [root@consul1 /]# mkdir /data/consul/ [root@consul1 /]# mkdir /data/consul/data [root@consul1 /]# chmod 777 /data/consul/data 2,运行consul\n-server:以server身份启动\n-bootstrap-expect=2:集群要求的最少server数量\n-bind:监听的ip\n-client:客户端ip,0.0.0.0表示不限制客户端ip\n-data-dir:指定存放数据的目录\n-node:指定节点id,注意：同一集群内节点id不允许重复\n[root@consul1 /]# nohup /usr/local/soft/consul agent -server -bind=172.17.0.2 -client=0.0.0.0 -bootstrap-expect=2 -data-dir=/data/consul/da -node=server-2 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; [1] 549 [root@consul1 /]# 查看是否在运行中，这里我们选择查看端口：\n[root@consul1 /]# ss -lntp State Recv-Q Send-Q Local Address:Port Peer Address:Port LISTEN 0 4096 172.17.0.2:8300 0.0.0.0:* users:((\u0026#34;consul\u0026#34;,pid=549,fd=6)) LISTEN 0 4096 172.17.0.2:8301 0.0.0.0:* users:((\u0026#34;consul\u0026#34;,pid=549,fd=12)) LISTEN 0 4096 172.17.0.2:8302 0.0.0.0:* users:((\u0026#34;consul\u0026#34;,pid=549,fd=8)) LISTEN 0 4096 *:8500 *:* users:((\u0026#34;consul\u0026#34;,pid=549,fd=16)) LISTEN 0 4096 *:8600 *:* users:((\u0026#34;consul\u0026#34;,pid=549,fd=15)) 可以看到consul已经在守护端口中,而且consul启用了多个端口\n3,查看consul的版本:\n[root@consul1 /]# /usr/local/soft/consul --version Consul v1.8.4 Revision 12b16df32 Protocol 2 spoken by default, understands 2 to 3 (agent will automatically use protocol \u0026gt;2 when speaking to compatible agents) 4,查看consul的集群成员数量\n[root@consul1 /]# /usr/local/soft/consul members Node Address Status Type Build Protocol DC Segment server-2 172.17.0.2:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; 只有一台机器，正常\n5,查看当前节点的信息:\n[root@consul1 /]# /usr/local/soft/consul info agent:  check_monitors = 0  check_ttls = 0  checks = 0  services = 0 build:  prerelease =  revision = 12b16df3  version = 1.8.4 consul:  acl = disabled  bootstrap = false  known_datacenters = 1  leader = false  leader_addr =  server = true raft:  applied_index = 0  commit_index = 0  fsm_pending = 0  last_contact = never  last_log_index = 0  last_log_term = 0  last_snapshot_index = 0  last_snapshot_term = 0  latest_configuration = []  latest_configuration_index = 0  num_peers = 0  protocol_version = 3  protocol_version_max = 3  protocol_version_min = 0  snapshot_version_max = 1  snapshot_version_min = 0  state = Follower  term = 0 runtime:  arch = amd64  cpu_count = 2  goroutines = 79  max_procs = 2  os = linux  version = go1.14.6 serf_lan:  coordinate_resets = 0  encrypted = false  event_queue = 0  event_time = 1  failed = 0  health_score = 0  intent_queue = 0  left = 0  member_time = 1  members = 1  query_queue = 0  query_time = 1 serf_wan:  coordinate_resets = 0  encrypted = false  event_queue = 0  event_time = 1  failed = 0  health_score = 0  intent_queue = 0  left = 0  member_time = 1  members = 1  query_queue = 0  query_time = 1 四、在第二台consul服务器上运行相同的操作， 启动的命令需要修改ip和节点名：\n[root@consul2 /]# nohup /usr/local/soft/consul agent -server -bind=172.17.0.3 -client=0.0.0.0 -bootstrap-expect=2 -data-dir=/data/consul/da -node=server-3 \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; [1] 371 查看集群成员：\n[root@consul2 /]# /usr/local/soft/consul members Node Address Status Type Build Protocol DC Segment server-3 172.17.0.3:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; 把当前节点加入到第一台consul服务器的ip:\n[root@consul2 /]# /usr/local/soft/consul join 172.17.0.2 Successfully joined cluster by contacting 1 nodes. 加入成功后再次查看节点:\n[root@consul2 /]# /usr/local/soft/consul members Node Address Status Type Build Protocol DC Segment server-2 172.17.0.2:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; server-3 172.17.0.3:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; 五、在第三台consul服务器上运行相同的操作: 命令修改ip和节点名：并允许访问ui\n-ui:允许访问web ui\n[root@consul3 /]# nohup /usr/local/soft/consul agent -server -bind=172.17.0.4 -client=0.0.0.0 -bootstrap-expect=2 -data-dir=/data/consul/da -node=server-4 -ui \u0026gt;/dev/null 2\u0026gt;\u0026amp;1 \u0026amp; [1] 229 [root@consul3 /]# 查看成员\n[root@consul3 /]# /usr/local/soft/consul members Node Address Status Type Build Protocol DC Segment server-4 172.17.0.4:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; 加入集群\n[root@consul3 /]# /usr/local/soft/consul join 172.17.0.2 Successfully joined cluster by contacting 1 nodes. 成功后再次查看成员\n[root@consul3 /]# /usr/local/soft/consul members Node Address Status Type Build Protocol DC Segment server-2 172.17.0.2:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; server-3 172.17.0.3:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; server-4 172.17.0.4:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; 六，访问在第三台consul服务器上启用的web ui: 访问:\n http://172.17.0.4:8500/ 返回:\n点击后可以查看集群内3个实例：\n如图：\n点击每台机器可查看其状态:\n七，consul的退出: leave指令触发一个优雅的离开动作并关闭agent，节点离开后不会尝试重新加入集群中\n在第二台consul服务器上执行:\n[root@consul2 /]# /usr/local/soft/consul leave Graceful leave complete 查看端口:\n[root@consul2 /]# ss -lntp State Recv-Q Send-Q Local Address:Port Peer Address:Port 可以看到守护进程已退出\n在其他节点查看成员:\n[root@consul3 /]# /usr/local/soft/consul members Node Address Status Type Build Protocol DC Segment server-2 172.17.0.2:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; server-3 172.17.0.3:8301 left server 1.8.4 2 dc1 \u0026lt;all\u0026gt; server-4 172.17.0.4:8301 alive server 1.8.4 2 dc1 \u0026lt;all\u0026gt; 可以看到server-3的状态已变更为left\n从web ui查看:\n已看不到 server-3\n八、docker安装consul docker run -d -p 8500:8500 \u0026ndash;restart=always \u0026ndash;name consul-8500 consul:1.8.8 agent -server -bootstrap -ui -node=consul_node_01 -client=\u0026lsquo;0.0.0.0\u0026rsquo;\n-d: 后台运行容器，并返回容器ID； -p: 指定端口映射，格式为：主机(宿主)端口:容器端口 --name consul-8500 ： 指定容器名称，自定义 consul:1.8.8 ： 指定镜像， 镜像名：标签名 agent: 表示启动 Agent 进程。 server：表示启动 Consul Server 模式 client：表示启动 Consul Cilent 模式。 bootstrap：表示这个节点是 Server-Leader ，每个数据中心只能运行一台服务器。技术角度上讲 Leader 是通过 Raft 算法选举的，但是集群第一次启动时需要一个引导Leader，在引导群集后，建议不要使用此标志。 ui：表示启动 Web UI 管理器 。 -node=consul_node_01：节点的名称，自定义；集群中必须是唯一的，默认是该节点的主机名。 -client：consul服务侦听地址，这个地址提供HTTP、DNS、RPC等服务，  默认是127.0.0.1所以不对外提供服务，如果你要对外提供服务改成0.0.0.0。 join：表示加入到某一个集群中去。 如：-join=192.168.1.169。 ","permalink":"https://iblog.zone/archives/centos%E6%90%AD%E5%BB%BAconsul%E9%9B%86%E7%BE%A4/","summary":"一、什么是consul 1、Consul 是 HashiCorp 公司推出的开源软件，用于实现分布式系统的服务发现与配置。\nConsul 是分布式的、高可用的、 可横向扩展的\n2、官方网站:\nhttps://www.consul.io/ 3、Consul 集群间使用了 Gossip 协议通信和 raft 一致性算法\n二、下载consul软件 1，下载地址:\nhttps://www.consul.io/downloads 选择64位linux版本下载\n2,把consul的安装文件解压:\n[root@localhost consul]# cd /usr/local/source/consul [root@localhost consul]# ls consul_1.8.4_linux_amd64.zip [root@localhost consul]# unzip consul_1.8.4_linux_amd64.zip Archive: consul_1.8.4_linux_amd64.zip  inflating: consul [root@localhost consul]# ls consul consul_1.8.4_linux_amd64.zip 3,复制到到各服务器的/usr/local/soft目录下\n三、在第一台consul服务器上运行: 1,生成数据目录:\n[root@consul1 /]# mkdir /data/ [root@consul1 /]# mkdir /data/consul/ [root@consul1 /]# mkdir /data/consul/data [root@consul1 /]# chmod 777 /data/consul/data 2,运行consul\n-server:以server身份启动\n-bootstrap-expect=2:集群要求的最少server数量\n-bind:监听的ip\n-client:客户端ip,0.0.0.0表示不限制客户端ip\n-data-dir:指定存放数据的目录\n-node:指定节点id,注意：同一集群内节点id不允许重复","title":"CentOS搭建Consul集群"},{"content":"私有镜像仓库搭建\ndocker run -d -p 5050:5000 --restart always --name registry registry:2 仓库配置\nvim /etc/docker/daemon.json\n{  \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://aqwaeuv8.mirror.aliyuncs.com\u0026#34;],  \u0026#34;insecure-registries\u0026#34;:[\u0026#34;10.3.55.134:5050\u0026#34;] // 取消仓库认证 } 测试，push和pull镜像没有问题，使用docker swarm update时出现找不到镜像错误\n问题1：\n update时提示，No such image: 10.3.55.134:5050/ms-group/ly-12320-server-ks_online:5f223018@sha256:261b7ef562bf95a7293046fb0524e5ff2fe70ad55dcd5ffe7981b35f9d50ff56\n 解决方案：\n需要在所有的docker swarm集群节点都配置daemon.json，并重启docker\nsystemctl restart docker 问题2：\n 修复完以上问题后，再更新时，出现starting container failed: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \u0026ldquo;/bin/sh\u0026rdquo;: stat /bin/sh: no such file or directory: unknown\n 解决方案：\n基础镜像没有/bin/sh导致，检查基础镜像\n注意：\nsave导出的镜像 用import导入有问题 建议使用load导入再打tag\n","permalink":"https://iblog.zone/archives/docker-swarm%E9%9B%86%E7%BE%A4%E5%90%AF%E5%8A%A8%E6%97%B6%E6%89%BE%E4%B8%8D%E5%88%B0%E9%95%9C%E5%83%8F%E7%9A%84%E9%97%AE%E9%A2%98%E8%A7%A3%E5%86%B3/","summary":"私有镜像仓库搭建\ndocker run -d -p 5050:5000 --restart always --name registry registry:2 仓库配置\nvim /etc/docker/daemon.json\n{  \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://aqwaeuv8.mirror.aliyuncs.com\u0026#34;],  \u0026#34;insecure-registries\u0026#34;:[\u0026#34;10.3.55.134:5050\u0026#34;] // 取消仓库认证 } 测试，push和pull镜像没有问题，使用docker swarm update时出现找不到镜像错误\n问题1：\n update时提示，No such image: 10.3.55.134:5050/ms-group/ly-12320-server-ks_online:5f223018@sha256:261b7ef562bf95a7293046fb0524e5ff2fe70ad55dcd5ffe7981b35f9d50ff56\n 解决方案：\n需要在所有的docker swarm集群节点都配置daemon.json，并重启docker\nsystemctl restart docker 问题2：\n 修复完以上问题后，再更新时，出现starting container failed: failed to create shim: OCI runtime create failed: container_linux.go:380: starting container process caused: exec: \u0026ldquo;/bin/sh\u0026rdquo;: stat /bin/sh: no such file or directory: unknown\n 解决方案：\n基础镜像没有/bin/sh导致，检查基础镜像","title":"Docker Swarm集群启动时找不到镜像的问题解决"},{"content":"一、下载 wget https://archive.apache.org/dist/rocketmq/4.6.0/rocketmq-all-4.6.0-bin-release.zip unzip rocketmq-all-4.6.0-bin-release.zip mv rocketmq-all-4.6.0-bin-release.zip /data/rocketmq # tree rocketmq rocketmq-all-4.6.0-bin-release ├── LICENSE ├── NOTICE ├── README.md ├── benchmark │ ├── consumer.sh │ ├── producer.sh │ ├── runclass.sh │ └── tproducer.sh ├── bin │ ├── README.md │ ├── cachedog.sh │ ├── cleancache.sh │ ├── cleancache.v1.sh │ ├── dledger │ │ └── fast-try.sh │ ├── mqadmin │ ├── mqadmin.cmd │ ├── mqbroker │ ├── mqbroker.cmd │ ├── mqbroker.numanode0 │ ├── mqbroker.numanode1 │ ├── mqbroker.numanode2 │ ├── mqbroker.numanode3 │ ├── mqnamesrv │ ├── mqnamesrv.cmd │ ├── mqshutdown │ ├── mqshutdown.cmd │ ├── os.sh │ ├── play.cmd │ ├── play.sh │ ├── runbroker.cmd │ ├── runbroker.sh │ ├── runserver.cmd │ ├── runserver.sh │ ├── setcache.sh │ ├── startfsrv.sh │ ├── tools.cmd │ └── tools.sh ├── conf │ ├── 2m-2s-async │ │ ├── broker-a-s.properties │ │ ├── broker-a.properties │ │ ├── broker-b-s.properties │ │ └── broker-b.properties │ ├── 2m-2s-sync │ │ ├── broker-a-s.properties │ │ ├── broker-a.properties │ │ ├── broker-b-s.properties │ │ └── broker-b.properties │ ├── 2m-noslave │ │ ├── broker-a.properties │ │ ├── broker-b.properties │ │ └── broker-trace.properties │ ├── broker.conf │ ├── dledger │ │ ├── broker-n0.conf │ │ ├── broker-n1.conf │ │ └── broker-n2.conf │ ├── logback_broker.xml │ ├── logback_namesrv.xml │ ├── logback_tools.xml │ ├── plain_acl.yml │ └── tools.yml └── lib  ├── commons-beanutils-1.9.2.jar  ├── commons-cli-1.2.jar  ├── commons-codec-1.9.jar  ├── commons-collections-3.2.2.jar  ├── commons-digester-1.8.1.jar  ├── commons-lang3-3.4.jar  ├── commons-logging-1.2.jar  ├── commons-validator-1.6.jar  ├── dledger-0.1.jar  ├── fastjson-1.2.61.jar  ├── guava-19.0.jar  ├── javassist-3.20.0-GA.jar  ├── jcommander-1.72.jar  ├── jna-4.2.2.jar  ├── logback-classic-1.0.13.jar  ├── logback-core-1.0.13.jar  ├── netty-all-4.0.42.Final.jar  ├── netty-tcnative-boringssl-static-1.1.33.Fork26.jar  ├── openmessaging-api-0.3.1-alpha.jar  ├── rocketmq-acl-4.6.0.jar  ├── rocketmq-broker-4.6.0.jar  ├── rocketmq-client-4.6.0.jar  ├── rocketmq-common-4.6.0.jar  ├── rocketmq-example-4.6.0.jar  ├── rocketmq-filter-4.6.0.jar  ├── rocketmq-logging-4.6.0.jar  ├── rocketmq-namesrv-4.6.0.jar  ├── rocketmq-openmessaging-4.6.0.jar  ├── rocketmq-remoting-4.6.0.jar  ├── rocketmq-srvutil-4.6.0.jar  ├── rocketmq-store-4.6.0.jar  ├── rocketmq-tools-4.6.0.jar  ├── slf4j-api-1.7.7.jar  └── snakeyaml-1.19.jar  9 directories, 89 files 二、集群配置  注意：需要关闭防火墙和selinux\n    服务器 ip 安装的服务 配置文件     node1 10.3.55.134 Broker，NameServer /data/rocketmq/conf/2m-noslave/broker-a.properties   node2 10.3.55.135 Broker，NameServer /data/rocketmq/conf/2m-noslave/broker-b.properties   node3 10.3.55.136 Broker，NameServer /data/rocketmq/conf/2m-noslave/broker-c.properties    由于是三主配置，所以配置文件修改如下：\nNode1：\nbrokerIP1=10.3.55.134 brokerIP2=10.3.55.134 brokerClusterName=rocketmq-cluster brokerName=broker-a brokerId=0 namesrvAddr=10.3.55.134:9876;10.3.55.135:9876;10.3.55.136:9876 defaultTopicQueueNums=4 deleteWhen=04 fileReservedTime=120 brokerRole=ASYNC_MASTER flushDiskType=ASYNC_FLUSH listenPort=10911 storePathRootDir=/data/rocketmq/store-a messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 diskMaxUsedSpaceRatio=88 Node2：\nbrokerIP1=10.3.55.135 brokerIP2=10.3.55.135 brokerClusterName=rocketmq-cluster brokerName=broker-b brokerId=1 namesrvAddr=10.3.55.134:9876;10.3.55.135:9876;10.3.55.136:9876 defaultTopicQueueNums=4 deleteWhen=04 fileReservedTime=120 brokerRole=ASYNC_MASTER flushDiskType=ASYNC_FLUSH listenPort=10911 storePathRootDir=/data/rocketmq/store-b messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 diskMaxUsedSpaceRatio=88 Node3:\nbrokerIP1=10.3.55.136 brokerIP2=10.3.55.136 brokerClusterName=rocketmq-cluster brokerName=broker-c brokerId=2 namesrvAddr=10.3.55.134:9876;10.3.55.135:9876;10.3.55.136:9876 defaultTopicQueueNums=4 deleteWhen=04 fileReservedTime=120 brokerRole=ASYNC_MASTER flushDiskType=ASYNC_FLUSH listenPort=10911 storePathRootDir=/data/rocketmq/store-c messageDelayLevel=1s 5s 10s 30s 1m 2m 3m 4m 5m 6m 7m 8m 9m 10m 20m 30m 1h 2h mapedFileSizeCommitLog=1073741824 mapedFileSizeConsumeQueue=300000 diskMaxUsedSpaceRatio=88 参数说明：\n   参数名 默认值 说明     listenPort 10911 接受客户端连接的监听端口   namesrvAddr null nameServer 地址   brokerIP1 网卡的 InetAddress 当前 broker 监听的 IP   brokerIP2 跟 brokerIP1 一样 存在主从 broker 时，如果在 broker 主节点上配置了 brokerIP2 属性，broker 从节点会连接主节点配置的 brokerIP2 进行同步   brokerName null broker 的名称   brokerClusterName DefaultCluster 本 broker 所属的 Cluser 名称   brokerId 0 broker id, 0 表示 master, 其他的正整数表示 slave   storePathCommitLog $HOME/store/commitlog/ 存储 commit log 的路径   storePathConsumerQueue $HOME/store/consumequeue/ 存储 consume queue 的路径   mappedFileSizeCommitLog 1024 * 1024 * 1024(1G) commit log 的映射文件大小   deleteWhen 04 在每天的什么时间删除已经超过文件保留时间的 commit log   fileReservedTime 72 以小时计算的文件保留时间   brokerRole ASYNC_MASTER SYNC_MASTER/ASYNC_MASTER/SLAVE   flushDiskType ASYNC_FLUSH SYNC_FLUSH/ASYNC_FLUSH SYNC_FLUSH 模式下的 broker 保证在收到确认生产者之前将消息刷盘。ASYNC_FLUSH 模式下的 broker 则利用刷盘一组消息的模式，可以取得更好的性能。   enableDLegerCommitLog true 是否启动 DLedger   dLegerGroup RaftNode00 DLedger Raft Group的名字，建议和 brokerName 保持一致   dLegerPeers n0-127.0.0.1:40911;n1-127.0.0.1:40912;n2-127.0.0.1:40913 DLedger Group 内各节点的端口信息，同一个 Group 内的各个节点配置必须要保证一致   dLegerSelfId n0 节点 id, 必须属于 dLegerPeers 中的一个；同 Group 内各个节点要唯一   sendMessageThreadPoolNums 16 发送线程个数，建议配置成 Cpu 核数    三、集群启动 # node1 nohup sh bin/mqnamesrv \u0026gt; nohubNameserv \u0026amp; nohup sh bin/mqbroker \u0026gt; nohubBroker -c conf/2m-noslave/broker-a.properties \u0026amp;  # node2 nohup sh bin/mqnamesrv \u0026gt; nohubNameserv \u0026amp; nohup sh bin/mqbroker \u0026gt; nohubBroker -c conf/2m-noslave/broker-b.properties \u0026amp;  # node3 nohup sh bin/mqnamesrv \u0026gt; nohubNameserv \u0026amp; nohup sh bin/mqbroker \u0026gt; nohubBroker -c conf/2m-noslave/broker-c.properties \u0026amp; 四、查看集群状态 [root@localhost rocketmq]# sh bin/mqadmin clusterList -n 127.0.0.1:9876 RocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0). RocketMQLog:WARN Please initialize the logger system properly. #Cluster Name #Broker Name #BID #Addr #Version #InTPS(LOAD) #OutTPS(LOAD) #PCWait(ms) #Hour #SPACE rocketmq-cluster broker-a 0 10.3.55.134:10911 V4_6_0 0.00(0,0ms) 0.00(0,0ms) 0 457977.21 -1.0000 rocketmq-cluster broker-b 0 10.3.55.135:10911 V4_6_0 0.00(0,0ms) 0.00(0,0ms) 0 457977.21 -1.0000 rocketmq-cluster broker-c 0 10.3.55.136:10911 V4_6_0 0.00(0,0ms) 0.00(0,0ms) 0 457977.21 -1.0000 如果执行上面的命令出现以下错误\nRocketMQLog:WARN No appenders could be found for logger (io.netty.util.internal.PlatformDependent0). RocketMQLog:WARN Please initialize the logger system properly. org.apache.rocketmq.tools.command.SubCommandException: ClusterListSubCommand command failed \tat org.apache.rocketmq.tools.command.cluster.ClusterListSubCommand.execute(ClusterListSubCommand.java:93) \tat org.apache.rocketmq.tools.command.MQAdminStartup.main0(MQAdminStartup.java:139) \tat org.apache.rocketmq.tools.command.MQAdminStartup.main(MQAdminStartup.java:90) Caused by: org.apache.rocketmq.acl.common.AclException: [10015:signature-failed] unable to calculate a request signature. error=[10015:signature-failed] unable to calculate a request signature. error=Algorithm HmacSHA1 not available \tat org.apache.rocketmq.acl.common.AclSigner.signAndBase64Encode(AclSigner.java:84) \tat org.apache.rocketmq.acl.common.AclSigner.calSignature(AclSigner.java:73) \tat org.apache.rocketmq.acl.common.AclSigner.calSignature(AclSigner.java:68) \tat org.apache.rocketmq.acl.common.AclUtils.calSignature(AclUtils.java:69) \tat org.apache.rocketmq.acl.common.AclClientRPCHook.doBeforeRequest(AclClientRPCHook.java:44) \tat org.apache.rocketmq.remoting.netty.NettyRemotingAbstract.doBeforeRpcHooks(NettyRemotingAbstract.java:172) \tat org.apache.rocketmq.remoting.netty.NettyRemotingClient.invokeSync(NettyRemotingClient.java:368) \tat org.apache.rocketmq.client.impl.MQClientAPIImpl.getBrokerClusterInfo(MQClientAPIImpl.java:1337) \tat org.apache.rocketmq.tools.admin.DefaultMQAdminExtImpl.examineBrokerClusterInfo(DefaultMQAdminExtImpl.java:306) \tat org.apache.rocketmq.tools.admin.DefaultMQAdminExt.examineBrokerClusterInfo(DefaultMQAdminExt.java:251) \tat org.apache.rocketmq.tools.command.cluster.ClusterListSubCommand.printClusterBaseInfo(ClusterListSubCommand.java:172) \tat org.apache.rocketmq.tools.command.cluster.ClusterListSubCommand.execute(ClusterListSubCommand.java:88) \t... 2 more Caused by: org.apache.rocketmq.acl.common.AclException: [10015:signature-failed] unable to calculate a request signature. error=Algorithm HmacSHA1 not available \tat org.apache.rocketmq.acl.common.AclSigner.sign(AclSigner.java:63) \tat org.apache.rocketmq.acl.common.AclSigner.signAndBase64Encode(AclSigner.java:79) \t... 13 more Caused by: java.security.NoSuchAlgorithmException: Algorithm HmacSHA1 not available \tat javax.crypto.Mac.getInstance(Mac.java:181) \tat org.apache.rocketmq.acl.common.AclSigner.sign(AclSigner.java:57) \t... 14 more 解决方案：\n确认自己的$JAVA_HOME/lib/ext目录下存在sunjce_provider.jar文件\n打开/data/rocketmq/bin/tools.sh\n#=========================================================================================== # JVM Configuration #=========================================================================================== JAVA_OPT=\u0026#34;${JAVA_OPT}-server -Xms1g -Xmx1g -Xmn256m -XX:MetaspaceSize=128m -XX:MaxMetaspaceSize=128m\u0026#34; #JAVA_OPT=\u0026#34;${JAVA_OPT} -Djava.ext.dirs=${BASE_DIR}/lib:${JAVA_HOME}/jre/lib/ext:${JAVA_HOME}/lib/ext\u0026#34; # 修改为以下内容 JAVA_OPT=\u0026#34;${JAVA_OPT}-Djava.ext.dirs=${BASE_DIR}/lib:${JAVA_HOME}/jre/lib/ext:/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.262.b10-1.el7.x86_64/jre/lib/ext\u0026#34; JAVA_OPT=\u0026#34;${JAVA_OPT}-cp ${CLASSPATH}\u0026#34; 启动内存不够 修改 bin/runbroker.sh 和 bin/runserver.sh 中的\nJAVA_OPT=\u0026#34;${JAVA_OPT} -server -Xms8g -Xmx8g -Xmn4g 将参数调低 （可以改为别的值）\nJAVA_OPT=\u0026#34;${JAVA_OPT} -server -Xms512m -Xmx512m -Xmn256m\u0026#34; ","permalink":"https://iblog.zone/archives/centos7%E6%90%AD%E5%BB%BArocketmq4.6%E4%B8%89%E4%B8%BB%E9%9B%86%E7%BE%A4/","summary":"一、下载 wget https://archive.apache.org/dist/rocketmq/4.6.0/rocketmq-all-4.6.0-bin-release.zip unzip rocketmq-all-4.6.0-bin-release.zip mv rocketmq-all-4.6.0-bin-release.zip /data/rocketmq # tree rocketmq rocketmq-all-4.6.0-bin-release ├── LICENSE ├── NOTICE ├── README.md ├── benchmark │ ├── consumer.sh │ ├── producer.sh │ ├── runclass.sh │ └── tproducer.sh ├── bin │ ├── README.md │ ├── cachedog.sh │ ├── cleancache.sh │ ├── cleancache.v1.sh │ ├── dledger │ │ └── fast-try.sh │ ├── mqadmin │ ├── mqadmin.cmd │ ├── mqbroker │ ├── mqbroker.cmd │ ├── mqbroker.numanode0 │ ├── mqbroker.","title":"CentOS7搭建Rocketmq4.6三主集群"},{"content":"基础规划 软件环境 #mongodb下载 # wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.4.10.tgz #mongodb解压 # tar -zxvf mongodb-linux-x86_64-rhel70-3.4.10.tgz #mongodb安装 # mv mv mongodb-linux-x86_64-rhel70-3.4.10 /usr/local/mongodb #mongodb环境配置 # echo \u0026#39;export PATH=/usr/local/mongodb/bin:$PATH\u0026#39; \u0026gt;\u0026gt; /etc/profile #使配置生效  # source /etc/profile  # 查看是否安装成功 # mongo --version 环境规划\u0026ndash;单机器部署多节点    节点名 节点用途 节点IP 节点端口 集群名     node1 主节点(PRIMARY) 10.0.2.11 27017 mgset   node2 从节点(SECONDARY) 10.0.2.11 27018    node3 仲裁节点(ARBITER) 10.0.2.11 27019        目录 用途 备注     /data/mongodb/node[x]/data Mongo集群数据文件目录    /data/mongodb/node[x]/logs Mongo集群系统日志目录    /data/mongodb/node[x]/conf Mongo集群配置文件目录     搭建步骤 配置文件 node1的配置文件/data/mongodb/node1/conf/mongod.conf (其他节点根据情况修改)\nsystemLog:  destination: file  path: \u0026#34;/data/mongodb/node1/logs/mongod.log\u0026#34;  logAppend: true storage:  dbPath: \u0026#34;/data/mongodb/node1/data\u0026#34;  journal:  enabled: true processManagement:  fork: true  pidFilePath: \u0026#34;/data/mongodb/node1/mongod.pid\u0026#34; net:  bindIp: 10.0.2.11  port: 27017 #security: # 认证配置，开启认证后取消注释 # keyFile: \u0026#34;/data/mongodb/node1/conf/access.key\u0026#34; # authorization: enabled replication:  oplogSizeMB: 500  replSetName: mgset 启动服务\n# mongod -f /data/mongodb/node1/conf/mongod.conf # mongod -f /data/mongodb/node2/conf/mongod.conf # mongod -f /data/mongodb/node3/conf/mongod.conf 配置集群 进入其中一个节点(主节点)的mongo控制台, 配置集群(务必保证节点防火墙关闭或开放mongo服务端口).\n# 仅在一个节点执行 # mongo 10.0.2.11:27017 #进入mongo控制台 \u0026gt; cfg = {_id: \u0026#39;mgset\u0026#39;, members: []} #生成集群配置变量 \u0026gt; cfg.members.push({_id: 1, host: \u0026#39;10.0.2.11:27017\u0026#39;}) #变量中加入节点1 \u0026gt; cfg.members.push({_id: 2, host: \u0026#39;10.0.2.11:27018\u0026#39;}) #变量中加入节点2 \u0026gt; cfg.members.push({_id: 3, host: \u0026#39;10.0.2.11:27019\u0026#39;, arbiterOnly: true}) #变量中加入节点3(仲裁节点) \u0026gt; rs.initiate(cfg) #根据变量配置集群 \u0026gt; rs.isMaster() #查看集群是否配置成功 \u0026gt; rs.status()  #健康状态 1表示正常 0表示故障  \u0026#34;health\u0026#34; : 1,  #表示状态 1是主库 2是从库 3表示恢复数据中 7表示投票者 8表示down机  \u0026#34;state\u0026#34; : 1,  #标注是主库还是从库  \u0026#34;stateStr\u0026#34; : \u0026#34;PRIMARY\u0026#34;,  #集群启动时间  \u0026#34;uptime\u0026#34; : 579,  #另一种格式的时间  \u0026#34;optime\u0026#34; : {  \u0026#34;ts\u0026#34; : Timestamp(1590593779, 1),  \u0026#34;t\u0026#34; : NumberLong(1)  },  #上一次心跳传过来数据的时间  \u0026#34;optimeDate\u0026#34; : ISODate(\u0026#34;2020-05-27T15:36:19Z\u0026#34;),  #检测上一次心跳时间  \u0026#34;lastHeartbeat\u0026#34; : ISODate(\u0026#34;2020-05-27T15:36:25.815Z\u0026#34;), \u0026gt; rs.printReplicationInfo()\t#oplog信息 configured oplog size: 1024MB log length start to end: 1543secs (0.43hrs) oplog first event time: Wed May 27 2020 23:26:46 GMT+0800 (CST) oplog last event time: Wed May 27 2020 23:52:29 GMT+0800 (CST) now: Wed May 27 2020 23:52:38 GMT+0800 (CST) \u0026gt; rs.printSlaveReplicationInfo()\t#查看延时从库信息 source: 10.0.0.93:28018  syncedTo: Wed May 27 2020 23:54:19 GMT+0800 (CST)  0 secs (0 hrs) behind the primary source: 10.0.0.93:28019  syncedTo: Wed May 27 2020 23:54:19 GMT+0800 (CST)  0 secs (0 hrs) behind the primary \u0026gt; rs.config()\t#打印副本集配置文件 添加用户 集群配置完成后, 仍然在主节点的mongo控制台中添加数据库管理员\n# 仅在一个节点执行 # mongo 10.0.2.11:27017 #进入mongo控制台 \u0026gt; use admin #使用内置的admin库  \u0026gt; db.createUser(\t#创建数据库管理员 { user:\u0026#34;root\u0026#34;, pwd:\u0026#34;root\u0026#34;, roles:[{role:\u0026#34;readWriteAnyDatabase\u0026#34;,db:\u0026#34;admin\u0026#34;},{role:\u0026#34;dbAdminAnyDatabase\u0026#34;,db:\u0026#34;admin\u0026#34;},{role:\u0026#34;userAdminAnyDatabase\u0026#34;,db:\u0026#34;admin\u0026#34;}] } )  # 其他用户创建 \u0026gt; db.createUser( #创建集群管理员 { user:\u0026#34;suroot\u0026#34;, pwd:\u0026#34;suroot\u0026#34;, roles:[{role:\u0026#34;clusterAdmin\u0026#34;,db:\u0026#34;admin\u0026#34;},{role:\u0026#34;clusterManager\u0026#34;,db:\u0026#34;admin\u0026#34;},{role:\u0026#34;clusterMonitor\u0026#34;,db:\u0026#34;admin\u0026#34;}] } ) \u0026gt; use testdb #切换到testdb数据库,不用事先创建 \u0026gt; db.createUser( #创建特定库的特定用户 { user:\u0026#34;test\u0026#34;, pwd:\u0026#34;test\u0026#34;, roles:[{role:\u0026#34;readWrite\u0026#34;,db:\u0026#34;testdb\u0026#34;},{role:\u0026#34;dbAdmin\u0026#34;,db:\u0026#34;testdb\u0026#34;},{role:\u0026#34;userAdmin\u0026#34;,db:\u0026#34;testdb\u0026#34;}] } ) \u0026gt; use admin \u0026gt; db.system.users.find() #查看创建的用户 　[注]:\n1. MongoDB的用户和数据库是绑定的, 必须指定某个用户归属于哪个数据库, 即在roles字段的每个role中指定db字段. 2. 数据库管理员通常需要具有读写,管理任意数据库和管理任意用户的role, 后续可以登录此用户进行数据库和用户的增删改查. 3. 集群管理员通常需要具有集群管理和集群监控的role, 只有集群管理员可以关闭集群. 4. 普通用户根据用途不同可以对特定或者多个数据库拥有各种不同的role, 本例中的test用户既可以读写testdb库, 同时也是testdb库的管理员. 5. 主节点上添加的用户应该能够在从节点上查询到(需要先键入rs.slaveOk()命令). 开启用户认证 用户添加完成后需要关闭所有节点:\nkillall mongod 生成keyFile(keyFile的用途是作为所有mongod后台进程允许加入集群的凭证, 所有集群中的节点共用一个keyFile, 避免其他mongod非法加入集群):\n# 仅在一个节点执行 # openssl rand -base64 756 \u0026gt; /data/mongodb/node1/conf/access.key #生成keyFile, keyFile的长度必须在6-1024个字符之间 # chmod 400 /data/mongodb/node1/conf/access.key #设置keyFile文件为只读 # cp /data/mongodb/node1/conf/access.key /data/mongodb/node2/conf/ #将keyFile复制到其他节点 # cp /data/mongodb/node1/conf/access.key /data/mongodb/node3/conf/ 取消三个节点mongod.conf文件中security部分的注释:\nsystemLog:  destination: file  path: \u0026#34;/data/mongodb/node1/logs/mongod.log\u0026#34;  logAppend: true storage:  dbPath: \u0026#34;/data/mongodb/node1/data\u0026#34;  journal:  enabled: true processManagement:  fork: true  pidFilePath: \u0026#34;/data/mongodb/node1/mongod.pid\u0026#34; net:  bindIp: 10.0.2.11  port: 27017 security: # 认证配置，开启认证后取消注释  keyFile: \u0026#34;/data/mongodb/node1/conf/access.key\u0026#34;  authorization: enabled replication:  oplogSizeMB: 500  replSetName: mgset 依次启动主节点, 从节点和仲裁节点的mongod后台进程:\n# mongod -f /data/mongodb/node1/conf/mongod.conf # mongod -f /data/mongodb/node2/conf/mongod.conf # mongod -f /data/mongodb/node3/conf/mongod.conf 使用认证用户登录:\n# mongo 10.0.2.11:27017 \u0026gt; use admin \u0026gt; db.auth(\u0026#39;root\u0026#39;, \u0026#39;root\u0026#39;) #使用数据库管理员认证 \u0026gt; rs.slaveOk() #默认读写操作均在主节点执行, 从节点上需要执行此命令才能读取数据库数据 \u0026gt; db.system.users.find() \u0026gt; use testdb \u0026gt; db.auth(\u0026#39;test\u0026#39;, \u0026#39;test\u0026#39;) #切换到test用户 \u0026gt; db.getCollectionNames() \u0026gt; use admin #切换到集群管理员用户, 关闭mongo服务 \u0026gt; db.auth(\u0026#39;suroot\u0026#39;, \u0026#39;suroot\u0026#39;) 　至此, 基于用户认证的MongoDB3.4三节点副本集集群环境已经搭建完成.\n常用命令：\nhttps://www.runoob.com/mongodb/mongodb-create-collection.html\n本文参考：\nhttps://www.itq168.com/article/3470\nhttps://xpbag.com/503.html\n","permalink":"https://iblog.zone/archives/centos7%E6%90%AD%E5%BB%BA%E5%9F%BA%E4%BA%8E%E7%94%A8%E6%88%B7%E8%AE%A4%E8%AF%81%E7%9A%84mongodb3.4%E4%B8%89%E8%8A%82%E7%82%B9%E5%89%AF%E6%9C%AC%E9%9B%86%E9%9B%86%E7%BE%A4/","summary":"基础规划 软件环境 #mongodb下载 # wget https://fastdl.mongodb.org/linux/mongodb-linux-x86_64-rhel70-3.4.10.tgz #mongodb解压 # tar -zxvf mongodb-linux-x86_64-rhel70-3.4.10.tgz #mongodb安装 # mv mv mongodb-linux-x86_64-rhel70-3.4.10 /usr/local/mongodb #mongodb环境配置 # echo \u0026#39;export PATH=/usr/local/mongodb/bin:$PATH\u0026#39; \u0026gt;\u0026gt; /etc/profile #使配置生效  # source /etc/profile  # 查看是否安装成功 # mongo --version 环境规划\u0026ndash;单机器部署多节点    节点名 节点用途 节点IP 节点端口 集群名     node1 主节点(PRIMARY) 10.0.2.11 27017 mgset   node2 从节点(SECONDARY) 10.0.2.11 27018    node3 仲裁节点(ARBITER) 10.0.2.11 27019        目录 用途 备注     /data/mongodb/node[x]/data Mongo集群数据文件目录    /data/mongodb/node[x]/logs Mongo集群系统日志目录    /data/mongodb/node[x]/conf Mongo集群配置文件目录     搭建步骤 配置文件 node1的配置文件/data/mongodb/node1/conf/mongod.","title":"CentOS7搭建基于用户认证的MongoDB3.4三节点副本集集群"},{"content":"一、Redis安装  安装基础依赖  #安装基础依赖包 sudo yum install -y gcc gcc-c++ make jemalloc-devel epel-release  下载Redis（ https://redis.io/download ）  #从官网获取最新版本的下载链接，然后通过wget命令下载 wget http://download.redis.io/releases/redis-4.0.2.tar.gz  解压到指定目录  #解压 sudo tar -zvxf redis-4.0.2.tar.gz -C /usr/local/src/  编译\u0026amp;安装  #进入目录 cd /usr/local/src/redis-4.0.2 #编译\u0026amp;安装 sudo make -j 16 \u0026amp; make install 二、Redis启动与测试  启动redis-server  #进入src目录 cd /usr/local/src/redis-4.0.2/src #启动服务端 sudo ./redis-server  启动redis客户端测试  #进入src目录 cd /usr/local/src/redis-4.0.2/src #启动客户端 sudo ./redis-cli  设置：set key1 value1 获取：get key1\n 三、Redis配置 1、 配置本机外访问  修改配置：绑定本机IP\u0026amp;关闭保护模式  #修改配置文件 sudo vi /usr/local/src/redis-4.0.2/redis.conf  #更换绑定 #将bind 127.0.0.1 更换为本机IP，例如：10.3.55.138 bind 10.3.55.138  #关闭保护模式 protected-mode no  # 设置密码 requirepass xxx  # 指定日志路径 logfile \u0026#34;/var/log/redis.log\u0026#34;  开放端口  #增加redis端口：6379 sudo firewall-cmd --add-port=6379/tcp --permanent #重新加载防火墙设置 sudo firewall-cmd --reload  Redis指定配置文件启动  #进入目录 cd /usr/local/src/redis-4.0.2 #指定配置文件启动 sudo ./src/redis-server redis.conf  Redis客户端连接指定Redis Server  #进入目录 cd /usr/redis/redis-4.0.2 #连接指定Redis Server sudo ./src/redis-cli -h 10.3.55.138 -a xxxx 2、配置Redis开机启动 将Redis配置成为系统服务，以支持开机启动\n 创建Redis服务  #创建服务文件 sudo vi /usr/lib/systemd/system/redis.service  #文件内容 [Unit] Description=Redis Server After=network.target  [Service] ExecStart=/usr/local/src/redis-4.0.2/src/redis-server /usr/local/src/redis-4.0.2/redis.conf --daemonize no ExecStop=/usr/local/src/redis-4.0.2/src/redis-cli -h 10.3.55.138 -p 6379 -a \u0026#34;xxx\u0026#34; shutdown Restart=always  [Install] WantedBy=multi-user.target  设置Redis服务开机启动\u0026amp;开启服务  #设置Redis服务开机启动 sudo systemctl enable redis #启动Redis服务 sudo systemctl start redis ","permalink":"https://iblog.zone/archives/centos7%E5%AE%89%E8%A3%85redis4.0/","summary":"一、Redis安装  安装基础依赖  #安装基础依赖包 sudo yum install -y gcc gcc-c++ make jemalloc-devel epel-release  下载Redis（ https://redis.io/download ）  #从官网获取最新版本的下载链接，然后通过wget命令下载 wget http://download.redis.io/releases/redis-4.0.2.tar.gz  解压到指定目录  #解压 sudo tar -zvxf redis-4.0.2.tar.gz -C /usr/local/src/  编译\u0026amp;安装  #进入目录 cd /usr/local/src/redis-4.0.2 #编译\u0026amp;安装 sudo make -j 16 \u0026amp; make install 二、Redis启动与测试  启动redis-server  #进入src目录 cd /usr/local/src/redis-4.0.2/src #启动服务端 sudo ./redis-server  启动redis客户端测试  #进入src目录 cd /usr/local/src/redis-4.0.2/src #启动客户端 sudo ./redis-cli  设置：set key1 value1 获取：get key1","title":"CentOS7安装Redis4.0"},{"content":"1、下载mysql的安装包\n#下载mysql wget https://cdn.mysql.com/archives/mysql-5.6/mysql-5.6.30.tar.gz 2、编译安装mysql\n#查看系统发行版本号 cat /etc/redhat-release CentOS Linux release 7.8.2003 (Core)  # 安装依赖包 yum install ncurses-devel libaio-devel -y yum install cmake gcc gcc-c++ make autoconf -y  #关闭SELinux #临时关闭selinux setenforce 0 #永久关闭selinux sed -i \u0026#39;s#SELINUX=enforcing#SELINUX=disabled#g\u0026#39; /etc/selinux/config  #关闭防火墙 #查看防火墙状态 systemctl status firewalld.service #临时关闭防火墙 systemctl stop firewalld.service #永久关闭防火墙 systemctl disable firewalld.service  #创建mysql帐号 groupadd -r mysql useradd -r -g mysql -s /sbin/nologin mysql  # 编译安装mysql tar xf mysql-5.6.30.tar.gz cd mysql-5.6.30/ cmake . -DCMAKE_INSTALL_PREFIX=/usr/local/mysql-5.6.30 \\  #指定安装目录 -DMYSQL_DATADIR=/usr/local/mysql-5.6.30/data \\  #指定数据目录 -DMYSQL_UNIX_ADDR=/usr/local/mysql-5.6.30/mysql.sock \\  #指定sock文件位置 -DDEFAULT_CHARSET=utf8 \\  #默认字符集 -DDEFAULT_COLLATION=utf8_general_ci \\  #默认编码 -DWITH_EXTRA_CHARSETS=all \\  #其它额外字符集 -DWITH_INNOBASE_STORAGE_ENGINE=1 \\  #支持的存储引擎 -DWITH_FEDERATED_STORAGE_ENGINE=1 \\ -DWITH_BLACKHOLE_STORAGE_ENGINE=1 \\ -DWITHOUT_EXAMPLE_STORAGE_ENGINE=1 \\ -DWITH_SSL=bundled \\  #编译其它功能SSL、ZLIB等 -DWITH_ZLIB=bundled \\ -DENABLED_LOCAL_INFILE=1 \\ -DWITH_EMBEDDED_SERVER=1 \\ -DENABLE_DOWNLOADS=1 \\ -DWITH_DEBUG=0 -DSYSCONFDIR=/etc #编译并安装数据库 make \u0026amp;\u0026amp; make install  #做软链接并给MySQL目录授权 ln -s /usr/local/mysql-5.6.30/ /usr/local/mysql # 必须要做 否则后面操作会报错 chown -R mysql.mysql /usr/local/mysql 3、初始化数据库\n#mysql 5.6初始化数据库的命令 cd /usr/local/mysql ./scripts/mysql_install_db --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --user=mysql #mysql 5.7初始化数据库的命令 /usr/local/mysql/bin/mysqld --initialize-insecure --basedir=/usr/local/mysql --datadir=/usr/local/mysql/data --user=mysql 4、设置mysql启动服务脚本\ncp /usr/local/mysql/support-files/mysql.server /etc/init.d/mysqld chmod 700 /etc/init.d/mysqld 5、设置mysql环境变量\necho \u0026#39;PATH=/usr/local/mysql/bin:$PATH\u0026#39; \u0026gt;\u0026gt;/etc/profile source /etc/profile 6、设置mysql配置文件\n#编辑mysql配置文件 /etc/my.cnf vim /etc/my.cnf  #在my.cnf中写入如下内容 [mysqld] server_id=1 port=3306 basedir=/usr/local/mysql datadir=/usr/local/mysql/data socket=/usr/local/mysql/mysql.sock log_bin=/usr/local/mysql/mysql-bin log_error=/var/log/mysql.log character-set-server=utf8 [client] socket=/usr/local/mysql/mysql.sock 7、启动mysql服务\n#启动mysql服务 /etc/init.d/mysqld start  #关闭mysql服务 /etc/init.d/mysqld stop  #重启mysql服务 /etc/init.d/mysqld restart 8、设置mysql服务密码\nmysqladmin -u root password 123456 9、开启mysql远程连接\n#登录mysql，操作mysql系统数据库 mysql -u root -p  #以下为sql命令 use mysql; #查看用户表中信息 select User,authentication_string,Host,Password from user; #使用如下命令创建可以远程连接的账号和密码 GRANT ALL PRIVILEGES ON *.* TO \u0026#39;账号\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;密码\u0026#39;; # \u0026#34;%\u0026#34;代表所有主机，也可以具体到你的主机IP地址  #刷新权限（从mysql数据库的grant表中重新加载权限数据到cache中，一定要做） flush privileges; ","permalink":"https://iblog.zone/archives/centos7%E7%BC%96%E8%AF%91%E5%AE%89%E8%A3%85mysql5.6/","summary":"1、下载mysql的安装包\n#下载mysql wget https://cdn.mysql.com/archives/mysql-5.6/mysql-5.6.30.tar.gz 2、编译安装mysql\n#查看系统发行版本号 cat /etc/redhat-release CentOS Linux release 7.8.2003 (Core)  # 安装依赖包 yum install ncurses-devel libaio-devel -y yum install cmake gcc gcc-c++ make autoconf -y  #关闭SELinux #临时关闭selinux setenforce 0 #永久关闭selinux sed -i \u0026#39;s#SELINUX=enforcing#SELINUX=disabled#g\u0026#39; /etc/selinux/config  #关闭防火墙 #查看防火墙状态 systemctl status firewalld.service #临时关闭防火墙 systemctl stop firewalld.service #永久关闭防火墙 systemctl disable firewalld.service  #创建mysql帐号 groupadd -r mysql useradd -r -g mysql -s /sbin/nologin mysql  # 编译安装mysql tar xf mysql-5.","title":"CentOS7编译安装MySQL5.6"},{"content":"一、Xtrabackup介绍 　MySQL冷备、mysqldump、MySQL热拷贝都无法实现对数据库进行增量备份。在实际生产环境中增量备份是非常实用的，如果数据大于50G或100G，存储空间足够的情况下，可以每天进行完整备份，如果每天产生的数据量较大，需要定制数据备份策略。例如每周实用完整备份，周一到周六实用增量备份。而Percona-Xtrabackup就是为了实现增量备份而出现的一款主流备份工具，xtrabakackup有2个工具，分别是xtrabakup、innobakupe。\n　Percona-xtrabackup是 Percona公司开发的一个用于MySQL数据库物理热备的备份工具，支持MySQL、Percona server和MariaDB，开源免费，是目前较为受欢迎的主流备份工具。xtrabackup只能备份innoDB和xtraDB两种数据引擎的表，而不能备份MyISAM数据表。\n二、Xtrabackup优点 （1）备份速度快，物理备份可靠\n（2）备份过程不会打断正在执行的事务（无需锁表）\n（3）能够基于压缩等功能节约磁盘空间和流量\n（4）自动备份校验\n（5）还原速度快\n（6）可以流传将备份传输到另外一台机器上\n（7）在不增加服务器负载的情况备份数据\n三、Xtrabackup备份原理 Xtrabackup备份流程图：\n（1）innobackupex启动后，会先fork一个进程，用于启动xtrabackup，然后等待xtrabackup备份ibd数据文件；\n（2）xtrabackup在备份innoDB数据是，有2种线程：redo拷贝线程和ibd数据拷贝线程。xtrabackup进程开始执行后，会启动一个redo拷贝的线程，用于从最新的checkpoint点开始顺序拷贝redo.log；再启动ibd数据拷贝线程，进行拷贝ibd数据。这里是先启动redo拷贝线程的。在此阶段，innobackupex进行处于等待状态（等待文件被创建）\n（4）xtrabackup拷贝完成ibd数据文件后，会通知innobackupex（通过创建文件），同时xtrabackup进入等待状态（redo线程依旧在拷贝redo.log）\n（5）innobackupex收到xtrabackup通知后哦，执行FLUSH TABLES WITH READ LOCK（FTWRL），取得一致性位点，然后开始备份非InnoDB文件（如frm、MYD、MYI、CSV、opt、par等格式的文件），在拷贝非InnoDB文件的过程当中，数据库处于全局只读状态。\n（6）当innobackup拷贝完所有的非InnoDB文件后，会通知xtrabackup，通知完成后，进入等待状态；\n（7）xtrabackup收到innobackupex备份完成的通知后，会停止redo拷贝线程，然后通知innobackupex，redo.log文件拷贝完成；\n（8）innobackupex收到redo.log备份完成后，就进行解锁操作，执行：UNLOCK TABLES；\n（9）最后innbackupex和xtrabackup进程各自释放资源，写备份元数据信息等，innobackupex等xtrabackup子进程结束后退出。\n四、xtrabackup的安装部署以及备份恢复实现 1、xtrabackup的安装 下载地址：https://www.percona.com/downloads/XtraBackup/LATEST/\n可以选择rpm包方式安装，也可以下载源码包编译安装，这里直接采用rpm包的方式进行安装\n[root@master tools]# wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.9/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.9-1.el7.x86_64.rpm [root@master tools]# yum install -y percona-xtrabackup-24-2.4.9-1.el7.x86_64.rpm  [root@master ~]# rpm -qa |grep xtrabackup percona-xtrabackup-24-2.4.9-1.el7.x86_64  Xtrabackup中主要包含两个工具： xtrabackup：是用于热备innodb，xtradb表中数据的工具，不能备份其他类型的表，也不能备份数据表结构； innobackupex：是将xtrabackup进行封装的perl脚本，提供了备份myisam表的能力。 常用选项:  --host 指定主机  --user 指定用户名  --password 指定密码  --port 指定端口  --databases 指定数据库  --incremental 创建增量备份  --incremental-basedir 指定包含完全备份的目录  --incremental-dir 指定包含增量备份的目录  --apply-log 对备份进行预处理操作  一般情况下，在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据文件仍处理不一致状态。“准备”的主要作用正是通过回滚未提交的事务及同步已经提交的事务至数据文件也使得数据文件处于一致性状态。  --redo-only 不回滚未提交事务  --copy-back 恢复备份目录 使用innobackupex备份时，其会调用xtrabackup备份所有的InnoDB表，复制所有关于表结构定义的相关文件(.frm)、以及MyISAM、MERGE、CSV和ARCHIVE表的相关文件，同时还会备份触发器和数据库配置信息相关的文件，这些文件会被保存到一个以时间命名的目录当中。在备份的同时，innobackupex还会在备份目录中创建如下文件：\n(1)xtrabackup_checkpoints -- 备份类型(如完全或增量)、备份状态(如是否已经为prepared状态)和LSN(日志序列号)范围信息：  每个InnoDB页(通常为16k大小) 都会包含一个日志序列号，即LSN，LSN是整个数据库系统的系统版本号，每个页面相关的LSN能够表明此页面最近是如何发生改变的。  (2)xtrabackup_binlog_info -- mysql服务器当前正在使用的二进制日志文件及备份这一刻位置二进制日志时间的位置。  (3)xtrabackup_binlog_pos_innodb -- 二进制日志文件及用于InnoDB或XtraDB表的二进制日志文件的当前position。  (4)xtrabackup_binary -- 备份中用到的xtrabackup的可执行文件；  (5)backup-my.cnf -- 备份命令用到的配置选项信息：  在使用innobackupex进行备份时，还可以使用--no-timestamp选项来阻止命令自动创建一个以时间命名的目录：如此一来，innobackupex命令将会创建一个BACKUP-DIR目录来存储备份数据。 如果要使用一个最小权限的用户进行备份，则可基于如下命令创建此类用户：如果要使用一个最小权限的用户进行备份，则可基于如下命令创建此类用户：\nmysql\u0026gt; CREATE USER \u0026#39;bkpuser\u0026#39;@\u0026#39;localhost\u0026#39; IDENTIFIED BY \u0026#39;123456\u0026#39;;　#创建用户 mysql\u0026gt; REVOKE ALL PRIVILEGES,GRANT OPTION FROM \u0026#39;bkpuser\u0026#39;;　#回收此用户所有权限 mysql\u0026gt; GRANT RELOAD,LOCK TABLES,REPLICATION CLIENT,PROCESS ON *.* TO \u0026#39;bkpuser\u0026#39;@\u0026#39;localhost\u0026#39;;　#授权刷新、锁定表、用户查看服务器状态 mysql\u0026gt; FLUSH PRIVILEGES;　#刷新授权表 *注意：备份时需启动MySQL,恢复时需关闭MySQL,清空mysql数据目录且不能重新初始化,恢复数据后应该立即进行一次完全备份*\n2、xtrabackup全量备份与恢复 备份： innobackupex --user=DBUSER --password=DBUSERPASS --defaults-file=/etc/my.cnf /path/to/BACKUP-DIR/  恢复： innobackupex --apply-log /backups/2018-07-30_11-04-55/ innobackupex --copy-back --defaults-file=/etc/my.cnf /backups/2018-07-30_11-04-55/ （1）准备(prepare)一个完全备份\n一般情况下，在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或者已经提交但尚未同步至数据文件中的事务。因此，此时数据文件仍处于不一致状态。\u0026ldquo;准备\u0026quot;的主要作用正是通过回滚未提交的事务及同步已经提交的事务至数据文件也使用得数据文件处于一致性状态。\ninnobackupex命令的\u0026ndash;apply-log选项可用于实现上述功能，如下面的命令：\n# innobackupex --apply-log /path/to/BACKUP-DIR 如果执行正确，其最后输出的几行信息通常如下：  120407 09:01:04 innobackupex: completed OK! 在实现\u0026quot;准备\u0026quot;的过程中，innobackupex通常还可以使用\u0026ndash;user-memory选项来指定其可以使用的内存的大小，默认为100M.如果有足够的内存空间可用，可以多划分一些内存给prepare的过程，以提高其完成备份的速度。\n（2）从一个完全备份中恢复数据\n注意：恢复不用启动MySQL\ninnobackupex命令的\u0026ndash;copy-back选项用于恢复操作，其通过复制所有数据相关的文件至mysql服务器DATADIR目录中来执行恢复过程。innobackupex通过backup-my.cnf来获取DATADIR目录的相关信息。\n# innobackupex --copy-back /path/to/BACKUP-DIR 当数据恢复至DATADIR目录以后，还需要确保所有的数据文件的属主和属组均为正确的用户，如mysql，否则，在启动mysqld之前还需要事先修改数据文件的属主和属组。如：\n# chown -R mysql.mysql /mydata/data/ （3）实战练习\n（1）全量备份 [root@master backups]# innobackupex --user=root --password=123456 --host=127.0.0.1 /backups/　#在master上进行全库备份#语法解释说明： #--user=root 指定备份用户 #--password=123456 指定备份用户密码 #--host　指定主机 #/backups　指定备份目录 [root@master backups]# ll total 0 drwxr-x--- 7 root root 232 Jul 30 11:01 2018-07-30_11-01-37 [root@master backups]# ll 2018-07-30_11-01-37/　#查看备份数据 total 77856 -rw-r----- 1 root root 418 Jul 30 11:01 backup-my.cnf　#备份用到的配置选项信息文件 -rw-r----- 1 root root 79691776 Jul 30 11:01 ibdata1　#数据文件 drwxr-x--- 2 root root 20 Jul 30 11:01 kim drwxr-x--- 2 root root 4096 Jul 30 11:01 mysql drwxr-x--- 2 root root 4096 Jul 30 11:01 performance_schema drwxr-x--- 2 root root 20 Jul 30 11:01 repppp drwxr-x--- 2 root root 4096 Jul 30 11:01 wordpress -rw-r----- 1 root root 21 Jul 30 11:01 xtrabackup_binlog_info　#mysql服务器当前正在使用的二进制日志文件和此时二进制日志时间的位置信息文件 -rw-r----- 1 root root 113 Jul 30 11:01 xtrabackup_checkpoints　#备份的类型、状态和LSN状态信息文件 -rw-r----- 1 root root 482 Jul 30 11:01 xtrabackup_info -rw-r----- 1 root root 2560 Jul 30 11:01 xtrabackup_logfile　#备份的日志文件  （2）恢复 [root@slave ~]# /etc/init.d/mysqld stop　#停止slave上的mysql Shutting down MySQL.. SUCCESS!  [root@slave tools]# yum install -y percona-xtrabackup-24-2.4.9-1.el7.x86_64.rpm #安装xtrabackup [root@master backups]# scp -r 2018-07-30_11-01-37/ root@192.168.56.12:/backups/　#从master上拷贝备份数据 [root@slave tools]# innobackupex --apply-log /backups/2018-07-30_11-01-37/　#合并数据，使数据文件处于一致性的状态 180729 23:18:23 innobackupex: Starting the apply-log operation  IMPORTANT: Please check that the apply-log run completes successfully.  At the end of a successful apply-log run innobackupex  prints \u0026#34;completed OK!\u0026#34;.  innobackupex version 2.4.9 based on MySQL server 5.7.13 Linux (x86_64) (revision id: a467167cdd4) xtrabackup: cd to /backups/2018-07-30_11-01-37/ xtrabackup: This target seems to be not prepared yet. InnoDB: Number of pools: 1 xtrabackup: xtrabackup_logfile detected: size=8388608, start_lsn=(3127097) ...... InnoDB: FTS optimize thread exiting. InnoDB: Starting shutdown... InnoDB: Shutdown completed; log sequence number 3129915 180729 23:18:30 completed OK! [root@slave ~]# rm -rf /usr/local/mysql/data/　#在slave上删除原有的数据 [root@slave ~]# vim /etc/my.cnf　#配置my.cnf的数据目录路径，否则会报错，要和master一致 datadir=/usr/local/mysql/data [root@slave ~]# innobackupex --copy-back /backups/2018-07-30_11-01-37/　#在slave上数据恢复 180729 23:32:03 innobackupex: Starting the copy-back operation  IMPORTANT: Please check that the copy-back run completes successfully.  At the end of a successful copy-back run innobackupex  prints \u0026#34;completed OK!\u0026#34;. ...... 180729 23:32:08 completed OK!　#看到completed OK就是恢复正常了 [root@slave ~]# ll /usr/local/mysql/data/　#slave上查看数据目录，可以看到数据已经恢复，但是属主会有问题，需要进行修改，所以一般使用mysql的运行用户进行恢复，否则需要进行修改属主和属组信息 total 188432 -rw-r----- 1 root root 79691776 Jul 29 23:32 ibdata1 -rw-r----- 1 root root 50331648 Jul 29 23:32 ib_logfile0 -rw-r----- 1 root root 50331648 Jul 29 23:32 ib_logfile1 -rw-r----- 1 root root 12582912 Jul 29 23:32 ibtmp1 drwxr-x--- 2 root root 20 Jul 29 23:32 kim drwxr-x--- 2 root root 4096 Jul 29 23:32 mysql drwxr-x--- 2 root root 4096 Jul 29 23:32 performance_schema drwxr-x--- 2 root root 20 Jul 29 23:32 repppp drwxr-x--- 2 root root 4096 Jul 29 23:32 wordpress -rw-r----- 1 root root 482 Jul 29 23:32 xtrabackup_info [root@slave ~]# chown -R mysql.mysql /usr/local/mysql/data/　#修改属主属组 [root@slave ~]# /etc/init.d/mysqld start　#启动mysql Starting MySQL. SUCCESS! [root@slave ~]# mysql -uroot -p -e \u0026#34;show databases;\u0026#34;　#查看数据，是否恢复 Enter password: +--------------------+ | Database | +--------------------+ | information_schema | | kim | | mysql | | performance_schema | | repppp | | wordpress | +--------------------+ 总结全库备份与恢复三步曲：\na.　innobackupex全量备份，并指定备份目录路径；\nb.　在恢复前，需要使用\u0026ndash;apply-log参数先进行合并数据文件，确保数据的一致性要求；\nc.　恢复时，直接使用\u0026ndash;copy-back参数进行恢复，需要注意的是，在my.cnf中要指定数据文件目录的路径。\n3、xtrabackup增量备份与恢复 　使用innobackupex进行增量备份，每个InnoDB的页面都会包含一个LSN信息，每当相关的数据发生改变，相关的页面的LSN就会自动增长。这正是InnoDB表可以进行增量备份的基础，即innobackupex通过备份上次完全备份之后发生改变的页面来实现。在进行增量备份时，首先要进行一次全量备份，第一次增量备份是基于全备的，之后的增量备份都是基于上一次的增量备份的，以此类推。\n要实现第一次增量备份，可以使用下面的命令进行：\n基于全量备份的增量备份与恢复 做一次增量备份（基于当前最新的全量备份） innobackupex --user=root --password=root --defaults-file=/etc/my.cnf --incremental /backups/ --incremental-basedir=/backups/2018-07-30_11-01-37 1. 准备基于全量 innobackupex --user=root --password=root --defaults-file=/etc/my.cnf --apply-log --redo-only /backups/2018-07-30_11-01-37 2. 准备基于增量 innobackupex --user=root --password=root --defaults-file=/etc/my.cnf --apply-log --redo-only /backups/2018-07-30_11-01-37 --incremental-dir=/backups/2018-07-30_13-51-47/ 3. 恢复 innobackupex --copy-back --defaults-file=/etc/my.cnf /opt/2017-01-05_11-04-55/ 解释： 1. 2018-07-30_11-01-37指的是完全备份所在的目录。 2. 2018-07-30_13-51-47指定是第一次基于2018-07-30_11-01-37增量备份的目录，其他类似以此类推，即如果有多次增量备份。每一次都要执行如上操作。 需要注意的是，增量备份仅能应用于InnoDB或XtraDB表，对于MyISAM表而言，执行增量备份时其实进行的是完全备份。\n\u0026ldquo;准备\u0026rdquo;(prepare)增量备份与整理完全备份有着一些不同，尤其要注意的是： ①需要在每个备份 (包括完全和各个增量备份)上，将已经提交的事务进行\u0026quot;重放\u0026rdquo;。\u0026ldquo;重放\u0026quot;之后，所有的备份数据将合并到完全备份上。 ②基于所有的备份将未提交的事务进行\u0026quot;回滚\u0026rdquo;\n（1）增量备份演示\n[root@master backups]# innobackupex --user=root --password=123456 --host=127.0.0.1 /backups/ #全备数据 [root@master ~]# mysql -uroot -p　#在master上创建student库并创建testtb表插入若干数据 Enter password: mysql\u0026gt; create database student; Query OK, 1 row affected (0.03 sec)  mysql\u0026gt; use student; Database changed mysql\u0026gt; create table testtb(id int); Query OK, 0 rows affected (0.07 sec)  mysql\u0026gt; insert into testtb values(1),(10),(99); Query OK, 3 rows affected (0.04 sec) Records: 3 Duplicates: 0 Warnings: 0  mysql\u0026gt; select * from testtb; +------+ | id | +------+ | 1 | | 10 | | 99 | +------+ 3 rows in set (0.00 sec)  mysql\u0026gt; quit; Bye  #使用innobackupex进行增量备份 [root@master backups]# innobackupex --user=root --password=123456 --host=127.0.0.1 --incremental /backups/ --incremental-basedir=/backups/2018-07-30_11-01-37/ ...... 180730 13:51:50 Executing UNLOCK TABLES 180730 13:51:50 All tables unlocked 180730 13:51:50 Backup created in directory \u0026#39;/backups/2018-07-30_13-51-47/\u0026#39; MySQL binlog position: filename \u0026#39;mysql-bin.000005\u0026#39;, position \u0026#39;664\u0026#39; 180730 13:51:50 [00] Writing /backups/2018-07-30_13-51-47/backup-my.cnf 180730 13:51:50 [00] ...done 180730 13:51:50 [00] Writing /backups/2018-07-30_13-51-47/xtrabackup_info 180730 13:51:50 [00] ...done xtrabackup: Transaction log of lsn (3158741) to (3158741) was copied. 180730 13:51:50 completed OK! [root@master backups]# ll　#查看备份数据 total 0 drwxr-x--- 7 root root 232 Jul 30 11:01 2018-07-30_11-01-37　#全量备份数据目录 drwxr-x--- 8 root root 273 Jul 30 13:51 2018-07-30_13-51-47　#增量备份数据目录 [root@master 2018-07-30_11-01-37]# cat xtrabackup_checkpoints #查看全量备份的xtrabackup_checkpoints backup_type = full-backuped　#备份类型为全量备份 from_lsn = 0　#lsn从0开始 to_lsn = 3127097　#lsn到3127097结束 last_lsn = 3127097 compact = 0 recover_binlog_info = 0  [root@master 2018-07-30_13-51-47]# cat xtrabackup_checkpoints #查看增量备份的xtrabackup_checkpoints backup_type = incremental　#备份类型为增量备份 from_lsn = 3127097　#lsn从3127097开始 to_lsn = 3158741　#lsn到啊3158741结束 last_lsn = 3158741　compact = 0 recover_binlog_info = 0 （2）增量备份后数据恢复演示\n（1）模拟mysql故障，删除数据目录所有数据 [root@master ~]# /etc/init.d/mysqld stop　#模拟mysql故障，停止mysql Shutting down MySQL.. SUCCESS! [root@master ~]# rm -rf /usr/local/mysql/data/*　#删除数据目录中的所有数据  （2）合并全备数据目录，确保数据的一致性 [root@master ~]# innobackupex --apply-log --redo-only /backups/2018-07-30_11-01-37/ 180730 14:05:27 innobackupex: Starting the apply-log operation  IMPORTANT: Please check that the apply-log run completes successfully.  At the end of a successful apply-log run innobackupex  prints \u0026#34;completed OK!\u0026#34;.  innobackupex version 2.4.9 based on MySQL server 5.7.13 Linux (x86_64) (revision id: a467167cdd4) xtrabackup: cd to /backups/2018-07-30_11-01-37/ ...... ...... xtrabackup: starting shutdown with innodb_fast_shutdown = 1 InnoDB: Starting shutdown... InnoDB: Shutdown completed; log sequence number 3127106 InnoDB: Number of pools: 1 180730 14:05:29 completed OK!  （3）将增量备份数据合并到全备数据目录当中 [root@master ~]# innobackupex --apply-log --redo-only /backups/2018-07-30_11-01-37/ --incremental-dir=/backups/2018-07-30_13-51-47/ 180730 14:06:42 innobackupex: Starting the apply-log operation  IMPORTANT: Please check that the apply-log run completes successfully.  At the end of a successful apply-log run innobackupex  prints \u0026#34;completed OK!\u0026#34;. ...... ...... 180730 14:06:44 [00] ...done 180730 14:06:44 completed OK! [root@master ~]# cat /backups/2018-07-30_11-01-37/xtrabackup_checkpoints  backup_type = log-applied　#查看到数据备份类型是增加 from_lsn = 0　#lsn从0开始 to_lsn = 3158741　#lsn结束号为最新的lsn last_lsn = 3158741 compact = 0 recover_binlog_info = 0  （4）恢复数据 [root@master ~]# innobackupex --copy-back /backups/2018-07-30_11-01-37/ 180730 14:07:51 innobackupex: Starting the copy-back operation  IMPORTANT: Please check that the copy-back run completes successfully.  At the end of a successful copy-back run innobackupex  prints \u0026#34;completed OK!\u0026#34;. ....... ....... 180730 14:08:17 [01] ...done 180730 14:08:17 completed OK! [root@master ~]# ll /usr/local/mysql/data/ total 77844 -rw-r----- 1 root root 79691776 Jul 30 14:08 ibdata1 drwxr-x--- 2 root root 20 Jul 30 14:08 kim drwxr-x--- 2 root root 4096 Jul 30 14:08 mysql drwxr-x--- 2 root root 4096 Jul 30 14:08 performance_schema drwxr-x--- 2 root root 20 Jul 30 14:08 repppp drwxr-x--- 2 root root 56 Jul 30 14:08 student drwxr-x--- 2 root root 4096 Jul 30 14:08 wordpress -rw-r----- 1 root root 21 Jul 30 14:08 xtrabackup_binlog_pos_innodb -rw-r----- 1 root root 554 Jul 30 14:08 xtrabackup_info [root@master ~]# chown -R mysql.mysql /usr/local/mysql/data　#更改数据的属主属组 [root@master ~]# /etc/init.d/mysqld start　#启动mysql Starting MySQL.Logging to \u0026#39;/usr/local/mysql/data/master.err\u0026#39;. .. SUCCESS! [root@master ~]# mysql -uroot -p -e \u0026#34;show databases;\u0026#34;　#查看数据是否恢复 Enter password: +--------------------+ | Database | +--------------------+ | information_schema | | kim | | mysql | | performance_schema | | repppp | | student | | wordpress | +--------------------+ 总结：\n（1）增量备份需要使用参数\u0026ndash;incremental指定需要备份到哪个目录，使用incremental-dir指定全备目录；\n（2）进行数据备份时，需要使用参数\u0026ndash;apply-log redo-only先合并全备数据目录数据，确保全备数据目录数据的一致性；\n（3）再将增量备份数据使用参数\u0026ndash;incremental-dir合并到全备数据当中；\n（4）最后通过最后的全备数据进行恢复数据，注意，如果有多个增量备份，需要逐一合并到全备数据当中，再进行恢复。\n#1. \u0026ndash;user=root 指定备份的用户\n#2. \u0026ndash;password=root指定备份用户的密码\n#3. \u0026ndash;defaults-file=/etc/my.cnf 指定的备份数据的配置文件\n#4. /opt/ 指定备份后的数据保存路径\n关于阿里云RDS物理备份数据使用xtrabackup工具恢复到本地mysql当中，请参考阿里云文档：https://help.aliyun.com/knowledge_detail/41817.html?spm=5176.11065259.1996646101.searchclickresult.53d420cclqekK3\n","permalink":"https://iblog.zone/archives/mysql%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8Dxtrabackup%E6%96%B9%E5%BC%8F/","summary":"一、Xtrabackup介绍 　MySQL冷备、mysqldump、MySQL热拷贝都无法实现对数据库进行增量备份。在实际生产环境中增量备份是非常实用的，如果数据大于50G或100G，存储空间足够的情况下，可以每天进行完整备份，如果每天产生的数据量较大，需要定制数据备份策略。例如每周实用完整备份，周一到周六实用增量备份。而Percona-Xtrabackup就是为了实现增量备份而出现的一款主流备份工具，xtrabakackup有2个工具，分别是xtrabakup、innobakupe。\n　Percona-xtrabackup是 Percona公司开发的一个用于MySQL数据库物理热备的备份工具，支持MySQL、Percona server和MariaDB，开源免费，是目前较为受欢迎的主流备份工具。xtrabackup只能备份innoDB和xtraDB两种数据引擎的表，而不能备份MyISAM数据表。\n二、Xtrabackup优点 （1）备份速度快，物理备份可靠\n（2）备份过程不会打断正在执行的事务（无需锁表）\n（3）能够基于压缩等功能节约磁盘空间和流量\n（4）自动备份校验\n（5）还原速度快\n（6）可以流传将备份传输到另外一台机器上\n（7）在不增加服务器负载的情况备份数据\n三、Xtrabackup备份原理 Xtrabackup备份流程图：\n（1）innobackupex启动后，会先fork一个进程，用于启动xtrabackup，然后等待xtrabackup备份ibd数据文件；\n（2）xtrabackup在备份innoDB数据是，有2种线程：redo拷贝线程和ibd数据拷贝线程。xtrabackup进程开始执行后，会启动一个redo拷贝的线程，用于从最新的checkpoint点开始顺序拷贝redo.log；再启动ibd数据拷贝线程，进行拷贝ibd数据。这里是先启动redo拷贝线程的。在此阶段，innobackupex进行处于等待状态（等待文件被创建）\n（4）xtrabackup拷贝完成ibd数据文件后，会通知innobackupex（通过创建文件），同时xtrabackup进入等待状态（redo线程依旧在拷贝redo.log）\n（5）innobackupex收到xtrabackup通知后哦，执行FLUSH TABLES WITH READ LOCK（FTWRL），取得一致性位点，然后开始备份非InnoDB文件（如frm、MYD、MYI、CSV、opt、par等格式的文件），在拷贝非InnoDB文件的过程当中，数据库处于全局只读状态。\n（6）当innobackup拷贝完所有的非InnoDB文件后，会通知xtrabackup，通知完成后，进入等待状态；\n（7）xtrabackup收到innobackupex备份完成的通知后，会停止redo拷贝线程，然后通知innobackupex，redo.log文件拷贝完成；\n（8）innobackupex收到redo.log备份完成后，就进行解锁操作，执行：UNLOCK TABLES；\n（9）最后innbackupex和xtrabackup进程各自释放资源，写备份元数据信息等，innobackupex等xtrabackup子进程结束后退出。\n四、xtrabackup的安装部署以及备份恢复实现 1、xtrabackup的安装 下载地址：https://www.percona.com/downloads/XtraBackup/LATEST/\n可以选择rpm包方式安装，也可以下载源码包编译安装，这里直接采用rpm包的方式进行安装\n[root@master tools]# wget https://www.percona.com/downloads/XtraBackup/Percona-XtraBackup-2.4.9/binary/redhat/7/x86_64/percona-xtrabackup-24-2.4.9-1.el7.x86_64.rpm [root@master tools]# yum install -y percona-xtrabackup-24-2.4.9-1.el7.x86_64.rpm  [root@master ~]# rpm -qa |grep xtrabackup percona-xtrabackup-24-2.4.9-1.el7.x86_64  Xtrabackup中主要包含两个工具： xtrabackup：是用于热备innodb，xtradb表中数据的工具，不能备份其他类型的表，也不能备份数据表结构； innobackupex：是将xtrabackup进行封装的perl脚本，提供了备份myisam表的能力。 常用选项:  --host 指定主机  --user 指定用户名  --password 指定密码  --port 指定端口  --databases 指定数据库  --incremental 创建增量备份  --incremental-basedir 指定包含完全备份的目录  --incremental-dir 指定包含增量备份的目录  --apply-log 对备份进行预处理操作  一般情况下，在备份完成后，数据尚且不能用于恢复操作，因为备份的数据中可能会包含尚未提交的事务或已经提交但尚未同步至数据文件中的事务。因此，此时数据文件仍处理不一致状态。“准备”的主要作用正是通过回滚未提交的事务及同步已经提交的事务至数据文件也使得数据文件处于一致性状态。  --redo-only 不回滚未提交事务  --copy-back 恢复备份目录 使用innobackupex备份时，其会调用xtrabackup备份所有的InnoDB表，复制所有关于表结构定义的相关文件(.","title":"Mysql备份和恢复（Xtrabackup方式）"},{"content":"一、备份单个数据库 1、备份命令：mysqldump\n　MySQL数据库自带的一个很好用的备份命令。是逻辑备份，导出的是SQL语句。也就是把数据从MySQL库中以逻辑的SQL语句的形式直接输出或生成备份的文件的过程。\n单实例语法（Syntax）: mysqldump -u \u0026lt;username\u0026gt; -p \u0026lt;dbname\u0026gt; \u0026gt; /path/to/***.sql  多实例的备份语法（Syntax）： mysqldump -u \u0026lt;username\u0026gt; -p \u0026lt;dbname\u0026gt; -S \u0026lt;sockPath\u0026gt; \u0026gt; /path/to/***.sql  eg: mysqldump -u root -p wordpress \u0026gt; /opt/wordpress_$(date +%F).sql 2、参数解析\n 1 -A --all-databases：导出全部数据库  2 -Y --all-tablespaces：导出全部表空间  3 -y --no-tablespaces：不导出任何表空间信息  4 --add-drop-database每个数据库创建之前添加drop数据库语句。  5 --add-drop-table每个数据表创建之前添加drop数据表语句。(默认为打开状态，使用--skip-add-drop-table取消选项)  6 --add-locks在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE。(默认为打开状态，使用--skip-add-locks取消选项)  7 --comments附加注释信息。默认为打开，可以用--skip-comments取消  8 --compact导出更少的输出信息(用于调试)。去掉注释和头尾等结构。可以使用选项：--skip-add-drop-table --skip-add-locks --skip-comments --skip-disable-keys  9 -c --complete-insert：使用完整的insert语句(包含列名称)。这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败。 10 -C --compress：在客户端和服务器之间启用压缩传递所有信息 11 -B--databases：导出几个数据库。参数后面所有名字参量都被看作数据库名。 12 --debug输出debug信息，用于调试。默认值为：d:t:o,/tmp/ 13 --debug-info输出调试信息并退出 14 --default-character-set设置默认字符集，默认值为utf8 15 --delayed-insert采用延时插入方式（INSERT DELAYED）导出数据 16 -E--events：导出事件。 17 --master-data：在备份文件中写入备份时的binlog文件，在恢复进，增量数据从这个文件之后的日志开始恢复。值为1时，binlog文件名和位置没有注释，为2时，则在备份文件中将binlog的文件名和位置进行注释 18 --flush-logs开始导出之前刷新日志。请注意：假如一次导出多个数据库(使用选项--databases或者--all-databases)，将会逐个数据库刷新日志。除使用--lock-all-tables或者--master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。因此，如果打算同时导出和刷新日志应该使用--lock-all-tables 或者--master-data 和--flush-logs。 19 --flush-privileges在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句。为了正确恢复，该选项应该用于导出mysql数据库和依赖mysql数据库数据的任何时候。 20 --force在导出过程中忽略出现的SQL错误。 21 -h --host：需要导出的主机信息 22 --ignore-table不导出指定表。指定忽略多个表时，需要重复多次，每次一个表。每个表必须同时指定数据库和表名。例如：--ignore-table=database.table1 --ignore-table=database.table2 …… 23 -x --lock-all-tables：提交请求锁定所有数据库中的所有表，以保证数据的一致性。这是一个全局读锁，并且自动关闭--single-transaction 和--lock-tables 选项。 24 -l --lock-tables：开始导出前，锁定所有表。用READ LOCAL锁定表以允许MyISAM表并行插入。对于支持事务的表例如InnoDB和BDB，--single-transaction是一个更好的选择，因为它根本不需要锁定表。请注意当导出多个数据库时，--lock-tables分别为每个数据库锁定表。因此，该选项不能保证导出文件中的表在数据库之间的逻辑一致性。不同数据库表的导出状态可以完全不同。 25 --single-transaction：适合innodb事务数据库的备份。保证备份的一致性，原理是设定本次会话的隔离级别为Repeatable read，来保证本次会话（也就是dump）时，不会看到其它会话已经提交了的数据。 26 -F：刷新binlog，如果binlog打开了，-F参数会在备份时自动刷新binlog进行切换。 27 -n --no-create-db：只导出数据，而不添加CREATE DATABASE 语句。 28 -t --no-create-info：只导出数据，而不添加CREATE TABLE 语句。 29 -d --no-data：不导出任何数据，只导出数据库表结构。 30 -p --password：连接数据库密码 31 -P --port：连接数据库端口号 32 -u --user：指定连接的用户名。 举例使用：\na、导出整个数据库(包括数据库中的数据） mysqldump -u username -p dbname \u0026gt; dbname.sql b、导出数据库结构（不含数据） mysqldump -u username -p -d dbname \u0026gt; dbname.sql c、导出数据库中的某张数据表（包含数据） mysqldump -u username -p dbname tablename \u0026gt; tablename.sql d、导出数据库中的某张数据表的表结构（不含数据） mysqldump -u username -p -d dbname tablename \u0026gt; tablename.sql 3、恢复操作\n语法（Syntax）： mysql -u\u0026lt;username\u0026gt; -p\u0026lt;password\u0026gt; \u0026lt;dbname\u0026gt; \u0026lt; /opt/mytest_bak.sql #库必须保留，空库也可 说明：指定dbname，相当于use \u0026lt;dbname\u0026gt; 4、示例\n（1）无参数备份数据库mytest和恢复\n（1）备份操作 a、备份 mysqldump -uroot -p‘123456’ mytest \u0026gt; /mnt/mytest_bak_$(date +%F).sql  （2）恢复操作 a、删除student表（库必须要保留，空库都行） mysql -uroot -p\u0026#39;123456\u0026#39; -e \u0026#34;use mytest;drop table student;\u0026#34; b、恢复数据 mysql -uroot -p\u0026#39;123456\u0026#39; mytest \u0026lt; /mnt/mytest_bak.sql c、查看数据 mysql -uroot -p\u0026#39;123456\u0026#39; -e \u0026#34;select * from mytest.student;\u0026#34; （2）-B参数备份和恢复（建议使用）\n（1）备份操作 a、备份 mysqldump -uroot -p\u0026#39;123456\u0026#39; -B mytest \u0026gt; /mnt/mytest_bak_B.sql  说明：加了-B参数后，备份文件中多的Create database和use mytest的命令 加-B参数的好处： 加上-B参数后，导出的数据文件中已存在创建库和使用库的语句，不需要手动在原库是创建库的操作，在恢复过程中不需要手动建库，可以直接还原恢复。  （2）恢复操作 a、删除mytest库 mysql -uroot -p\u0026#39;123456\u0026#39; -e \u0026#34;drop database mytest;\u0026#34; b、恢复数据 （1）使用不带参数的导出文件导入（导入时不指定要恢复的数据库），报错 mysql -uroot - p\u0026#39;123456\u0026#39; \u0026lt; /mnt/mytest_bak.sql ERROR 1046 (3D000) at line 22: No database selected （2）使用带-B参数的导出文件导入（导入时也不指定要恢复的数据库），成功 mysql -uroot -p\u0026#39;123456\u0026#39; \u0026lt; /mnt/mytest_bak_B.sql c、查看数据 mysql -uroot -p\u0026#39;123456\u0026#39; -e \u0026#34;select * from mytest.student;\u0026#34; （3）\u0026ndash;compact参数优化备份文小大小，减少输出注释（一般用于Debug调试）\n（1）备份 mysqldump -uroot -p\u0026#39;123456\u0026#39; --compact -B mytest \u0026gt; /mnt/mytest_bak_Compact.sql 说明： 使用--compact参数，可以优化输出内容的大小，让容量更少，适合调试。便会忽略--skip-add-drop-table，--no-set-names，--skip-disable-keys，--skip-add-locks等几个参数的功能。 （4）指定压缩命令来压缩备份文件\n（1）备份 mysqldump -uroot -p\u0026#39;123456\u0026#39; -B mytest | gzip \u0026gt; /mnt/mytest_bak_.sql.gz 说明： mysqldump导出的文件是文本文件，压缩效率很高 （5）备份多个数据库\n（1）说明 通过-B参数指定相关数据库，每个数据库名之前用空格分格。当使用-B参数后，将所有数据库全部列全，则此时等同于-A参数。 （2）备份 mysqldump -uroot -p\u0026#39;123456\u0026#39; -B mytest wiki | gzip \u0026gt; /mnt/mytestAndWiki_bak.sql.gz （6）分库备份\n　分库备份实际上就是执行一个备份语句就备份一个库，有多个库时，就执行多条相同的备份语句，只是备份的库名和备份文件名不同而已。可能通过shell脚本自动生成并执行相应的操作，也可以把所有单个备份语句写在一个shell脚本中，通过cron定时任务来备份。\n分库备份的意义是在所有库都备份成一个备份文件时，恢复其中一个库的数据是比较麻烦的，所以分库备份，利于恢复。分库备份脚本如下：\nfor dbname in ` mysql -uroot -p\u0026#39;123456\u0026#39; -e \u0026#34;show databases;\u0026#34; | grep -Evi \u0026#34;database|infor|perfor\u0026#34;` do  mysqldump -uroot -p\u0026#34;123456\u0026#34; --events -B $dbname | gzip \u0026gt; /mnt/${dbname}_bak.sql.gz done 说明：${dbname}_bak**，由于要求备份文件名以$dbname_bak.sql.gz格式命令，但系统无法辨别变量是$dbname还是$dbname_bak****，所以此时就需要用大括号“{}”将变量括起来，就是${dbname}_bak.sql.gz了。**\n（7）-d参数，只备份数据库中表结构\nmysqldump -uroot -p\u0026#39;123456\u0026#39; -d mytest \u0026gt; /mnt/mytestDesc_bak.sql （8）-A参数备份全库，并且-F刷新和切换binlog\nmysqldump -uroot -p\u0026#39;123456\u0026#39; -A -B -F \u0026gt; /mnt/All_bak.sql （9）\u0026ndash;master-data参数在备份文件中写入当前binlog文件号\nmysqldump -uroot -p\u0026#39;123456\u0026#39; --master-data=1 --compact mytest \u0026gt; /mnt/All_bak.sql  mysqldump -uroot -p\u0026#39;123456\u0026#39; --master-data=2 --compact mytest \u0026gt; /mnt/All_bak.sql 二、备份单个表 语法（Syntax）：不能加-B参数 mysqldump -u\u0026lt;username\u0026gt; -p\u0026lt;password\u0026gt; dbname tablename1 tablename2... \u0026gt; /path/to/***.sql 示例：\n示例1：备份mytest库中的student表 mysqldump -uroot -p\u0026#39;123456\u0026#39; mytest student \u0026gt; /mnt/table_bak/student_bak.sql  示例2：备份mytest库中所有表，就是备份mytest库 mysqldump -uroot -p\u0026#39;123456\u0026#39; mytest \u0026gt; /mnt/table_bak/all_bak.sql  示例3：备份mytest库中的student和test表 mysqldump -uroot -p\u0026#39;123456\u0026#39; mytest student test \u0026gt; /mnt/table_bak/two_bak.sql  示例4：-d参数，只备份表结构 mysqldump -uroot -p\u0026#39;123456\u0026#39; -d mytest stusent \u0026gt; /mnt/studentDesc_bak.sql  示例5：-t参数，只备份数据 mysqldump -uroot -p\u0026#39;123456\u0026#39; --compact -t mytest stusent \u0026gt; /mnt/studentData_bak.sql INSERT INTO `student` VALUES (1,\u0026#39;Tom\u0026#39;,20,\u0026#39;S11\u0026#39;),(2,\u0026#39;Jary\u0026#39;,21,\u0026#39;S12\u0026#39;),(3,\u0026#39;King\u0026#39;,25,\u0026#39;S10\u0026#39;),(4,\u0026#39;Smith\u0026#39;,19,\u0026#39;S11\u0026#39;),(5,\u0026#39;??\u0026#39;,20,\u0026#39;S11\u0026#39;),(6,\u0026#39;张三\u0026#39;,20,\u0026#39;S11\u0026#39;); 三、企业生产场景不同引擎备份命令参数 1、mysqldump的关键参数\n-B：指定多个库，在备份文件中增加建库语句和use语句 --compact：去掉备份文件中的注释，适合调试，生产场景不用 -A：备份所有库 -F：刷新binlog日志 --master-data：在备份文件中增加binlog日志文件名及对应的位置点 -x --lock-all-tables：锁表 -l：只读锁表 -d：只备份表结构 -t：只备份数据 --single-transaction：适合innodb事务数据库的备份  InnoDB表在备份时，通常启用选项--single-transaction来保证备份的一致性，原理是设定本次会话的隔离级别为Repeatable read，来保证本次会话（也就是dump）时，不会看到其它会话已经提交了的数据。 2、不同引擎备份命令参数用法\n1）Myisam引擎： mysqldump -uroot -p123456 -A -B --master-data=1 -x| gzip \u0026gt; /data/all_$(date +%F).sql.gz  （2）InnoDB引擎： mysqldump -uroot -p123456 -A -B --master-data=1 --single-transaction \u0026gt; /data/bak.sql  （3）生产环境DBA给出的命令 a、for MyISAM mysqldump --user=root --all-databases --flush-privileges --lock-all-tables \\ --master-data=1 --flush-logs --triggers --routines --events \\ --hex-blob \u0026gt; $BACKUP_DIR/full_dump_$BACKUP_TIMESTAMP.sql  b、for InnoDB mysqldump --user=root --all-databases --flush-privileges --single-transaction \\ --master-data=1 --flush-logs --triggers --routines --events \\ --hex-blob \u0026gt; $BACKUP_DIR/full_dump_$BACKUP_TIMESTAMP.sql 常见错误\n但是意外发生了出现\u0026quot;Warning: Using a password on the command line interface can be insecure.\n第一种方法、修改数据库配置文件\n1、我们需要修改数据库配置文件，这个要看我们数据库的配置的，有些是在/etc/my.cnf，有些是/etc/my.conf\n我们需要在[client]部分添加脚本（可以自己创建个文件，导入时指定）：\n host=localhost user=数据库用户 password=\u0026lsquo;数据库密码\u0026rsquo;\n 2、采用命令导出和导入数据库\n#导出数据库\n mysqldump \u0026ndash;defaults-extra-file=/etc/my.cnf database \u0026gt; database.sql\n #导入数据库\n mysql \u0026ndash;defaults-extra-file=/etc/my.cnf database \u0026lt; database.sql\n ","permalink":"https://iblog.zone/archives/mysql%E5%A4%87%E4%BB%BD%E5%92%8C%E6%81%A2%E5%A4%8Dmysqldump%E6%96%B9%E5%BC%8F/","summary":"一、备份单个数据库 1、备份命令：mysqldump\n　MySQL数据库自带的一个很好用的备份命令。是逻辑备份，导出的是SQL语句。也就是把数据从MySQL库中以逻辑的SQL语句的形式直接输出或生成备份的文件的过程。\n单实例语法（Syntax）: mysqldump -u \u0026lt;username\u0026gt; -p \u0026lt;dbname\u0026gt; \u0026gt; /path/to/***.sql  多实例的备份语法（Syntax）： mysqldump -u \u0026lt;username\u0026gt; -p \u0026lt;dbname\u0026gt; -S \u0026lt;sockPath\u0026gt; \u0026gt; /path/to/***.sql  eg: mysqldump -u root -p wordpress \u0026gt; /opt/wordpress_$(date +%F).sql 2、参数解析\n 1 -A --all-databases：导出全部数据库  2 -Y --all-tablespaces：导出全部表空间  3 -y --no-tablespaces：不导出任何表空间信息  4 --add-drop-database每个数据库创建之前添加drop数据库语句。  5 --add-drop-table每个数据表创建之前添加drop数据表语句。(默认为打开状态，使用--skip-add-drop-table取消选项)  6 --add-locks在每个表导出之前增加LOCK TABLES并且之后UNLOCK TABLE。(默认为打开状态，使用--skip-add-locks取消选项)  7 --comments附加注释信息。默认为打开，可以用--skip-comments取消  8 --compact导出更少的输出信息(用于调试)。去掉注释和头尾等结构。可以使用选项：--skip-add-drop-table --skip-add-locks --skip-comments --skip-disable-keys  9 -c --complete-insert：使用完整的insert语句(包含列名称)。这么做能提高插入效率，但是可能会受到max_allowed_packet参数的影响而导致插入失败。 10 -C --compress：在客户端和服务器之间启用压缩传递所有信息 11 -B--databases：导出几个数据库。参数后面所有名字参量都被看作数据库名。 12 --debug输出debug信息，用于调试。默认值为：d:t:o,/tmp/ 13 --debug-info输出调试信息并退出 14 --default-character-set设置默认字符集，默认值为utf8 15 --delayed-insert采用延时插入方式（INSERT DELAYED）导出数据 16 -E--events：导出事件。 17 --master-data：在备份文件中写入备份时的binlog文件，在恢复进，增量数据从这个文件之后的日志开始恢复。值为1时，binlog文件名和位置没有注释，为2时，则在备份文件中将binlog的文件名和位置进行注释 18 --flush-logs开始导出之前刷新日志。请注意：假如一次导出多个数据库(使用选项--databases或者--all-databases)，将会逐个数据库刷新日志。除使用--lock-all-tables或者--master-data外。在这种情况下，日志将会被刷新一次，相应的所以表同时被锁定。因此，如果打算同时导出和刷新日志应该使用--lock-all-tables 或者--master-data 和--flush-logs。 19 --flush-privileges在导出mysql数据库之后，发出一条FLUSH PRIVILEGES 语句。为了正确恢复，该选项应该用于导出mysql数据库和依赖mysql数据库数据的任何时候。 20 --force在导出过程中忽略出现的SQL错误。 21 -h --host：需要导出的主机信息 22 --ignore-table不导出指定表。指定忽略多个表时，需要重复多次，每次一个表。每个表必须同时指定数据库和表名。例如：--ignore-table=database.","title":"Mysql备份和恢复（mysqldump方式）"},{"content":"Mysql 5.6 -- 创建用户 CREATE USER \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;qweasd11\u0026#39;;  -- 授权用户访问的数据库以及权限 grant all privileges on test.* to \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39;; -- test为访问数据库 -- all privileges 表示可以对数据进行任意的操作, -- all privileges 可以替换为 select,delete,update,create,drop  -- 修改用户的密码 update mysql.user set password=password(\u0026#39;qweasd11\u0026#39;) where user=\u0026#39;joker\u0026#39;;  -- 修改密码必须刷新才会起作用 flush privileges; Mysql 5.7 -- 创建用户 CREATE USER \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;qweasd11\u0026#39;;  -- 授权用户访问的数据库以及权限 grant all privileges on test.* to \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39;; -- test为访问数据库 -- all privileges 表示可以对数据进行任意的操作, -- all privileges 可以替换为 select,delete,update,create,drop  -- 修改用户的密码 update mysql.user set authentication_string=password(\u0026#39;qweasd11\u0026#39;) where user=\u0026#39;joker\u0026#39;;  -- 修改密码必须刷新才会起作用 flush privileges;  -- 删除用户权限 revoke all privileges on test.* from \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39;; Mysql 8 -- 创建用户 create user \u0026#39;joker11\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;Password=qweasd11\u0026#39;;  -- 授权用户访问的数据库以及权限 grant all privileges on test.* to \u0026#39;joker11\u0026#39;@\u0026#39;localhost\u0026#39; with grant option; -- test为访问数据库 可以使用*表示所有的数据库 -- all privileges 表示可以对数据进行任意的操作, -- all privileges 可以替换为 select,delete,update,create,drop  -- 取消授权 revoke all privileges on test.* from \u0026#39;joker11\u0026#39;@\u0026#39;localhost\u0026#39;;  -- 查看用户授权信息 show grants for \u0026#39;joker11\u0026#39;@\u0026#39;localhost\u0026#39;;  -- 修改用户的密码 Alter user \u0026#39;joker11\u0026#39;@\u0026#39;localhost\u0026#39; identified by \u0026#39;新密码\u0026#39;;  -- 修改密码必须刷新才会起作用 flush privileges;  -- 删除用户 drop user \u0026#39;joker11\u0026#39;@\u0026#39;localhost\u0026#39;; ","permalink":"https://iblog.zone/archives/mysql%E7%94%A8%E6%88%B7%E6%93%8D%E4%BD%9C/","summary":"Mysql 5.6 -- 创建用户 CREATE USER \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;qweasd11\u0026#39;;  -- 授权用户访问的数据库以及权限 grant all privileges on test.* to \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39;; -- test为访问数据库 -- all privileges 表示可以对数据进行任意的操作, -- all privileges 可以替换为 select,delete,update,create,drop  -- 修改用户的密码 update mysql.user set password=password(\u0026#39;qweasd11\u0026#39;) where user=\u0026#39;joker\u0026#39;;  -- 修改密码必须刷新才会起作用 flush privileges; Mysql 5.7 -- 创建用户 CREATE USER \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39; IDENTIFIED BY \u0026#39;qweasd11\u0026#39;;  -- 授权用户访问的数据库以及权限 grant all privileges on test.* to \u0026#39;joker\u0026#39;@\u0026#39;%\u0026#39;; -- test为访问数据库 -- all privileges 表示可以对数据进行任意的操作, -- all privileges 可以替换为 select,delete,update,create,drop  -- 修改用户的密码 update mysql.","title":"Mysql用户操作"},{"content":"今天正式把之前的博客文章迁移至hugo了，终于不用再忍受hexo缓慢的生成速度了，以下记录一下迁移过程\n一、hugo安装 brew install hugo 二、创建网站 hugo new site iblog 三、安装主题 git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 cd themes/PaperMod git pull 四、配置config.yml（不使用config.toml） baseURL: \u0026#34;https://iblog.zone\u0026#34; title: ylw\u0026#39;s blog paginate: 10 theme: PaperMod defaultContentLanguage: zh  permalinks:  posts: /archives/:slug/  enableInlineShortcodes: true enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false enableEmoji: true  # googleAnalytics: UA-123-45  minify:  disableXML: true  minifyOutput: false  params:  env: production # to enable google analytics, opengraph, twitter-cards and schema.  #title: iblog.zone  description: \u0026#34;本站主要用来收集整理资料、记录笔记，方便自己查询使用\u0026#34;  author:  # author: [\u0026#34;Me\u0026#34;, \u0026#34;You\u0026#34;] # multiple authors  #images: [\u0026#34;\u0026lt;link or path of image for opengraph, twitter-cards\u0026gt;\u0026#34;]  DateFormat: \u0026#34;2006-01-02\u0026#34;  defaultTheme: auto # dark, light  disableThemeToggle: false   ShowReadingTime: true  ShowShareButtons: false  disableSpecial1stPost: true  displayFullLangName: true  ShowPostNavLinks: true  ShowBreadCrumbs: true  ShowCodeCopyButtons: true  ShowToc: true  comments: true  mainSections:  - posts    assets:  # disableHLJS: true # to disable highlight.js  # disableFingerprinting: true  favicon: \u0026#34;/images/favicon.ico\u0026#34;  favicon16x16: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34;  favicon32x32: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34;  apple_touch_icon: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34;  safari_pinned_tab: \u0026#34;\u0026lt;link / abs url\u0026gt;\u0026#34;   #label:  # text: \u0026#34;iblog.zone\u0026#34;  # icon:   # iconHeight: 35   # profile-mode  #profileMode:  # enabled: false # needs to be explicitly set  # title: ExampleSite  # subtitle: \u0026#34;This is subtitle\u0026#34;  # imageUrl: \u0026#34;\u0026lt;img location\u0026gt;\u0026#34;  # imageWidth: 120  # imageHeight: 120  # imageTitle: my image  # buttons:  # - name: Posts  # url: posts  # - name: Tags  # url: tags   # home-info mode  #homeInfoParams:  # Title: \u0026#34;Hi there \\U0001F44B\u0026#34;  # Content: Welcome to my blog   cover:  hidden: true # hide everywhere but not in structured data  hiddenInList: true # hide on list pages and home  hiddenInSingle: true # hide on single page   # for search  fuseOpts:  isCaseSensitive: false  shouldSort: true  location: 0  distance: 1000  threshold: 0.4  minMatchCharLength: 0  keys: [\u0026#34;title\u0026#34;, \u0026#34;permalink\u0026#34;, \u0026#34;summary\u0026#34;, \u0026#34;content\u0026#34;] menu:  main:  - identifier: home  name: Home  url: /  weight: 5  - identifier: categories  name: Categories  url: /categories/  weight: 15  - identifier: archives  name: Archives  url: /archives/  weight: 10  - identifier: tags  name: Tags  url: /tags/  weight: 20  - identifier: search  name: Search  url: /search/  weight: 30 # for search outputs:  home:  - HTML  - RSS  - JSON # is necessary  taxonomies:  category: categories  tag: tags  series: series  markup:  goldmark:  renderer:  unsafe: true 五、迁移hexo博客内容 主要修改categories和tags的格式和图片映射路径\n由{% asset_img xxxx.png%} 到 ![](/images/文章同名目录/xxx.png)\n六、生成预览 hugo server 七、部署 # 生成public静态文件 hugo # 将hexo的.git目录拷贝到public目录 cp -r hexo/.git hugo/public/ # 提交文章 验证 git add . git commit -m \u0026#34;xx\u0026#34; git push ","permalink":"https://iblog.zone/archives/hexo%E8%BF%81%E7%A7%BB%E8%87%B3hugo/","summary":"今天正式把之前的博客文章迁移至hugo了，终于不用再忍受hexo缓慢的生成速度了，以下记录一下迁移过程\n一、hugo安装 brew install hugo 二、创建网站 hugo new site iblog 三、安装主题 git clone https://github.com/adityatelange/hugo-PaperMod themes/PaperMod --depth=1 cd themes/PaperMod git pull 四、配置config.yml（不使用config.toml） baseURL: \u0026#34;https://iblog.zone\u0026#34; title: ylw\u0026#39;s blog paginate: 10 theme: PaperMod defaultContentLanguage: zh  permalinks:  posts: /archives/:slug/  enableInlineShortcodes: true enableRobotsTXT: true buildDrafts: false buildFuture: false buildExpired: false enableEmoji: true  # googleAnalytics: UA-123-45  minify:  disableXML: true  minifyOutput: false  params:  env: production # to enable google analytics, opengraph, twitter-cards and schema.","title":"Hexo迁移至hugo"},{"content":"#项目异常信息 org.apache.http.TruncatedChunkException: Truncated chunk ( expected size: 7752; actual size: 4077)  at org.apache.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:186)  at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)  at \u0026lt;mypackage\u0026gt;.\u0026lt;MyServlet\u0026gt;.service(\u0026lt;MyServlet\u0026gt;.java:XXX)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  at org.jboss.resteasy.plugins.server.servlet.FilterDispatcher.doFilter(FilterDispatcher.java:63)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)  at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)  at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)  at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)  at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)  at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859)  at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:602)  at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)  at java.lang.Thread.run(Thread.java:724) 当系统报以上错误时，接口表现出来的是数据传输不完整， 比如说接口返回 json ， 那么接收到的数据可能会少一截，json 数据说不定会少个 ｝ ，此时json就无法反序列化了。说白了就是丢包了。\n通过大半天的查找，终于定位到是nginx的问题，因为nginx处理chunked传输有问题。 所以最简单的处理方式是把nginx的缓存关闭。\nproxy_buffering off; ","permalink":"https://iblog.zone/archives/nignx%E5%AF%BC%E8%87%B4java%E7%A8%8B%E5%BA%8Ftruncatedchunkexception%E8%A7%A3%E5%86%B3%E5%8A%9E%E6%B3%95/","summary":"#项目异常信息 org.apache.http.TruncatedChunkException: Truncated chunk ( expected size: 7752; actual size: 4077)  at org.apache.http.impl.io.ChunkedInputStream.read(ChunkedInputStream.java:186)  at org.apache.http.conn.EofSensorInputStream.read(EofSensorInputStream.java:138)  at \u0026lt;mypackage\u0026gt;.\u0026lt;MyServlet\u0026gt;.service(\u0026lt;MyServlet\u0026gt;.java:XXX)  at javax.servlet.http.HttpServlet.service(HttpServlet.java:717)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:290)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  at org.jboss.resteasy.plugins.server.servlet.FilterDispatcher.doFilter(FilterDispatcher.java:63)  at org.apache.catalina.core.ApplicationFilterChain.internalDoFilter(ApplicationFilterChain.java:235)  at org.apache.catalina.core.ApplicationFilterChain.doFilter(ApplicationFilterChain.java:206)  at org.apache.catalina.core.StandardWrapperValve.invoke(StandardWrapperValve.java:233)  at org.apache.catalina.core.StandardContextValve.invoke(StandardContextValve.java:191)  at org.apache.catalina.core.StandardHostValve.invoke(StandardHostValve.java:127)  at org.apache.catalina.valves.ErrorReportValve.invoke(ErrorReportValve.java:102)  at org.apache.catalina.core.StandardEngineValve.invoke(StandardEngineValve.java:109)  at org.apache.catalina.connector.CoyoteAdapter.service(CoyoteAdapter.java:293)  at org.apache.coyote.http11.Http11Processor.process(Http11Processor.java:859)  at org.apache.coyote.http11.Http11Protocol$Http11ConnectionHandler.process(Http11Protocol.java:602)  at org.apache.tomcat.util.net.JIoEndpoint$Worker.run(JIoEndpoint.java:489)  at java.lang.Thread.run(Thread.java:724) 当系统报以上错误时，接口表现出来的是数据传输不完整， 比如说接口返回 json ， 那么接收到的数据可能会少一截，json 数据说不定会少个 ｝ ，此时json就无法反序列化了。说白了就是丢包了。","title":"Nignx导致java程序TruncatedChunkException解决办法"},{"content":"说明：\n通过MySQL的 information_schema 数据库，可查询数据库中每个表占用的空间、表记录的行数；该库中有一个 TABLES 表，这个表主要字段分别是：\n TABLE_SCHEMA : 数据库名 TABLE_NAME：表名 ENGINE：所使用的存储引擎 TABLES_ROWS：记录数 DATA_LENGTH：数据大小 INDEX_LENGTH：索引大小  其他字段请参考MySQL的手册，查看一个表占用空间的大小，那就相当于是 数据大小 + 索引大小 。\n查看所有库的大小\nmysql\u0026gt; use information_schema; Database changed mysql\u0026gt; select concat(round(sum(DATA_LENGTH/1024/1024),2),\u0026#39;MB\u0026#39;) as data from TABLES; +----------+ | data | +----------+ | 104.21MB | +----------+ 1 row in set (0.11 sec) 查看指定库的大小\nmysql\u0026gt; select concat(round(sum(DATA_LENGTH/1024/1024),2),\u0026#39;MB\u0026#39;) as data from TABLES where table_schema=\u0026#39;jishi\u0026#39;; +---------+ | data | +---------+ | 26.17MB | +---------+ 1 row in set (0.01 sec) 查看指定库的指定表的大小\nmysql\u0026gt; select concat(round(sum(DATA_LENGTH/1024/1024),2),\u0026#39;MB\u0026#39;) as data from TABLES where table_schema=\u0026#39;jishi\u0026#39; and table_name=\u0026#39;a_ya\u0026#39;; +--------+ | data | +--------+ | 0.02MB | +--------+ 1 row in set (0.00 sec) 查看指定库的索引大小\nmysql\u0026gt; SELECT CONCAT(ROUND(SUM(index_length)/(1024*1024), 2), \u0026#39; MB\u0026#39;) AS \u0026#39;Total Index Size\u0026#39; FROM TABLES WHERE table_schema = \u0026#39;jishi\u0026#39;; +------------------+ | Total Index Size | +------------------+ | 0.94 MB | +------------------+ 1 row in set (0.01 sec) 查看指定库的指定表的索引大小\nmysql\u0026gt; SELECT CONCAT(ROUND(SUM(index_length)/(1024*1024), 2), \u0026#39; MB\u0026#39;) AS \u0026#39;Total Index Size\u0026#39; FROM TABLES WHERE table_schema = \u0026#39;test\u0026#39; and table_name=\u0026#39;a_yuser\u0026#39;; +------------------+ | Total Index Size | +------------------+ | 21.84 MB | +------------------+ 1 row in set (0.00 sec) mysql\u0026gt; show create table test.a_yuser\\G; *************************** 1. row ***************************  Table: a_yuser Create Table: CREATE TABLE `a_yuser` (  `email` varchar(60) NOT NULL DEFAULT \u0026#39;\u0026#39;,  `user_name` varchar(60) NOT NULL DEFAULT \u0026#39;\u0026#39;,  KEY `cc` (`email`(5)),  KEY `ccb` (`user_name`(5)),  KEY `ccbc` (`email`(5),`user_name`(5)) ) ENGINE=MyISAM DEFAULT CHARSET=utf8 1 row in set (0.00 sec)  ERROR: No query specified  mysql\u0026gt; select count(*) from test.a_yuser; +----------+ | count(*) | +----------+ | 1073607 | +----------+ 1 row in set (0.00 sec) 查看一个库中的情况\nmysql\u0026gt; SELECT CONCAT(table_schema,\u0026#39;.\u0026#39;,table_name) AS \u0026#39;Table Name\u0026#39;, CONCAT(ROUND(table_rows/1000000,4),\u0026#39;M\u0026#39;) AS \u0026#39;Number of Rows\u0026#39;, CONCAT(ROUND(data_length/(1024*1024*1024),4),\u0026#39;G\u0026#39;) AS \u0026#39;Data Size\u0026#39;, CONCAT(ROUND(index_length/(1024*1024*1024),4),\u0026#39;G\u0026#39;) AS \u0026#39;Index Size\u0026#39;, CONCAT(ROUND((data_length+index_length)/(1024*1024*1024),4),\u0026#39;G\u0026#39;) AS\u0026#39;Total\u0026#39;FROM information_schema.TABLES WHERE table_schema LIKE \u0026#39;test\u0026#39;; +---------------+----------------+-----------+------------+---------+ | Table Name | Number of Rows | Data Size | Index Size | Total | +---------------+----------------+-----------+------------+---------+ | test.a_br | 0.4625M | 0.0259G | 0.0171G | 0.0431G | | test.a_skuclr | 0.7099M | 0.0660G | 0.0259G | 0.0919G | | test.a_yuser | 1.0736M | 0.0497G | 0.0213G | 0.0710G | | test.test | 0.0000M | 0.0000G | 0.0000G | 0.0000G | +---------------+----------------+-----------+------------+---------+ 4 rows in set (0.13 sec) ","permalink":"https://iblog.zone/archives/%E6%9F%A5%E7%9C%8Bmysql%E5%BA%93%E5%A4%A7%E5%B0%8F%E8%A1%A8%E5%A4%A7%E5%B0%8F%E7%B4%A2%E5%BC%95%E5%A4%A7%E5%B0%8F/","summary":"说明：\n通过MySQL的 information_schema 数据库，可查询数据库中每个表占用的空间、表记录的行数；该库中有一个 TABLES 表，这个表主要字段分别是：\n TABLE_SCHEMA : 数据库名 TABLE_NAME：表名 ENGINE：所使用的存储引擎 TABLES_ROWS：记录数 DATA_LENGTH：数据大小 INDEX_LENGTH：索引大小  其他字段请参考MySQL的手册，查看一个表占用空间的大小，那就相当于是 数据大小 + 索引大小 。\n查看所有库的大小\nmysql\u0026gt; use information_schema; Database changed mysql\u0026gt; select concat(round(sum(DATA_LENGTH/1024/1024),2),\u0026#39;MB\u0026#39;) as data from TABLES; +----------+ | data | +----------+ | 104.21MB | +----------+ 1 row in set (0.11 sec) 查看指定库的大小\nmysql\u0026gt; select concat(round(sum(DATA_LENGTH/1024/1024),2),\u0026#39;MB\u0026#39;) as data from TABLES where table_schema=\u0026#39;jishi\u0026#39;; +---------+ | data | +---------+ | 26.17MB | +---------+ 1 row in set (0.","title":"查看mysql库大小，表大小，索引大小"},{"content":"1.NIO 1.1 NIO通道客户端【应用】   客户端实现步骤\n 打开通道 指定IP和端口号 写出数据 释放资源    示例代码\npublic class NIOClient {  public static void main(String[] args) throws IOException {  //1.打开通道  SocketChannel socketChannel = SocketChannel.open();   //2.指定IP和端口号  socketChannel.connect(new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;,10000));   //3.写出数据  ByteBuffer byteBuffer = ByteBuffer.wrap(\u0026#34;一点寒毛先制\u0026#34;.getBytes());  socketChannel.write(byteBuffer);   //4.释放资源  socketChannel.close();  } }   1.2 NIO通道服务端【应用】   NIO通道\n  服务端通道\n只负责建立建立，不负责传递数据\n  客户端通道\n建立建立并将数据传递给服务端\n  缓冲区\n客户端发送的数据都在缓冲区中\n  服务端通道内部创建出来的客户端通道\n相当于客户端通道的延伸用来传递数据\n    服务端实现步骤\n 打开一个服务端通道 绑定对应的端口号 通道默认是阻塞的，需要设置为非阻塞 此时没有门卫大爷，所以需要经常看一下有没有连接发过来没？ 如果有客户端来连接了,则在服务端通道内部,再创建一个客户端通道,相当于是客户端通道的延伸 获取客户端传递过来的数据,并把数据放在byteBuffer1这个缓冲区中 给客户端回写数据 释放资源    示例代码\npublic class NIOServer {  public static void main(String[] args) throws IOException { // 1.打开一个服务端通道  ServerSocketChannel serverSocketChannel = ServerSocketChannel.open(); // 2.绑定对应的端口号  serverSocketChannel.bind(new InetSocketAddress(10000)); // 3.通道默认是阻塞的，需要设置为非阻塞  //如果传递true 表示通道设置为阻塞通道...默认值  //如果传递false 表示通道设置为非阻塞通道  serverSocketChannel.configureBlocking(false); // 4.此时没有门卫大爷，所以需要经常看一下有没有连接发过来没？  while (true) { // 5.如果有客户端来连接了，则在服务端通道内部，再创建一个客户端通道，相当于是客户端通道的延伸  //此时已经设置了通道为非阻塞  //所以在调用方法的时候,如果有客户端来连接,那么会创建一个SocketChannel对象.  //如果在调用方法的时候,没有客户端来连接,那么他会返回一个null  SocketChannel socketChannel = serverSocketChannel.accept();  //System.out.println(socketChannel);  if(socketChannel != null){ // 6.客户端将缓冲区通过通道传递给服务端,就到了这个延伸通道socketChannel里面 // 7.服务端创建一个空的缓冲区装数据并输出  ByteBuffer byteBuffer = ByteBuffer.allocate(1024);  //获取传递过来的数据,并把他们放到byteBuffer缓冲区中.  //返回值:  //正数: 表示本次读到的有效字节个数.  //0 : 表示本次没有读到有效字节.  //-1 : 表示读到了末尾  int len = socketChannel.read(byteBuffer);  System.out.println(new String(byteBuffer.array(),0,len));  //8.释放资源  socketChannel.close();  }  }  } }   1.3 NIO通道练习【应用】   客户端\n  实现步骤\n 打开通道 指定IP和端口号 写出数据 读取服务器写回的数据 释放资源    示例代码\npublic class Clinet {  public static void main(String[] args) throws IOException {  // 1.打开通道  SocketChannel socketChannel = SocketChannel.open();  // 2.指定IP和端口号  socketChannel.connect(new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;,10000));  // 3.写出数据  ByteBuffer byteBuffer1 = ByteBuffer.wrap(\u0026#34;吃俺老孙一棒棒\u0026#34;.getBytes());  socketChannel.write(byteBuffer1);  // 手动写入结束标记  socketChannel.shutdownOutput();   System.out.println(\u0026#34;数据已经写给服务器\u0026#34;);  // 4.读取服务器写回的数据  ByteBuffer byteBuffer2 = ByteBuffer.allocate(1024);  int len;  while((len = socketChannel.read(byteBuffer2)) != -1){  byteBuffer2.flip();  System.out.println(new String(byteBuffer2.array(),0,len));  byteBuffer2.clear();  }  // 5.释放资源  socketChannel.close();  } }     服务端\n  实现步骤\n 打开一个服务端通道 绑定对应的端口号 通道默认是阻塞的，需要设置为非阻塞 此时没有门卫大爷，所以需要经常看一下有没有连接发过来没？ 如果有客户端来连接了,则在服务端通道内部,再创建一个客户端通道,相当于是客户端通道的延伸 获取客户端传递过来的数据,并把数据放在byteBuffer1这个缓冲区中 给客户端回写数据 释放资源    示例代码\npublic class Sever {  public static void main(String[] args) throws IOException {  // 1，打开一个服务端通道  ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();  // 2，绑定对应的端口号  serverSocketChannel.bind(new InetSocketAddress(10000));  // 3，通道默认是阻塞的，需要设置为非阻塞  serverSocketChannel.configureBlocking(false);  // 4，此时没有门卫大爷，所以需要经常看一下有没有连接发过来没？  while(true){  // 5，如果有客户端来连接了，则在服务端通道内部，再创建一个客户端通道，相当于是客户端通道的延伸  SocketChannel socketChannel = serverSocketChannel.accept();  if(socketChannel != null){  System.out.println(\u0026#34;此时有客户端来连接了\u0026#34;);  // 6,获取客户端传递过来的数据,并把数据放在byteBuffer1这个缓冲区中  ByteBuffer byteBuffer1 = ByteBuffer.allocate(1024);  //socketChannel.read(byteBuffer1);  int len;  //针对于缓冲区来讲  //如果 从添加数据 ----\u0026gt; 获取数据 flip  //如果 从获取数据 ----\u0026gt; 添加数据 clear  while((len = socketChannel.read(byteBuffer1)) != -1){  byteBuffer1.flip();  System.out.println(new String(byteBuffer1.array(),0,len));  byteBuffer1.clear();  }   System.out.println(\u0026#34;接收数据完毕,准备开始往客户端回写数据\u0026#34;);  // 7,给客户端回写数据  ByteBuffer byteBuffer2 = ByteBuffer.wrap(\u0026#34;哎哟,真疼啊!!!\u0026#34;.getBytes());  socketChannel.write(byteBuffer2);  // 8,释放资源  socketChannel.close();  }  }  } }     1.4 NIO通道练习优化【应用】   存在问题\n服务端内部获取的客户端通道在读取时,如果读取不到结束标记就会一直阻塞\n  解决方案\n将服务端内部获取的客户端通道设置为非阻塞的\n  示例代码\n// 客户端 public class Clinet {  public static void main(String[] args) throws IOException {  SocketChannel socketChannel = SocketChannel.open();   socketChannel.connect(new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;,10000));   ByteBuffer byteBuffer1 = ByteBuffer.wrap(\u0026#34;吃俺老孙一棒棒\u0026#34;.getBytes());  socketChannel.write(byteBuffer1);   System.out.println(\u0026#34;数据已经写给服务器\u0026#34;);   ByteBuffer byteBuffer2 = ByteBuffer.allocate(1024);  int len;  while((len = socketChannel.read(byteBuffer2)) != -1){  System.out.println(\u0026#34;客户端接收回写数据\u0026#34;);  byteBuffer2.flip();  System.out.println(new String(byteBuffer2.array(),0,len));  byteBuffer2.clear();  }  socketChannel.close();  } } // 服务端 public class Sever {  public static void main(String[] args) throws IOException {  ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();   serverSocketChannel.bind(new InetSocketAddress(10000));   serverSocketChannel.configureBlocking(false);   while(true){  SocketChannel socketChannel = serverSocketChannel.accept();  if(socketChannel != null){  System.out.println(\u0026#34;此时有客户端来连接了\u0026#34;);  // 将服务端内部获取的客户端通道设置为非阻塞的  socketChannel.configureBlocking(false);  //获取客户端传递过来的数据,并把数据放在byteBuffer1这个缓冲区中  ByteBuffer byteBuffer1 = ByteBuffer.allocate(1024);  //socketChannel.read(byteBuffer1);  int len;  //针对于缓冲区来讲  //如果 从添加数据 ----\u0026gt; 获取数据 flip  //如果 从获取数据 ----\u0026gt; 添加数据 clear  while((len = socketChannel.read(byteBuffer1)) \u0026gt; 0){  System.out.println(\u0026#34;服务端接收发送数据\u0026#34;);  byteBuffer1.flip();  System.out.println(new String(byteBuffer1.array(),0,len));  byteBuffer1.clear();  }   System.out.println(\u0026#34;接收数据完毕,准备开始往客户端回写数据\u0026#34;);   ByteBuffer byteBuffer2 = ByteBuffer.wrap(\u0026#34;哎哟,真疼啊!!!\u0026#34;.getBytes());  socketChannel.write(byteBuffer2);   socketChannel.close();  }  }  } }   1.5NIO选择器【理解】   概述\n选择器可以监视通道的状态,多路复用\n  选择器对象\n  Selector\n选择器对象\n  SelectionKey\n绑定的key\n  SelectableChannel\n能使用选择器的通道\n SocketChannel ServerSocketChannel      1.6NIO选择器改写服务端【应用】   实现步骤\n  打开一个服务端通道(open)\n  绑定对应的端口号\n  通道默认是阻塞的，需要设置为非阻塞\n  打开一个选择器（门卫大爷）\n  将选择器绑定服务端通道，并监视服务端是否准备好\n  如果有客户端来连接了，大爷会遍历所有的服务端通道，谁准备好了，就让谁来连接 连接后，在服务端通道内部，再创建一个客户端延伸通道\n  如果客户端把数据传递过来了，大爷会遍历所有的延伸通道，谁准备好了，谁去接收数据\n    代码实现\n// 客户端 public class Clinet {  public static void main(String[] args) throws IOException {  SocketChannel socketChannel = SocketChannel.open();   socketChannel.connect(new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;,10000));   ByteBuffer byteBuffer1 = ByteBuffer.wrap(\u0026#34;吃俺老孙一棒棒\u0026#34;.getBytes());  socketChannel.write(byteBuffer1);   System.out.println(\u0026#34;数据已经写给服务器\u0026#34;);   ByteBuffer byteBuffer2 = ByteBuffer.allocate(1024);  int len;  while((len = socketChannel.read(byteBuffer2)) != -1){  System.out.println(\u0026#34;客户端接收回写数据\u0026#34;);  byteBuffer2.flip();  System.out.println(new String(byteBuffer2.array(),0,len));  byteBuffer2.clear();  }  socketChannel.close();  } } // 服务端 public class Server {  public static void main(String[] args) throws IOException {  //1.打开服务端通道  ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();  //2.让这个通道绑定一个端口  serverSocketChannel.bind(new InetSocketAddress(10000));  //3.设置通道为非阻塞  serverSocketChannel.configureBlocking(false);  //4.打开一个选择器  //Selector --- 选择器 // SelectionKey --- 绑定通道后返回那个令牌  // SelectableChannel --- 可以使用选择器的通道  Selector selector = Selector.open();  //5.绑定选择器和服务端通道  serverSocketChannel.register(selector,SelectionKey.OP_ACCEPT);   while(true){  System.out.println(\u0026#34;11\u0026#34;);  //选择器会监视客户端通道的状态.  //6.返回值就表示此时有多少个客户端来连接.  int count = selector.select();  System.out.println(\u0026#34;222\u0026#34;);  if(count != 0){  System.out.println(\u0026#34;有客户端来连接了\u0026#34;);  //7.会遍历所有的服务端通道.看谁准备好了,谁准备好了,就让谁去连接.  //获取所有服务端通道的令牌,并将它们都放到一个集合中,将集合返回.  Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys();  Iterator\u0026lt;SelectionKey\u0026gt; iterator = selectionKeys.iterator();  while(iterator.hasNext()){  //selectionKey 依次表示每一个服务端通道的令牌  SelectionKey selectionKey = iterator.next();  if(selectionKey.isAcceptable()){  //可以通过令牌来获取到了一个已经就绪的服务端通道  ServerSocketChannel ssc = (ServerSocketChannel) selectionKey.channel();  //客户端的延伸通道  SocketChannel socketChannel = ssc.accept();  //将客户端延伸通道设置为非阻塞的  socketChannel.configureBlocking(false);  socketChannel.register(selector,SelectionKey.OP_READ);  //当客户端来连接的时候,所有的步骤已经全部执行完毕.  }else if(selectionKey.isReadable()){  //当前通道已经做好了读取的准备(延伸通道)  SocketChannel socketChannel = (SocketChannel) selectionKey.channel();  ByteBuffer byteBuffer1 = ByteBuffer.allocate(1024);  //socketChannel.read(byteBuffer1);  int len;  while((len = socketChannel.read(byteBuffer1)) \u0026gt; 0){  byteBuffer1.flip();  System.out.println(new String(byteBuffer1.array(),0,len));  byteBuffer1.clear();  }  //给客户端的回写数据  socketChannel.write(ByteBuffer.wrap(\u0026#34;哎哟喂好疼啊!!!\u0026#34;.getBytes()));  socketChannel.close();  }  iterator.remove();  }  }  }  } }   2.HTTP协议 2.1概述【理解】 超文本传输协议(关于超文本的概念JavaWeb在进行学习)，是建立在TCP/IP协议基础上,是网络应用层的协议。\n由请求和响应构成,是一个标准的客户端和服务器模型\n2.2URL【理解】   概述\n统一资源定位符,常见的如http://bbs.itheima.com/forum.php\n完整的格式为 http://bbs.itheima.com:80/forum.php\n  详解\n  2.3抓包工具的使用【应用】   使用步骤\n  在谷歌浏览器网页中按F12 或者网页空白处右键,点击检查,可以调出工具\n  点击network,进入到查看网络相关信息界面\n  这时在浏览器中发起请求,进行访问,工具中就会显示出请求和响应相关的信息\n    2.4请求信息【理解】   组成\n 请求行 请求头 请求空行 请求体    请求行\n  格式\n  请求方式\nGET,POST,HEAD,PUT,DELETE,CONNECT,OPTIONS,TRACE,PATCH\n其中用的比较多的是GET和POST\n  URI\n请求资源路径,统一资源标识符\n  协议版本\n HTTP1.0: 每次请求和响应都需要建立一个单独的连接 HTTP1.1:支持长连接      请求头\n  格式\n  请求头名称\n  Host: 用来指定请求的服务端地址\n  Connection: 取值为keep-alive表示需要持久连接\n  User-Agent: 客户端的信息\n  Accept: 指定客户端能够接收的内容类型\n  Accept-Encoding: 指定浏览器可以支持的服务器返回内容压缩编码类型\n  Accept-Language: 浏览器可接受的语言\n      小结\n  2.5响应信息【理解】   组成\n 响应行 响应头 响应空行 响应体    响应行\n  格式\n  协议版本\n HTTP1.0: 每次请求和响应都需要建立一个单独的连接 HTTP1.1: 支持长连接    响应状态码\n 1xx: 指示信息(表示请求已接收，继续处理) 2xx: 成功(表示请求已被成功接收、理解、接受) 3xx: 请求重定向(要完成请求必须进行更进一步的操作) 4xx: 客户端错误(请求有语法错误或请求无法实现) 5xx: 服务器端错误(服务器未能实现合法的请求)    状态信息\n 200 ok 404 Not Found 500 Internal Server Error      响应头\n  响应头名称\n Content-Type: 告诉客户端实际返回内容的网络媒体类型(互联网媒体类型,也叫做MIME类型)    响应头值\n  text/html \u0026mdash;-\u0026gt; 文本类型\n  image/png \u0026mdash;-\u0026gt; png格式文件\n  image/jpeg \u0026mdash;-\u0026gt; jpg格式文件\n      小结\n  3.HTTP服务器 3.1需求【理解】  编写服务器端代码,实现可以解析浏览器的请求,给浏览器响应数据  3.2环境搭建【理解】   实现步骤\n 编写HttpServer类,实现可以接收浏览器发出的请求 其中获取连接的代码可以单独抽取到一个类中    代码实现\n// 服务端代码 public class HttpServer {  public static void main(String[] args) throws IOException {  //1.打开服务端通道  ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();  //2.让这个通道绑定一个端口  serverSocketChannel.bind(new InetSocketAddress(10000));  //3.设置通道为非阻塞  serverSocketChannel.configureBlocking(false);  //4.打开一个选择器  Selector selector = Selector.open();   //5.绑定选择器和服务端通道  serverSocketChannel.register(selector,SelectionKey.OP_ACCEPT);   while(true){  //6.选择器会监视通道的状态.  int count = selector.select();  if(count != 0){  //7.会遍历所有的服务端通道.看谁准备好了,谁准备好了,就让谁去连接.  //获取所有服务端通道的令牌,并将它们都放到一个集合中,将集合返回.  Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys();  Iterator\u0026lt;SelectionKey\u0026gt; iterator = selectionKeys.iterator();  while(iterator.hasNext()){  //selectionKey 依次表示每一个服务端通道的令牌  SelectionKey selectionKey = iterator.next();  if(selectionKey.isAcceptable()){  //获取连接  AcceptHandler acceptHandler = new AcceptHandler();  acceptHandler.connSocketChannel(selectionKey);  }else if(selectionKey.isReadable()){   }  //任务处理完毕以后,将SelectionKey从集合中移除  iterator.remove();  }  }  }  } } // 将获取连接的代码抽取到这个类中 public class AcceptHandler {   public SocketChannel connSocketChannel(SelectionKey selectionKey){  try {  //获取到已经就绪的服务端通道  ServerSocketChannel ssc = (ServerSocketChannel) selectionKey.channel();  SocketChannel socketChannel = ssc.accept();  //设置为非阻塞状态  socketChannel.configureBlocking(false);  //把socketChannel注册到选择器上  socketChannel.register(selectionKey.selector(), SelectionKey.OP_READ);  return socketChannel;  } catch (IOException e) {  e.printStackTrace();  }  return null;  } }   3.3获取请求信息并解析【理解】   实现步骤\n 将请求信息封装到HttpRequest类中 在类中定义方法,实现获取请求信息并解析    代码实现\n/** * 用来封装请求数据的类 */ public class HttpRequest {  private String method; //请求方式  private String requestURI; //请求的uri  private String version; //http的协议版本   private HashMap\u0026lt;String,String\u0026gt; hm = new HashMap\u0026lt;\u0026gt;();//所有的请求头   //parse --- 获取请求数据 并解析  public void parse(SelectionKey selectionKey){  try {  SocketChannel socketChannel = (SocketChannel) selectionKey.channel();   StringBuilder sb = new StringBuilder();  //创建一个缓冲区  ByteBuffer byteBuffer = ByteBuffer.allocate(1024);  int len;  //循环读取  while((len = socketChannel.read(byteBuffer)) \u0026gt; 0){  byteBuffer.flip();  sb.append(new String(byteBuffer.array(),0,len));  //System.out.println(new String(byteBuffer.array(),0,len));  byteBuffer.clear();  }  //System.out.println(sb);  parseHttpRequest(sb);   } catch (IOException e) {  e.printStackTrace();  }  }   //解析http请求协议中的数据  private void parseHttpRequest(StringBuilder sb) {  //1.需要把StringBuilder先变成一个字符串  String httpRequestStr = sb.toString();  //2.获取每一行数据  String[] split = httpRequestStr.split(\u0026#34;\\r\\n\u0026#34;);  //3.获取请求行  String httpRequestLine = split[0];//GET / HTTP/1.1  //4.按照空格进行切割,得到请求行中的三部分  String[] httpRequestInfo = httpRequestLine.split(\u0026#34; \u0026#34;);  this.method = httpRequestInfo[0];  this.requestURI = httpRequestInfo[1];  this.version = httpRequestInfo[2];  //5.操作每一个请求头  for (int i = 1; i \u0026lt; split.length; i++) {  String httpRequestHeaderInfo = split[i];//Host: 127.0.0.1:10000  String[] httpRequestHeaderInfoArr = httpRequestHeaderInfo.split(\u0026#34;: \u0026#34;);  hm.put(httpRequestHeaderInfoArr[0],httpRequestHeaderInfoArr[1]);  }   }   public String getMethod() {  return method;  }   public void setMethod(String method) {  this.method = method;  }   public String getRequestURI() {  return requestURI;  }   public void setRequestURI(String requestURI) {  this.requestURI = requestURI;  }   public String getVersion() {  return version;  }   public void setVersion(String version) {  this.version = version;  }   public HashMap\u0026lt;String, String\u0026gt; getHm() {  return hm;  }   public void setHm(HashMap\u0026lt;String, String\u0026gt; hm) {  this.hm = hm;  }   @Override  public String toString() {  return \u0026#34;HttpRequest{\u0026#34; +  \u0026#34;method=\u0026#39;\u0026#34; + method + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, requestURI=\u0026#39;\u0026#34; + requestURI + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, version=\u0026#39;\u0026#34; + version + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, hm=\u0026#34; + hm +  \u0026#39;}\u0026#39;;  } }   3.4给浏览器响应数据【理解】   实现步骤\n 将响应信息封装HttpResponse类中 定义方法,封装响应信息,给浏览器响应数据    代码实现\npublic class HttpResponse {  private String version; //协议版本  private String status; //响应状态码  private String desc; //状态码的描述信息   //响应头数据  private HashMap\u0026lt;String, String\u0026gt; hm = new HashMap\u0026lt;\u0026gt;();   private HttpRequest httpRequest; //我们后面要根据请求的数据,来进行一些判断   //给浏览器响应数据的方法  public void sendStaticResource(SelectionKey selectionKey) {  //1.给响应行赋值  this.version = \u0026#34;HTTP/1.1\u0026#34;;  this.status = \u0026#34;200\u0026#34;;  this.desc = \u0026#34;ok\u0026#34;;  //2.将响应行拼接成一个单独的字符串 // HTTP/1.1 200 ok  String responseLine = this.version + \u0026#34; \u0026#34; + this.status + \u0026#34; \u0026#34; + this.desc + \u0026#34;\\r\\n\u0026#34;;   //3.给响应头赋值  hm.put(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html;charset=UTF-8\u0026#34;);   //4.将所有的响应头拼接成一个单独的字符串  StringBuilder sb = new StringBuilder();  Set\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; entries = hm.entrySet();  for (Map.Entry\u0026lt;String, String\u0026gt; entry : entries) {  sb.append(entry.getKey()).append(\u0026#34;: \u0026#34;).append(entry.getValue()).append(\u0026#34;\\r\\n\u0026#34;);  }   //5.响应空行  String emptyLine = \u0026#34;\\r\\n\u0026#34;;   //6.响应行,响应头,响应空行拼接成一个大字符串  String responseLineStr = responseLine + sb.toString() + emptyLine;   try {  //7.将上面三个写给浏览器  SocketChannel socketChannel = (SocketChannel) selectionKey.channel();  ByteBuffer byteBuffer1 = ByteBuffer.wrap(responseLineStr.getBytes());  socketChannel.write(byteBuffer1);   //8.单独操作响应体  //因为在以后响应体不一定是一个字符串  //有可能是一个文件,所以单独操作  String s = \u0026#34;哎哟,妈呀,终于写完了.\u0026#34;;  ByteBuffer byteBuffer2 = ByteBuffer.wrap(s.getBytes());  socketChannel.write(byteBuffer2);   //9.释放资源  socketChannel.close();  } catch (IOException e) {  e.printStackTrace();  }  }   public String getVersion() {  return version;  }   public void setVersion(String version) {  this.version = version;  }   public String getStatus() {  return status;  }   public void setStatus(String status) {  this.status = status;  }   public String getDesc() {  return desc;  }   public void setDesc(String desc) {  this.desc = desc;  }   public HashMap\u0026lt;String, String\u0026gt; getHm() {  return hm;  }   public void setHm(HashMap\u0026lt;String, String\u0026gt; hm) {  this.hm = hm;  }   public HttpRequest getHttpRequest() {  return httpRequest;  }   public void setHttpRequest(HttpRequest httpRequest) {  this.httpRequest = httpRequest;  }   @Override  public String toString() {  return \u0026#34;HttpResponse{\u0026#34; +  \u0026#34;version=\u0026#39;\u0026#34; + version + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, status=\u0026#39;\u0026#34; + status + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, desc=\u0026#39;\u0026#34; + desc + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, hm=\u0026#34; + hm +  \u0026#34;, httpRequest=\u0026#34; + httpRequest +  \u0026#39;}\u0026#39;;  } }   3.5代码优化【理解】   实现步骤\n 根据请求资源路径不同,响应不同的数据 服务端健壮性处理 访问不存在的资源处理    代码实现\n/** * 接收连接的任务处理类 */ public class AcceptHandler {   public SocketChannel connSocketChannel(SelectionKey selectionKey){  try {  //获取到已经就绪的服务端通道  ServerSocketChannel ssc = (ServerSocketChannel) selectionKey.channel();  SocketChannel socketChannel = ssc.accept();  //设置为非阻塞状态  socketChannel.configureBlocking(false);  //把socketChannel注册到选择器上  socketChannel.register(selectionKey.selector(), SelectionKey.OP_READ);  return socketChannel;  } catch (IOException e) {  e.printStackTrace();  }  return null;  } } /** * 接收客户端请求的类 */ public class HttpServer {  public static void main(String[] args) throws IOException {  //1.打开服务端通道  ServerSocketChannel serverSocketChannel = ServerSocketChannel.open();  //2.让这个通道绑定一个端口  serverSocketChannel.bind(new InetSocketAddress(10000));  //3.设置通道为非阻塞  serverSocketChannel.configureBlocking(false);  //4.打开一个选择器  Selector selector = Selector.open();  //5.绑定选择器和服务端通道  serverSocketChannel.register(selector,SelectionKey.OP_ACCEPT);   while(true){  //6.选择器会监视通道的状态.  int count = selector.select();  if(count != 0){  //7.会遍历所有的服务端通道.看谁准备好了,谁准备好了,就让谁去连接.  //获取所有服务端通道的令牌,并将它们都放到一个集合中,将集合返回.  Set\u0026lt;SelectionKey\u0026gt; selectionKeys = selector.selectedKeys();  Iterator\u0026lt;SelectionKey\u0026gt; iterator = selectionKeys.iterator();  while(iterator.hasNext()){  //selectionKey 依次表示每一个服务端通道的令牌  SelectionKey selectionKey = iterator.next();  if(selectionKey.isAcceptable()){  //获取连接  AcceptHandler acceptHandler = new AcceptHandler();  acceptHandler.connSocketChannel(selectionKey);   }else if(selectionKey.isReadable()){  //读取数据  HttpRequest httpRequest = new HttpRequest();  httpRequest.parse(selectionKey);  System.out.println(\u0026#34;http请求的数据为 ----\u0026gt;\u0026#34; + httpRequest);   if(httpRequest.getRequestURI() == null || \u0026#34;\u0026#34;.equals(httpRequest.getRequestURI())){  selectionKey.channel();  continue;  }  System.out.println(\u0026#34;...数据解析完毕,准备响应数据....\u0026#34;);   //响应数据  HttpResponse httpResponse = new HttpResponse();  httpResponse.setHttpRequest(httpRequest);  httpResponse.sendStaticResource(selectionKey);  }  //任务处理完毕以后,将SelectionKey从集合中移除  iterator.remove();  }  }  }  } } /** * 用来封装请求数据的类 */ public class HttpRequest {  private String method; //请求方式  private String requestURI; //请求的uri  private String version; //http的协议版本   private HashMap\u0026lt;String,String\u0026gt; hm = new HashMap\u0026lt;\u0026gt;();//所有的请求头   //parse --- 获取请求数据 并解析  public void parse(SelectionKey selectionKey){  try {  SocketChannel socketChannel = (SocketChannel) selectionKey.channel();   StringBuilder sb = new StringBuilder();  //创建一个缓冲区  ByteBuffer byteBuffer = ByteBuffer.allocate(1024);  int len;  //循环读取  while((len = socketChannel.read(byteBuffer)) \u0026gt; 0){  byteBuffer.flip();  sb.append(new String(byteBuffer.array(),0,len));  //System.out.println(new String(byteBuffer.array(),0,len));  byteBuffer.clear();  }  //System.out.println(sb);  parseHttpRequest(sb);   } catch (IOException e) {  e.printStackTrace();  }  }   //解析http请求协议中的数据  private void parseHttpRequest(StringBuilder sb) {  //1.需要把StringBuilder先变成一个字符串  String httpRequestStr = sb.toString();  if(!(httpRequestStr == null || \u0026#34;\u0026#34;.equals(httpRequestStr))){  //2.获取每一行数据  String[] split = httpRequestStr.split(\u0026#34;\\r\\n\u0026#34;);  //3.获取请求行  String httpRequestLine = split[0];//GET / HTTP/1.1  //4.按照空格进行切割,得到请求行中的三部分  String[] httpRequestInfo = httpRequestLine.split(\u0026#34; \u0026#34;);  this.method = httpRequestInfo[0];  this.requestURI = httpRequestInfo[1];  this.version = httpRequestInfo[2];  //5.操作每一个请求头  for (int i = 1; i \u0026lt; split.length; i++) {  String httpRequestHeaderInfo = split[i];//Host: 127.0.0.1:10000  String[] httpRequestHeaderInfoArr = httpRequestHeaderInfo.split(\u0026#34;: \u0026#34;);  hm.put(httpRequestHeaderInfoArr[0],httpRequestHeaderInfoArr[1]);  }  }  }   public String getMethod() {  return method;  }   public void setMethod(String method) {  this.method = method;  }   public String getRequestURI() {  return requestURI;  }   public void setRequestURI(String requestURI) {  this.requestURI = requestURI;  }   public String getVersion() {  return version;  }   public void setVersion(String version) {  this.version = version;  }   public HashMap\u0026lt;String, String\u0026gt; getHm() {  return hm;  }   public void setHm(HashMap\u0026lt;String, String\u0026gt; hm) {  this.hm = hm;  }   @Override  public String toString() {  return \u0026#34;HttpRequest{\u0026#34; +  \u0026#34;method=\u0026#39;\u0026#34; + method + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, requestURI=\u0026#39;\u0026#34; + requestURI + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, version=\u0026#39;\u0026#34; + version + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, hm=\u0026#34; + hm +  \u0026#39;}\u0026#39;;  } } /** * 用来封装响应数据的类 */ public class HttpResponse {  private String version; //协议版本  private String status; //响应状态码  private String desc; //状态码的描述信息   //响应头数据  private HashMap\u0026lt;String, String\u0026gt; hm = new HashMap\u0026lt;\u0026gt;();   private HttpRequest httpRequest; //我们后面要根据请求的数据,来进行一些判断   //给浏览器响应数据的方法  public void sendStaticResource(SelectionKey selectionKey) {  //1.给响应行赋值  this.version = \u0026#34;HTTP/1.1\u0026#34;;  this.status = \u0026#34;200\u0026#34;;  this.desc = \u0026#34;ok\u0026#34;;   //3.给响应头赋值  //先获取浏览器请求的URI  String requestURI = this.getHttpRequest().getRequestURI();  if(requestURI != null){   File file = new File(WEB_APP_PATH + requestURI);  //判断这个路径是否存在  if(!file.exists()){  this.status = \u0026#34;404\u0026#34;;  this.desc = \u0026#34;NOT FOUNG\u0026#34;;  }   if(\u0026#34;200\u0026#34;.equals(this.status)){  if(\u0026#34;/\u0026#34;.equals(requestURI)){  hm.put(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html;charset=UTF-8\u0026#34;);  }else if(\u0026#34;/favicon.ico\u0026#34;.equals(requestURI)){  hm.put(\u0026#34;Content-Type\u0026#34;, \u0026#34;image/x-icon\u0026#34;);  }else if(\u0026#34;/a.txt\u0026#34;.equals(requestURI)){  hm.put(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html;charset=UTF-8\u0026#34;);  }else if(\u0026#34;/1.jpg\u0026#34;.equals(requestURI)){  hm.put(\u0026#34;Content-Type\u0026#34;, \u0026#34;image/jpeg\u0026#34;);  }else if(\u0026#34;/1.png\u0026#34;.equals(requestURI)){  hm.put(\u0026#34;Content-Type\u0026#34;, \u0026#34;image/png\u0026#34;);  }  }else{  hm.put(\u0026#34;Content-Type\u0026#34;, \u0026#34;text/html;charset=UTF-8\u0026#34;);  }   }   //2.将响应行拼接成一个单独的字符串 // HTTP/1.1 200 ok  String responseLine = this.version + \u0026#34; \u0026#34; + this.status + \u0026#34; \u0026#34; + this.desc + \u0026#34;\\r\\n\u0026#34;;   //4.将所有的响应头拼接成一个单独的字符串  StringBuilder sb = new StringBuilder();  Set\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; entries = hm.entrySet();  for (Map.Entry\u0026lt;String, String\u0026gt; entry : entries) {  sb.append(entry.getKey()).append(\u0026#34;: \u0026#34;).append(entry.getValue()).append(\u0026#34;\\r\\n\u0026#34;);  }   //5.响应空行  String emptyLine = \u0026#34;\\r\\n\u0026#34;;   //6.响应行,响应头,响应空行拼接成一个大字符串  String responseLineStr = responseLine + sb.toString() + emptyLine;   try {  //7.将上面三个写给浏览器  SocketChannel socketChannel = (SocketChannel) selectionKey.channel();  ByteBuffer byteBuffer1 = ByteBuffer.wrap(responseLineStr.getBytes());  socketChannel.write(byteBuffer1);   //8.单独操作响应体  //因为在以后响应体不一定是一个字符串  //有可能是一个文件,所以单独操作  // String s = \u0026#34;哎哟,妈呀,终于写完了.\u0026#34;;  byte [] bytes = getContent();  ByteBuffer byteBuffer2 = ByteBuffer.wrap(bytes);  socketChannel.write(byteBuffer2);   //9.释放资源  socketChannel.close();  } catch (IOException e) {  e.printStackTrace();  }  }   public static final String WEB_APP_PATH = \u0026#34;mynio\\\\webapp\u0026#34;;  private byte[] getContent() {  try {  //1.获取浏览器请求的URI  String requestURI = this.getHttpRequest().getRequestURI();  if(requestURI != null){   if(\u0026#34;200\u0026#34;.equals(this.status)){  //2.判断一下请求的URI,根据不同的URI来响应不同的东西  if(\u0026#34;/\u0026#34;.equals(requestURI)){  String s = \u0026#34;哎哟,妈呀,终于写完了.\u0026#34;;  return s.getBytes();  }else/* if(\u0026#34;/favicon.ico\u0026#34;.equals(requestURI))*/{  //获取一个ico文件  FileInputStream fis = new FileInputStream(WEB_APP_PATH + requestURI);  //把ico文件变成一个字节数组返回  return IOUtils.toByteArray(fis);  }  }else{  return \u0026#34;访问的资源不存在\u0026#34;.getBytes();  }  }  } catch (IOException e) {  e.printStackTrace();  }  return new byte[0];  }   public String getVersion() {  return version;  }   public void setVersion(String version) {  this.version = version;  }   public String getStatus() {  return status;  }   public void setStatus(String status) {  this.status = status;  }   public String getDesc() {  return desc;  }   public void setDesc(String desc) {  this.desc = desc;  }   public HashMap\u0026lt;String, String\u0026gt; getHm() {  return hm;  }   public void setHm(HashMap\u0026lt;String, String\u0026gt; hm) {  this.hm = hm;  }   public HttpRequest getHttpRequest() {  return httpRequest;  }   public void setHttpRequest(HttpRequest httpRequest) {  this.httpRequest = httpRequest;  }   @Override  public String toString() {  return \u0026#34;HttpResponse{\u0026#34; +  \u0026#34;version=\u0026#39;\u0026#34; + version + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, status=\u0026#39;\u0026#34; + status + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, desc=\u0026#39;\u0026#34; + desc + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, hm=\u0026#34; + hm +  \u0026#34;, httpRequest=\u0026#34; + httpRequest +  \u0026#39;}\u0026#39;;  } }   ","permalink":"https://iblog.zone/archives/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B02/","summary":"1.NIO 1.1 NIO通道客户端【应用】   客户端实现步骤\n 打开通道 指定IP和端口号 写出数据 释放资源    示例代码\npublic class NIOClient {  public static void main(String[] args) throws IOException {  //1.打开通道  SocketChannel socketChannel = SocketChannel.open();   //2.指定IP和端口号  socketChannel.connect(new InetSocketAddress(\u0026#34;127.0.0.1\u0026#34;,10000));   //3.写出数据  ByteBuffer byteBuffer = ByteBuffer.wrap(\u0026#34;一点寒毛先制\u0026#34;.getBytes());  socketChannel.write(byteBuffer);   //4.释放资源  socketChannel.close();  } }   1.2 NIO通道服务端【应用】   NIO通道\n  服务端通道\n只负责建立建立，不负责传递数据","title":"Java网络编程02"},{"content":"1.网络编程入门 1.1 网络编程概述【理解】   计算机网络\n是指将地理位置不同的具有独立功能的多台计算机及其外部设备，通过通信线路连接起来，在网络操作系统，网络管理软件及网络通信协议的管理和协调下，实现资源共享和信息传递的计算机系统\n  网络编程\n在网络通信协议下，不同计算机上运行的程序，可以进行数据传输\n  1.2 网络编程三要素【理解】   IP地址\n要想让网络中的计算机能够互相通信，必须为每台计算机指定一个标识号，通过这个标识号来指定要接收数据的计算机和识别发送的计算机，而IP地址就是这个标识号。也就是设备的标识\n  端口\n网络的通信，本质上是两个应用程序的通信。每台计算机都有很多的应用程序，那么在网络通信时，如何区分这些应用程序呢？如果说IP地址可以唯一标识网络中的设备，那么端口号就可以唯一标识设备中的应用程序了。也就是应用程序的标识\n  协议\n通过计算机网络可以使多台计算机实现连接，位于同一个网络中的计算机在进行连接和通信时需要遵守一定的规则，这就好比在道路中行驶的汽车一定要遵守交通规则一样。在计算机网络中，这些连接和通信的规则被称为网络通信协议，它对数据的传输格式、传输速率、传输步骤等做了统一规定，通信双方必须同时遵守才能完成数据交换。常见的协议有UDP协议和TCP协议\n  1.3 IP地址【理解】 IP地址：是网络中设备的唯一标识\n  IP地址分为两大类\n  IPv4：是给每个连接在网络上的主机分配一个32bit地址。按照TCP/IP规定，IP地址用二进制来表示，每个IP地址长32bit，也就是4个字节。例如一个采用二进制形式的IP地址是“11000000 10101000 00000001 01000010”，这么长的地址，处理起来也太费劲了。为了方便使用，IP地址经常被写成十进制的形式，中间使用符号“.”分隔不同的字节。于是，上面的IP地址可以表示为“192.168.1.66”。IP地址的这种表示法叫做“点分十进制表示法”，这显然比1和0容易记忆得多\n  IPv6：由于互联网的蓬勃发展，IP地址的需求量愈来愈大，但是网络地址资源有限，使得IP的分配越发紧张。为了扩大地址空间，通过IPv6重新定义地址空间，采用128位地址长度，每16个字节一组，分成8组十六进制数，这样就解决了网络地址资源数量不够的问题\n    DOS常用命令：\n  ipconfig：查看本机IP地址\n  ping IP地址：检查网络是否连通\n    特殊IP地址：\n 127.0.0.1：是回送地址，可以代表本机地址，一般用来测试使用    1.4 InetAddress【应用】 InetAddress：此类表示Internet协议（IP）地址\n  相关方法\n   方法名 说明     static InetAddress getByName(String host) 确定主机名称的IP地址。主机名称可以是机器名称，也可以是IP地址   String getHostName() 获取此IP地址的主机名   String getHostAddress() 返回文本显示中的IP地址字符串      代码演示\npublic class InetAddressDemo {  public static void main(String[] args) throws UnknownHostException { \t//InetAddress address = InetAddress.getByName(\u0026#34;itheima\u0026#34;);  InetAddress address = InetAddress.getByName(\u0026#34;192.168.1.66\u0026#34;);   //public String getHostName()：获取此IP地址的主机名  String name = address.getHostName();  //public String getHostAddress()：返回文本显示中的IP地址字符串  String ip = address.getHostAddress();   System.out.println(\u0026#34;主机名：\u0026#34; + name);  System.out.println(\u0026#34;IP地址：\u0026#34; + ip);  } }   1.5 端口和协议【理解】   端口\n 设备上应用程序的唯一标识    端口号\n 用两个字节表示的整数，它的取值范围是0~65535。其中，0~1023之间的端口号用于一些知名的网络服务和应用，普通的应用程序需要使用1024以上的端口号。如果端口号被另外一个服务或应用所占用，会导致当前程序启动失败    协议\n 计算机网络中，连接和通信的规则被称为网络通信协议    UDP协议\n 用户数据报协议(User Datagram Protocol) UDP是无连接通信协议，即在数据传输时，数据的发送端和接收端不建立逻辑连接。简单来说，当一台计算机向另外一台计算机发送数据时，发送端不会确认接收端是否存在，就会发出数据，同样接收端在收到数据时，也不会向发送端反馈是否收到数据。 由于使用UDP协议消耗系统资源小，通信效率高，所以通常都会用于音频、视频和普通数据的传输 例如视频会议通常采用UDP协议，因为这种情况即使偶尔丢失一两个数据包，也不会对接收结果产生太大影响。但是在使用UDP协议传送数据时，由于UDP的面向无连接性，不能保证数据的完整性，因此在传输重要数据时不建议使用UDP协议    TCP协议\n  传输控制协议 (Transmission Control Protocol)\n  TCP协议是面向连接的通信协议，即传输数据之前，在发送端和接收端建立逻辑连接，然后再传输数据，它提供了两台计算机之间可靠无差错的数据传输。在TCP连接中必须要明确客户端与服务器端，由客户端向服务端发出连接请求，每次连接的创建都需要经过“三次握手”\n  三次握手：TCP协议中，在发送数据的准备阶段，客户端与服务器之间的三次交互，以保证连接的可靠\n第一次握手，客户端向服务器端发出连接请求，等待服务器确认\n第二次握手，服务器端向客户端回送一个响应，通知客户端收到了连接请求\n第三次握手，客户端再次向服务器端发送确认信息，确认连接\n  完成三次握手，连接建立后，客户端和服务器就可以开始进行数据传输了。由于这种面向连接的特性，TCP协议可以保证传输数据的安全，所以应用十分广泛。例如上传文件、下载文件、浏览网页等\n    2.UDP通信程序 2.1 UDP发送数据【应用】   Java中的UDP通信\n UDP协议是一种不可靠的网络协议，它在通信的两端各建立一个Socket对象，但是这两个Socket只是发送，接收数据的对象，因此对于基于UDP协议的通信双方而言，没有所谓的客户端和服务器的概念 Java提供了DatagramSocket类作为基于UDP协议的Socket    构造方法\n   方法名 说明     DatagramSocket() 创建数据报套接字并将其绑定到本机地址上的任何可用端口   DatagramPacket(byte[] buf,int len,InetAddress add,int port) 创建数据包,发送长度为len的数据包到指定主机的指定端口      相关方法\n   方法名 说明     void send(DatagramPacket p) 发送数据报包   void close() 关闭数据报套接字   void receive(DatagramPacket p) 从此套接字接受数据报包      发送数据的步骤\n 创建发送端的Socket对象(DatagramSocket) 创建数据，并把数据打包 调用DatagramSocket对象的方法发送数据 关闭发送端    代码演示\npublic class SendDemo {  public static void main(String[] args) throws IOException {  //创建发送端的Socket对象(DatagramSocket)  // DatagramSocket() 构造数据报套接字并将其绑定到本地主机上的任何可用端口  DatagramSocket ds = new DatagramSocket();   //创建数据，并把数据打包  //DatagramPacket(byte[] buf, int length, InetAddress address, int port)  //构造一个数据包，发送长度为 length的数据包到指定主机上的指定端口号。  byte[] bys = \u0026#34;hello,udp,我来了\u0026#34;.getBytes();   DatagramPacket dp = new DatagramPacket(bys,bys.length,InetAddress.getByName(\u0026#34;127.0.0.1\u0026#34;),10086);   //调用DatagramSocket对象的方法发送数据  //void send(DatagramPacket p) 从此套接字发送数据报包  ds.send(dp);   //关闭发送端  //void close() 关闭此数据报套接字  ds.close();  } }   2.2UDP接收数据【应用】   接收数据的步骤\n 创建接收端的Socket对象(DatagramSocket) 创建一个数据包，用于接收数据 调用DatagramSocket对象的方法接收数据 解析数据包，并把数据在控制台显示 关闭接收端    构造方法\n   方法名 说明     DatagramPacket(byte[] buf, int len) 创建一个DatagramPacket用于接收长度为len的数据包      相关方法\n   方法名 说明     byte[] getData() 返回数据缓冲区   int getLength() 返回要发送的数据的长度或接收的数据的长度      示例代码\npublic class ReceiveDemo {  public static void main(String[] args) throws IOException {  //创建接收端的Socket对象(DatagramSocket)  DatagramSocket ds = new DatagramSocket(12345);   //创建一个数据包，用于接收数据  byte[] bys = new byte[1024];  DatagramPacket dp = new DatagramPacket(bys, bys.length);   //调用DatagramSocket对象的方法接收数据  ds.receive(dp);   //解析数据包，并把数据在控制台显示  System.out.println(\u0026#34;数据是：\u0026#34; + new String(dp.getData(), 0, dp.getLength()));  }  } }   2.3UDP通信程序练习【应用】   案例需求\nUDP发送数据：数据来自于键盘录入，直到输入的数据是886，发送数据结束\nUDP接收数据：因为接收端不知道发送端什么时候停止发送，故采用死循环接收\n  代码实现\n/* UDP发送数据： 数据来自于键盘录入，直到输入的数据是886，发送数据结束 */ public class SendDemo {  public static void main(String[] args) throws IOException {  //创建发送端的Socket对象(DatagramSocket)  DatagramSocket ds = new DatagramSocket();  //键盘录入数据  Scanner sc = new Scanner(System.in);  while (true) {  String s = sc.nextLine();  //输入的数据是886，发送数据结束  if (\u0026#34;886\u0026#34;.equals(s)) {  break;  }  //创建数据，并把数据打包  byte[] bys = s.getBytes();  DatagramPacket dp = new DatagramPacket(bys, bys.length, InetAddress.getByName(\u0026#34;192.168.1.66\u0026#34;), 12345);   //调用DatagramSocket对象的方法发送数据  ds.send(dp);  }  //关闭发送端  ds.close();  } }  /* UDP接收数据： 因为接收端不知道发送端什么时候停止发送，故采用死循环接收 */ public class ReceiveDemo {  public static void main(String[] args) throws IOException {  //创建接收端的Socket对象(DatagramSocket)  DatagramSocket ds = new DatagramSocket(12345);  while (true) {  //创建一个数据包，用于接收数据  byte[] bys = new byte[1024];  DatagramPacket dp = new DatagramPacket(bys, bys.length);  //调用DatagramSocket对象的方法接收数据  ds.receive(dp);  //解析数据包，并把数据在控制台显示  System.out.println(\u0026#34;数据是：\u0026#34; + new String(dp.getData(), 0, dp.getLength()));  }  //关闭接收端 // ds.close();  } }   2.4UDP三种通讯方式【理解】   单播\n单播用于两个主机之间的端对端通信\n  组播\n组播用于对一组特定的主机进行通信\n  广播\n广播用于一个主机对整个局域网上所有主机上的数据通信\n  2.5UDP组播实现【理解】   实现步骤\n 发送端  创建发送端的Socket对象(DatagramSocket) 创建数据，并把数据打包(DatagramPacket) 调用DatagramSocket对象的方法发送数据(在单播中,这里是发给指定IP的电脑但是在组播当中,这里是发给组播地址) 释放资源   接收端  创建接收端Socket对象(MulticastSocket) 创建一个箱子,用于接收数据 把当前计算机绑定一个组播地址 将数据接收到箱子中 解析数据包,并打印数据 释放资源      代码实现\n// 发送端 public class ClinetDemo {  public static void main(String[] args) throws IOException {  // 1. 创建发送端的Socket对象(DatagramSocket)  DatagramSocket ds = new DatagramSocket();  String s = \u0026#34;hello 组播\u0026#34;;  byte[] bytes = s.getBytes();  InetAddress address = InetAddress.getByName(\u0026#34;224.0.1.0\u0026#34;);  int port = 10000;  // 2. 创建数据，并把数据打包(DatagramPacket)  DatagramPacket dp = new DatagramPacket(bytes,bytes.length,address,port);  // 3. 调用DatagramSocket对象的方法发送数据(在单播中,这里是发给指定IP的电脑但是在组播当中,这里是发给组播地址)  ds.send(dp);  // 4. 释放资源  ds.close();  } } // 接收端 public class ServerDemo {  public static void main(String[] args) throws IOException {  // 1. 创建接收端Socket对象(MulticastSocket)  MulticastSocket ms = new MulticastSocket(10000);  // 2. 创建一个箱子,用于接收数据  DatagramPacket dp = new DatagramPacket(new byte[1024],1024);  // 3. 把当前计算机绑定一个组播地址,表示添加到这一组中.  ms.joinGroup(InetAddress.getByName(\u0026#34;224.0.1.0\u0026#34;));  // 4. 将数据接收到箱子中  ms.receive(dp);  // 5. 解析数据包,并打印数据  byte[] data = dp.getData();  int length = dp.getLength();  System.out.println(new String(data,0,length));  // 6. 释放资源  ms.close();  } }   2.6UDP广播实现【理解】   实现步骤\n 发送端  创建发送端Socket对象(DatagramSocket) 创建存储数据的箱子,将广播地址封装进去 发送数据 释放资源   接收端  创建接收端的Socket对象(DatagramSocket) 创建一个数据包，用于接收数据 调用DatagramSocket对象的方法接收数据 解析数据包，并把数据在控制台显示 关闭接收端      代码实现\n// 发送端 public class ClientDemo {  public static void main(String[] args) throws IOException {  // 1. 创建发送端Socket对象(DatagramSocket)  DatagramSocket ds = new DatagramSocket(); \t// 2. 创建存储数据的箱子,将广播地址封装进去  String s = \u0026#34;广播 hello\u0026#34;;  byte[] bytes = s.getBytes();  InetAddress address = InetAddress.getByName(\u0026#34;255.255.255.255\u0026#34;);  int port = 10000;  DatagramPacket dp = new DatagramPacket(bytes,bytes.length,address,port); \t// 3. 发送数据  ds.send(dp); \t// 4. 释放资源  ds.close();  } } // 接收端 public class ServerDemo {  public static void main(String[] args) throws IOException {  // 1. 创建接收端的Socket对象(DatagramSocket)  DatagramSocket ds = new DatagramSocket(10000);  // 2. 创建一个数据包，用于接收数据  DatagramPacket dp = new DatagramPacket(new byte[1024],1024);  // 3. 调用DatagramSocket对象的方法接收数据  ds.receive(dp);  // 4. 解析数据包，并把数据在控制台显示  byte[] data = dp.getData();  int length = dp.getLength();  System.out.println(new String(data,0,length));  // 5. 关闭接收端  ds.close();  } }   3.TCP通信程序 3.1TCP发送数据【应用】   Java中的TCP通信\n Java对基于TCP协议的的网络提供了良好的封装，使用Socket对象来代表两端的通信端口，并通过Socket产生IO流来进行网络通信。 Java为客户端提供了Socket类，为服务器端提供了ServerSocket类    构造方法\n   方法名 说明     Socket(InetAddress address,int port) 创建流套接字并将其连接到指定IP指定端口号   Socket(String host, int port) 创建流套接字并将其连接到指定主机上的指定端口号      相关方法\n   方法名 说明     InputStream getInputStream() 返回此套接字的输入流   OutputStream getOutputStream() 返回此套接字的输出流      示例代码\npublic class ClientDemo {  public static void main(String[] args) throws IOException {  //创建客户端的Socket对象(Socket)  //Socket(String host, int port) 创建流套接字并将其连接到指定主机上的指定端口号  Socket s = new Socket(\u0026#34;127.0.0.1\u0026#34;,10000);   //获取输出流，写数据  //OutputStream getOutputStream() 返回此套接字的输出流  OutputStream os = s.getOutputStream();  os.write(\u0026#34;hello,tcp,我来了\u0026#34;.getBytes());   //释放资源  s.close();  } }   3.2TCP接收数据【应用】   构造方法\n   方法名 说明     ServletSocket(int port) 创建绑定到指定端口的服务器套接字      相关方法\n   方法名 说明     Socket accept() 监听要连接到此的套接字并接受它      注意事项\n accept方法是阻塞的,作用就是等待客户端连接 客户端创建对象并连接服务器,此时是通过三次握手协议,保证跟服务器之间的连接 针对客户端来讲,是往外写的,所以是输出流 针对服务器来讲,是往里读的,所以是输入流 read方法也是阻塞的 客户端在关流的时候,还多了一个往服务器写结束标记的动作 最后一步断开连接,通过四次挥手协议保证连接终止    三次握手和四次挥手\n  三次握手\n  四次挥手\n    示例代码\npublic class ServerDemo {  public static void main(String[] args) throws IOException {  //创建服务器端的Socket对象(ServerSocket)  //ServerSocket(int port) 创建绑定到指定端口的服务器套接字  ServerSocket ss = new ServerSocket(10000);   //Socket accept() 侦听要连接到此套接字并接受它  Socket s = ss.accept();   //获取输入流，读数据，并把数据显示在控制台  InputStream is = s.getInputStream();  byte[] bys = new byte[1024];  int len = is.read(bys);  String data = new String(bys,0,len);  System.out.println(\u0026#34;数据是：\u0026#34; + data);   //释放资源  s.close();  ss.close();  } }   3.3TCP程序练习【应用】   案例需求\n客户端：发送数据，接受服务器反馈\n服务器：收到消息后给出反馈\n  案例分析\n 客户端创建对象，使用输出流输出数据 服务端创建对象，使用输入流接受数据 服务端使用输出流给出反馈数据 客户端使用输入流接受反馈数据    代码实现\n// 客户端 public class ClientDemo {  public static void main(String[] args) throws IOException {  Socket socket = new Socket(\u0026#34;127.0.0.1\u0026#34;,10000);   OutputStream os = socket.getOutputStream();  os.write(\u0026#34;hello\u0026#34;.getBytes());  // os.close();如果在这里关流,会导致整个socket都无法使用  socket.shutdownOutput();//仅仅关闭输出流.并写一个结束标记,对socket没有任何影响   BufferedReader br = new BufferedReader(new InputStreamReader(socket.getInputStream()));  String line;  while((line = br.readLine())!=null){  System.out.println(line);  }  br.close();  os.close();  socket.close();  } } // 服务器 public class ServerDemo {  public static void main(String[] args) throws IOException {  ServerSocket ss = new ServerSocket(10000);   Socket accept = ss.accept();   InputStream is = accept.getInputStream();  int b;  while((b = is.read())!=-1){  System.out.println((char) b);  }   System.out.println(\u0026#34;看看我执行了吗?\u0026#34;);   BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(accept.getOutputStream()));  bw.write(\u0026#34;你谁啊?\u0026#34;);  bw.newLine();  bw.flush();   bw.close();  is.close();  accept.close();  ss.close();  } }   3.4TCP程序文件上传练习【应用】   案例需求\n客户端：数据来自于本地文件，接收服务器反馈\n服务器：接收到的数据写入本地文件，给出反馈\n  案例分析\n 创建客户端对象，创建输入流对象指向文件，每读一次数据就给服务器输出一次数据，输出结束后使用shutdownOutput()方法告知服务端传输结束 创建服务器对象，创建输出流对象指向文件，每接受一次数据就使用输出流输出到文件中，传输结束后。使用输出流给客户端反馈信息 客户端接受服务端的回馈信息    相关方法\n   方法名 说明     void shutdownInput() 将此套接字的输入流放置在“流的末尾”   void shutdownOutput() 禁止用此套接字的输出流      代码实现\n// 客户端 public class ClientDemo {  public static void main(String[] args) throws IOException {  Socket socket = new Socket(\u0026#34;127.0.0.1\u0026#34;,10000);   //是本地的流,用来读取本地文件的.  BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;socketmodule\\\\ClientDir\\\\1.jpg\u0026#34;));   //写到服务器 --- 网络中的流  OutputStream os = socket.getOutputStream();  BufferedOutputStream bos = new BufferedOutputStream(os);   int b;  while((b = bis.read())!=-1){  bos.write(b);//通过网络写到服务器中  }  bos.flush();  //给服务器一个结束标记,告诉服务器文件已经传输完毕  socket.shutdownOutput();   BufferedReader br = new BufferedReader(new InputStreamReader(socket.getInputStream()));  String line;  while((line = br.readLine()) !=null){  System.out.println(line);  }  bis.close();  socket.close();  } } // 服务器 public class ServerDemo {  public static void main(String[] args) throws IOException {  ServerSocket ss = new ServerSocket(10000);   Socket accept = ss.accept();   //网络中的流,从客户端读取数据的  BufferedInputStream bis = new BufferedInputStream(accept.getInputStream());  //本地的IO流,把数据写到本地中,实现永久化存储  BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;socketmodule\\\\ServerDir\\\\copy.jpg\u0026#34;));   int b;  while((b = bis.read()) !=-1){  bos.write(b);  }   BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(accept.getOutputStream()));  bw.write(\u0026#34;上传成功\u0026#34;);  bw.newLine();  bw.flush();   bos.close();  accept.close();  ss.close();  } }   3.5TCP程序服务器优化【应用】   优化方案一\n  需求\n服务器只能处理一个客户端请求，接收完一个图片之后，服务器就关闭了。\n  解决方案\n使用循环\n  代码实现\n// 服务器代码如下,客户端代码同上个案例,此处不再给出 public class ServerDemo {  public static void main(String[] args) throws IOException {  ServerSocket ss = new ServerSocket(10000);   while (true) {  Socket accept = ss.accept();   //网络中的流,从客户端读取数据的  BufferedInputStream bis = new BufferedInputStream(accept.getInputStream());  //本地的IO流,把数据写到本地中,实现永久化存储  BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;optimizeserver\\\\ServerDir\\\\copy.jpg\u0026#34;));   int b;  while((b = bis.read()) !=-1){  bos.write(b);  }   BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(accept.getOutputStream()));  bw.write(\u0026#34;上传成功\u0026#34;);  bw.newLine();  bw.flush();   bos.close();  accept.close();  }  //ss.close();   } }     优化方案二\n  需求\n第二次上传文件的时候，会把第一次的文件给覆盖。\n  解决方案\nUUID. randomUUID()方法生成随机的文件名\n  代码实现\n// 服务器代码如下,客户端代码同上个案例,此处不再给出 public class ServerDemo {  public static void main(String[] args) throws IOException {  ServerSocket ss = new ServerSocket(10000);   while (true) {  Socket accept = ss.accept();   //网络中的流,从客户端读取数据的  BufferedInputStream bis = new BufferedInputStream(accept.getInputStream());  //本地的IO流,把数据写到本地中,实现永久化存储  BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;optimizeserver\\\\ServerDir\\\\\u0026#34; + UUID.randomUUID().toString() + \u0026#34;.jpg\u0026#34;));   int b;  while((b = bis.read()) !=-1){  bos.write(b);  }   BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(accept.getOutputStream()));  bw.write(\u0026#34;上传成功\u0026#34;);  bw.newLine();  bw.flush();   bos.close();  accept.close();  }  //ss.close();   } }     优化方案三\n  需求\n使用循环虽然可以让服务器处理多个客户端请求。但是还是无法同时跟多个客户端进行通信。\n  解决方案\n开启多线程处理\n  代码实现\n// 线程任务类 public class ThreadSocket implements Runnable {  private Socket acceptSocket;   public ThreadSocket(Socket accept) {  this.acceptSocket = accept;  }   @Override  public void run() {  BufferedOutputStream bos = null;  try {  //网络中的流,从客户端读取数据的  BufferedInputStream bis = new BufferedInputStream(acceptSocket.getInputStream());  //本地的IO流,把数据写到本地中,实现永久化存储  bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;optimizeserver\\\\ServerDir\\\\\u0026#34; + UUID.randomUUID().toString() + \u0026#34;.jpg\u0026#34;));   int b;  while((b = bis.read()) !=-1){  bos.write(b);  }   BufferedWriter bw = new BufferedWriter(new OutputStreamWriter(acceptSocket.getOutputStream()));  bw.write(\u0026#34;上传成功\u0026#34;);  bw.newLine();  bw.flush();  } catch (IOException e) {  e.printStackTrace();  } finally {  if(bos != null){  try {  bos.close();  } catch (IOException e) {  e.printStackTrace();  }  }   if (acceptSocket != null){  try {  acceptSocket.close();  } catch (IOException e) {  e.printStackTrace();  }  }  }  } } // 服务器代码 public class ServerDemo {  public static void main(String[] args) throws IOException {  ServerSocket ss = new ServerSocket(10000);   while (true) {  Socket accept = ss.accept();  ThreadSocket ts = new ThreadSocket(accept);  new Thread(ts).start();  }  //ss.close();  } }     优化方案四\n  需求\n使用多线程虽然可以让服务器同时处理多个客户端请求。但是资源消耗太大。\n  解决方案\n加入线程池\n  代码实现\n// 服务器代码如下,线程任务类代码同上,此处不再给出 public class ServerDemo {  public static void main(String[] args) throws IOException {  ServerSocket ss = new ServerSocket(10000);  ThreadPoolExecutor pool = new ThreadPoolExecutor(  3,//核心线程数量  10, //线程池的总数量  60, //临时线程空闲时间  TimeUnit.SECONDS, //临时线程空闲时间的单位  new ArrayBlockingQueue\u0026lt;\u0026gt;(5),//阻塞队列  Executors.defaultThreadFactory(),//创建线程的方式  new ThreadPoolExecutor.AbortPolicy()//任务拒绝策略  );   while (true) {  Socket accept = ss.accept();  ThreadSocket ts = new ThreadSocket(accept);  //new Thread(ts).start();  pool.submit(ts);  }  //ss.close();  } }     4.NIO 4.1概述【理解】   BIO\nBlocking IO,阻塞型IO\n  NIO\nNo Blocking IO,非阻塞型IO\n  阻塞IO的弊端\n在等待的过程中,什么事也做不了\n  非阻塞IO的好处\n不需要一直等待,当一切就绪了再去做\n  4.2NIO与BIO的区别【理解】   区别一\nBIO是阻塞的，NIO是非阻塞的\n  区别二\nBIO是面向流的，NIO是面向缓冲区的\nBIO中数据传输是单向的，NIO中的缓冲区是双向的\n  4.3NIO三大模块【理解】   缓冲区\n用来存储数据\n  通道\n用来建立连接和传输数据\n  选择器\n监视通道状态\n  4.4NIO创建缓冲区对象【应用】   方法介绍\n   方法名 说明     static ByteBuffer allocate(长度) 创建byte类型的缓冲区   static ByteBuffer wrap(byte[] array) 创建一个有内容的byte类型缓冲区      代码示例\npublic class CreateByteBufferDemo1 {  public static void main(String[] args) {  //method1();   //method2();   ByteBuffer wrap = ByteBuffer.wrap(\u0026#34;aaa\u0026#34;.getBytes());  for (int i = 0; i \u0026lt; 3; i++) {  System.out.println(wrap.get());  }  }   private static void method2() {  byte [] bytes = {97,98,99};  ByteBuffer byteBuffer2 = ByteBuffer.wrap(bytes);  //缓冲区的长度3  //缓冲区里面的内容就是字节数组的内容.  for (int i = 0; i \u0026lt; 3; i++) {  System.out.println(byteBuffer2.get());  }  System.out.println(byteBuffer2.get());  }   private static void method1() {  ByteBuffer byteBuffer1 = ByteBuffer.allocate(5);  //get  for (int i = 0; i \u0026lt; 5; i++) {  System.out.println(byteBuffer1.get());  }  System.out.println(byteBuffer1.get());  } }   4.5NIO缓冲区添加数据【应用】   方法介绍\n  代码示例\npublic class ByteBufferDemo2 {  public static void main(String[] args) { // int position()\t当前要操作的索引 // int limit() 最多能操作到哪个索引 // int capacity()\t缓冲区的总长度  ByteBuffer byteBuffer = ByteBuffer.allocate(10);  System.out.println(byteBuffer.position());//0  System.out.println(byteBuffer.limit());//10  System.out.println(byteBuffer.capacity());//10  // put(byte b)\t一次添加一个字节 // byteBuffer.put((byte) 97); // System.out.println(byteBuffer.position()); // System.out.println(byteBuffer.limit()); // System.out.println(byteBuffer.capacity());  // put(byte[] src)\t一次添加一个字节数组 // byteBuffer.put(\u0026#34;aaa\u0026#34;.getBytes()); // System.out.println(byteBuffer.position());//3 // System.out.println(byteBuffer.limit());//10 // System.out.println(byteBuffer.capacity());//10  // position(int newPosition) 修改position // byteBuffer.position(1);  // limit(int newLimit)\t修改limit // byteBuffer.limit(5); // System.out.println(byteBuffer.position()); // System.out.println(byteBuffer.limit()); // System.out.println(byteBuffer.capacity());  // int remaining()\t还有多少能操作 // boolean hasRemaining()\t是否还有能操作的   byteBuffer.put(\u0026#34;0123456789\u0026#34;.getBytes());  System.out.println(byteBuffer.remaining());  System.out.println(byteBuffer.hasRemaining());  } }   4.6NIO缓冲区获取数据【应用】   方法介绍\n   方法名 介绍     flip() 切换读写模式（写à读）   get() 读一个字节   get(byte[] dst) 读多个字节   get(int index) 读指定索引的字节   rewind() 将position设置为0，可以重复读   clear() 数据读写完毕（读-\u0026gt;写）   array() 将缓冲区转换成字节数组返回      代码示例\npublic class ByteBufferDemo3 {  public static void main(String[] args) {  ByteBuffer byteBuffer = ByteBuffer.allocate(10);  byteBuffer.put(\u0026#34;abc\u0026#34;.getBytes());  // flip() 切换读写模式（写读）  byteBuffer.flip(); // get() 读一个字节 // while(byteBuffer.limit() != byteBuffer.position()){ // System.out.println((char) byteBuffer.get()); // }   for (int i = 0; i \u0026lt; byteBuffer.limit(); i++) {  System.out.println((char) byteBuffer.get());  }  // get(byte[] dst) 读多个字节 // byte [] bytes = new byte[byteBuffer.limit()]; // byteBuffer.get(bytes); // System.out.println(new String(bytes));  // get(int index) 读指定索引的字节 // System.out.println((char) byteBuffer.get(0));  // rewind() 将position设置为0，可以重复读 // byteBuffer.rewind(); // for (int i = 0; i \u0026lt; byteBuffer.limit(); i++) { // System.out.println((char) byteBuffer.get()); // }  // clear() 数据读写完毕（读-\u0026gt;写）  byteBuffer.clear();  byteBuffer.put(\u0026#34;qqq\u0026#34;.getBytes()); // array() 将缓冲区转换成字节数组返回   byte[] bytes = byteBuffer.array();  System.out.println(new String(bytes));  } }   4.7小结【理解】   需求：我要把数据写到缓冲区中。\n数据是从外面进入到缓冲区的，所以缓冲区在做读数据的操作。\n  需求：我要把数据从缓冲区中读出来。\n数据是从缓冲区里面到外面的。所以缓冲区在做写数据的操作。\n  capacity：容量（长度） limit： 界限（最多能读/写到哪里） posotion：位置（读/写哪个索引）\n  获取缓冲区里面数据之前，需要调用flip方法\n  再次写数据之前，需要调用clear方法，\n但是数据还未消失，等再次写入数据，被覆盖了才会消失。\n  ","permalink":"https://iblog.zone/archives/java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B01/","summary":"1.网络编程入门 1.1 网络编程概述【理解】   计算机网络\n是指将地理位置不同的具有独立功能的多台计算机及其外部设备，通过通信线路连接起来，在网络操作系统，网络管理软件及网络通信协议的管理和协调下，实现资源共享和信息传递的计算机系统\n  网络编程\n在网络通信协议下，不同计算机上运行的程序，可以进行数据传输\n  1.2 网络编程三要素【理解】   IP地址\n要想让网络中的计算机能够互相通信，必须为每台计算机指定一个标识号，通过这个标识号来指定要接收数据的计算机和识别发送的计算机，而IP地址就是这个标识号。也就是设备的标识\n  端口\n网络的通信，本质上是两个应用程序的通信。每台计算机都有很多的应用程序，那么在网络通信时，如何区分这些应用程序呢？如果说IP地址可以唯一标识网络中的设备，那么端口号就可以唯一标识设备中的应用程序了。也就是应用程序的标识\n  协议\n通过计算机网络可以使多台计算机实现连接，位于同一个网络中的计算机在进行连接和通信时需要遵守一定的规则，这就好比在道路中行驶的汽车一定要遵守交通规则一样。在计算机网络中，这些连接和通信的规则被称为网络通信协议，它对数据的传输格式、传输速率、传输步骤等做了统一规定，通信双方必须同时遵守才能完成数据交换。常见的协议有UDP协议和TCP协议\n  1.3 IP地址【理解】 IP地址：是网络中设备的唯一标识\n  IP地址分为两大类\n  IPv4：是给每个连接在网络上的主机分配一个32bit地址。按照TCP/IP规定，IP地址用二进制来表示，每个IP地址长32bit，也就是4个字节。例如一个采用二进制形式的IP地址是“11000000 10101000 00000001 01000010”，这么长的地址，处理起来也太费劲了。为了方便使用，IP地址经常被写成十进制的形式，中间使用符号“.”分隔不同的字节。于是，上面的IP地址可以表示为“192.168.1.66”。IP地址的这种表示法叫做“点分十进制表示法”，这显然比1和0容易记忆得多\n  IPv6：由于互联网的蓬勃发展，IP地址的需求量愈来愈大，但是网络地址资源有限，使得IP的分配越发紧张。为了扩大地址空间，通过IPv6重新定义地址空间，采用128位地址长度，每16个字节一组，分成8组十六进制数，这样就解决了网络地址资源数量不够的问题\n    DOS常用命令：\n  ipconfig：查看本机IP地址\n  ping IP地址：检查网络是否连通\n    特殊IP地址：\n 127.0.0.1：是回送地址，可以代表本机地址，一般用来测试使用    1.4 InetAddress【应用】 InetAddress：此类表示Internet协议（IP）地址\n  相关方法","title":"Java网络编程01"},{"content":"用yum安装的nginx，报unknown directive “stream”\n解决方法：yum install nginx-mod-stream -y\n[root@localhost modules]# yum install nginx-mod-stream -y  已安装:  nginx-mod-stream.x86_64 1:1.20.1-2.el7 安装位置\n[root@localhost ~]# ll /usr/lib64/nginx/modules 总用量 176 -rwxr-xr-x 1 root root 179864 6月 2 08:24 ngx_stream_module.so nginx配置文件引入\n[root@localhost ~]# cat /etc/nginx/nginx.conf # 加载stream模块 load_module /usr/lib64/nginx/modules/ngx_stream_module.so; user nginx; worker_processes auto; ... ","permalink":"https://iblog.zone/archives/%E7%94%A8yum%E5%AE%89%E8%A3%85%E7%9A%84nginx%E6%8A%A5unknown-directive-stream/","summary":"用yum安装的nginx，报unknown directive “stream”\n解决方法：yum install nginx-mod-stream -y\n[root@localhost modules]# yum install nginx-mod-stream -y  已安装:  nginx-mod-stream.x86_64 1:1.20.1-2.el7 安装位置\n[root@localhost ~]# ll /usr/lib64/nginx/modules 总用量 176 -rwxr-xr-x 1 root root 179864 6月 2 08:24 ngx_stream_module.so nginx配置文件引入\n[root@localhost ~]# cat /etc/nginx/nginx.conf # 加载stream模块 load_module /usr/lib64/nginx/modules/ngx_stream_module.so; user nginx; worker_processes auto; ... ","title":"用yum安装的nginx，报unknown directive “stream”"},{"content":"1.线程池 1.1 线程状态介绍 当线程被创建并启动以后，它既不是一启动就进入了执行状态，也不是一直处于执行状态。线程对象在不同的时期有不同的状态。那么Java中的线程存在哪几种状态呢？Java中的线程\n状态被定义在了java.lang.Thread.State枚举类中，State枚举类的源码如下：\npublic class Thread {   public enum State {   /* 新建 */  NEW ,   /* 可运行状态 */  RUNNABLE ,   /* 阻塞状态 */  BLOCKED ,   /* 无限等待状态 */  WAITING ,   /* 计时等待 */  TIMED_WAITING ,   /* 终止 */  TERMINATED;  \t}   // 获取当前线程的状态  public State getState() {  return jdk.internal.misc.VM.toThreadState(threadStatus);  }  } 通过源码我们可以看到Java中的线程存在6种状态，每种线程状态的含义如下\n   线程状态 具体含义     NEW 一个尚未启动的线程的状态。也称之为初始状态、开始状态。线程刚被创建，但是并未启动。还没调用start方法。MyThread t = new MyThread()只有线程象，没有线程特征。   RUNNABLE 当我们调用线程对象的start方法，那么此时线程对象进入了RUNNABLE状态。那么此时才是真正的在JVM进程中创建了一个线程，线程一经启动并不是立即得到执行，线程的运行与否要听令与CPU的调度，那么我们把这个中间状态称之为可执行状态(RUNNABLE)也就是说它具备执行的资格，但是并没有真正的执行起来而是在等待CPU的度。   BLOCKED 当一个线程试图获取一个对象锁，而该对象锁被其他的线程持有，则该线程进入Blocked状态；当该线程持有锁时，该线程将变成Runnable状态。   WAITING 一个正在等待的线程的状态。也称之为等待状态。造成线程等待的原因有两种，分别是调用Object.wait()、join()方法。处于等待状态的线程，正在等待其他线程去执行一个特定的操作。例如：因为wait()而等待的线程正在等待另一个线程去调用notify()或notifyAll()；一个因为join()而等待的线程正在等待另一个线程结束。   TIMED_WAITING 一个在限定时间内等待的线程的状态。也称之为限时等待状态。造成线程限时等待状态的原因有三种，分别是：Thread.sleep(long)，Object.wait(long)、join(long)。   TERMINATED 一个完全运行完成的线程的状态。也称之为终止状态、结束状态    各个状态的转换，如下图所示：\n1.2 线程的状态-练习1 目的 : 本案例主要演示TIME_WAITING的状态转换。\n**需求：**编写一段代码，依次显示一个线程的这些状态：NEW -\u0026gt; RUNNABLE -\u0026gt; TIME_WAITING -\u0026gt; RUNNABLE -\u0026gt; TERMINATED\n为了简化我们的开发，本次我们使用匿名内部类结合lambda表达式的方式使用多线程。\n代码实现\npublic class ThreadStateDemo01 {   public static void main(String[] args) throws InterruptedException {   //定义一个内部线程  Thread thread = new Thread(() -\u0026gt; {  System.out.println(\u0026#34;2.执行thread.start()之后，线程的状态：\u0026#34; + Thread.currentThread().getState());  try {  //休眠100毫秒  Thread.sleep(100);  } catch (InterruptedException e) {  e.printStackTrace();  }  System.out.println(\u0026#34;4.执行Thread.sleep(long)完成之后，线程的状态：\u0026#34; + Thread.currentThread().getState());  });   //获取start()之前的状态  System.out.println(\u0026#34;1.通过new初始化一个线程，但是还没有start()之前，线程的状态：\u0026#34; + thread.getState());   //启动线程  thread.start();   //休眠50毫秒  Thread.sleep(50);   //因为thread1需要休眠100毫秒，所以在第50毫秒，thread处于sleep状态  System.out.println(\u0026#34;3.执行Thread.sleep(long)时，线程的状态：\u0026#34; + thread.getState());   //thread1和main线程主动休眠150毫秒，所以在第150毫秒,thread早已执行完毕  Thread.sleep(100);   System.out.println(\u0026#34;5.线程执行完毕之后，线程的状态：\u0026#34; + thread.getState() + \u0026#34;\\n\u0026#34;);   }  } 控制台输出\n1.通过new初始化一个线程，但是还没有start()之前，线程的状态：NEW 2.执行thread.start()之后，线程的状态：RUNNABLE 3.执行Thread.sleep(long)时，线程的状态：TIMED_WAITING 4.执行Thread.sleep(long)完成之后，线程的状态：RUNNABLE 5.线程执行完毕之后，线程的状态：TERMINATED  1.3 线程的状态-练习2 目的 : 本案例主要演示WAITING的状态转换。\n**需求 ：**编写一段代码，依次显示一个线程的这些状态：NEW -\u0026gt; RUNNABLE -\u0026gt; WAITING -\u0026gt; RUNNABLE -\u0026gt; TERMINATED\n代码实现 :\npublic class ThreadStateDemo02 {   public static void main(String[] args) throws InterruptedException {   //定义一个对象，用来加锁和解锁  Object obj = new Object();   //定义一个内部线程  Thread thread1 = new Thread(() -\u0026gt; {  System.out.println(\u0026#34;2.执行thread.start()之后，线程的状态：\u0026#34; + Thread.currentThread().getState());  synchronized (obj) {  try {   //thread1需要休眠100毫秒  Thread.sleep(100);   //thread1100毫秒之后，通过wait()方法释放obj对象是锁  obj.wait();   } catch (InterruptedException e) {  e.printStackTrace();  }  }  System.out.println(\u0026#34;4.被object.notify()方法唤醒之后，线程的状态：\u0026#34; + Thread.currentThread().getState());  });   //获取start()之前的状态  System.out.println(\u0026#34;1.通过new初始化一个线程，但是还没有start()之前，线程的状态：\u0026#34; + thread1.getState());   //启动线程  thread1.start();   //main线程休眠150毫秒  Thread.sleep(150);   //因为thread1在第100毫秒进入wait等待状态，所以第150秒肯定可以获取其状态  System.out.println(\u0026#34;3.执行object.wait()时，线程的状态：\u0026#34; + thread1.getState());   //声明另一个线程进行解锁  new Thread(() -\u0026gt; {  synchronized (obj) {  //唤醒等待的线程  obj.notify();  }  }).start();   //main线程休眠10毫秒等待thread1线程能够苏醒  Thread.sleep(10);   //获取thread1运行结束之后的状态  System.out.println(\u0026#34;5.线程执行完毕之后，线程的状态：\u0026#34; + thread1.getState() + \u0026#34;\\n\u0026#34;);   }  } 控制台输出结果\n1.通过new初始化一个线程，但是还没有start()之前，线程的状态：NEW 2.执行thread.start()之后，线程的状态：RUNNABLE 3.执行object.wait()时，线程的状态：WAITING 4.被object.notify()方法唤醒之后，线程的状态：RUNNABLE 5.线程执行完毕之后，线程的状态：TERMINATED  1.4 线程的状态-练习3 目的 : 本案例主要演示BLOCKED的状态转换。\n**需求 ：**编写一段代码，依次显示一个线程的这些状态：NEW -\u0026gt; RUNNABLE -\u0026gt; BLOCKED -\u0026gt; RUNNABLE -\u0026gt; TERMINATED\npublic class ThreadStateDemo03 {   public static void main(String[] args) throws InterruptedException {   //定义一个对象，用来加锁和解锁  Object obj2 = new Object();   //定义一个线程，先抢占了obj2对象的锁  new Thread(() -\u0026gt; {  synchronized (obj2) {  try {  Thread.sleep(100); //第一个线程要持有锁100毫秒  obj2.wait(); //然后通过wait()方法进行等待状态，并释放obj2的对象锁  } catch (InterruptedException e) {  e.printStackTrace();  }  }  }).start();   //定义目标线程，获取等待获取obj2的锁  Thread thread = new Thread(() -\u0026gt; {  System.out.println(\u0026#34;2.执行thread.start()之后，线程的状态：\u0026#34; + Thread.currentThread().getState());  synchronized (obj2) {  try {  Thread.sleep(100); //thread3要持有对象锁100毫秒  obj2.notify(); //然后通过notify()方法唤醒所有在ojb2上等待的线程继续执行后续操作  } catch (InterruptedException e) {  e.printStackTrace();  }  }  System.out.println(\u0026#34;4.阻塞结束后，线程的状态：\u0026#34; + Thread.currentThread().getState());  });   //获取start()之前的状态  System.out.println(\u0026#34;1.通过new初始化一个线程，但是还没有thread.start()之前，线程的状态：\u0026#34; + thread.getState());   //启动线程  thread.start();   //先等100毫秒  Thread.sleep(50);   //第一个线程释放锁至少需要100毫秒，所以在第50毫秒时，thread正在因等待obj的对象锁而阻塞  System.out.println(\u0026#34;3.因为等待锁而阻塞时，线程的状态：\u0026#34; + thread.getState());   //再等300毫秒  Thread.sleep(300);   //两个线程的执行时间加上之前等待的50毫秒总共是250毫秒，所以第300毫秒，所有的线程都已经执行完毕  System.out.println(\u0026#34;5.线程执行完毕之后，线程的状态：\u0026#34; + thread.getState());   }  } 控制台输出结果\n1.通过new初始化一个线程，但是还没有thread.start()之前，线程的状态：NEW 2.执行thread.start()之后，线程的状态：RUNNABLE 3.因为等待锁而阻塞时，线程的状态：BLOCKED 4.阻塞结束后，线程的状态：RUNNABLE 5.线程执行完毕之后，线程的状态：TERMINATED 1.5 线程池-基本原理 概述 :\n​\t提到池，大家应该能想到的就是水池。水池就是一个容器，在该容器中存储了很多的水。那么什么是线程池呢？线程池也是可以看做成一个池子，在该池子中存储很多个线程。\n线程池存在的意义：\n​\t系统创建一个线程的成本是比较高的，因为它涉及到与操作系统交互，当程序中需要创建大量生存期很短暂的线程时，频繁的创建和销毁线程对系统的资源消耗有可能大于业务处理是对系\n​\t统资源的消耗，这样就有点\u0026quot;舍本逐末\u0026quot;了。针对这一种情况，为了提高性能，我们就可以采用线程池。线程池在启动的时，会创建大量空闲线程，当我们向线程池提交任务的时，线程池就\n​\t会启动一个线程来执行该任务。等待任务执行完毕以后，线程并不会死亡，而是再次返回到线程池中称为空闲状态。等待下一次任务的执行。\n线程池的设计思路 :\n 准备一个任务容器 一次性启动多个(2个)消费者线程 刚开始任务容器是空的，所以线程都在wait 直到一个外部线程向这个任务容器中扔了一个\u0026quot;任务\u0026quot;，就会有一个消费者线程被唤醒 这个消费者线程取出\u0026quot;任务\u0026quot;，并且执行这个任务，执行完毕后，继续等待下一次任务的到来  1.6 线程池-Executors默认线程池 概述 : JDK对线程池也进行了相关的实现，在真实企业开发中我们也很少去自定义线程池，而是使用JDK中自带的线程池。\n我们可以使用Executors中所提供的静态方法来创建线程池\n​\tstatic ExecutorService newCachedThreadPool() 创建一个默认的线程池 ​\tstatic newFixedThreadPool(int nThreads)\t创建一个指定最多线程数量的线程池\n代码实现 :\npackage com.itheima.mythreadpool;   //static ExecutorService newCachedThreadPool() 创建一个默认的线程池 //static newFixedThreadPool(int nThreads)\t创建一个指定最多线程数量的线程池  import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors;  public class MyThreadPoolDemo {  public static void main(String[] args) throws InterruptedException {   //1,创建一个默认的线程池对象.池子中默认是空的.默认最多可以容纳int类型的最大值.  ExecutorService executorService = Executors.newCachedThreadPool();  //Executors --- 可以帮助我们创建线程池对象  //ExecutorService --- 可以帮助我们控制线程池   executorService.submit(()-\u0026gt;{  System.out.println(Thread.currentThread().getName() + \u0026#34;在执行了\u0026#34;);  });   //Thread.sleep(2000);   executorService.submit(()-\u0026gt;{  System.out.println(Thread.currentThread().getName() + \u0026#34;在执行了\u0026#34;);  });   executorService.shutdown();  } } 1.7 线程池-Executors创建指定上限的线程池 使用Executors中所提供的静态方法来创建线程池\n​\tstatic ExecutorService newFixedThreadPool(int nThreads) : 创建一个指定最多线程数量的线程池\n代码实现 :\npackage com.itheima.mythreadpool;  //static ExecutorService newFixedThreadPool(int nThreads) //创建一个指定最多线程数量的线程池  import java.util.concurrent.ExecutorService; import java.util.concurrent.Executors; import java.util.concurrent.ThreadPoolExecutor;  public class MyThreadPoolDemo2 {  public static void main(String[] args) {  //参数不是初始值而是最大值  ExecutorService executorService = Executors.newFixedThreadPool(10);   ThreadPoolExecutor pool = (ThreadPoolExecutor) executorService;  System.out.println(pool.getPoolSize());//0   executorService.submit(()-\u0026gt;{  System.out.println(Thread.currentThread().getName() + \u0026#34;在执行了\u0026#34;);  });   executorService.submit(()-\u0026gt;{  System.out.println(Thread.currentThread().getName() + \u0026#34;在执行了\u0026#34;);  });   System.out.println(pool.getPoolSize());//2 // executorService.shutdown();  } } 1.8 线程池-ThreadPoolExecutor 创建线程池对象 :\nThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(核心线程数量,最大线程数量,空闲线程最大存活时间,任务队列,创建线程工厂,任务的拒绝策略);\n代码实现 :\npackage com.itheima.mythreadpool;  import java.util.concurrent.ArrayBlockingQueue; import java.util.concurrent.Executors; import java.util.concurrent.ThreadPoolExecutor; import java.util.concurrent.TimeUnit;  public class MyThreadPoolDemo3 { // 参数一：核心线程数量 // 参数二：最大线程数 // 参数三：空闲线程最大存活时间 // 参数四：时间单位 // 参数五：任务队列 // 参数六：创建线程工厂 // 参数七：任务的拒绝策略  public static void main(String[] args) {  ThreadPoolExecutor pool = new ThreadPoolExecutor(2,5,2,TimeUnit.SECONDS,new ArrayBlockingQueue\u0026lt;\u0026gt;(10), Executors.defaultThreadFactory(),new ThreadPoolExecutor.AbortPolicy());  pool.submit(new MyRunnable());  pool.submit(new MyRunnable());   pool.shutdown();  } } 1.9 线程池-参数详解 public ThreadPoolExecutor(int corePoolSize,  int maximumPoolSize,  long keepAliveTime,  TimeUnit unit,  BlockingQueue\u0026lt;Runnable\u0026gt; workQueue,  ThreadFactory threadFactory,  RejectedExecutionHandler handler)  corePoolSize： 核心线程的最大值，不能小于0 maximumPoolSize：最大线程数，不能小于等于0，maximumPoolSize \u0026gt;= corePoolSize keepAliveTime： 空闲线程最大存活时间,不能小于0 unit： 时间单位 workQueue： 任务队列，不能为null threadFactory： 创建线程工厂,不能为null handler： 任务的拒绝策略,不能为null 1.10 线程池-非默认任务拒绝策略 RejectedExecutionHandler是jdk提供的一个任务拒绝策略接口，它下面存在4个子类。\nThreadPoolExecutor.AbortPolicy: 丢弃任务并抛出RejectedExecutionException异常。是默认的策略。 ThreadPoolExecutor.DiscardPolicy： 丢弃任务，但是不抛出异常 这是不推荐的做法。 ThreadPoolExecutor.DiscardOldestPolicy： 抛弃队列中等待最久的任务 然后把当前任务加入队列中。 ThreadPoolExecutor.CallerRunsPolicy: 调用任务的run()方法绕过线程池直接执行。 注：明确线程池对多可执行的任务数 = 队列容量 + 最大线程数\n案例演示1：演示ThreadPoolExecutor.AbortPolicy任务处理策略\npublic class ThreadPoolExecutorDemo01 {   public static void main(String[] args) {   /** * 核心线程数量为1 ， 最大线程池数量为3, 任务容器的容量为1 ,空闲线程的最大存在时间为20s */  ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(1 , 3 , 20 , TimeUnit.SECONDS ,  new ArrayBlockingQueue\u0026lt;\u0026gt;(1) , Executors.defaultThreadFactory() , new ThreadPoolExecutor.AbortPolicy()) ;   // 提交5个任务，而该线程池最多可以处理4个任务，当我们使用AbortPolicy这个任务处理策略的时候，就会抛出异常  for(int x = 0 ; x \u0026lt; 5 ; x++) {  threadPoolExecutor.submit(() -\u0026gt; {  System.out.println(Thread.currentThread().getName() + \u0026#34;----\u0026gt;\u0026gt; 执行了任务\u0026#34;);  });  }  } } 控制台输出结果\npool-1-thread-1----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-3----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-2----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-3----\u0026gt;\u0026gt; 执行了任务 控制台报错，仅仅执行了4个任务，有一个任务被丢弃了\n案例演示2：演示ThreadPoolExecutor.DiscardPolicy任务处理策略\npublic class ThreadPoolExecutorDemo02 {  public static void main(String[] args) {  /** * 核心线程数量为1 ， 最大线程池数量为3, 任务容器的容量为1 ,空闲线程的最大存在时间为20s */  ThreadPoolExecutor threadPoolExecutor = new ThreadPoolExecutor(1 , 3 , 20 , TimeUnit.SECONDS ,  new ArrayBlockingQueue\u0026lt;\u0026gt;(1) , Executors.defaultThreadFactory() , new ThreadPoolExecutor.DiscardPolicy()) ;   // 提交5个任务，而该线程池最多可以处理4个任务，当我们使用DiscardPolicy这个任务处理策略的时候，控制台不会报错  for(int x = 0 ; x \u0026lt; 5 ; x++) {  threadPoolExecutor.submit(() -\u0026gt; {  System.out.println(Thread.currentThread().getName() + \u0026#34;----\u0026gt;\u0026gt; 执行了任务\u0026#34;);  });  }  } } 控制台输出结果\npool-1-thread-1----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-1----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-3----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-2----\u0026gt;\u0026gt; 执行了任务 控制台没有报错，仅仅执行了4个任务，有一个任务被丢弃了\n案例演示3：演示ThreadPoolExecutor.DiscardOldestPolicy任务处理策略\npublic class ThreadPoolExecutorDemo02 {  public static void main(String[] args) {  /** * 核心线程数量为1 ， 最大线程池数量为3, 任务容器的容量为1 ,空闲线程的最大存在时间为20s */  ThreadPoolExecutor threadPoolExecutor;  threadPoolExecutor = new ThreadPoolExecutor(1 , 3 , 20 , TimeUnit.SECONDS ,  new ArrayBlockingQueue\u0026lt;\u0026gt;(1) , Executors.defaultThreadFactory() , new ThreadPoolExecutor.DiscardOldestPolicy());  // 提交5个任务  for(int x = 0 ; x \u0026lt; 5 ; x++) {  // 定义一个变量，来指定指定当前执行的任务;这个变量需要被final修饰  final int y = x ;  threadPoolExecutor.submit(() -\u0026gt; {  System.out.println(Thread.currentThread().getName() + \u0026#34;----\u0026gt;\u0026gt; 执行了任务\u0026#34; + y);  });  }  } } 控制台输出结果\npool-1-thread-2----\u0026gt;\u0026gt; 执行了任务2 pool-1-thread-1----\u0026gt;\u0026gt; 执行了任务0 pool-1-thread-3----\u0026gt;\u0026gt; 执行了任务3 pool-1-thread-1----\u0026gt;\u0026gt; 执行了任务4 由于任务1在线程池中等待时间最长，因此任务1被丢弃。\n案例演示4：演示ThreadPoolExecutor.CallerRunsPolicy任务处理策略\npublic class ThreadPoolExecutorDemo04 {  public static void main(String[] args) {   /** * 核心线程数量为1 ， 最大线程池数量为3, 任务容器的容量为1 ,空闲线程的最大存在时间为20s */  ThreadPoolExecutor threadPoolExecutor;  threadPoolExecutor = new ThreadPoolExecutor(1 , 3 , 20 , TimeUnit.SECONDS ,  new ArrayBlockingQueue\u0026lt;\u0026gt;(1) , Executors.defaultThreadFactory() , new ThreadPoolExecutor.CallerRunsPolicy());   // 提交5个任务  for(int x = 0 ; x \u0026lt; 5 ; x++) {  threadPoolExecutor.submit(() -\u0026gt; {  System.out.println(Thread.currentThread().getName() + \u0026#34;----\u0026gt;\u0026gt; 执行了任务\u0026#34;);  });  }  } } 控制台输出结果\npool-1-thread-1----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-3----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-2----\u0026gt;\u0026gt; 执行了任务 pool-1-thread-1----\u0026gt;\u0026gt; 执行了任务 main----\u0026gt;\u0026gt; 执行了任务 通过控制台的输出，我们可以看到次策略没有通过线程池中的线程执行任务，而是直接调用任务的run()方法绕过线程池直接执行。\n2. 原子性 2.1 volatile-问题 代码分析 :\npackage com.itheima.myvolatile;  public class Demo {  public static void main(String[] args) {  MyThread1 t1 = new MyThread1();  t1.setName(\u0026#34;小路同学\u0026#34;);  t1.start();   MyThread2 t2 = new MyThread2();  t2.setName(\u0026#34;小皮同学\u0026#34;);  t2.start();  } } package com.itheima.myvolatile;  public class Money {  public static int money = 100000; } package com.itheima.myvolatile;  public class MyThread1 extends Thread {  @Override  public void run() {  while(Money.money == 100000){   }   System.out.println(\u0026#34;结婚基金已经不是十万了\u0026#34;);  } } package com.itheima.myvolatile;  public class MyThread2 extends Thread {  @Override  public void run() {  try {  Thread.sleep(10);  } catch (InterruptedException e) {  e.printStackTrace();  }   Money.money = 90000;  } } 程序问题 : 女孩虽然知道结婚基金是十万，但是当基金的余额发生变化的时候，女孩无法知道最新的余额。\n2.2 volatile解决 以上案例出现的问题 :\n​\t当A线程修改了共享数据时，B线程没有及时获取到最新的值，如果还在使用原先的值，就会出现问题\n​\t1，堆内存是唯一的，每一个线程都有自己的线程栈。\n​\t2 ，每一个线程在使用堆里面变量的时候，都会先拷贝一份到变量的副本中。\n​\t3 ，在线程中，每一次使用是从变量的副本中获取的。\nVolatile关键字 : 强制线程每次在使用的时候，都会看一下共享区域最新的值\n代码实现 : 使用volatile关键字解决\npackage com.itheima.myvolatile;  public class Demo {  public static void main(String[] args) {  MyThread1 t1 = new MyThread1();  t1.setName(\u0026#34;小路同学\u0026#34;);  t1.start();   MyThread2 t2 = new MyThread2();  t2.setName(\u0026#34;小皮同学\u0026#34;);  t2.start();  } } package com.itheima.myvolatile;  public class Money {  public static volatile int money = 100000; } package com.itheima.myvolatile;  public class MyThread1 extends Thread {  @Override  public void run() {  while(Money.money == 100000){   }   System.out.println(\u0026#34;结婚基金已经不是十万了\u0026#34;);  } } package com.itheima.myvolatile;  public class MyThread2 extends Thread {  @Override  public void run() {  try {  Thread.sleep(10);  } catch (InterruptedException e) {  e.printStackTrace();  }   Money.money = 90000;  } } 2.3 synchronized解决 synchronized解决 :\n​\t1 ，线程获得锁\n​\t2 ，清空变量副本\n​\t3 ，拷贝共享变量最新的值到变量副本中\n​\t4 ，执行代码\n​\t5 ，将修改后变量副本中的值赋值给共享数据\n​\t6 ，释放锁\n代码实现 :\npackage com.itheima.myvolatile2;  public class Demo {  public static void main(String[] args) {  MyThread1 t1 = new MyThread1();  t1.setName(\u0026#34;小路同学\u0026#34;);  t1.start();   MyThread2 t2 = new MyThread2();  t2.setName(\u0026#34;小皮同学\u0026#34;);  t2.start();  } } package com.itheima.myvolatile2;  public class Money {  public static Object lock = new Object();  public static volatile int money = 100000; } package com.itheima.myvolatile2;  public class MyThread1 extends Thread {  @Override  public void run() {  while(true){  synchronized (Money.lock){  if(Money.money != 100000){  System.out.println(\u0026#34;结婚基金已经不是十万了\u0026#34;);  break;  }  }  }  } } package com.itheima.myvolatile2;  public class MyThread2 extends Thread {  @Override  public void run() {  synchronized (Money.lock) {  try {  Thread.sleep(10);  } catch (InterruptedException e) {  e.printStackTrace();  }   Money.money = 90000;  }  } } 2.4 原子性 概述 : 所谓的原子性是指在一次操作或者多次操作中，要么所有的操作全部都得到了执行并且不会受到任何因素的干扰而中断，要么所有的操作都不执行，多个操作是一个不可以分割的整体。\n代码实现 :\npackage com.itheima.threadatom;  public class AtomDemo {  public static void main(String[] args) {  MyAtomThread atom = new MyAtomThread();   for (int i = 0; i \u0026lt; 100; i++) {  new Thread(atom).start();  }  } } class MyAtomThread implements Runnable {  private volatile int count = 0; //送冰淇淋的数量   @Override  public void run() {  for (int i = 0; i \u0026lt; 100; i++) {  //1,从共享数据中读取数据到本线程栈中.  //2,修改本线程栈中变量副本的值  //3,会把本线程栈中变量副本的值赋值给共享数据.  count++;  System.out.println(\u0026#34;已经送了\u0026#34; + count + \u0026#34;个冰淇淋\u0026#34;);  }  } } 代码总结 : count++ 不是一个原子性操作, 他在执行的过程中,有可能被其他线程打断\n2.5 volatile关键字不能保证原子性 解决方案 : 我们可以给count++操作添加锁，那么count++操作就是临界区中的代码，临界区中的代码一次只能被一个线程去执行，所以count++就变成了原子操作。\npackage com.itheima.threadatom2;  public class AtomDemo {  public static void main(String[] args) {  MyAtomThread atom = new MyAtomThread();   for (int i = 0; i \u0026lt; 100; i++) {  new Thread(atom).start();  }  } } class MyAtomThread implements Runnable {  private volatile int count = 0; //送冰淇淋的数量  private Object lock = new Object();   @Override  public void run() {  for (int i = 0; i \u0026lt; 100; i++) {  //1,从共享数据中读取数据到本线程栈中.  //2,修改本线程栈中变量副本的值  //3,会把本线程栈中变量副本的值赋值给共享数据.  synchronized (lock) {  count++;  System.out.println(\u0026#34;已经送了\u0026#34; + count + \u0026#34;个冰淇淋\u0026#34;);  }  }  } } 2.6 原子性_AtomicInteger 概述：java从JDK1.5开始提供了java.util.concurrent.atomic包(简称Atomic包)，这个包中的原子操作类提供了一种用法简单，性能高效，线程安全地更新一个变量的方式。因为变\n量的类型有很多种，所以在Atomic包里一共提供了13个类，属于4种类型的原子更新方式，分别是原子更新基本类型、原子更新数组、原子更新引用和原子更新属性(字段)。本次我们只讲解\n使用原子的方式更新基本类型，使用原子的方式更新基本类型Atomic包提供了以下3个类：\nAtomicBoolean： 原子更新布尔类型\nAtomicInteger： 原子更新整型\nAtomicLong：\t原子更新长整型\n以上3个类提供的方法几乎一模一样，所以本节仅以AtomicInteger为例进行讲解，AtomicInteger的常用方法如下：\npublic AtomicInteger()：\t初始化一个默认值为0的原子型Integer public AtomicInteger(int initialValue)： 初始化一个指定值的原子型Integer  int get(): 获取值 int getAndIncrement(): 以原子方式将当前值加1，注意，这里返回的是自增前的值。 int incrementAndGet(): 以原子方式将当前值加1，注意，这里返回的是自增后的值。 int addAndGet(int data):\t以原子方式将输入的数值与实例中的值（AtomicInteger里的value）相加，并返回结果。 int getAndSet(int value): 以原子方式设置为newValue的值，并返回旧值。 代码实现 :\npackage com.itheima.threadatom3;  import java.util.concurrent.atomic.AtomicInteger;  public class MyAtomIntergerDemo1 { // public AtomicInteger()：\t初始化一个默认值为0的原子型Integer // public AtomicInteger(int initialValue)： 初始化一个指定值的原子型Integer  public static void main(String[] args) {  AtomicInteger ac = new AtomicInteger();  System.out.println(ac);   AtomicInteger ac2 = new AtomicInteger(10);  System.out.println(ac2);  }  } package com.itheima.threadatom3;  import java.lang.reflect.Field; import java.util.concurrent.atomic.AtomicInteger;  public class MyAtomIntergerDemo2 { // int get(): 获取值 // int getAndIncrement(): 以原子方式将当前值加1，注意，这里返回的是自增前的值。 // int incrementAndGet(): 以原子方式将当前值加1，注意，这里返回的是自增后的值。 // int addAndGet(int data):\t以原子方式将参数与对象中的值相加，并返回结果。 // int getAndSet(int value): 以原子方式设置为newValue的值，并返回旧值。  public static void main(String[] args) { // AtomicInteger ac1 = new AtomicInteger(10); // System.out.println(ac1.get());  // AtomicInteger ac2 = new AtomicInteger(10); // int andIncrement = ac2.getAndIncrement(); // System.out.println(andIncrement); // System.out.println(ac2.get());  // AtomicInteger ac3 = new AtomicInteger(10); // int i = ac3.incrementAndGet(); // System.out.println(i);//自增后的值 // System.out.println(ac3.get());  // AtomicInteger ac4 = new AtomicInteger(10); // int i = ac4.addAndGet(20); // System.out.println(i); // System.out.println(ac4.get());   AtomicInteger ac5 = new AtomicInteger(100);  int andSet = ac5.getAndSet(20);  System.out.println(andSet);  System.out.println(ac5.get());  } } 2.7 AtomicInteger-内存解析 AtomicInteger原理 : 自旋锁 + CAS 算法\nCAS算法：\n​\t有3个操作数（内存值V， 旧的预期值A，要修改的值B）\n​\t当旧的预期值A == 内存值 此时修改成功，将V改为B\n​\t当旧的预期值A！=内存值 此时修改失败，不做任何操作\n​\t并重新获取现在的最新值（这个重新获取的动作就是自旋）\n2.8 AtomicInteger-源码解析 代码实现 :\npackage com.itheima.threadatom4;  public class AtomDemo {  public static void main(String[] args) {  MyAtomThread atom = new MyAtomThread();   for (int i = 0; i \u0026lt; 100; i++) {  new Thread(atom).start();  }  } } package com.itheima.threadatom4;  import java.util.concurrent.atomic.AtomicInteger;  public class MyAtomThread implements Runnable {  //private volatile int count = 0; //送冰淇淋的数量  //private Object lock = new Object();  AtomicInteger ac = new AtomicInteger(0);   @Override  public void run() {  for (int i = 0; i \u0026lt; 100; i++) {  //1,从共享数据中读取数据到本线程栈中.  //2,修改本线程栈中变量副本的值  //3,会把本线程栈中变量副本的值赋值给共享数据.  //synchronized (lock) { // count++; // ac++;  int count = ac.incrementAndGet();  System.out.println(\u0026#34;已经送了\u0026#34; + count + \u0026#34;个冰淇淋\u0026#34;);  // }  }  } } 源码解析 :\n//先自增，然后获取自增后的结果 public final int incrementAndGet() {  //+ 1 自增后的结果  //this 就表示当前的atomicInteger（值）  //1 自增一次  return U.getAndAddInt(this, VALUE, 1) + 1; }  public final int getAndAddInt(Object o, long offset, int delta) {  //v 旧值  int v;  //自旋的过程  do {  //不断的获取旧值  v = getIntVolatile(o, offset);  //如果这个方法的返回值为false，那么继续自旋  //如果这个方法的返回值为true，那么自旋结束  //o 表示的就是内存值  //v 旧值  //v + delta 修改后的值  } while (!weakCompareAndSetInt(o, offset, v, v + delta));  //作用：比较内存中的值，旧值是否相等，如果相等就把修改后的值写到内存中，返回true。表示修改成功。  // 如果不相等，无法把修改后的值写到内存中，返回false。表示修改失败。  //如果修改失败，那么继续自旋。  return v; } 2.9 悲观锁和乐观锁 synchronized和CAS的区别 :\n**相同点：**在多线程情况下，都可以保证共享数据的安全性。\n**不同点：**synchronized总是从最坏的角度出发，认为每次获取数据的时候，别人都有可能修改。所以在每次操作共享数据之前，都会上锁。（悲观锁）\n​\tcas是从乐观的角度出发，假设每次获取数据别人都不会修改，所以不会上锁。只不过在修改共享数据的时候，会检查一下，别人有没有修改过这个数据。\n​\t如果别人修改过，那么我再次获取现在最新的值。\n​\t如果别人没有修改过，那么我现在直接修改共享数据的值.(乐观锁）\n3. 并发工具类 3.1 并发工具类-Hashtable ​\tHashtable出现的原因 : 在集合类中HashMap是比较常用的集合对象，但是HashMap是线程不安全的(多线程环境下可能会存在问题)。为了保证数据的安全性我们可以使用Hashtable，但是Hashtable的效率低下。\n代码实现 :\npackage com.itheima.mymap;  import java.util.HashMap; import java.util.Hashtable;  public class MyHashtableDemo {  public static void main(String[] args) throws InterruptedException {  Hashtable\u0026lt;String, String\u0026gt; hm = new Hashtable\u0026lt;\u0026gt;();   Thread t1 = new Thread(() -\u0026gt; {  for (int i = 0; i \u0026lt; 25; i++) {  hm.put(i + \u0026#34;\u0026#34;, i + \u0026#34;\u0026#34;);  }  });    Thread t2 = new Thread(() -\u0026gt; {  for (int i = 25; i \u0026lt; 51; i++) {  hm.put(i + \u0026#34;\u0026#34;, i + \u0026#34;\u0026#34;);  }  });   t1.start();  t2.start();   System.out.println(\u0026#34;----------------------------\u0026#34;);  //为了t1和t2能把数据全部添加完毕  Thread.sleep(1000);   //0-0 1-1 ..... 50- 50   for (int i = 0; i \u0026lt; 51; i++) {  System.out.println(hm.get(i + \u0026#34;\u0026#34;));  }//0 1 2 3 .... 50    } } 3.2 并发工具类-ConcurrentHashMap基本使用 ​\tConcurrentHashMap出现的原因 : 在集合类中HashMap是比较常用的集合对象，但是HashMap是线程不安全的(多线程环境下可能会存在问题)。为了保证数据的安全性我们可以使用Hashtable，但是Hashtable的效率低下。\n基于以上两个原因我们可以使用JDK1.5以后所提供的ConcurrentHashMap。\n体系结构 :\n总结 :\n​\t1 ，HashMap是线程不安全的。多线程环境下会有数据安全问题\n​\t2 ，Hashtable是线程安全的，但是会将整张表锁起来，效率低下\n​\t3，ConcurrentHashMap也是线程安全的，效率较高。 在JDK7和JDK8中，底层原理不一样。\n代码实现 :\npackage com.itheima.mymap;  import java.util.Hashtable; import java.util.concurrent.ConcurrentHashMap;  public class MyConcurrentHashMapDemo {  public static void main(String[] args) throws InterruptedException {  ConcurrentHashMap\u0026lt;String, String\u0026gt; hm = new ConcurrentHashMap\u0026lt;\u0026gt;(100);   Thread t1 = new Thread(() -\u0026gt; {  for (int i = 0; i \u0026lt; 25; i++) {  hm.put(i + \u0026#34;\u0026#34;, i + \u0026#34;\u0026#34;);  }  });    Thread t2 = new Thread(() -\u0026gt; {  for (int i = 25; i \u0026lt; 51; i++) {  hm.put(i + \u0026#34;\u0026#34;, i + \u0026#34;\u0026#34;);  }  });   t1.start();  t2.start();   System.out.println(\u0026#34;----------------------------\u0026#34;);  //为了t1和t2能把数据全部添加完毕  Thread.sleep(1000);   //0-0 1-1 ..... 50- 50   for (int i = 0; i \u0026lt; 51; i++) {  System.out.println(hm.get(i + \u0026#34;\u0026#34;));  }//0 1 2 3 .... 50  } } 3.3 并发工具类-ConcurrentHashMap1.7原理 3.4 并发工具类-ConcurrentHashMap1.8原理 总结 :\n​\t1，如果使用空参构造创建ConcurrentHashMap对象，则什么事情都不做。 在第一次添加元素的时候创建哈希表\n​\t2，计算当前元素应存入的索引。\n​\t3，如果该索引位置为null，则利用cas算法，将本结点添加到数组中。\n​\t4，如果该索引位置不为null，则利用volatile关键字获得当前位置最新的结点地址，挂在他下面，变成链表。\n​\t5，当链表的长度大于等于8时，自动转换成红黑树6，以链表或者红黑树头结点为锁对象，配合悲观锁保证多线程操作集合时数据的安全性\n3.5 并发工具类-CountDownLatch CountDownLatch类 :\n   方法 解释     public CountDownLatch(int count) 参数传递线程数，表示等待线程数量   public void await() 让线程等待   public void countDown() 当前线程执行完毕    使用场景： 让某一条线程等待其他线程执行完毕之后再执行\n代码实现 :\npackage com.itheima.mycountdownlatch;  import java.util.concurrent.CountDownLatch;  public class ChileThread1 extends Thread {   private CountDownLatch countDownLatch;  public ChileThread1(CountDownLatch countDownLatch) {  this.countDownLatch = countDownLatch;  }   @Override  public void run() {  //1.吃饺子  for (int i = 1; i \u0026lt;= 10; i++) {  System.out.println(getName() + \u0026#34;在吃第\u0026#34; + i + \u0026#34;个饺子\u0026#34;);  }  //2.吃完说一声  //每一次countDown方法的时候，就让计数器-1  countDownLatch.countDown();  } } package com.itheima.mycountdownlatch;  import java.util.concurrent.CountDownLatch;  public class ChileThread2 extends Thread {   private CountDownLatch countDownLatch;  public ChileThread2(CountDownLatch countDownLatch) {  this.countDownLatch = countDownLatch;  }  @Override  public void run() {  //1.吃饺子  for (int i = 1; i \u0026lt;= 15; i++) {  System.out.println(getName() + \u0026#34;在吃第\u0026#34; + i + \u0026#34;个饺子\u0026#34;);  }  //2.吃完说一声  //每一次countDown方法的时候，就让计数器-1  countDownLatch.countDown();  } } package com.itheima.mycountdownlatch;  import java.util.concurrent.CountDownLatch;  public class ChileThread3 extends Thread {   private CountDownLatch countDownLatch;  public ChileThread3(CountDownLatch countDownLatch) {  this.countDownLatch = countDownLatch;  }  @Override  public void run() {  //1.吃饺子  for (int i = 1; i \u0026lt;= 20; i++) {  System.out.println(getName() + \u0026#34;在吃第\u0026#34; + i + \u0026#34;个饺子\u0026#34;);  }  //2.吃完说一声  //每一次countDown方法的时候，就让计数器-1  countDownLatch.countDown();  } } package com.itheima.mycountdownlatch;  import java.util.concurrent.CountDownLatch;  public class MotherThread extends Thread {  private CountDownLatch countDownLatch;  public MotherThread(CountDownLatch countDownLatch) {  this.countDownLatch = countDownLatch;  }   @Override  public void run() {  //1.等待  try {  //当计数器变成0的时候，会自动唤醒这里等待的线程。  countDownLatch.await();  } catch (InterruptedException e) {  e.printStackTrace();  }  //2.收拾碗筷  System.out.println(\u0026#34;妈妈在收拾碗筷\u0026#34;);  } } package com.itheima.mycountdownlatch;  import java.util.concurrent.CountDownLatch;  public class MyCountDownLatchDemo {  public static void main(String[] args) {  //1.创建CountDownLatch的对象，需要传递给四个线程。  //在底层就定义了一个计数器，此时计数器的值就是3  CountDownLatch countDownLatch = new CountDownLatch(3);  //2.创建四个线程对象并开启他们。  MotherThread motherThread = new MotherThread(countDownLatch);  motherThread.start();   ChileThread1 t1 = new ChileThread1(countDownLatch);  t1.setName(\u0026#34;小明\u0026#34;);   ChileThread2 t2 = new ChileThread2(countDownLatch);  t2.setName(\u0026#34;小红\u0026#34;);   ChileThread3 t3 = new ChileThread3(countDownLatch);  t3.setName(\u0026#34;小刚\u0026#34;);   t1.start();  t2.start();  t3.start();  } } 总结 :\n​\t1. CountDownLatch(int count)：参数写等待线程的数量。并定义了一个计数器。\n​\t2. await()：让线程等待，当计数器为0时，会唤醒等待的线程\n​\t3. countDown()： 线程执行完毕时调用，会将计数器-1。\n3.6 并发工具类-Semaphore 使用场景 :\n​\t可以控制访问特定资源的线程数量。\n实现步骤 :\n​\t1，需要有人管理这个通道\n​\t2，当有车进来了，发通行许可证\n​\t3，当车出去了，收回通行许可证\n​\t4，如果通行许可证发完了，那么其他车辆只能等着\n代码实现 :\npackage com.itheima.mysemaphore;  import java.util.concurrent.Semaphore;  public class MyRunnable implements Runnable {  //1.获得管理员对象，  private Semaphore semaphore = new Semaphore(2);  @Override  public void run() {  //2.获得通行证  try {  semaphore.acquire();  //3.开始行驶  System.out.println(\u0026#34;获得了通行证开始行驶\u0026#34;);  Thread.sleep(2000);  System.out.println(\u0026#34;归还通行证\u0026#34;);  //4.归还通行证  semaphore.release();  } catch (InterruptedException e) {  e.printStackTrace();  }  } } package com.itheima.mysemaphore;  public class MySemaphoreDemo {  public static void main(String[] args) {  MyRunnable mr = new MyRunnable();   for (int i = 0; i \u0026lt; 100; i++) {  new Thread(mr).start();  }  } } ","permalink":"https://iblog.zone/archives/java%E5%A4%9A%E7%BA%BF%E7%A8%8B02/","summary":"1.线程池 1.1 线程状态介绍 当线程被创建并启动以后，它既不是一启动就进入了执行状态，也不是一直处于执行状态。线程对象在不同的时期有不同的状态。那么Java中的线程存在哪几种状态呢？Java中的线程\n状态被定义在了java.lang.Thread.State枚举类中，State枚举类的源码如下：\npublic class Thread {   public enum State {   /* 新建 */  NEW ,   /* 可运行状态 */  RUNNABLE ,   /* 阻塞状态 */  BLOCKED ,   /* 无限等待状态 */  WAITING ,   /* 计时等待 */  TIMED_WAITING ,   /* 终止 */  TERMINATED;  \t}   // 获取当前线程的状态  public State getState() {  return jdk.","title":"Java多线程02"},{"content":"1.实现多线程 1.1简单了解多线程【理解】 是指从软件或者硬件上实现多个线程并发执行的技术。 具有多线程能力的计算机因有硬件支持而能够在同一时间执行多个线程，提升性能。\n1.2并发和并行【理解】   并行：在同一时刻，有多个指令在多个CPU上同时执行。\n  并发：在同一时刻，有多个指令在单个CPU上交替执行。\n  1.3进程和线程【理解】   进程：是正在运行的程序\n独立性：进程是一个能独立运行的基本单位，同时也是系统分配资源和调度的独立单位 动态性：进程的实质是程序的一次执行过程，进程是动态产生，动态消亡的 并发性：任何进程都可以同其他进程一起并发执行\n  线程：是进程中的单个顺序控制流，是一条执行路径\n​\t单线程：一个进程如果只有一条执行路径，则称为单线程程序\n​\t多线程：一个进程如果有多条执行路径，则称为多线程程序\n  1.4实现多线程方式一：继承Thread类【应用】   方法介绍\n   方法名 说明     void run() 在线程开启后，此方法将被调用执行   void start() 使此线程开始执行，Java虚拟机会调用run方法()      实现步骤\n 定义一个类MyThread继承Thread类 在MyThread类中重写run()方法 创建MyThread类的对象 启动线程    代码演示\npublic class MyThread extends Thread {  @Override  public void run() {  for(int i=0; i\u0026lt;100; i++) {  System.out.println(i);  }  } } public class MyThreadDemo {  public static void main(String[] args) {  MyThread my1 = new MyThread();  MyThread my2 = new MyThread();  // my1.run(); // my2.run();   //void start() 导致此线程开始执行; Java虚拟机调用此线程的run方法  my1.start();  my2.start();  } }   两个小问题\n  为什么要重写run()方法？\n因为run()是用来封装被线程执行的代码\n  run()方法和start()方法的区别？\nrun()：封装线程执行的代码，直接调用，相当于普通方法的调用\nstart()：启动线程；然后由JVM调用此线程的run()方法\n    1.5实现多线程方式二：实现Runnable接口【应用】   Thread构造方法\n   方法名 说明     Thread(Runnable target) 分配一个新的Thread对象   Thread(Runnable target, String name) 分配一个新的Thread对象      实现步骤\n 定义一个类MyRunnable实现Runnable接口 在MyRunnable类中重写run()方法 创建MyRunnable类的对象 创建Thread类的对象，把MyRunnable对象作为构造方法的参数 启动线程    代码演示\npublic class MyRunnable implements Runnable {  @Override  public void run() {  for(int i=0; i\u0026lt;100; i++) {  System.out.println(Thread.currentThread().getName()+\u0026#34;:\u0026#34;+i);  }  } } public class MyRunnableDemo {  public static void main(String[] args) {  //创建MyRunnable类的对象  MyRunnable my = new MyRunnable();   //创建Thread类的对象，把MyRunnable对象作为构造方法的参数  //Thread(Runnable target) // Thread t1 = new Thread(my); // Thread t2 = new Thread(my);  //Thread(Runnable target, String name)  Thread t1 = new Thread(my,\u0026#34;坦克\u0026#34;);  Thread t2 = new Thread(my,\u0026#34;飞机\u0026#34;);   //启动线程  t1.start();  t2.start();  } }   1.6实现多线程方式三: 实现Callable接口【应用】   方法介绍\n   方法名 说明     V call() 计算结果，如果无法计算结果，则抛出一个异常   FutureTask(Callable callable) 创建一个 FutureTask，一旦运行就执行给定的 Callable   V get() 如有必要，等待计算完成，然后获取其结果      实现步骤\n 定义一个类MyCallable实现Callable接口 在MyCallable类中重写call()方法 创建MyCallable类的对象 创建Future的实现类FutureTask对象，把MyCallable对象作为构造方法的参数 创建Thread类的对象，把FutureTask对象作为构造方法的参数 启动线程 再调用get方法，就可以获取线程结束之后的结果。    代码演示\npublic class MyCallable implements Callable\u0026lt;String\u0026gt; {  @Override  public String call() throws Exception {  for (int i = 0; i \u0026lt; 100; i++) {  System.out.println(\u0026#34;跟女孩表白\u0026#34; + i);  }  //返回值就表示线程运行完毕之后的结果  return \u0026#34;答应\u0026#34;;  } } public class Demo {  public static void main(String[] args) throws ExecutionException, InterruptedException {  //线程开启之后需要执行里面的call方法  MyCallable mc = new MyCallable();   //Thread t1 = new Thread(mc);   //可以获取线程执行完毕之后的结果.也可以作为参数传递给Thread对象  FutureTask\u0026lt;String\u0026gt; ft = new FutureTask\u0026lt;\u0026gt;(mc);   //创建线程对象  Thread t1 = new Thread(ft);   String s = ft.get();  //开启线程  t1.start();   //String s = ft.get();  System.out.println(s);  } }   三种实现方式的对比\n 实现Runnable、Callable接口  好处: 扩展性强，实现该接口的同时还可以继承其他的类 缺点: 编程相对复杂，不能直接使用Thread类中的方法   继承Thread类  好处: 编程比较简单，可以直接使用Thread类中的方法 缺点: 可以扩展性较差，不能再继承其他的类      1.7设置和获取线程名称【应用】   方法介绍\n   方法名 说明     void setName(String name) 将此线程的名称更改为等于参数name   String getName() 返回此线程的名称   Thread currentThread() 返回对当前正在执行的线程对象的引用      代码演示\npublic class MyThread extends Thread {  public MyThread() {}  public MyThread(String name) {  super(name);  }   @Override  public void run() {  for (int i = 0; i \u0026lt; 100; i++) {  System.out.println(getName()+\u0026#34;:\u0026#34;+i);  }  } } public class MyThreadDemo {  public static void main(String[] args) {  MyThread my1 = new MyThread();  MyThread my2 = new MyThread();   //void setName(String name)：将此线程的名称更改为等于参数 name  my1.setName(\u0026#34;高铁\u0026#34;);  my2.setName(\u0026#34;飞机\u0026#34;);   //Thread(String name)  MyThread my1 = new MyThread(\u0026#34;高铁\u0026#34;);  MyThread my2 = new MyThread(\u0026#34;飞机\u0026#34;);   my1.start();  my2.start();   //static Thread currentThread() 返回对当前正在执行的线程对象的引用  System.out.println(Thread.currentThread().getName());  } }   1.8线程休眠【应用】   相关方法\n   方法名 说明     static void sleep(long millis) 使当前正在执行的线程停留（暂停执行）指定的毫秒数      代码演示\npublic class MyRunnable implements Runnable {  @Override  public void run() {  for (int i = 0; i \u0026lt; 100; i++) {  try {  Thread.sleep(100);  } catch (InterruptedException e) {  e.printStackTrace();  }   System.out.println(Thread.currentThread().getName() + \u0026#34;---\u0026#34; + i);  }  } } public class Demo {  public static void main(String[] args) throws InterruptedException {  /*System.out.println(\u0026#34;睡觉前\u0026#34;); Thread.sleep(3000); System.out.println(\u0026#34;睡醒了\u0026#34;);*/   MyRunnable mr = new MyRunnable();   Thread t1 = new Thread(mr);  Thread t2 = new Thread(mr);   t1.start();  t2.start();  } }   1.9线程优先级【应用】   线程调度\n  两种调度方式\n 分时调度模型：所有线程轮流使用 CPU 的使用权，平均分配每个线程占用 CPU 的时间片 抢占式调度模型：优先让优先级高的线程使用 CPU，如果线程的优先级相同，那么会随机选择一个，优先级高的线程获取的 CPU 时间片相对多一些    Java使用的是抢占式调度模型\n  随机性\n假如计算机只有一个 CPU，那么 CPU 在某一个时刻只能执行一条指令，线程只有得到CPU时间片，也就是使用权，才可以执行指令。所以说多线程程序的执行是有随机性，因为谁抢到CPU的使用权是不一定的\n    优先级相关方法\n   方法名 说明     final int getPriority() 返回此线程的优先级   final void setPriority(int newPriority) 更改此线程的优先级线程默认优先级是5；线程优先级的范围是：1-10      代码演示\npublic class MyCallable implements Callable\u0026lt;String\u0026gt; {  @Override  public String call() throws Exception {  for (int i = 0; i \u0026lt; 100; i++) {  System.out.println(Thread.currentThread().getName() + \u0026#34;---\u0026#34; + i);  }  return \u0026#34;线程执行完毕了\u0026#34;;  } } public class Demo {  public static void main(String[] args) {  //优先级: 1 - 10 默认值:5  MyCallable mc = new MyCallable();   FutureTask\u0026lt;String\u0026gt; ft = new FutureTask\u0026lt;\u0026gt;(mc);   Thread t1 = new Thread(ft);  t1.setName(\u0026#34;飞机\u0026#34;);  t1.setPriority(10);  //System.out.println(t1.getPriority());//5  t1.start();   MyCallable mc2 = new MyCallable();   FutureTask\u0026lt;String\u0026gt; ft2 = new FutureTask\u0026lt;\u0026gt;(mc2);   Thread t2 = new Thread(ft2);  t2.setName(\u0026#34;坦克\u0026#34;);  t2.setPriority(1);  //System.out.println(t2.getPriority());//5  t2.start();  } }   1.10守护线程【应用】   相关方法\n   方法名 说明     void setDaemon(boolean on) 将此线程标记为守护线程，当运行的线程都是守护线程时，Java虚拟机将退出      代码演示\npublic class MyThread1 extends Thread {  @Override  public void run() {  for (int i = 0; i \u0026lt; 10; i++) {  System.out.println(getName() + \u0026#34;---\u0026#34; + i);  }  } } public class MyThread2 extends Thread {  @Override  public void run() {  for (int i = 0; i \u0026lt; 100; i++) {  System.out.println(getName() + \u0026#34;---\u0026#34; + i);  }  } } public class Demo {  public static void main(String[] args) {  MyThread1 t1 = new MyThread1();  MyThread2 t2 = new MyThread2();   t1.setName(\u0026#34;女神\u0026#34;);  t2.setName(\u0026#34;备胎\u0026#34;);   //把第二个线程设置为守护线程  //当普通线程执行完之后,那么守护线程也没有继续运行下去的必要了.  t2.setDaemon(true);   t1.start();  t2.start();  } }   2.线程同步 2.1卖票【应用】   案例需求\n某电影院目前正在上映国产大片，共有100张票，而它有3个窗口卖票，请设计一个程序模拟该电影院卖票\n  实现步骤\n  定义一个类SellTicket实现Runnable接口，里面定义一个成员变量：private int tickets = 100;\n  在SellTicket类中重写run()方法实现卖票，代码步骤如下\n  判断票数大于0，就卖票，并告知是哪个窗口卖的\n  卖了票之后，总票数要减1\n  票卖没了，线程停止\n  定义一个测试类SellTicketDemo，里面有main方法，代码步骤如下\n  创建SellTicket类的对象\n  创建三个Thread类的对象，把SellTicket对象作为构造方法的参数，并给出对应的窗口名称\n  启动线程\n    代码实现\npublic class SellTicket implements Runnable {  private int tickets = 100;  //在SellTicket类中重写run()方法实现卖票，代码步骤如下  @Override  public void run() {  while (true) {  if(ticket \u0026lt;= 0){  //卖完了  break;  }else{  try {  Thread.sleep(100);  } catch (InterruptedException e) {  e.printStackTrace();  }  ticket--;  System.out.println(Thread.currentThread().getName() + \u0026#34;在卖票,还剩下\u0026#34; + ticket + \u0026#34;张票\u0026#34;);  }  }  } } public class SellTicketDemo {  public static void main(String[] args) {  //创建SellTicket类的对象  SellTicket st = new SellTicket();   //创建三个Thread类的对象，把SellTicket对象作为构造方法的参数，并给出对应的窗口名称  Thread t1 = new Thread(st,\u0026#34;窗口1\u0026#34;);  Thread t2 = new Thread(st,\u0026#34;窗口2\u0026#34;);  Thread t3 = new Thread(st,\u0026#34;窗口3\u0026#34;);   //启动线程  t1.start();  t2.start();  t3.start();  } }   2.2卖票案例的问题【理解】   卖票出现了问题\n  相同的票出现了多次\n  出现了负数的票\n    问题产生原因\n线程执行的随机性导致的,可能在卖票过程中丢失cpu的执行权,导致出现问题\n  2.3同步代码块解决数据安全问题【应用】   安全问题出现的条件\n  是多线程环境\n  有共享数据\n  有多条语句操作共享数据\n    如何解决多线程安全问题呢?\n 基本思想：让程序没有安全问题的环境    怎么实现呢?\n  把多条语句操作共享数据的代码给锁起来，让任意时刻只能有一个线程执行即可\n  Java提供了同步代码块的方式来解决\n    同步代码块格式：\nsynchronized(任意对象) { \t多条语句操作共享数据的代码 } synchronized(任意对象)：就相当于给代码加锁了，任意对象就可以看成是一把锁\n  同步的好处和弊端\n  好处：解决了多线程的数据安全问题\n  弊端：当线程很多时，因为每个线程都会去判断同步上的锁，这是很耗费资源的，无形中会降低程序的运行效率\n    代码演示\npublic class SellTicket implements Runnable {  private int tickets = 100;  private Object obj = new Object();   @Override  public void run() {  while (true) {  synchronized (obj) { // 对可能有安全问题的代码加锁,多个线程必须使用同一把锁  //t1进来后，就会把这段代码给锁起来  if (tickets \u0026gt; 0) {  try {  Thread.sleep(100);  //t1休息100毫秒  } catch (InterruptedException e) {  e.printStackTrace();  }  //窗口1正在出售第100张票  System.out.println(Thread.currentThread().getName() + \u0026#34;正在出售第\u0026#34; + tickets + \u0026#34;张票\u0026#34;);  tickets--; //tickets = 99;  }  }  //t1出来了，这段代码的锁就被释放了  }  } }  public class SellTicketDemo {  public static void main(String[] args) {  SellTicket st = new SellTicket();   Thread t1 = new Thread(st, \u0026#34;窗口1\u0026#34;);  Thread t2 = new Thread(st, \u0026#34;窗口2\u0026#34;);  Thread t3 = new Thread(st, \u0026#34;窗口3\u0026#34;);   t1.start();  t2.start();  t3.start();  } }   2.4同步方法解决数据安全问题【应用】   同步方法的格式\n同步方法：就是把synchronized关键字加到方法上\n修饰符 synchronized 返回值类型 方法名(方法参数) { \t方法体； } 同步方法的锁对象是什么呢?\n​\tthis\n  静态同步方法\n同步静态方法：就是把synchronized关键字加到静态方法上\n修饰符 static synchronized 返回值类型 方法名(方法参数) { \t方法体； } 同步静态方法的锁对象是什么呢?\n​\t类名.class\n  代码演示\npublic class MyRunnable implements Runnable {  private static int ticketCount = 100;   @Override  public void run() {  while(true){  if(\u0026#34;窗口一\u0026#34;.equals(Thread.currentThread().getName())){  //同步方法  boolean result = synchronizedMthod();  if(result){  break;  }  }   if(\u0026#34;窗口二\u0026#34;.equals(Thread.currentThread().getName())){  //同步代码块  synchronized (MyRunnable.class){  if(ticketCount == 0){  break;  }else{  try {  Thread.sleep(10);  } catch (InterruptedException e) {  e.printStackTrace();  }  ticketCount--;  System.out.println(Thread.currentThread().getName() + \u0026#34;在卖票,还剩下\u0026#34; + ticketCount + \u0026#34;张票\u0026#34;);  }  }  }   }  }   private static synchronized boolean synchronizedMthod() {  if(ticketCount == 0){  return true;  }else{  try {  Thread.sleep(10);  } catch (InterruptedException e) {  e.printStackTrace();  }  ticketCount--;  System.out.println(Thread.currentThread().getName() + \u0026#34;在卖票,还剩下\u0026#34; + ticketCount + \u0026#34;张票\u0026#34;);  return false;  }  } } public class Demo { \tpublic static void main(String[] args) {  MyRunnable mr = new MyRunnable();  Thread t1 = new Thread(mr);  Thread t2 = new Thread(mr);   t1.setName(\u0026#34;窗口一\u0026#34;);  t2.setName(\u0026#34;窗口二\u0026#34;);   t1.start();  t2.start();  } }   2.5Lock锁【应用】 虽然我们可以理解同步代码块和同步方法的锁对象问题，但是我们并没有直接看到在哪里加上了锁，在哪里释放了锁，为了更清晰的表达如何加锁和释放锁，JDK5以后提供了一个新的锁对象Lock\nLock是接口不能直接实例化，这里采用它的实现类ReentrantLock来实例化\n  ReentrantLock构造方法\n   方法名 说明     ReentrantLock() 创建一个ReentrantLock的实例      加锁解锁方法\n   方法名 说明     void lock() 获得锁   void unlock() 释放锁      代码演示\npublic class Ticket implements Runnable {  //票的数量  private int ticket = 100;  private Object obj = new Object();  private ReentrantLock lock = new ReentrantLock();   @Override  public void run() {  while (true) {  //synchronized (obj){//多个线程必须使用同一把锁.  try {  lock.lock();  if (ticket \u0026lt;= 0) {  //卖完了  break;  } else {  Thread.sleep(100);  ticket--;  System.out.println(Thread.currentThread().getName() + \u0026#34;在卖票,还剩下\u0026#34; + ticket + \u0026#34;张票\u0026#34;);  }  } catch (InterruptedException e) {  e.printStackTrace();  } finally {  lock.unlock();  }  // }  }  } }  public class Demo {  public static void main(String[] args) {  Ticket ticket = new Ticket();   Thread t1 = new Thread(ticket);  Thread t2 = new Thread(ticket);  Thread t3 = new Thread(ticket);   t1.setName(\u0026#34;窗口一\u0026#34;);  t2.setName(\u0026#34;窗口二\u0026#34;);  t3.setName(\u0026#34;窗口三\u0026#34;);   t1.start();  t2.start();  t3.start();  } }   2.6死锁【理解】   概述\n线程死锁是指由于两个或者多个线程互相持有对方所需要的资源，导致这些线程处于等待状态，无法前往执行\n  什么情况下会产生死锁\n 资源有限 同步嵌套    代码演示\npublic class Demo {  public static void main(String[] args) {  Object objA = new Object();  Object objB = new Object();   new Thread(()-\u0026gt;{  while(true){  synchronized (objA){  //线程一  synchronized (objB){  System.out.println(\u0026#34;小康同学正在走路\u0026#34;);  }  }  }  }).start();   new Thread(()-\u0026gt;{  while(true){  synchronized (objB){  //线程二  synchronized (objA){  System.out.println(\u0026#34;小薇同学正在走路\u0026#34;);  }  }  }  }).start();  } }   3.生产者消费者 3.1生产者和消费者模式概述【应用】   概述\n生产者消费者模式是一个十分经典的多线程协作的模式，弄懂生产者消费者问题能够让我们对多线程编程的理解更加深刻。\n所谓生产者消费者问题，实际上主要是包含了两类线程：\n​\t一类是生产者线程用于生产数据\n​\t一类是消费者线程用于消费数据\n为了解耦生产者和消费者的关系，通常会采用共享的数据区域，就像是一个仓库\n生产者生产数据之后直接放置在共享数据区中，并不需要关心消费者的行为\n消费者只需要从共享数据区中去获取数据，并不需要关心生产者的行为\n  Object类的等待和唤醒方法\n   方法名 说明     void wait() 导致当前线程等待，直到另一个线程调用该对象的 notify()方法或 notifyAll()方法   void notify() 唤醒正在等待对象监视器的单个线程   void notifyAll() 唤醒正在等待对象监视器的所有线程      3.2生产者和消费者案例【应用】   案例需求\n  桌子类(Desk)：定义表示包子数量的变量,定义锁对象变量,定义标记桌子上有无包子的变量\n  生产者类(Cooker)：实现Runnable接口，重写run()方法，设置线程任务\n1.判断是否有包子,决定当前线程是否执行\n2.如果有包子,就进入等待状态,如果没有包子,继续执行,生产包子\n3.生产包子之后,更新桌子上包子状态,唤醒消费者消费包子\n  消费者类(Foodie)：实现Runnable接口，重写run()方法，设置线程任务\n1.判断是否有包子,决定当前线程是否执行\n2.如果没有包子,就进入等待状态,如果有包子,就消费包子\n3.消费包子后,更新桌子上包子状态,唤醒生产者生产包子\n  测试类(Demo)：里面有main方法，main方法中的代码步骤如下\n创建生产者线程和消费者线程对象\n分别开启两个线程\n    代码实现\npublic class Desk {   //定义一个标记  //true 就表示桌子上有汉堡包的,此时允许吃货执行  //false 就表示桌子上没有汉堡包的,此时允许厨师执行  public static boolean flag = false;   //汉堡包的总数量  public static int count = 10;   //锁对象  public static final Object lock = new Object(); }  public class Cooker extends Thread { // 生产者步骤： // 1，判断桌子上是否有汉堡包 // 如果有就等待，如果没有才生产。 // 2，把汉堡包放在桌子上。 // 3，叫醒等待的消费者开吃。  @Override  public void run() {  while(true){  synchronized (Desk.lock){  if(Desk.count == 0){  break;  }else{  if(!Desk.flag){  //生产  System.out.println(\u0026#34;厨师正在生产汉堡包\u0026#34;);  Desk.flag = true;  Desk.lock.notifyAll();  }else{  try {  Desk.lock.wait();  } catch (InterruptedException e) {  e.printStackTrace();  }  }  }  }  }  } }  public class Foodie extends Thread {  @Override  public void run() { // 1，判断桌子上是否有汉堡包。 // 2，如果没有就等待。 // 3，如果有就开吃 // 4，吃完之后，桌子上的汉堡包就没有了 // 叫醒等待的生产者继续生产 // 汉堡包的总数量减一   //套路:  //1. while(true)死循环  //2. synchronized 锁,锁对象要唯一  //3. 判断,共享数据是否结束. 结束  //4. 判断,共享数据是否结束. 没有结束  while(true){  synchronized (Desk.lock){  if(Desk.count == 0){  break;  }else{  if(Desk.flag){  //有  System.out.println(\u0026#34;吃货在吃汉堡包\u0026#34;);  Desk.flag = false;  Desk.lock.notifyAll();  Desk.count--;  }else{  //没有就等待  //使用什么对象当做锁,那么就必须用这个对象去调用等待和唤醒的方法.  try {  Desk.lock.wait();  } catch (InterruptedException e) {  e.printStackTrace();  }  }  }  }  }   } }  public class Demo {  public static void main(String[] args) {  /*消费者步骤： 1，判断桌子上是否有汉堡包。 2，如果没有就等待。 3，如果有就开吃 4，吃完之后，桌子上的汉堡包就没有了 叫醒等待的生产者继续生产 汉堡包的总数量减一*/   /*生产者步骤： 1，判断桌子上是否有汉堡包 如果有就等待，如果没有才生产。 2，把汉堡包放在桌子上。 3，叫醒等待的消费者开吃。*/   Foodie f = new Foodie();  Cooker c = new Cooker();   f.start();  c.start();   } }   3.3生产者和消费者案例优化【应用】   需求\n 将Desk类中的变量,采用面向对象的方式封装起来 生产者和消费者类中构造方法接收Desk类对象,之后在run方法中进行使用 创建生产者和消费者线程对象,构造方法中传入Desk类对象 开启两个线程    代码实现\npublic class Desk {   //定义一个标记  //true 就表示桌子上有汉堡包的,此时允许吃货执行  //false 就表示桌子上没有汉堡包的,此时允许厨师执行  //public static boolean flag = false;  private boolean flag;   //汉堡包的总数量  //public static int count = 10;  //以后我们在使用这种必须有默认值的变量  // private int count = 10;  private int count;   //锁对象  //public static final Object lock = new Object();  private final Object lock = new Object();   public Desk() {  this(false,10); // 在空参内部调用带参,对成员变量进行赋值,之后就可以直接使用成员变量了  }   public Desk(boolean flag, int count) {  this.flag = flag;  this.count = count;  }   public boolean isFlag() {  return flag;  }   public void setFlag(boolean flag) {  this.flag = flag;  }   public int getCount() {  return count;  }   public void setCount(int count) {  this.count = count;  }   public Object getLock() {  return lock;  }   @Override  public String toString() {  return \u0026#34;Desk{\u0026#34; +  \u0026#34;flag=\u0026#34; + flag +  \u0026#34;, count=\u0026#34; + count +  \u0026#34;, lock=\u0026#34; + lock +  \u0026#39;}\u0026#39;;  } }  public class Cooker extends Thread {   private Desk desk;   public Cooker(Desk desk) {  this.desk = desk;  } // 生产者步骤： // 1，判断桌子上是否有汉堡包 // 如果有就等待，如果没有才生产。 // 2，把汉堡包放在桌子上。 // 3，叫醒等待的消费者开吃。   @Override  public void run() {  while(true){  synchronized (desk.getLock()){  if(desk.getCount() == 0){  break;  }else{  //System.out.println(\u0026#34;验证一下是否执行了\u0026#34;);  if(!desk.isFlag()){  //生产  System.out.println(\u0026#34;厨师正在生产汉堡包\u0026#34;);  desk.setFlag(true);  desk.getLock().notifyAll();  }else{  try {  desk.getLock().wait();  } catch (InterruptedException e) {  e.printStackTrace();  }  }  }  }  }  } }  public class Foodie extends Thread {  private Desk desk;   public Foodie(Desk desk) {  this.desk = desk;  }   @Override  public void run() { // 1，判断桌子上是否有汉堡包。 // 2，如果没有就等待。 // 3，如果有就开吃 // 4，吃完之后，桌子上的汉堡包就没有了 // 叫醒等待的生产者继续生产 // 汉堡包的总数量减一   //套路:  //1. while(true)死循环  //2. synchronized 锁,锁对象要唯一  //3. 判断,共享数据是否结束. 结束  //4. 判断,共享数据是否结束. 没有结束  while(true){  synchronized (desk.getLock()){  if(desk.getCount() == 0){  break;  }else{  //System.out.println(\u0026#34;验证一下是否执行了\u0026#34;);  if(desk.isFlag()){  //有  System.out.println(\u0026#34;吃货在吃汉堡包\u0026#34;);  desk.setFlag(false);  desk.getLock().notifyAll();  desk.setCount(desk.getCount() - 1);  }else{  //没有就等待  //使用什么对象当做锁,那么就必须用这个对象去调用等待和唤醒的方法.  try {  desk.getLock().wait();  } catch (InterruptedException e) {  e.printStackTrace();  }  }  }  }  }   } }  public class Demo {  public static void main(String[] args) {  /*消费者步骤： 1，判断桌子上是否有汉堡包。 2，如果没有就等待。 3，如果有就开吃 4，吃完之后，桌子上的汉堡包就没有了 叫醒等待的生产者继续生产 汉堡包的总数量减一*/   /*生产者步骤： 1，判断桌子上是否有汉堡包 如果有就等待，如果没有才生产。 2，把汉堡包放在桌子上。 3，叫醒等待的消费者开吃。*/   Desk desk = new Desk();   Foodie f = new Foodie(desk);  Cooker c = new Cooker(desk);   f.start();  c.start();   } }   3.4阻塞队列基本使用【理解】   阻塞队列继承结构\n  常见BlockingQueue:\nArrayBlockingQueue: 底层是数组,有界\nLinkedBlockingQueue: 底层是链表,无界.但不是真正的无界,最大为int的最大值\n  BlockingQueue的核心方法:\nput(anObject): 将参数放入队列,如果放不进去会阻塞\ntake(): 取出第一个数据,取不到会阻塞\n  代码示例\npublic class Demo02 {  public static void main(String[] args) throws Exception {  // 创建阻塞队列的对象,容量为 1  ArrayBlockingQueue\u0026lt;String\u0026gt; arrayBlockingQueue = new ArrayBlockingQueue\u0026lt;\u0026gt;(1);   // 存储元素  arrayBlockingQueue.put(\u0026#34;汉堡包\u0026#34;);   // 取元素  System.out.println(arrayBlockingQueue.take());  System.out.println(arrayBlockingQueue.take()); // 取不到会阻塞   System.out.println(\u0026#34;程序结束了\u0026#34;);  } }   3.5阻塞队列实现等待唤醒机制【理解】   案例需求\n  生产者类(Cooker)：实现Runnable接口，重写run()方法，设置线程任务\n1.构造方法中接收一个阻塞队列对象\n2.在run方法中循环向阻塞队列中添加包子\n3.打印添加结果\n  消费者类(Foodie)：实现Runnable接口，重写run()方法，设置线程任务\n1.构造方法中接收一个阻塞队列对象\n2.在run方法中循环获取阻塞队列中的包子\n3.打印获取结果\n  测试类(Demo)：里面有main方法，main方法中的代码步骤如下\n创建阻塞队列对象\n创建生产者线程和消费者线程对象,构造方法中传入阻塞队列对象\n分别开启两个线程\n    代码实现\npublic class Cooker extends Thread {   private ArrayBlockingQueue\u0026lt;String\u0026gt; bd;   public Cooker(ArrayBlockingQueue\u0026lt;String\u0026gt; bd) {  this.bd = bd;  } // 生产者步骤： // 1，判断桌子上是否有汉堡包 // 如果有就等待，如果没有才生产。 // 2，把汉堡包放在桌子上。 // 3，叫醒等待的消费者开吃。   @Override  public void run() {  while (true) {  try {  bd.put(\u0026#34;汉堡包\u0026#34;);  System.out.println(\u0026#34;厨师放入一个汉堡包\u0026#34;);  } catch (InterruptedException e) {  e.printStackTrace();  }  }  } }  public class Foodie extends Thread {  private ArrayBlockingQueue\u0026lt;String\u0026gt; bd;   public Foodie(ArrayBlockingQueue\u0026lt;String\u0026gt; bd) {  this.bd = bd;  }   @Override  public void run() { // 1，判断桌子上是否有汉堡包。 // 2，如果没有就等待。 // 3，如果有就开吃 // 4，吃完之后，桌子上的汉堡包就没有了 // 叫醒等待的生产者继续生产 // 汉堡包的总数量减一   //套路:  //1. while(true)死循环  //2. synchronized 锁,锁对象要唯一  //3. 判断,共享数据是否结束. 结束  //4. 判断,共享数据是否结束. 没有结束  while (true) {  try {  String take = bd.take();  System.out.println(\u0026#34;吃货将\u0026#34; + take + \u0026#34;拿出来吃了\u0026#34;);  } catch (InterruptedException e) {  e.printStackTrace();  }  }   } }  public class Demo {  public static void main(String[] args) {  ArrayBlockingQueue\u0026lt;String\u0026gt; bd = new ArrayBlockingQueue\u0026lt;\u0026gt;(1);   Foodie f = new Foodie(bd);  Cooker c = new Cooker(bd);   f.start();  c.start();  } }   ","permalink":"https://iblog.zone/archives/java%E5%A4%9A%E7%BA%BF%E7%A8%8B01/","summary":"1.实现多线程 1.1简单了解多线程【理解】 是指从软件或者硬件上实现多个线程并发执行的技术。 具有多线程能力的计算机因有硬件支持而能够在同一时间执行多个线程，提升性能。\n1.2并发和并行【理解】   并行：在同一时刻，有多个指令在多个CPU上同时执行。\n  并发：在同一时刻，有多个指令在单个CPU上交替执行。\n  1.3进程和线程【理解】   进程：是正在运行的程序\n独立性：进程是一个能独立运行的基本单位，同时也是系统分配资源和调度的独立单位 动态性：进程的实质是程序的一次执行过程，进程是动态产生，动态消亡的 并发性：任何进程都可以同其他进程一起并发执行\n  线程：是进程中的单个顺序控制流，是一条执行路径\n​\t单线程：一个进程如果只有一条执行路径，则称为单线程程序\n​\t多线程：一个进程如果有多条执行路径，则称为多线程程序\n  1.4实现多线程方式一：继承Thread类【应用】   方法介绍\n   方法名 说明     void run() 在线程开启后，此方法将被调用执行   void start() 使此线程开始执行，Java虚拟机会调用run方法()      实现步骤\n 定义一个类MyThread继承Thread类 在MyThread类中重写run()方法 创建MyThread类的对象 启动线程    代码演示\npublic class MyThread extends Thread {  @Override  public void run() {  for(int i=0; i\u0026lt;100; i++) {  System.","title":"Java多线程01"},{"content":"1.字符流 1.1为什么会出现字符流【理解】   字符流的介绍\n由于字节流操作中文不是特别的方便，所以Java就提供字符流\n字符流 = 字节流 + 编码表\n  中文的字节存储方式\n用字节流复制文本文件时，文本文件也会有中文，但是没有问题，原因是最终底层操作会自动进行字节拼接成中文，如何识别是中文的呢？\n汉字在存储的时候，无论选择哪种编码存储，第一个字节都是负数\n  1.2编码表【理解】   什么是字符集\n是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等\nl计算机要准确的存储和识别各种字符集符号，就需要进行字符编码，一套字符集必然至少有一套字符编码。常见字符集有ASCII字符集、GBXXX字符集、Unicode字符集等\n  常见的字符集\n  ASCII字符集：\nlASCII：是基于拉丁字母的一套电脑编码系统，用于显示现代英语，主要包括控制字符(回车键、退格、换行键等)和可显示字符(英文大小写字符、阿拉伯数字和西文符号)\n基本的ASCII字符集，使用7位表示一个字符，共128字符。ASCII的扩展字符集使用8位表示一个字符，共256字符，方便支持欧洲常用字符。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等\n  GBXXX字符集：\nGBK：最常用的中文码表。是在GB2312标准基础上的扩展规范，使用了双字节编码方案，共收录了21003个汉字，完全兼容GB2312标准，同时支持繁体汉字以及日韩汉字等\n  Unicode字符集：\nUTF-8编码：可以用来表示Unicode标准中任意字符，它是电子邮件、网页及其他存储或传送文字的应用 中，优先采用的编码。互联网工程工作小组（IETF）要求所有互联网协议都必须支持UTF-8编码。它使用一至四个字节为每个字符编码\n编码规则：\n128个US-ASCII字符，只需一个字节编码\n拉丁文等字符，需要二个字节编码\n大部分常用字（含中文），使用三个字节编码\n其他极少使用的Unicode辅助字符，使用四字节编码\n    1.3字符串中的编码解码问题【应用】   相关方法\n   方法名 说明     byte[] getBytes() 使用平台的默认字符集将该 String编码为一系列字节   byte[] getBytes(String charsetName) 使用指定的字符集将该 String编码为一系列字节   String(byte[] bytes) 使用平台的默认字符集解码指定的字节数组来创建字符串   String(byte[] bytes, String charsetName) 通过指定的字符集解码指定的字节数组来创建字符串      代码演示\npublic class StringDemo {  public static void main(String[] args) throws UnsupportedEncodingException {  //定义一个字符串  String s = \u0026#34;中国\u0026#34;;   //byte[] bys = s.getBytes(); //[-28, -72, -83, -27, -101, -67]  //byte[] bys = s.getBytes(\u0026#34;UTF-8\u0026#34;); //[-28, -72, -83, -27, -101, -67]  byte[] bys = s.getBytes(\u0026#34;GBK\u0026#34;); //[-42, -48, -71, -6]  System.out.println(Arrays.toString(bys));   //String ss = new String(bys);  //String ss = new String(bys,\u0026#34;UTF-8\u0026#34;);  String ss = new String(bys,\u0026#34;GBK\u0026#34;);  System.out.println(ss);  } }   1.4字符流写数据【应用】   介绍\nWriter: 用于写入字符流的抽象父类\nFileWriter: 用于写入字符流的常用子类\n  构造方法\n   方法名 说明     FileWriter(File file) 根据给定的 File 对象构造一个 FileWriter 对象   FileWriter(File file, boolean append) 根据给定的 File 对象构造一个 FileWriter 对象   FileWriter(String fileName) 根据给定的文件名构造一个 FileWriter 对象   FileWriter(String fileName, boolean append) 根据给定的文件名以及指示是否附加写入数据的 boolean 值来构造 FileWriter 对象      成员方法\n   方法名 说明     void write(int c) 写一个字符   void write(char[] cbuf) 写入一个字符数组   void write(char[] cbuf, int off, int len) 写入字符数组的一部分   void write(String str) 写一个字符串   void write(String str, int off, int len) 写一个字符串的一部分      刷新和关闭的方法\n   方法名 说明     flush() 刷新流，之后还可以继续写数据   close() 关闭流，释放资源，但是在关闭之前会先刷新流。一旦关闭，就不能再写数据      代码演示\npublic class OutputStreamWriterDemo {  public static void main(String[] args) throws IOException {  FileWriter fw = new FileWriter(\u0026#34;myCharStream\\\\a.txt\u0026#34;);   //void write(int c)：写一个字符 // fw.write(97); // fw.write(98); // fw.write(99);   //void writ(char[] cbuf)：写入一个字符数组  char[] chs = {\u0026#39;a\u0026#39;, \u0026#39;b\u0026#39;, \u0026#39;c\u0026#39;, \u0026#39;d\u0026#39;, \u0026#39;e\u0026#39;}; // fw.write(chs);   //void write(char[] cbuf, int off, int len)：写入字符数组的一部分 // fw.write(chs, 0, chs.length); // fw.write(chs, 1, 3);   //void write(String str)：写一个字符串 // fw.write(\u0026#34;abcde\u0026#34;);   //void write(String str, int off, int len)：写一个字符串的一部分 // fw.write(\u0026#34;abcde\u0026#34;, 0, \u0026#34;abcde\u0026#34;.length());  fw.write(\u0026#34;abcde\u0026#34;, 1, 3);   //释放资源  fw.close();  } }   1.5字符流读数据【应用】   介绍\nReader: 用于读取字符流的抽象父类\nFileReader: 用于读取字符流的常用子类\n  构造方法\n   方法名 说明     FileReader(File file) 在给定从中读取数据的 File 的情况下创建一个新 FileReader   FileReader(String fileName) 在给定从中读取数据的文件名的情况下创建一个新 FileReader        成员方法\n   方法名 说明     int read() 一次读一个字符数据   int read(char[] cbuf) 一次读一个字符数组数据      代码演示\npublic class InputStreamReaderDemo {  public static void main(String[] args) throws IOException {   FileReader fr = new FileReader(\u0026#34;myCharStream\\\\b.txt\u0026#34;);   //int read()：一次读一个字符数据 // int ch; // while ((ch=fr.read())!=-1) { // System.out.print((char)ch); // }   //int read(char[] cbuf)：一次读一个字符数组数据  char[] chs = new char[1024];  int len;  while ((len = fr.read(chs)) != -1) {  System.out.print(new String(chs, 0, len));  }   //释放资源  fr.close();  } }   1.6字符流用户注册案例【应用】   案例需求\n将键盘录入的用户名和密码保存到本地实现永久化存储\n  实现步骤\n 获取用户输入的用户名和密码 将用户输入的用户名和密码写入到本地文件中 关流,释放资源    代码实现\npublic class CharStreamDemo8 {  public static void main(String[] args) throws IOException {  //需求: 将键盘录入的用户名和密码保存到本地实现永久化存储  //要求：用户名独占一行，密码独占一行   //分析：  //1，实现键盘录入，把用户名和密码录入进来  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请录入用户名\u0026#34;);  String username = sc.next();  System.out.println(\u0026#34;请录入密码\u0026#34;);  String password = sc.next();   //2.分别把用户名和密码写到本地文件。  FileWriter fw = new FileWriter(\u0026#34;charstream\\\\a.txt\u0026#34;);  //将用户名和密码写到文件中  fw.write(username);  //表示写出一个回车换行符 windows \\r\\n MacOS \\r Linux \\n  fw.write(\u0026#34;\\r\\n\u0026#34;);  fw.write(password);  //刷新流  fw.flush();  //3.关流,释放资源  fw.close();  } }   1.7字符缓冲流【应用】   字符缓冲流介绍\n  BufferedWriter：将文本写入字符输出流，缓冲字符，以提供单个字符，数组和字符串的高效写入，可以指定缓冲区大小，或者可以接受默认大小。默认值足够大，可用于大多数用途\n  BufferedReader：从字符输入流读取文本，缓冲字符，以提供字符，数组和行的高效读取，可以指定缓冲区大小，或者可以使用默认大小。 默认值足够大，可用于大多数用途\n    构造方法\n   方法名 说明     BufferedWriter(Writer out) 创建字符缓冲输出流对象   BufferedReader(Reader in) 创建字符缓冲输入流对象      代码演示\npublic class BufferedStreamDemo01 {  public static void main(String[] args) throws IOException {  //BufferedWriter(Writer out)  BufferedWriter bw = new BufferedWriter(new FileWriter(\u0026#34;myCharStream\\\\bw.txt\u0026#34;));  bw.write(\u0026#34;hello\\r\\n\u0026#34;);  bw.write(\u0026#34;world\\r\\n\u0026#34;);  bw.close();   //BufferedReader(Reader in)  BufferedReader br = new BufferedReader(new FileReader(\u0026#34;myCharStream\\\\bw.txt\u0026#34;));   //一次读取一个字符数据 // int ch; // while ((ch=br.read())!=-1) { // System.out.print((char)ch); // }   //一次读取一个字符数组数据  char[] chs = new char[1024];  int len;  while ((len=br.read(chs))!=-1) {  System.out.print(new String(chs,0,len));  }   br.close();  } }   1.8字符缓冲流特有功能【应用】   方法介绍\nBufferedWriter：\n   方法名 说明     void newLine() 写一行行分隔符，行分隔符字符串由系统属性定义    BufferedReader:\n   方法名 说明     String readLine() 读一行文字。 结果包含行的内容的字符串，不包括任何行终止字符如果流的结尾已经到达，则为null      代码演示\npublic class BufferedStreamDemo02 {  public static void main(String[] args) throws IOException {   //创建字符缓冲输出流  BufferedWriter bw = new BufferedWriter(new FileWriter(\u0026#34;myCharStream\\\\bw.txt\u0026#34;));   //写数据  for (int i = 0; i \u0026lt; 10; i++) {  bw.write(\u0026#34;hello\u0026#34; + i);  //bw.write(\u0026#34;\\r\\n\u0026#34;);  bw.newLine();  bw.flush();  }   //释放资源  bw.close();   //创建字符缓冲输入流  BufferedReader br = new BufferedReader(new FileReader(\u0026#34;myCharStream\\\\bw.txt\u0026#34;));   String line;  while ((line=br.readLine())!=null) {  System.out.println(line);  }   br.close();  } }   1.9字符缓冲流操作文件中数据排序案例【应用】   案例需求\n使用字符缓冲流读取文件中的数据，排序后再次写到本地文件\n  实现步骤\n 将文件中的数据读取到程序中 对读取到的数据进行处理 将处理后的数据添加到集合中 对集合中的数据进行排序 将排序后的集合中的数据写入到文件中    代码实现\npublic class CharStreamDemo14 {  public static void main(String[] args) throws IOException {  //需求：读取文件中的数据，排序后再次写到本地文件  //分析：  //1.要把文件中的数据读取进来。  BufferedReader br = new BufferedReader(new FileReader(\u0026#34;charstream\\\\sort.txt\u0026#34;));  //输出流一定不能写在这里，因为会清空文件中的内容  //BufferedWriter bw = new BufferedWriter(new FileWriter(\u0026#34;charstream\\\\sort.txt\u0026#34;));   String line = br.readLine();  System.out.println(\u0026#34;读取到的数据为\u0026#34; + line);  br.close();   //2.按照空格进行切割  String[] split = line.split(\u0026#34; \u0026#34;);//9 1 2 5 3 10 4 6 7 8  //3.把字符串类型的数组变成int类型  int [] arr = new int[split.length];  //遍历split数组，可以进行类型转换。  for (int i = 0; i \u0026lt; split.length; i++) {  String smallStr = split[i];  //类型转换  int number = Integer.parseInt(smallStr);  //把转换后的结果存入到arr中  arr[i] = number;  }  //4.排序  Arrays.sort(arr);  System.out.println(Arrays.toString(arr));   //5.把排序之后结果写回到本地 1 2 3 4...  BufferedWriter bw = new BufferedWriter(new FileWriter(\u0026#34;charstream\\\\sort.txt\u0026#34;));  //写出  for (int i = 0; i \u0026lt; arr.length; i++) {  bw.write(arr[i] + \u0026#34; \u0026#34;);  bw.flush();  }  //释放资源  bw.close();   } }   1.10IO流小结【理解】   IO流小结\n  2.转换流 2.1字符流中和编码解码问题相关的两个类【理解】   InputStreamReader：是从字节流到字符流的桥梁,父类是Reader\n​\t它读取字节，并使用指定的编码将其解码为字符\n​\t它使用的字符集可以由名称指定，也可以被明确指定，或者可以接受平台的默认字符集\n  OutputStreamWriter：是从字符流到字节流的桥梁,父类是Writer\n​\t是从字符流到字节流的桥梁，使用指定的编码将写入的字符编码为字节\n​\t它使用的字符集可以由名称指定，也可以被明确指定，或者可以接受平台的默认字符集\n  2.2转换流读写数据【应用】   构造方法\n   方法名 说明     InputStreamReader(InputStream in) 使用默认字符编码创建InputStreamReader对象   InputStreamReader(InputStream in,String chatset) 使用指定的字符编码创建InputStreamReader对象   OutputStreamWriter(OutputStream out) 使用默认字符编码创建OutputStreamWriter对象   OutputStreamWriter(OutputStream out,String charset) 使用指定的字符编码创建OutputStreamWriter对象      代码演示\npublic class ConversionStreamDemo {  public static void main(String[] args) throws IOException {  //OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(\u0026#34;myCharStream\\\\osw.txt\u0026#34;));  OutputStreamWriter osw = new OutputStreamWriter(new FileOutputStream(\u0026#34;myCharStream\\\\osw.txt\u0026#34;),\u0026#34;GBK\u0026#34;);  osw.write(\u0026#34;中国\u0026#34;);  osw.close();   //InputStreamReader isr = new InputStreamReader(new FileInputStream(\u0026#34;myCharStream\\\\osw.txt\u0026#34;));  InputStreamReader isr = new InputStreamReader(new FileInputStream(\u0026#34;myCharStream\\\\osw.txt\u0026#34;),\u0026#34;GBK\u0026#34;);  //一次读取一个字符数据  int ch;  while ((ch=isr.read())!=-1) {  System.out.print((char)ch);  }  isr.close();  } }   3.对象操作流 3.1对象序列化流【应用】   对象序列化介绍\n 对象序列化：就是将对象保存到磁盘中，或者在网络中传输对象 这种机制就是使用一个字节序列表示一个对象，该字节序列包含：对象的类型、对象的数据和对象中存储的属性等信息 字节序列写到文件之后，相当于文件中持久保存了一个对象的信息 反之，该字节序列还可以从文件中读取回来，重构对象，对它进行反序列化    对象序列化流： ObjectOutputStream\n 将Java对象的原始数据类型和图形写入OutputStream。 可以使用ObjectInputStream读取（重构）对象。 可以通过使用流的文件来实现对象的持久存储。 如果流是网络套接字流，则可以在另一个主机上或另一个进程中重构对象    构造方法\n   方法名 说明     ObjectOutputStream(OutputStream out) 创建一个写入指定的OutputStream的ObjectOutputStream      序列化对象的方法\n   方法名 说明     void writeObject(Object obj) 将指定的对象写入ObjectOutputStream      示例代码\n学生类\npublic class Student implements Serializable {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } } 测试类\npublic class ObjectOutputStreamDemo {  public static void main(String[] args) throws IOException {  //ObjectOutputStream(OutputStream out)：创建一个写入指定的OutputStream的ObjectOutputStream  ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;myOtherStream\\\\oos.txt\u0026#34;));   //创建对象  Student s = new Student(\u0026#34;佟丽娅\u0026#34;,30);   //void writeObject(Object obj)：将指定的对象写入ObjectOutputStream  oos.writeObject(s);   //释放资源  oos.close();  } }   注意事项\n 一个对象要想被序列化，该对象所属的类必须必须实现Serializable 接口 Serializable是一个标记接口，实现该接口，不需要重写任何方法    3.2对象反序列化流【应用】   对象反序列化流： ObjectInputStream\n ObjectInputStream反序列化先前使用ObjectOutputStream编写的原始数据和对象    构造方法\n   方法名 说明     ObjectInputStream(InputStream in) 创建从指定的InputStream读取的ObjectInputStream      反序列化对象的方法\n   方法名 说明     Object readObject() 从ObjectInputStream读取一个对象      示例代码\npublic class ObjectInputStreamDemo {  public static void main(String[] args) throws IOException, ClassNotFoundException {  //ObjectInputStream(InputStream in)：创建从指定的InputStream读取的ObjectInputStream  ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\u0026#34;myOtherStream\\\\oos.txt\u0026#34;));   //Object readObject()：从ObjectInputStream读取一个对象  Object obj = ois.readObject();   Student s = (Student) obj;  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge());   ois.close();  } }   3.3serialVersionUID\u0026amp;transient【应用】   serialVersionUID\n 用对象序列化流序列化了一个对象后，假如我们修改了对象所属的类文件，读取数据会不会出问题呢？  会出问题，会抛出InvalidClassException异常   如果出问题了，如何解决呢？  重新序列化 给对象所属的类加一个serialVersionUID  private static final long serialVersionUID = 42L;        transient\n 如果一个对象中的某个成员变量的值不想被序列化，又该如何实现呢？  给该成员变量加transient关键字修饰，该关键字标记的成员变量不参与序列化过程      示例代码\n学生类\npublic class Student implements Serializable {  private static final long serialVersionUID = 42L;  private String name; // private int age;  private transient int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }  // @Override // public String toString() { // return \u0026#34;Student{\u0026#34; + // \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; + // \u0026#34;, age=\u0026#34; + age + // \u0026#39;}\u0026#39;; // } } 测试类\npublic class ObjectStreamDemo {  public static void main(String[] args) throws IOException, ClassNotFoundException { // write();  read();  }   //反序列化  private static void read() throws IOException, ClassNotFoundException {  ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\u0026#34;myOtherStream\\\\oos.txt\u0026#34;));  Object obj = ois.readObject();  Student s = (Student) obj;  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge());  ois.close();  }   //序列化  private static void write() throws IOException {  ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;myOtherStream\\\\oos.txt\u0026#34;));  Student s = new Student(\u0026#34;佟丽娅\u0026#34;, 30);  oos.writeObject(s);  oos.close();  } }   3.4对象操作流练习【应用】   案例需求\n创建多个学生类对象写到文件中,再次读取到内存中\n  实现步骤\n 创建序列化流对象 创建多个学生对象 将学生对象添加到集合中 将集合对象序列化到文件中 创建反序列化流对象 将文件中的对象数据,读取到内存中    代码实现\n学生类\npublic class Student implements Serializable{   private static final long serialVersionUID = 2L;   private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  } } 测试类\npublic class Demo03 {  /** * read(): * 读取到文件末尾返回值是 -1 * readLine(): * 读取到文件的末尾返回值 null * readObject(): * 读取到文件的末尾 直接抛出异常 * 如果要序列化的对象有多个,不建议直接将多个对象序列化到文件中,因为反序列化时容易出异常 * 建议: 将要序列化的多个对象存储到集合中,然后将集合序列化到文件中 */  public static void main(String[] args) throws Exception {  /*// 序列化 //1.创建序列化流对象 ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;myCode\\\\oos.txt\u0026#34;)); ArrayList\u0026lt;Student\u0026gt; arrayList = new ArrayList\u0026lt;\u0026gt;(); //2.创建多个学生对象 Student s = new Student(\u0026#34;佟丽娅\u0026#34;,30); Student s01 = new Student(\u0026#34;佟丽娅\u0026#34;,30); //3.将学生对象添加到集合中 arrayList.add(s); arrayList.add(s01); //4.将集合对象序列化到文件中 oos.writeObject(arrayList); oos.close();*/   // 反序列化  //5.创建反序列化流对象  ObjectInputStream ois = new ObjectInputStream(new FileInputStream(\u0026#34;myCode\\\\oos.txt\u0026#34;));  //6.将文件中的对象数据,读取到内存中  Object obj = ois.readObject();  ArrayList\u0026lt;Student\u0026gt; arrayList = (ArrayList\u0026lt;Student\u0026gt;)obj;  ois.close();  for (Student s : arrayList) {  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge());  }  } }   4.Properties集合 4.1Properties作为Map集合的使用【应用】   Properties介绍\n 是一个Map体系的集合类 Properties可以保存到流中或从流中加载 属性列表中的每个键及其对应的值都是一个字符串    Properties基本使用\npublic class PropertiesDemo01 {  public static void main(String[] args) {  //创建集合对象 // Properties\u0026lt;String,String\u0026gt; prop = new Properties\u0026lt;String,String\u0026gt;(); //错误  Properties prop = new Properties();   //存储元素  prop.put(\u0026#34;itheima001\u0026#34;, \u0026#34;佟丽娅\u0026#34;);  prop.put(\u0026#34;itheima002\u0026#34;, \u0026#34;赵丽颖\u0026#34;);  prop.put(\u0026#34;itheima003\u0026#34;, \u0026#34;刘诗诗\u0026#34;);   //遍历集合  Set\u0026lt;Object\u0026gt; keySet = prop.keySet();  for (Object key : keySet) {  Object value = prop.get(key);  System.out.println(key + \u0026#34;,\u0026#34; + value);  }  } }   4.2Properties作为Map集合的特有方法【应用】   特有方法\n   方法名 说明     Object setProperty(String key, String value) 设置集合的键和值，都是String类型，底层调用 Hashtable方法 put   String getProperty(String key) 使用此属性列表中指定的键搜索属性   Set stringPropertyNames() 从该属性列表中返回一个不可修改的键集，其中键及其对应的值是字符串      示例代码\npublic class PropertiesDemo02 {  public static void main(String[] args) {  //创建集合对象  Properties prop = new Properties();   //Object setProperty(String key, String value)：设置集合的键和值，都是String类型  prop.setProperty(\u0026#34;itheima001\u0026#34;, \u0026#34;佟丽娅\u0026#34;);  prop.setProperty(\u0026#34;itheima002\u0026#34;, \u0026#34;赵丽颖\u0026#34;);  prop.setProperty(\u0026#34;itheima003\u0026#34;, \u0026#34;刘诗诗\u0026#34;);   //String getProperty(String key)：使用此属性列表中指定的键搜索属性 // System.out.println(prop.getProperty(\u0026#34;itheima001\u0026#34;)); // System.out.println(prop.getProperty(\u0026#34;itheima0011\u0026#34;));  // System.out.println(prop);   //Set\u0026lt;String\u0026gt; stringPropertyNames()：从该属性列表中返回一个不可修改的键集，其中键及其对应的值是字符串  Set\u0026lt;String\u0026gt; names = prop.stringPropertyNames();  for (String key : names) { // System.out.println(key);  String value = prop.getProperty(key);  System.out.println(key + \u0026#34;,\u0026#34; + value);  }  } }   4.3Properties和IO流相结合的方法【应用】   和IO流结合的方法\n   方法名 说明     void load(Reader reader) 从输入字符流读取属性列表（键和元素对）   void store(Writer writer, String comments) 将此属性列表（键和元素对）写入此 Properties表中，以适合使用 load(Reader)方法的格式写入输出字符流      示例代码\npublic class PropertiesDemo03 {  public static void main(String[] args) throws IOException {  //把集合中的数据保存到文件 // myStore();   //把文件中的数据加载到集合  myLoad();   }   private static void myLoad() throws IOException {  Properties prop = new Properties();   //void load(Reader reader)：  FileReader fr = new FileReader(\u0026#34;myOtherStream\\\\fw.txt\u0026#34;);  prop.load(fr);  fr.close();   System.out.println(prop);  }   private static void myStore() throws IOException {  Properties prop = new Properties();   prop.setProperty(\u0026#34;itheima001\u0026#34;,\u0026#34;佟丽娅\u0026#34;);  prop.setProperty(\u0026#34;itheima002\u0026#34;,\u0026#34;赵丽颖\u0026#34;);  prop.setProperty(\u0026#34;itheima003\u0026#34;,\u0026#34;刘诗诗\u0026#34;);   //void store(Writer writer, String comments)：  FileWriter fw = new FileWriter(\u0026#34;myOtherStream\\\\fw.txt\u0026#34;);  prop.store(fw,null);  fw.close();  } }   4.4Properties集合练习【应用】   案例需求\n在Properties文件中手动写上姓名和年龄,读取到集合中,将该数据封装成学生对象,写到本地文件\n  实现步骤\n 创建Properties集合,将本地文件中的数据加载到集合中 获取集合中的键值对数据,封装到学生对象中 创建序列化流对象,将学生对象序列化到本地文件中    代码实现\n学生类\npublic class Student implements Serializable {  private static final long serialVersionUID = 1L;   private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } } 测试类\npublic class Test {   public static void main(String[] args) throws IOException {  //1.创建Properties集合,将本地文件中的数据加载到集合中  Properties prop = new Properties();  FileReader fr = new FileReader(\u0026#34;prop.properties\u0026#34;);  prop.load(fr);  fr.close(); \t//2.获取集合中的键值对数据,封装到学生对象中  String name = prop.getProperty(\u0026#34;name\u0026#34;);  int age = Integer.parseInt(prop.getProperty(\u0026#34;age\u0026#34;));  Student s = new Student(name,age); \t//3.创建序列化流对象,将学生对象序列化到本地文件中  ObjectOutputStream oos = new ObjectOutputStream(new FileOutputStream(\u0026#34;a.txt\u0026#34;));  oos.writeObject(s);  oos.close();  } }   ","permalink":"https://iblog.zone/archives/java-io%E6%B5%8102/","summary":"1.字符流 1.1为什么会出现字符流【理解】   字符流的介绍\n由于字节流操作中文不是特别的方便，所以Java就提供字符流\n字符流 = 字节流 + 编码表\n  中文的字节存储方式\n用字节流复制文本文件时，文本文件也会有中文，但是没有问题，原因是最终底层操作会自动进行字节拼接成中文，如何识别是中文的呢？\n汉字在存储的时候，无论选择哪种编码存储，第一个字节都是负数\n  1.2编码表【理解】   什么是字符集\n是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等\nl计算机要准确的存储和识别各种字符集符号，就需要进行字符编码，一套字符集必然至少有一套字符编码。常见字符集有ASCII字符集、GBXXX字符集、Unicode字符集等\n  常见的字符集\n  ASCII字符集：\nlASCII：是基于拉丁字母的一套电脑编码系统，用于显示现代英语，主要包括控制字符(回车键、退格、换行键等)和可显示字符(英文大小写字符、阿拉伯数字和西文符号)\n基本的ASCII字符集，使用7位表示一个字符，共128字符。ASCII的扩展字符集使用8位表示一个字符，共256字符，方便支持欧洲常用字符。是一个系统支持的所有字符的集合，包括各国家文字、标点符号、图形符号、数字等\n  GBXXX字符集：\nGBK：最常用的中文码表。是在GB2312标准基础上的扩展规范，使用了双字节编码方案，共收录了21003个汉字，完全兼容GB2312标准，同时支持繁体汉字以及日韩汉字等\n  Unicode字符集：\nUTF-8编码：可以用来表示Unicode标准中任意字符，它是电子邮件、网页及其他存储或传送文字的应用 中，优先采用的编码。互联网工程工作小组（IETF）要求所有互联网协议都必须支持UTF-8编码。它使用一至四个字节为每个字符编码\n编码规则：\n128个US-ASCII字符，只需一个字节编码\n拉丁文等字符，需要二个字节编码\n大部分常用字（含中文），使用三个字节编码\n其他极少使用的Unicode辅助字符，使用四字节编码\n    1.3字符串中的编码解码问题【应用】   相关方法\n   方法名 说明     byte[] getBytes() 使用平台的默认字符集将该 String编码为一系列字节   byte[] getBytes(String charsetName) 使用指定的字符集将该 String编码为一系列字节   String(byte[] bytes) 使用平台的默认字符集解码指定的字节数组来创建字符串   String(byte[] bytes, String charsetName) 通过指定的字符集解码指定的字节数组来创建字符串      代码演示","title":"Java IO流02"},{"content":"前端发布完页面后，无法正常显示，console控制台报\n在nginx配置文件中，增加header配置，有严格的顺序\n add_header Content-Security-Policy \u0026#34;default-src \u0026#39;none\u0026#39; 域名 \u0026#39;unsafe-inline\u0026#39; \u0026#39;unsafe-eval\u0026#39; blob: data: ;\u0026#34;;  add_header X-Xss-Protection \u0026#34;1;mode=block\u0026#34;;  add_header X-Content-Type-Options nosniff; 执行结果，页面正常显示\n","permalink":"https://iblog.zone/archives/nginx%E8%A7%A3%E5%86%B3%E5%86%85%E5%AE%B9%E5%AE%89%E5%85%A8%E7%AD%96%E7%95%A5cspcontent-security-policy%E9%85%8D%E7%BD%AE%E6%96%B9%E5%BC%8F/","summary":"前端发布完页面后，无法正常显示，console控制台报\n在nginx配置文件中，增加header配置，有严格的顺序\n add_header Content-Security-Policy \u0026#34;default-src \u0026#39;none\u0026#39; 域名 \u0026#39;unsafe-inline\u0026#39; \u0026#39;unsafe-eval\u0026#39; blob: data: ;\u0026#34;;  add_header X-Xss-Protection \u0026#34;1;mode=block\u0026#34;;  add_header X-Content-Type-Options nosniff; 执行结果，页面正常显示","title":"nginx解决内容安全策略CSP（Content-Security-Policy）配置方式"},{"content":"跨域脚本攻击 XSS 是最常见、危害最大的网页安全漏洞。\n为了防止它们，要采取很多编程措施，非常麻烦。很多人提出，能不能根本上解决问题，浏览器自动禁止外部注入恶意脚本？\n这就是\u0026quot;网页安全政策\u0026quot;（Content Security Policy，缩写 CSP）的来历。本文详细介绍如何使用 CSP 防止 XSS 攻击。\n一、简介 CSP 的实质就是白名单制度，开发者明确告诉客户端，哪些外部资源可以加载和执行，等同于提供白名单。它的实现和执行全部由浏览器完成，开发者只需提供配置。\nCSP 大大增强了网页的安全性。攻击者即使发现了漏洞，也没法注入脚本，除非还控制了一台列入了白名单的可信主机。\n两种方法可以启用 CSP。一种是通过 HTTP 头信息的Content-Security-Policy的字段。\n Content-Security-Policy: script-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; style-src cdn.example.org third-party.org; child-src https:  另一种是通过网页的\u0026lt;meta\u0026gt;标签。\n \u0026lt;meta http-equiv=\u0026#34;Content-Security-Policy\u0026#34; content=\u0026#34;script-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; style-src cdn.example.org third-party.org; child-src https:\u0026#34;\u0026gt;  上面代码中，CSP 做了如下配置。\n  脚本：只信任当前域名 \u0026lt;object\u0026gt;标签：不信任任何URL，即不加载任何资源 样式表：只信任cdn.example.org和third-party.org 框架（frame）：必须使用HTTPS协议加载 其他资源：没有限制   启用后，不符合 CSP 的外部资源就会被阻止加载。\nChrome 的报错信息。\nFirefox 的报错信息。\n二、限制选项 CSP 提供了很多限制选项，涉及安全的各个方面。\n2.1 资源加载限制 以下选项限制各类资源的加载。\n  script-src：外部脚本 style-src：样式表 img-src：图像 media-src：媒体文件（音频和视频） font-src：字体文件 object-src：插件（比如 Flash） child-src：框架 frame-ancestors：嵌入的外部资源（比如\u0026lt;frame\u0026gt;、\u0026lt;iframe\u0026gt;、\u0026lt;embed\u0026gt;和\u0026lt;applet\u0026gt;） connect-src：HTTP 连接（通过 XHR、WebSockets、EventSource等） worker-src：worker脚本 manifest-src：manifest 文件   2.2 default-src default-src用来设置上面各个选项的默认值。\n Content-Security-Policy: default-src \u0026#39;self\u0026#39;  上面代码限制所有的外部资源，都只能从当前域名加载。\n如果同时设置某个单项限制（比如font-src）和default-src，前者会覆盖后者，即字体文件会采用font-src的值，其他资源依然采用default-src的值。\n2.3 URL 限制 有时，网页会跟其他 URL 发生联系，这时也可以加以限制。\n  frame-ancestors：限制嵌入框架的网页 base-uri：限制\u0026lt;base#href\u0026gt; form-action：限制\u0026lt;form#action\u0026gt;   2.4 其他限制 其他一些安全相关的功能，也放在了 CSP 里面。\n  block-all-mixed-content：HTTPS 网页不得加载 HTTP 资源（浏览器已经默认开启） upgrade-insecure-requests：自动将网页上所有加载外部资源的 HTTP 链接换成 HTTPS 协议 plugin-types：限制可以使用的插件格式 sandbox：浏览器行为的限制，比如不能有弹出窗口等。   2.5 report-uri 有时，我们不仅希望防止 XSS，还希望记录此类行为。report-uri就用来告诉浏览器，应该把注入行为报告给哪个网址。\n Content-Security-Policy: default-src \u0026#39;self\u0026#39;; ...; report-uri /my_amazing_csp_report_parser;  上面代码指定，将注入行为报告给/my_amazing_csp_report_parser这个 URL。\n浏览器会使用POST方法，发送一个JSON对象，下面是一个例子。\n {  \u0026#34;csp-report\u0026#34;: {  \u0026#34;document-uri\u0026#34;: \u0026#34;http://example.org/page.html\u0026#34;,  \u0026#34;referrer\u0026#34;: \u0026#34;http://evil.example.com/\u0026#34;,  \u0026#34;blocked-uri\u0026#34;: \u0026#34;http://evil.example.com/evil.js\u0026#34;,  \u0026#34;violated-directive\u0026#34;: \u0026#34;script-src \u0026#39;self\u0026#39; https://apis.google.com\u0026#34;,  \u0026#34;original-policy\u0026#34;: \u0026#34;script-src \u0026#39;self\u0026#39; https://apis.google.com; report-uri http://example.org/my_amazing_csp_report_parser\u0026#34;  } }  三、Content-Security-Policy-Report-Only 除了Content-Security-Policy，还有一个Content-Security-Policy-Report-Only字段，表示不执行限制选项，只是记录违反限制的行为。\n它必须与report-uri选项配合使用。\n Content-Security-Policy-Report-Only: default-src \u0026#39;self\u0026#39;; ...; report-uri /my_amazing_csp_report_parser;  四、选项值 每个限制选项可以设置以下几种值，这些值就构成了白名单。\n  主机名：example.org，https://example.com:443 路径名：example.org/resources/js/ 通配符：*.example.org，*://*.example.com:*（表示任意协议、任意子域名、任意端口） 协议名：https:、data: 关键字'self'：当前域名，需要加引号 关键字'none'：禁止加载任何外部资源，需要加引号   多个值也可以并列，用空格分隔。\n Content-Security-Policy: script-src \u0026#39;self\u0026#39; https://apis.google.com  如果同一个限制选项使用多次，只有第一次会生效。\n # 错误的写法 script-src https://host1.com; script-src https://host2.com # 正确的写法 script-src https://host1.com https://host2.com  如果不设置某个限制选项，就是默认允许任何值。\n五、script-src 的特殊值 除了常规值，script-src还可以设置一些特殊值。注意，下面这些值都必须放在单引号里面。\n  'unsafe-inline'：允许执行页面内嵌的\u0026lt;script\u0026gt;标签和事件监听函数 unsafe-eval：允许将字符串当作代码执行，比如使用eval、setTimeout、setInterval和Function等函数。 nonce值：每次HTTP回应给出一个授权token，页面内嵌脚本必须有这个token，才会执行 hash值：列出允许执行的脚本代码的Hash值，页面内嵌脚本的哈希值只有吻合的情况下，才能执行。   nonce值的例子如下，服务器发送网页的时候，告诉浏览器一个随机生成的token。\n Content-Security-Policy: script-src \u0026#39;nonce-EDNnf03nceIOfn39fn3e9h3sdfa\u0026#39;  页面内嵌脚本，必须有这个token才能执行。\n \u0026lt;script nonce=EDNnf03nceIOfn39fn3e9h3sdfa\u0026gt;  // some code \u0026lt;/script\u0026gt;  hash值的例子如下，服务器给出一个允许执行的代码的hash值。\n Content-Security-Policy: script-src \u0026#39;sha256-qznLcsROx4GACP2dm0UCKCzCG-HiZ1guq6ZZDob_Tng=\u0026#39;  下面的代码就会允许执行，因为hash值相符。\n \u0026lt;script\u0026gt;alert(\u0026#39;Hello, world.\u0026#39;);\u0026lt;/script\u0026gt;  注意，计算hash值的时候，\u0026lt;script\u0026gt;标签不算在内。\n除了script-src选项，nonce值和hash值还可以用在style-src选项，控制页面内嵌的样式表。\n六、注意点 （1）script-src和object-src是必设的，除非设置了default-src。\n因为攻击者只要能注入脚本，其他限制都可以规避。而object-src必设是因为 Flash 里面可以执行外部脚本。\n（2）script-src不能使用unsafe-inline关键字（除非伴随一个nonce值），也不能允许设置data:URL。\n下面是两个恶意攻击的例子。\n \u0026lt;img src=\u0026#34;x\u0026#34; onerror=\u0026#34;evil()\u0026#34;\u0026gt; \u0026lt;script src=\u0026#34;data:text/javascript,evil()\u0026#34;\u0026gt;\u0026lt;/script\u0026gt;  （3）必须特别注意 JSONP 的回调函数。\n \u0026lt;script src=\u0026#34;/path/jsonp?callback=alert(document.domain)//\u0026#34;\u0026gt; \u0026lt;/script\u0026gt;  上面的代码中，虽然加载的脚本来自当前域名，但是通过改写回调函数，攻击者依然可以执行恶意代码。\n七、参考链接  CSP Is Dead, Long Live CSP! , by Lukas Weichselbaum An Introduction to Content Security Policy, by Mike West  ","permalink":"https://iblog.zone/archives/content-security-policy-%E5%85%A5%E9%97%A8%E6%95%99%E7%A8%8B/","summary":"跨域脚本攻击 XSS 是最常见、危害最大的网页安全漏洞。\n为了防止它们，要采取很多编程措施，非常麻烦。很多人提出，能不能根本上解决问题，浏览器自动禁止外部注入恶意脚本？\n这就是\u0026quot;网页安全政策\u0026quot;（Content Security Policy，缩写 CSP）的来历。本文详细介绍如何使用 CSP 防止 XSS 攻击。\n一、简介 CSP 的实质就是白名单制度，开发者明确告诉客户端，哪些外部资源可以加载和执行，等同于提供白名单。它的实现和执行全部由浏览器完成，开发者只需提供配置。\nCSP 大大增强了网页的安全性。攻击者即使发现了漏洞，也没法注入脚本，除非还控制了一台列入了白名单的可信主机。\n两种方法可以启用 CSP。一种是通过 HTTP 头信息的Content-Security-Policy的字段。\n Content-Security-Policy: script-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; style-src cdn.example.org third-party.org; child-src https:  另一种是通过网页的\u0026lt;meta\u0026gt;标签。\n \u0026lt;meta http-equiv=\u0026#34;Content-Security-Policy\u0026#34; content=\u0026#34;script-src \u0026#39;self\u0026#39;; object-src \u0026#39;none\u0026#39;; style-src cdn.example.org third-party.org; child-src https:\u0026#34;\u0026gt;  上面代码中，CSP 做了如下配置。\n  脚本：只信任当前域名 \u0026lt;object\u0026gt;标签：不信任任何URL，即不加载任何资源 样式表：只信任cdn.example.org和third-party.org 框架（frame）：必须使用HTTPS协议加载 其他资源：没有限制   启用后，不符合 CSP 的外部资源就会被阻止加载。\nChrome 的报错信息。\nFirefox 的报错信息。\n二、限制选项 CSP 提供了很多限制选项，涉及安全的各个方面。\n2.1 资源加载限制 以下选项限制各类资源的加载。","title":"Content Security Policy 入门教程"},{"content":"1.File类 1.1File类概述和构造方法【应用】   File类介绍\n 它是文件和目录路径名的抽象表示 文件和目录是可以通过File封装成对象的 对于File而言,其封装的并不是一个真正存在的文件,仅仅是一个路径名而已.它可以是存在的,也可以是不存在的.将来是要通过具体的操作把这个路径的内容转换为具体存在的    File类的构造方法\n   方法名 说明     File(String pathname) 通过将给定的路径名字符串转换为抽象路径名来创建新的 File实例   File(String parent, String child) 从父路径名字符串和子路径名字符串创建新的 File实例   File(File parent, String child) 从父抽象路径名和子路径名字符串创建新的 File实例      示例代码\npublic class FileDemo01 {  public static void main(String[] args) {  //File(String pathname): 通过将给定的路径名字符串转换为抽象路径名来创建新的 File实例  File f1 = new File(\u0026#34;E:\\\\itcast\\\\java.txt\u0026#34;);  System.out.println(f1);   //File(String parent, String child): 从父路径名字符串和子路径名字符串创建新的 File实例  File f2 = new File(\u0026#34;E:\\\\itcast\u0026#34;,\u0026#34;java.txt\u0026#34;);  System.out.println(f2);   //File(File parent, String child): 从父抽象路径名和子路径名字符串创建新的 File实例  File f3 = new File(\u0026#34;E:\\\\itcast\u0026#34;);  File f4 = new File(f3,\u0026#34;java.txt\u0026#34;);  System.out.println(f4);  } }   1.2绝对路径和相对路径【理解】   绝对路径\n是一个完整的路径,从盘符开始\n  相对路径\n是一个简化的路径,相对当前项目下的路径\n  示例代码\npublic class FileDemo02 {  public static void main(String[] args) {  // 是一个完整的路径,从盘符开始  File file1 = new File(\u0026#34;D:\\\\itheima\\\\a.txt\u0026#34;);   // 是一个简化的路径,从当前项目根目录开始  File file2 = new File(\u0026#34;a.txt\u0026#34;);  File file3 = new File(\u0026#34;模块名\\\\a.txt\u0026#34;);  } }   1.3File类创建功能【应用】   方法分类\n   方法名 说明     public boolean createNewFile() 当具有该名称的文件不存在时，创建一个由该抽象路径名命名的新空文件   public boolean mkdir() 创建由此抽象路径名命名的目录   public boolean mkdirs() 创建由此抽象路径名命名的目录，包括任何必需但不存在的父目录      示例代码\npublic class FileDemo02 {  public static void main(String[] args) throws IOException {  //需求1：我要在E:\\\\itcast目录下创建一个文件java.txt  File f1 = new File(\u0026#34;E:\\\\itcast\\\\java.txt\u0026#34;);  System.out.println(f1.createNewFile());  System.out.println(\u0026#34;--------\u0026#34;);   //需求2：我要在E:\\\\itcast目录下创建一个目录JavaSE  File f2 = new File(\u0026#34;E:\\\\itcast\\\\JavaSE\u0026#34;);  System.out.println(f2.mkdir());  System.out.println(\u0026#34;--------\u0026#34;);   //需求3：我要在E:\\\\itcast目录下创建一个多级目录JavaWEB\\\\HTML  File f3 = new File(\u0026#34;E:\\\\itcast\\\\JavaWEB\\\\HTML\u0026#34;); // System.out.println(f3.mkdir());  System.out.println(f3.mkdirs());  System.out.println(\u0026#34;--------\u0026#34;);   //需求4：我要在E:\\\\itcast目录下创建一个文件javase.txt  File f4 = new File(\u0026#34;E:\\\\itcast\\\\javase.txt\u0026#34;); // System.out.println(f4.mkdir());  System.out.println(f4.createNewFile());  } }   1.4File类删除功能【应用】   方法分类\n   方法名 说明     public boolean delete() 删除由此抽象路径名表示的文件或目录      示例代码\npublic class FileDemo03 {  public static void main(String[] args) throws IOException { // File f1 = new File(\u0026#34;E:\\\\itcast\\\\java.txt\u0026#34;);  //需求1：在当前模块目录下创建java.txt文件  File f1 = new File(\u0026#34;myFile\\\\java.txt\u0026#34;); // System.out.println(f1.createNewFile());   //需求2：删除当前模块目录下的java.txt文件  System.out.println(f1.delete());  System.out.println(\u0026#34;--------\u0026#34;);   //需求3：在当前模块目录下创建itcast目录  File f2 = new File(\u0026#34;myFile\\\\itcast\u0026#34;); // System.out.println(f2.mkdir());   //需求4：删除当前模块目录下的itcast目录  System.out.println(f2.delete());  System.out.println(\u0026#34;--------\u0026#34;);   //需求5：在当前模块下创建一个目录itcast,然后在该目录下创建一个文件java.txt  File f3 = new File(\u0026#34;myFile\\\\itcast\u0026#34;); // System.out.println(f3.mkdir());  File f4 = new File(\u0026#34;myFile\\\\itcast\\\\java.txt\u0026#34;); // System.out.println(f4.createNewFile());   //需求6：删除当前模块下的目录itcast  System.out.println(f4.delete());  System.out.println(f3.delete());  } }   1.5File类判断和获取功能【应用】   判断功能\n   方法名 说明     public boolean isDirectory() 测试此抽象路径名表示的File是否为目录   public boolean isFile() 测试此抽象路径名表示的File是否为文件   public boolean exists() 测试此抽象路径名表示的File是否存在      获取功能\n   方法名 说明     public String getAbsolutePath() 返回此抽象路径名的绝对路径名字符串   public String getPath() 将此抽象路径名转换为路径名字符串   public String getName() 返回由此抽象路径名表示的文件或目录的名称   public File[] listFiles() 返回此抽象路径名表示的目录中的文件和目录的File对象数组      示例代码\npublic class FileDemo04 {  public static void main(String[] args) {  //创建一个File对象  File f = new File(\u0026#34;myFile\\\\java.txt\u0026#34;);  // public boolean isDirectory()：测试此抽象路径名表示的File是否为目录 // public boolean isFile()：测试此抽象路径名表示的File是否为文件 // public boolean exists()：测试此抽象路径名表示的File是否存在  System.out.println(f.isDirectory());  System.out.println(f.isFile());  System.out.println(f.exists());  // public String getAbsolutePath()：返回此抽象路径名的绝对路径名字符串 // public String getPath()：将此抽象路径名转换为路径名字符串 // public String getName()：返回由此抽象路径名表示的文件或目录的名称  System.out.println(f.getAbsolutePath());  System.out.println(f.getPath());  System.out.println(f.getName());  System.out.println(\u0026#34;--------\u0026#34;);  // public File[] listFiles()：返回此抽象路径名表示的目录中的文件和目录的File对象数组  File f2 = new File(\u0026#34;E:\\\\itcast\u0026#34;);  File[] fileArray = f2.listFiles();  for(File file : fileArray) { // System.out.println(file); // System.out.println(file.getName());  if(file.isFile()) {  System.out.println(file.getName());  }  }  } }   1.6File类练习一【应用】   案例需求\n在当前模块下的aaa文件夹中创建一个a.txt文件\n  实现步骤\n 创建File对象,指向aaa文件夹 判断aaa文件夹是否存在,如果不存在则创建 创建File对象,指向aaa文件夹下的a.txt文件 创建这个文件    代码实现\npublic class Test1 {  public static void main(String[] args) throws IOException {  //练习一：在当前模块下的aaa文件夹中创建一个a.txt文件  /* File file = new File(\u0026#34;filemodule\\\\aaa\\\\a.txt\u0026#34;); file.createNewFile();*/  //注意点:文件所在的文件夹必须要存在.   //1.创建File对象,指向aaa文件夹  File file = new File(\u0026#34;filemodule\\\\aaa\u0026#34;);  //2.判断aaa文件夹是否存在,如果不存在则创建  if(!file.exists()){  //如果文件夹不存在,就创建出来  file.mkdirs();  }  //3.创建File对象,指向aaa文件夹下的a.txt文件  File newFile = new File(file,\u0026#34;a.txt\u0026#34;);  //4.创建这个文件  newFile.createNewFile();  } }   1.7File类练习二【应用】   案例需求\n删除一个多级文件夹\n  实现步骤\n 定义一个方法,接收一个File对象 遍历这个File对象,获取它下边的每个文件和文件夹对象 判断当前遍历到的File对象是文件还是文件夹 如果是文件,直接删除 如果是文件夹,递归调用自己,将当前遍历到的File对象当做参数传递 参数传递过来的文件夹File对象已经处理完成,最后直接删除这个空文件夹    代码实现\npublic class Test2 {  public static void main(String[] args) {  //练习二：删除一个多级文件夹  //delete方法  //只能删除文件和空文件夹.  //如果现在要删除一个有内容的文件夹?  //先删掉这个文件夹里面所有的内容.  //最后再删除这个文件夹   File src = new File(\u0026#34;C:\\\\Users\\\\apple\\\\Desktop\\\\src\u0026#34;);  deleteDir(src);  }  \t//1.定义一个方法,接收一个File对象  private static void deleteDir(File src) {  //先删掉这个文件夹里面所有的内容.  //递归 方法在方法体中自己调用自己.  //注意: 可以解决所有文件夹和递归相结合的题目  //2.遍历这个File对象,获取它下边的每个文件和文件夹对象  File[] files = src.listFiles();  //3.判断当前遍历到的File对象是文件还是文件夹  for (File file : files) {  //4.如果是文件,直接删除  if(file.isFile()){  file.delete();  }else{  //5.如果是文件夹,递归调用自己,将当前遍历到的File对象当做参数传递  deleteDir(file);//参数一定要是src文件夹里面的文件夹File对象  }  }  //6.参数传递过来的文件夹File对象已经处理完成,最后直接删除这个空文件夹  src.delete();  }  }   1.8File类练习三【应用】   案例需求\n统计一个文件夹中每种文件的个数并打印\n打印格式如下：\n txt:3个 doc:4个 jpg:6个 …    实现步骤\n 定义一个方法,参数是HashMap集合用来统计次数和File对象要统计的文件夹 遍历File对象,获取它下边的每一个文件和文件夹对象 判断当前File对象是文件还是文件夹 如果是文件,判断这种类型文件后缀名在HashMap集合中是否出现过  没出现过,将这种类型文件的后缀名存入集合中,次数存1 出现过,获取这种类型文件的后缀名出现的次数,对其+1,在存回集合中   如果是文件夹,递归调用自己,HashMap集合就是参数集合,File对象是当前文件夹对象    代码实现\npublic class Test3 {  public static void main(String[] args) {  //统计一个文件夹中,每种文件出现的次数.  //统计 --- 定义一个变量用来统计. ---- 弊端:同时只能统计一种文件  //利用map集合进行数据统计,键 --- 文件后缀名 值 ---- 次数   File file = new File(\u0026#34;filemodule\u0026#34;);  HashMap\u0026lt;String, Integer\u0026gt; hm = new HashMap\u0026lt;\u0026gt;();  getCount(hm, file);  System.out.println(hm);  }  \t//1.定义一个方法,参数是HashMap集合用来统计次数和File对象要统计的文件夹  private static void getCount(HashMap\u0026lt;String, Integer\u0026gt; hm, File file) {  //2.遍历File对象,获取它下边的每一个文件和文件夹对象  File[] files = file.listFiles();  for (File f : files) {  //3.判断当前File对象是文件还是文件夹  if(f.isFile()){  //如果是文件,判断这种类型文件后缀名在HashMap集合中是否出现过  String fileName = f.getName();  String[] fileNameArr = fileName.split(\u0026#34;\\\\.\u0026#34;);  if(fileNameArr.length == 2){  String fileEndName = fileNameArr[1];  if(hm.containsKey(fileEndName)){  //出现过,获取这种类型文件的后缀名出现的次数,对其+1,在存回集合中  Integer count = hm.get(fileEndName);  //这种文件又出现了一次.  count++;  //把已经出现的次数给覆盖掉.  hm.put(fileEndName,count);  }else{  // 没出现过,将这种类型文件的后缀名存入集合中,次数存1  hm.put(fileEndName,1);  }  }  }else{  //如果是文件夹,递归调用自己,HashMap集合就是参数集合,File对象是当前文件夹对象代码实现  getCount(hm,f);  }  }  }  }   2.字节流 2.1 IO流概述和分类【理解】  IO流介绍  IO：输入/输出(Input/Output) 流：是一种抽象概念,是对数据传输的总称.也就是说数据在设备间的传输称为流,流的本质是数据传输 IO流就是用来处理设备间数据传输问题的.常见的应用: 文件复制; 文件上传; 文件下载   IO流的分类  按照数据的流向  输入流：读数据 输出流：写数据   按照数据类型来分  字节流  字节输入流 字节输出流   字符流  字符输入流 字符输出流       IO流的使用场景  如果操作的是纯文本文件,优先使用字符流 如果操作的是图片、视频、音频等二进制文件,优先使用字节流 如果不确定文件类型,优先使用字节流.字节流是万能的流    2.2字节流写数据【应用】   字节流抽象基类\n InputStream：这个抽象类是表示字节输入流的所有类的超类 OutputStream：这个抽象类是表示字节输出流的所有类的超类 子类名特点：子类名称都是以其父类名作为子类名的后缀    字节输出流\n FileOutputStream(String name)：创建文件输出流以指定的名称写入文件    使用字节输出流写数据的步骤\n 创建字节输出流对象(调用系统功能创建了文件,创建字节输出流对象,让字节输出流对象指向文件) 调用字节输出流对象的写数据方法 释放资源(关闭此文件输出流并释放与此流相关联的任何系统资源)    示例代码\npublic class FileOutputStreamDemo01 {  public static void main(String[] args) throws IOException {  //创建字节输出流对象  /* 注意点: 1.如果文件不存在,会帮我们创建 2.如果文件存在,会把文件清空 */  //FileOutputStream(String name)：创建文件输出流以指定的名称写入文件  FileOutputStream fos = new FileOutputStream(\u0026#34;myByteStream\\\\fos.txt\u0026#34;);   //void write(int b)：将指定的字节写入此文件输出流  fos.write(97); // fos.write(57); // fos.write(55);   //最后都要释放资源  //void close()：关闭此文件输出流并释放与此流相关联的任何系统资源。  fos.close();  } }   2.3字节流写数据的三种方式【应用】   写数据的方法分类\n   方法名 说明     void write(int b) 将指定的字节写入此文件输出流 一次写一个字节数据   void write(byte[] b) 将 b.length字节从指定的字节数组写入此文件输出流 一次写一个字节数组数据   void write(byte[] b, int off, int len) 将 len字节从指定的字节数组开始，从偏移量off开始写入此文件输出流 一次写一个字节数组的部分数据      示例代码\npublic class FileOutputStreamDemo02 {  public static void main(String[] args) throws IOException {  //FileOutputStream(String name)：创建文件输出流以指定的名称写入文件  FileOutputStream fos = new FileOutputStream(\u0026#34;myByteStream\\\\fos.txt\u0026#34;);  //FileOutputStream(File file)：创建文件输出流以写入由指定的 File对象表示的文件 // FileOutputStream fos = new FileOutputStream(new File(\u0026#34;myByteStream\\\\fos.txt\u0026#34;));   //void write(int b)：将指定的字节写入此文件输出流 // fos.write(97); // fos.write(98); // fos.write(99); // fos.write(100); // fos.write(101);  // void write(byte[] b)：将 b.length字节从指定的字节数组写入此文件输出流 // byte[] bys = {97, 98, 99, 100, 101};  //byte[] getBytes()：返回字符串对应的字节数组  byte[] bys = \u0026#34;abcde\u0026#34;.getBytes(); // fos.write(bys);   //void write(byte[] b, int off, int len)：将 len字节从指定的字节数组开始，从偏移量off开始写入此文件输出流 // fos.write(bys,0,bys.length);  fos.write(bys,1,3);   //释放资源  fos.close();  } }   2.4字节流写数据的两个小问题【应用】   字节流写数据如何实现换行\n windows:\\r\\n linux:\\n mac:\\r    字节流写数据如何实现追加写入\n public FileOutputStream(String name,boolean append) 创建文件输出流以指定的名称写入文件。如果第二个参数为true ，则字节将写入文件的末尾而不是开头    示例代码\npublic class FileOutputStreamDemo03 {  public static void main(String[] args) throws IOException {  //创建字节输出流对象 // FileOutputStream fos = new FileOutputStream(\u0026#34;myByteStream\\\\fos.txt\u0026#34;);  FileOutputStream fos = new FileOutputStream(\u0026#34;myByteStream\\\\fos.txt\u0026#34;,true);   //写数据  for (int i = 0; i \u0026lt; 10; i++) {  fos.write(\u0026#34;hello\u0026#34;.getBytes());  fos.write(\u0026#34;\\r\\n\u0026#34;.getBytes());  }   //释放资源  fos.close();  } }   2.5字节流写数据加异常处理【应用】   异常处理格式\n  try-catch-finally\ntry{ \t可能出现异常的代码; }catch(异常类名 变量名){ \t异常的处理代码; }finally{ \t执行所有清除操作; }   finally特点\n 被finally控制的语句一定会执行，除非JVM退出      示例代码\npublic class FileOutputStreamDemo04 {  public static void main(String[] args) {  //加入finally来实现释放资源  FileOutputStream fos = null;  try {  fos = new FileOutputStream(\u0026#34;myByteStream\\\\fos.txt\u0026#34;);  fos.write(\u0026#34;hello\u0026#34;.getBytes());  } catch (IOException e) {  e.printStackTrace();  } finally {  if(fos != null) {  try {  fos.close();  } catch (IOException e) {  e.printStackTrace();  }  }  }  } }   2.6字节流读数据(一次读一个字节数据)【应用】   字节输入流\n FileInputStream(String name)：通过打开与实际文件的连接来创建一个FileInputStream,该文件由文件系统中的路径名name命名    字节输入流读取数据的步骤\n 创建字节输入流对象 调用字节输入流对象的读数据方法 释放资源    示例代码\npublic class FileInputStreamDemo01 {  public static void main(String[] args) throws IOException {  //创建字节输入流对象  //FileInputStream(String name)  FileInputStream fis = new FileInputStream(\u0026#34;myByteStream\\\\fos.txt\u0026#34;);   int by;  /* fis.read()：读数据 by=fis.read()：把读取到的数据赋值给by by != -1：判断读取到的数据是否是-1 */  while ((by=fis.read())!=-1) {  System.out.print((char)by);  }   //释放资源  fis.close();  } }   2.7字节流复制文件【应用】   案例需求\n​\t把“E:\\itcast\\窗里窗外.txt”复制到模块目录下的“窗里窗外.txt” (文件可以是任意文件)\n  实现步骤\n  复制文本文件，其实就把文本文件的内容从一个文件中读取出来(数据源)，然后写入到另一个文件中(目的地)\n  数据源：\n​\tE:\\itcast\\窗里窗外.txt \u0026mdash; 读数据 \u0026mdash; InputStream \u0026mdash; FileInputStream\n  目的地：\n​\tmyByteStream\\窗里窗外.txt \u0026mdash; 写数据 \u0026mdash; OutputStream \u0026mdash; FileOutputStream\n    代码实现\npublic class CopyTxtDemo {  public static void main(String[] args) throws IOException {  //根据数据源创建字节输入流对象  FileInputStream fis = new FileInputStream(\u0026#34;E:\\\\itcast\\\\窗里窗外.txt\u0026#34;);  //根据目的地创建字节输出流对象  FileOutputStream fos = new FileOutputStream(\u0026#34;myByteStream\\\\窗里窗外.txt\u0026#34;);   //读写数据，复制文本文件(一次读取一个字节，一次写入一个字节)  int by;  while ((by=fis.read())!=-1) {  fos.write(by);  }   //释放资源  fos.close();  fis.close();  } }   2.8字节流读数据(一次读一个字节数组数据)【应用】   一次读一个字节数组的方法\n public int read(byte[] b)：从输入流读取最多b.length个字节的数据 返回的是读入缓冲区的总字节数,也就是实际的读取字节个数    示例代码\npublic class FileInputStreamDemo02 {  public static void main(String[] args) throws IOException {  //创建字节输入流对象  FileInputStream fis = new FileInputStream(\u0026#34;myByteStream\\\\fos.txt\u0026#34;);   byte[] bys = new byte[1024]; //1024及其整数倍  int len;  //循环读取  while ((len=fis.read(bys))!=-1) {  System.out.print(new String(bys,0,len));  }   //释放资源  fis.close();  } }   2.9字节流复制文件【应用】   案例需求\n​\t把“E:\\itcast\\mn.jpg”复制到模块目录下的“mn.jpg” (文件可以是任意文件去)\n  实现步骤\n 根据数据源创建字节输入流对象 根据目的地创建字节输出流对象 读写数据，复制图片(一次读取一个字节数组，一次写入一个字节数组) 释放资源    代码实现\npublic class CopyJpgDemo {  public static void main(String[] args) throws IOException {  //根据数据源创建字节输入流对象  FileInputStream fis = new FileInputStream(\u0026#34;E:\\\\itcast\\\\mn.jpg\u0026#34;);  //根据目的地创建字节输出流对象  FileOutputStream fos = new FileOutputStream(\u0026#34;myByteStream\\\\mn.jpg\u0026#34;);   //读写数据，复制图片(一次读取一个字节数组，一次写入一个字节数组)  byte[] bys = new byte[1024];  int len;  while ((len=fis.read(bys))!=-1) {  fos.write(bys,0,len);  }   //释放资源  fos.close();  fis.close();  } }   3.字节缓冲流 3.1字节缓冲流构造方法【应用】   字节缓冲流介绍\n lBufferOutputStream：该类实现缓冲输出流.通过设置这样的输出流,应用程序可以向底层输出流写入字节,而不必为写入的每个字节导致底层系统的调用 lBufferedInputStream：创建BufferedInputStream将创建一个内部缓冲区数组.当从流中读取或跳过字节时,内部缓冲区将根据需要从所包含的输入流中重新填充,一次很多字节    构造方法：\n   方法名 说明     BufferedOutputStream(OutputStream out) 创建字节缓冲输出流对象   BufferedInputStream(InputStream in) 创建字节缓冲输入流对象      示例代码\npublic class BufferStreamDemo {  public static void main(String[] args) throws IOException {  //字节缓冲输出流：BufferedOutputStream(OutputStream out)   BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;myByteStream\\\\bos.txt\u0026#34;));  //写数据  bos.write(\u0026#34;hello\\r\\n\u0026#34;.getBytes());  bos.write(\u0026#34;world\\r\\n\u0026#34;.getBytes());  //释放资源  bos.close();    //字节缓冲输入流：BufferedInputStream(InputStream in)  BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;myByteStream\\\\bos.txt\u0026#34;));   //一次读取一个字节数据 // int by; // while ((by=bis.read())!=-1) { // System.out.print((char)by); // }   //一次读取一个字节数组数据  byte[] bys = new byte[1024];  int len;  while ((len=bis.read(bys))!=-1) {  System.out.print(new String(bys,0,len));  }   //释放资源  bis.close();  } }   3.2字节缓冲流复制视频【应用】   案例需求\n把“E:\\itcast\\字节流复制图片.avi”复制到模块目录下的“字节流复制图片.avi”\n  实现步骤\n 根据数据源创建字节输入流对象 根据目的地创建字节输出流对象 读写数据，复制视频 释放资源    代码实现\npublic class CopyAviDemo {  public static void main(String[] args) throws IOException {   //复制视频 // method1();  method2();   }   //字节缓冲流一次读写一个字节数组  public static void method2() throws IOException {  BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;E:\\\\itcast\\\\字节流复制图片.avi\u0026#34;));  BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;myByteStream\\\\字节流复制图片.avi\u0026#34;));   byte[] bys = new byte[1024];  int len;  while ((len=bis.read(bys))!=-1) {  bos.write(bys,0,len);  }   bos.close();  bis.close();  }   //字节缓冲流一次读写一个字节  public static void method1() throws IOException {  BufferedInputStream bis = new BufferedInputStream(new FileInputStream(\u0026#34;E:\\\\itcast\\\\字节流复制图片.avi\u0026#34;));  BufferedOutputStream bos = new BufferedOutputStream(new FileOutputStream(\u0026#34;myByteStream\\\\字节流复制图片.avi\u0026#34;));   int by;  while ((by=bis.read())!=-1) {  bos.write(by);  }   bos.close();  bis.close();  }  }   ","permalink":"https://iblog.zone/archives/java-io%E6%B5%8101/","summary":"1.File类 1.1File类概述和构造方法【应用】   File类介绍\n 它是文件和目录路径名的抽象表示 文件和目录是可以通过File封装成对象的 对于File而言,其封装的并不是一个真正存在的文件,仅仅是一个路径名而已.它可以是存在的,也可以是不存在的.将来是要通过具体的操作把这个路径的内容转换为具体存在的    File类的构造方法\n   方法名 说明     File(String pathname) 通过将给定的路径名字符串转换为抽象路径名来创建新的 File实例   File(String parent, String child) 从父路径名字符串和子路径名字符串创建新的 File实例   File(File parent, String child) 从父抽象路径名和子路径名字符串创建新的 File实例      示例代码\npublic class FileDemo01 {  public static void main(String[] args) {  //File(String pathname): 通过将给定的路径名字符串转换为抽象路径名来创建新的 File实例  File f1 = new File(\u0026#34;E:\\\\itcast\\\\java.txt\u0026#34;);  System.","title":"Java IO流01"},{"content":"1.Map集合 1.1Map集合概述和特点【理解】   Map集合概述\ninterface Map\u0026lt;K,V\u0026gt; K：键的类型；V：值的类型   Map集合的特点\n 双列集合,一个键对应一个值 键不可以重复,值可以重复    Map集合的基本使用\npublic class MapDemo01 {  public static void main(String[] args) {  //创建集合对象  Map\u0026lt;String,String\u0026gt; map = new HashMap\u0026lt;String,String\u0026gt;();   //V put(K key, V value) 将指定的值与该映射中的指定键相关联  map.put(\u0026#34;itheima001\u0026#34;,\u0026#34;林青霞\u0026#34;);  map.put(\u0026#34;itheima002\u0026#34;,\u0026#34;张曼玉\u0026#34;);  map.put(\u0026#34;itheima003\u0026#34;,\u0026#34;王祖贤\u0026#34;);  map.put(\u0026#34;itheima003\u0026#34;,\u0026#34;柳岩\u0026#34;);   //输出集合对象  System.out.println(map);  } }   1.2Map集合的基本功能【应用】   方法介绍\n   方法名 说明     V put(K key,V value) 添加元素   V remove(Object key) 根据键删除键值对元素   void clear() 移除所有的键值对元素   boolean containsKey(Object key) 判断集合是否包含指定的键   boolean containsValue(Object value) 判断集合是否包含指定的值   boolean isEmpty() 判断集合是否为空   int size() 集合的长度，也就是集合中键值对的个数      示例代码\npublic class MapDemo02 {  public static void main(String[] args) {  //创建集合对象  Map\u0026lt;String,String\u0026gt; map = new HashMap\u0026lt;String,String\u0026gt;();   //V put(K key,V value)：添加元素  map.put(\u0026#34;张无忌\u0026#34;,\u0026#34;赵敏\u0026#34;);  map.put(\u0026#34;郭靖\u0026#34;,\u0026#34;黄蓉\u0026#34;);  map.put(\u0026#34;杨过\u0026#34;,\u0026#34;小龙女\u0026#34;);   //V remove(Object key)：根据键删除键值对元素 // System.out.println(map.remove(\u0026#34;郭靖\u0026#34;)); // System.out.println(map.remove(\u0026#34;郭襄\u0026#34;));   //void clear()：移除所有的键值对元素 // map.clear();   //boolean containsKey(Object key)：判断集合是否包含指定的键 // System.out.println(map.containsKey(\u0026#34;郭靖\u0026#34;)); // System.out.println(map.containsKey(\u0026#34;郭襄\u0026#34;));   //boolean isEmpty()：判断集合是否为空 // System.out.println(map.isEmpty());   //int size()：集合的长度，也就是集合中键值对的个数  System.out.println(map.size());   //输出集合对象  System.out.println(map);  } }   1.3Map集合的获取功能【应用】   方法介绍\n   方法名 说明     V get(Object key) 根据键获取值   Set\u0026lt;K\u0026gt; keySet() 获取所有键的集合   Collection\u0026lt;V\u0026gt; values() 获取所有值的集合   Set\u0026lt;Map.Entry\u0026lt;K,V\u0026raquo; entrySet() 获取所有键值对对象的集合      示例代码\npublic class MapDemo03 {  public static void main(String[] args) {  //创建集合对象  Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;String, String\u0026gt;();   //添加元素  map.put(\u0026#34;张无忌\u0026#34;, \u0026#34;赵敏\u0026#34;);  map.put(\u0026#34;郭靖\u0026#34;, \u0026#34;黄蓉\u0026#34;);  map.put(\u0026#34;杨过\u0026#34;, \u0026#34;小龙女\u0026#34;);   //V get(Object key):根据键获取值 // System.out.println(map.get(\u0026#34;张无忌\u0026#34;)); // System.out.println(map.get(\u0026#34;张三丰\u0026#34;));   //Set\u0026lt;K\u0026gt; keySet():获取所有键的集合 // Set\u0026lt;String\u0026gt; keySet = map.keySet(); // for(String key : keySet) { // System.out.println(key); // }   //Collection\u0026lt;V\u0026gt; values():获取所有值的集合  Collection\u0026lt;String\u0026gt; values = map.values();  for(String value : values) {  System.out.println(value);  }  } }   1.4Map集合的遍历(方式1)【应用】   遍历思路\n 我们刚才存储的元素都是成对出现的，所以我们把Map看成是一个夫妻对的集合  把所有的丈夫给集中起来 遍历丈夫的集合，获取到每一个丈夫 根据丈夫去找对应的妻子      步骤分析\n 获取所有键的集合。用keySet()方法实现 遍历键的集合，获取到每一个键。用增强for实现 根据键去找值。用get(Object key)方法实现    代码实现\npublic class MapDemo01 {  public static void main(String[] args) {  //创建集合对象  Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;String, String\u0026gt;();   //添加元素  map.put(\u0026#34;张无忌\u0026#34;, \u0026#34;赵敏\u0026#34;);  map.put(\u0026#34;郭靖\u0026#34;, \u0026#34;黄蓉\u0026#34;);  map.put(\u0026#34;杨过\u0026#34;, \u0026#34;小龙女\u0026#34;);   //获取所有键的集合。用keySet()方法实现  Set\u0026lt;String\u0026gt; keySet = map.keySet();  //遍历键的集合，获取到每一个键。用增强for实现  for (String key : keySet) {  //根据键去找值。用get(Object key)方法实现  String value = map.get(key);  System.out.println(key + \u0026#34;,\u0026#34; + value);  }  } }   1.5Map集合的遍历(方式2)【应用】   遍历思路\n 我们刚才存储的元素都是成对出现的，所以我们把Map看成是一个夫妻对的集合  获取所有结婚证的集合 遍历结婚证的集合，得到每一个结婚证 根据结婚证获取丈夫和妻子      步骤分析\n 获取所有键值对对象的集合  Set\u0026lt;Map.Entry\u0026lt;K,V\u0026raquo; entrySet()：获取所有键值对对象的集合   遍历键值对对象的集合，得到每一个键值对对象  用增强for实现，得到每一个Map.Entry   根据键值对对象获取键和值  用getKey()得到键 用getValue()得到值      代码实现\npublic class MapDemo02 {  public static void main(String[] args) {  //创建集合对象  Map\u0026lt;String, String\u0026gt; map = new HashMap\u0026lt;String, String\u0026gt;();   //添加元素  map.put(\u0026#34;张无忌\u0026#34;, \u0026#34;赵敏\u0026#34;);  map.put(\u0026#34;郭靖\u0026#34;, \u0026#34;黄蓉\u0026#34;);  map.put(\u0026#34;杨过\u0026#34;, \u0026#34;小龙女\u0026#34;);   //获取所有键值对对象的集合  Set\u0026lt;Map.Entry\u0026lt;String, String\u0026gt;\u0026gt; entrySet = map.entrySet();  //遍历键值对对象的集合，得到每一个键值对对象  for (Map.Entry\u0026lt;String, String\u0026gt; me : entrySet) {  //根据键值对对象获取键和值  String key = me.getKey();  String value = me.getValue();  System.out.println(key + \u0026#34;,\u0026#34; + value);  }  } }   2.HashMap集合 2.1HashMap集合概述和特点【理解】  HashMap底层是哈希表结构的 依赖hashCode方法和equals方法保证键的唯一 如果键要存储的是自定义对象，需要重写hashCode和equals方法  2.2HashMap集合应用案例【应用】   案例需求\n 创建一个HashMap集合，键是学生对象(Student)，值是居住地 (String)。存储多个元素，并遍历。 要求保证键的唯一性：如果学生对象的成员变量值相同，我们就认为是同一个对象    代码实现\n学生类\npublic class Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public boolean equals(Object o) {  if (this == o) return true;  if (o == null || getClass() != o.getClass()) return false;   Student student = (Student) o;   if (age != student.age) return false;  return name != null ? name.equals(student.name) : student.name == null;  }   @Override  public int hashCode() {  int result = name != null ? name.hashCode() : 0;  result = 31 * result + age;  return result;  } } 测试类\npublic class HashMapDemo {  public static void main(String[] args) {  //创建HashMap集合对象  HashMap\u0026lt;Student, String\u0026gt; hm = new HashMap\u0026lt;Student, String\u0026gt;();   //创建学生对象  Student s1 = new Student(\u0026#34;林青霞\u0026#34;, 30);  Student s2 = new Student(\u0026#34;张曼玉\u0026#34;, 35);  Student s3 = new Student(\u0026#34;王祖贤\u0026#34;, 33);  Student s4 = new Student(\u0026#34;王祖贤\u0026#34;, 33);   //把学生添加到集合  hm.put(s1, \u0026#34;西安\u0026#34;);  hm.put(s2, \u0026#34;武汉\u0026#34;);  hm.put(s3, \u0026#34;郑州\u0026#34;);  hm.put(s4, \u0026#34;北京\u0026#34;);   //遍历集合  Set\u0026lt;Student\u0026gt; keySet = hm.keySet();  for (Student key : keySet) {  String value = hm.get(key);  System.out.println(key.getName() + \u0026#34;,\u0026#34; + key.getAge() + \u0026#34;,\u0026#34; + value);  }  } }   3.TreeMap集合 3.1TreeMap集合概述和特点【理解】  TreeMap底层是红黑树结构 依赖自然排序或者比较器排序,对键进行排序 如果键存储的是自定义对象,需要实现Comparable接口或者在创建TreeMap对象时候给出比较器排序规则  3.2TreeMap集合应用案例一【应用】   案例需求\n 创建一个TreeMap集合,键是学生对象(Student),值是籍贯(String),学生属性姓名和年龄,按照年龄进行排序并遍历 要求按照学生的年龄进行排序,如果年龄相同则按照姓名进行排序    代码实现\n学生类\npublic class Student implements Comparable\u0026lt;Student\u0026gt;{  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  }   @Override  public int compareTo(Student o) {  //按照年龄进行排序  int result = o.getAge() - this.getAge();  //次要条件，按照姓名排序。  result = result == 0 ? o.getName().compareTo(this.getName()) : result;  return result;  } } 测试类\npublic class Test1 {  public static void main(String[] args) {  // 创建TreeMap集合对象  TreeMap\u0026lt;Student,String\u0026gt; tm = new TreeMap\u0026lt;\u0026gt;();  \t// 创建学生对象  Student s1 = new Student(\u0026#34;xiaohei\u0026#34;,23);  Student s2 = new Student(\u0026#34;dapang\u0026#34;,22);  Student s3 = new Student(\u0026#34;xiaomei\u0026#34;,22);  \t// 将学生对象添加到TreeMap集合中  tm.put(s1,\u0026#34;江苏\u0026#34;);  tm.put(s2,\u0026#34;北京\u0026#34;);  tm.put(s3,\u0026#34;天津\u0026#34;);  \t// 遍历TreeMap集合,打印每个学生的信息  tm.forEach(  (Student key, String value)-\u0026gt;{  System.out.println(key + \u0026#34;---\u0026#34; + value);  }  );  } }   3.3TreeMap集合应用案例二【应用】   案例需求\n 给定一个字符串,要求统计字符串中每个字符出现的次数。 举例: 给定字符串是“aababcabcdabcde”,在控制台输出: “a(5)b(4)c(3)d(2)e(1)”    代码实现\npublic class Test2 {  public static void main(String[] args) {  // 给定字符串  String s = \u0026#34;aababcabcdabcde\u0026#34;; \t// 创建TreeMap集合对象,键是Character,值是Integer  TreeMap\u0026lt;Character,Integer\u0026gt; tm = new TreeMap\u0026lt;\u0026gt;();   //遍历字符串，得到每一个字符  for (int i = 0; i \u0026lt; s.length(); i++) {  //c依次表示字符串中的每一个字符  char c = s.charAt(i);  // 判断当前遍历到的字符是否在集合中出现过  if(!tm.containsKey(c)){  //表示当前字符是第一次出现。  tm.put(c,1);  }else{  //存在，表示当前字符已经出现过了  //先获取这个字符已经出现的次数  Integer count = tm.get(c);  //自增，表示这个字符又出现了依次  count++;  //将自增后的结果再次添加到集合中。  tm.put(c,count);  }  }  // a（5）b（4）c（3）d（2）e（1）  //System.out.println(tm);  tm.forEach(  (Character key,Integer value)-\u0026gt;{  System.out.print(key + \u0026#34;（\u0026#34; + value + \u0026#34;）\u0026#34;);  }  );  } }   4.可变参数 4.1可变参数【应用】   可变参数介绍\n 可变参数又称参数个数可变，用作方法的形参出现，那么方法参数个数就是可变的了 方法的参数类型已经确定,个数不确定,我们可以使用可变参数    可变参数定义格式\n修饰符 返回值类型 方法名(数据类型… 变量名) { }   可变参数的注意事项\n 这里的变量其实是一个数组 如果一个方法有多个参数，包含可变参数，可变参数要放在最后    可变参数的基本使用\npublic class ArgsDemo01 {  public static void main(String[] args) {  System.out.println(sum(10, 20));  System.out.println(sum(10, 20, 30));  System.out.println(sum(10, 20, 30, 40));   System.out.println(sum(10,20,30,40,50));  System.out.println(sum(10,20,30,40,50,60));  System.out.println(sum(10,20,30,40,50,60,70));  System.out.println(sum(10,20,30,40,50,60,70,80,90,100));  }  // public static int sum(int b,int... a) { // return 0; // }   public static int sum(int... a) {  int sum = 0;  for(int i : a) {  sum += i;  }  return sum;  } }   4.2创建不可变集合【理解】   方法介绍\n 在List、Set、Map接口中,都存在of方法,可以创建一个不可变的集合  这个集合不能添加,不能删除,不能修改 但是可以结合集合的带参构造,实现集合的批量添加   在Map接口中,还有一个ofEntries方法可以提高代码的阅读性  首先会把键值对封装成一个Entry对象,再把这个Entry对象添加到集合当中      示例代码\npublic class MyVariableParameter4 {  public static void main(String[] args) {  // static \u0026lt;E\u0026gt; List\u0026lt;E\u0026gt; of(E…elements) 创建一个具有指定元素的List集合对象  //static \u0026lt;E\u0026gt; Set\u0026lt;E\u0026gt; of(E…elements) 创建一个具有指定元素的Set集合对象  //static \u0026lt;K , V\u0026gt; Map\u0026lt;K，V\u0026gt; of(E…elements) 创建一个具有指定元素的Map集合对象   //method1();  //method2();  //method3();  //method4();   }   private static void method4() {  Map\u0026lt;String, String\u0026gt; map = Map.ofEntries(  Map.entry(\u0026#34;zhangsan\u0026#34;, \u0026#34;江苏\u0026#34;),  Map.entry(\u0026#34;lisi\u0026#34;, \u0026#34;北京\u0026#34;));  System.out.println(map);  }   private static void method3() {  Map\u0026lt;String, String\u0026gt; map = Map.of(\u0026#34;zhangsan\u0026#34;, \u0026#34;江苏\u0026#34;, \u0026#34;lisi\u0026#34;, \u0026#34;北京\u0026#34;, \u0026#34;wangwu\u0026#34;, \u0026#34;天津\u0026#34;);  System.out.println(map);  }   private static void method2() {  //传递的参数当中，不能存在重复的元素。  Set\u0026lt;String\u0026gt; set = Set.of(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;,\u0026#34;a\u0026#34;);  System.out.println(set);  }   private static void method1() {  List\u0026lt;String\u0026gt; list = List.of(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;);  System.out.println(list);  //list.add(\u0026#34;Q\u0026#34;);  //list.remove(\u0026#34;a\u0026#34;);  //list.set(0,\u0026#34;A\u0026#34;);  //System.out.println(list);  // ArrayList\u0026lt;String\u0026gt; list2 = new ArrayList\u0026lt;\u0026gt;(); // list2.add(\u0026#34;aaa\u0026#34;); // list2.add(\u0026#34;aaa\u0026#34;); // list2.add(\u0026#34;aaa\u0026#34;); // list2.add(\u0026#34;aaa\u0026#34;);   //集合的批量添加。  //首先是通过调用List.of方法来创建一个不可变的集合，of方法的形参就是一个可变参数。  //再创建一个ArrayList集合，并把这个不可变的集合中所有的数据，都添加到ArrayList中。  ArrayList\u0026lt;String\u0026gt; list3 = new ArrayList\u0026lt;\u0026gt;(List.of(\u0026#34;a\u0026#34;, \u0026#34;b\u0026#34;, \u0026#34;c\u0026#34;, \u0026#34;d\u0026#34;));  System.out.println(list3);  } }   5.Stream流 5.1体验Stream流【理解】   案例需求\n按照下面的要求完成集合的创建和遍历\n 创建一个集合，存储多个字符串元素 把集合中所有以\u0026quot;张\u0026quot;开头的元素存储到一个新的集合 把\u0026quot;张\u0026quot;开头的集合中的长度为3的元素存储到一个新的集合 遍历上一步得到的集合    原始方式示例代码\npublic class StreamDemo {  public static void main(String[] args) {  //创建一个集合，存储多个字符串元素  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();   list.add(\u0026#34;林青霞\u0026#34;);  list.add(\u0026#34;张曼玉\u0026#34;);  list.add(\u0026#34;王祖贤\u0026#34;);  list.add(\u0026#34;柳岩\u0026#34;);  list.add(\u0026#34;张敏\u0026#34;);  list.add(\u0026#34;张无忌\u0026#34;);   //把集合中所有以\u0026#34;张\u0026#34;开头的元素存储到一个新的集合  ArrayList\u0026lt;String\u0026gt; zhangList = new ArrayList\u0026lt;String\u0026gt;();   for(String s : list) {  if(s.startsWith(\u0026#34;张\u0026#34;)) {  zhangList.add(s);  }  }  // System.out.println(zhangList);   //把\u0026#34;张\u0026#34;开头的集合中的长度为3的元素存储到一个新的集合  ArrayList\u0026lt;String\u0026gt; threeList = new ArrayList\u0026lt;String\u0026gt;();   for(String s : zhangList) {  if(s.length() == 3) {  threeList.add(s);  }  }  // System.out.println(threeList);   //遍历上一步得到的集合  for(String s : threeList) {  System.out.println(s);  }  System.out.println(\u0026#34;--------\u0026#34;);   //Stream流来改进 // list.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;张\u0026#34;)).filter(s -\u0026gt; s.length() == 3).forEach(s -\u0026gt; System.out.println(s));  list.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;张\u0026#34;)).filter(s -\u0026gt; s.length() == 3).forEach(System.out::println);  } }   使用Stream流示例代码\npublic class StreamDemo {  public static void main(String[] args) {  //创建一个集合，存储多个字符串元素  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();   list.add(\u0026#34;林青霞\u0026#34;);  list.add(\u0026#34;张曼玉\u0026#34;);  list.add(\u0026#34;王祖贤\u0026#34;);  list.add(\u0026#34;柳岩\u0026#34;);  list.add(\u0026#34;张敏\u0026#34;);  list.add(\u0026#34;张无忌\u0026#34;);   //Stream流来改进  list.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;张\u0026#34;)).filter(s -\u0026gt; s.length() == 3).forEach(System.out::println);  } }   Stream流的好处\n 直接阅读代码的字面意思即可完美展示无关逻辑方式的语义：获取流、过滤姓张、过滤长度为3、逐一打印 Stream流把真正的函数式编程风格引入到Java中 代码简洁    5.2Stream流的常见生成方式【应用】   Stream流的思想\n  Stream流的三类方法\n 获取Stream流  创建一条流水线,并把数据放到流水线上准备进行操作   中间方法  流水线上的操作 一次操作完毕之后,还可以继续进行其他操作   终结方法  一个Stream流只能有一个终结方法 是流水线上的最后一个操作      生成Stream流的方式\n  Collection体系集合\n使用默认方法stream()生成流， default Stream stream()\n  Map体系集合\n把Map转成Set集合，间接的生成流\n  数组\n通过Arrays中的静态方法stream生成流\n  同种数据类型的多个数据\n通过Stream接口的静态方法of(T\u0026hellip; values)生成流\n    代码演示\npublic class StreamDemo {  public static void main(String[] args) {  //Collection体系的集合可以使用默认方法stream()生成流  List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();  Stream\u0026lt;String\u0026gt; listStream = list.stream();   Set\u0026lt;String\u0026gt; set = new HashSet\u0026lt;String\u0026gt;();  Stream\u0026lt;String\u0026gt; setStream = set.stream();   //Map体系的集合间接的生成流  Map\u0026lt;String,Integer\u0026gt; map = new HashMap\u0026lt;String, Integer\u0026gt;();  Stream\u0026lt;String\u0026gt; keyStream = map.keySet().stream();  Stream\u0026lt;Integer\u0026gt; valueStream = map.values().stream();  Stream\u0026lt;Map.Entry\u0026lt;String, Integer\u0026gt;\u0026gt; entryStream = map.entrySet().stream();   //数组可以通过Arrays中的静态方法stream生成流  String[] strArray = {\u0026#34;hello\u0026#34;,\u0026#34;world\u0026#34;,\u0026#34;java\u0026#34;};  Stream\u0026lt;String\u0026gt; strArrayStream = Arrays.stream(strArray);   //同种数据类型的多个数据可以通过Stream接口的静态方法of(T... values)生成流  Stream\u0026lt;String\u0026gt; strArrayStream2 = Stream.of(\u0026#34;hello\u0026#34;, \u0026#34;world\u0026#34;, \u0026#34;java\u0026#34;);  Stream\u0026lt;Integer\u0026gt; intStream = Stream.of(10, 20, 30);  } }   5.3Stream流中间操作方法【应用】   概念\n中间操作的意思是,执行完此方法之后,Stream流依然可以继续执行其他操作\n  常见方法\n   方法名 说明     Stream\u0026lt;T\u0026gt; filter(Predicate predicate) 用于对流中的数据进行过滤   Stream\u0026lt;T\u0026gt; limit(long maxSize) 返回此流中的元素组成的流，截取前指定参数个数的数据   Stream\u0026lt;T\u0026gt; skip(long n) 跳过指定参数个数的数据，返回由该流的剩余元素组成的流   static \u0026lt;T\u0026gt; Stream\u0026lt;T\u0026gt; concat(Stream a, Stream b) 合并a和b两个流为一个流   Stream\u0026lt;T\u0026gt; distinct() 返回由该流的不同元素（根据Object.equals(Object) ）组成的流      filter代码演示\npublic class StreamDemo01 {  public static void main(String[] args) {  //创建一个集合，存储多个字符串元素  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();   list.add(\u0026#34;林青霞\u0026#34;);  list.add(\u0026#34;张曼玉\u0026#34;);  list.add(\u0026#34;王祖贤\u0026#34;);  list.add(\u0026#34;柳岩\u0026#34;);  list.add(\u0026#34;张敏\u0026#34;);  list.add(\u0026#34;张无忌\u0026#34;);   //需求1：把list集合中以张开头的元素在控制台输出  list.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;张\u0026#34;)).forEach(System.out::println);  System.out.println(\u0026#34;--------\u0026#34;);   //需求2：把list集合中长度为3的元素在控制台输出  list.stream().filter(s -\u0026gt; s.length() == 3).forEach(System.out::println);  System.out.println(\u0026#34;--------\u0026#34;);   //需求3：把list集合中以张开头的，长度为3的元素在控制台输出  list.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;张\u0026#34;)).filter(s -\u0026gt; s.length() == 3).forEach(System.out::println);  } }   limit\u0026amp;skip代码演示\npublic class StreamDemo02 {  public static void main(String[] args) {  //创建一个集合，存储多个字符串元素  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();   list.add(\u0026#34;林青霞\u0026#34;);  list.add(\u0026#34;张曼玉\u0026#34;);  list.add(\u0026#34;王祖贤\u0026#34;);  list.add(\u0026#34;柳岩\u0026#34;);  list.add(\u0026#34;张敏\u0026#34;);  list.add(\u0026#34;张无忌\u0026#34;);   //需求1：取前3个数据在控制台输出  list.stream().limit(3).forEach(System.out::println);  System.out.println(\u0026#34;--------\u0026#34;);   //需求2：跳过3个元素，把剩下的元素在控制台输出  list.stream().skip(3).forEach(System.out::println);  System.out.println(\u0026#34;--------\u0026#34;);   //需求3：跳过2个元素，把剩下的元素中前2个在控制台输出  list.stream().skip(2).limit(2).forEach(System.out::println);  } }   concat\u0026amp;distinct代码演示\npublic class StreamDemo03 {  public static void main(String[] args) {  //创建一个集合，存储多个字符串元素  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();   list.add(\u0026#34;林青霞\u0026#34;);  list.add(\u0026#34;张曼玉\u0026#34;);  list.add(\u0026#34;王祖贤\u0026#34;);  list.add(\u0026#34;柳岩\u0026#34;);  list.add(\u0026#34;张敏\u0026#34;);  list.add(\u0026#34;张无忌\u0026#34;);   //需求1：取前4个数据组成一个流  Stream\u0026lt;String\u0026gt; s1 = list.stream().limit(4);   //需求2：跳过2个数据组成一个流  Stream\u0026lt;String\u0026gt; s2 = list.stream().skip(2);   //需求3：合并需求1和需求2得到的流，并把结果在控制台输出 // Stream.concat(s1,s2).forEach(System.out::println);   //需求4：合并需求1和需求2得到的流，并把结果在控制台输出，要求字符串元素不能重复  Stream.concat(s1,s2).distinct().forEach(System.out::println);  } }   5.4Stream流终结操作方法【应用】   概念\n终结操作的意思是,执行完此方法之后,Stream流将不能再执行其他操作\n  常见方法\n   方法名 说明     void forEach(Consumer action) 对此流的每个元素执行操作   long count() 返回此流中的元素数      代码演示\npublic class StreamDemo {  public static void main(String[] args) {  //创建一个集合，存储多个字符串元素  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();   list.add(\u0026#34;林青霞\u0026#34;);  list.add(\u0026#34;张曼玉\u0026#34;);  list.add(\u0026#34;王祖贤\u0026#34;);  list.add(\u0026#34;柳岩\u0026#34;);  list.add(\u0026#34;张敏\u0026#34;);  list.add(\u0026#34;张无忌\u0026#34;);   //需求1：把集合中的元素在控制台输出 // list.stream().forEach(System.out::println);   //需求2：统计集合中有几个以张开头的元素，并把统计结果在控制台输出  long count = list.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;张\u0026#34;)).count();  System.out.println(count);  } }   5.5Stream流的收集操作【应用】   概念\n对数据使用Stream流的方式操作完毕后,可以把流中的数据收集到集合中\n  常用方法\n   方法名 说明     R collect(Collector collector) 把结果收集到集合中      工具类Collectors提供了具体的收集方式\n   方法名 说明     public static \u0026lt;T\u0026gt; Collector toList() 把元素收集到List集合中   public static \u0026lt;T\u0026gt; Collector toSet() 把元素收集到Set集合中   public static Collector toMap(Function keyMapper,Function valueMapper) 把元素收集到Map集合中      代码演示\npublic class CollectDemo {  public static void main(String[] args) {  //创建List集合对象  List\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;String\u0026gt;();  list.add(\u0026#34;林青霞\u0026#34;);  list.add(\u0026#34;张曼玉\u0026#34;);  list.add(\u0026#34;王祖贤\u0026#34;);  list.add(\u0026#34;柳岩\u0026#34;);   /* //需求1：得到名字为3个字的流 Stream\u0026lt;String\u0026gt; listStream = list.stream().filter(s -\u0026gt; s.length() == 3); //需求2：把使用Stream流操作完毕的数据收集到List集合中并遍历 List\u0026lt;String\u0026gt; names = listStream.collect(Collectors.toList()); for(String name : names) { System.out.println(name); } */   //创建Set集合对象  Set\u0026lt;Integer\u0026gt; set = new HashSet\u0026lt;Integer\u0026gt;();  set.add(10);  set.add(20);  set.add(30);  set.add(33);  set.add(35);   /* //需求3：得到年龄大于25的流 Stream\u0026lt;Integer\u0026gt; setStream = set.stream().filter(age -\u0026gt; age \u0026gt; 25); //需求4：把使用Stream流操作完毕的数据收集到Set集合中并遍历 Set\u0026lt;Integer\u0026gt; ages = setStream.collect(Collectors.toSet()); for(Integer age : ages) { System.out.println(age); } */  //定义一个字符串数组，每一个字符串数据由姓名数据和年龄数据组合而成  String[] strArray = {\u0026#34;林青霞,30\u0026#34;, \u0026#34;张曼玉,35\u0026#34;, \u0026#34;王祖贤,33\u0026#34;, \u0026#34;柳岩,25\u0026#34;};   //需求5：得到字符串中年龄数据大于28的流  Stream\u0026lt;String\u0026gt; arrayStream = Stream.of(strArray).filter(s -\u0026gt; Integer.parseInt(s.split(\u0026#34;,\u0026#34;)[1]) \u0026gt; 28);   //需求6：把使用Stream流操作完毕的数据收集到Map集合中并遍历，字符串中的姓名作键，年龄作值  Map\u0026lt;String, Integer\u0026gt; map = arrayStream.collect(Collectors.toMap(s -\u0026gt; s.split(\u0026#34;,\u0026#34;)[0], s -\u0026gt; Integer.parseInt(s.split(\u0026#34;,\u0026#34;)[1])));   Set\u0026lt;String\u0026gt; keySet = map.keySet();  for (String key : keySet) {  Integer value = map.get(key);  System.out.println(key + \u0026#34;,\u0026#34; + value);  }  } }   5.6Stream流综合练习【应用】   案例需求\n现在有两个ArrayList集合，分别存储6名男演员名称和6名女演员名称，要求完成如下的操作\n 男演员只要名字为3个字的前三人 女演员只要姓林的，并且不要第一个 把过滤后的男演员姓名和女演员姓名合并到一起 把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据  演员类Actor已经提供，里面有一个成员变量，一个带参构造方法，以及成员变量对应的get/set方法\n  代码实现\n演员类\npublic class Actor {  private String name;   public Actor(String name) {  this.name = name;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  } } 测试类\npublic class StreamTest {  public static void main(String[] args) {  //创建集合  ArrayList\u0026lt;String\u0026gt; manList = new ArrayList\u0026lt;String\u0026gt;();  manList.add(\u0026#34;周润发\u0026#34;);  manList.add(\u0026#34;成龙\u0026#34;);  manList.add(\u0026#34;刘德华\u0026#34;);  manList.add(\u0026#34;吴京\u0026#34;);  manList.add(\u0026#34;周星驰\u0026#34;);  manList.add(\u0026#34;李连杰\u0026#34;);   ArrayList\u0026lt;String\u0026gt; womanList = new ArrayList\u0026lt;String\u0026gt;();  womanList.add(\u0026#34;林心如\u0026#34;);  womanList.add(\u0026#34;张曼玉\u0026#34;);  womanList.add(\u0026#34;林青霞\u0026#34;);  womanList.add(\u0026#34;柳岩\u0026#34;);  womanList.add(\u0026#34;林志玲\u0026#34;);  womanList.add(\u0026#34;王祖贤\u0026#34;);   /* //男演员只要名字为3个字的前三人 Stream\u0026lt;String\u0026gt; manStream = manList.stream().filter(s -\u0026gt; s.length() == 3).limit(3); //女演员只要姓林的，并且不要第一个 Stream\u0026lt;String\u0026gt; womanStream = womanList.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;林\u0026#34;)).skip(1); //把过滤后的男演员姓名和女演员姓名合并到一起 Stream\u0026lt;String\u0026gt; stream = Stream.concat(manStream, womanStream); //把上一步操作后的元素作为构造方法的参数创建演员对象,遍历数据 // stream.map(Actor::new).forEach(System.out::println); stream.map(Actor::new).forEach(p -\u0026gt; System.out.println(p.getName())); */   Stream.concat(manList.stream().filter(s -\u0026gt; s.length() == 3).limit(3),  womanList.stream().filter(s -\u0026gt; s.startsWith(\u0026#34;林\u0026#34;)).skip(1)).map(Actor::new).  forEach(p -\u0026gt; System.out.println(p.getName()));  } }   ","permalink":"https://iblog.zone/archives/java%E9%9B%86%E5%90%8803/","summary":"1.Map集合 1.1Map集合概述和特点【理解】   Map集合概述\ninterface Map\u0026lt;K,V\u0026gt; K：键的类型；V：值的类型   Map集合的特点\n 双列集合,一个键对应一个值 键不可以重复,值可以重复    Map集合的基本使用\npublic class MapDemo01 {  public static void main(String[] args) {  //创建集合对象  Map\u0026lt;String,String\u0026gt; map = new HashMap\u0026lt;String,String\u0026gt;();   //V put(K key, V value) 将指定的值与该映射中的指定键相关联  map.put(\u0026#34;itheima001\u0026#34;,\u0026#34;林青霞\u0026#34;);  map.put(\u0026#34;itheima002\u0026#34;,\u0026#34;张曼玉\u0026#34;);  map.put(\u0026#34;itheima003\u0026#34;,\u0026#34;王祖贤\u0026#34;);  map.put(\u0026#34;itheima003\u0026#34;,\u0026#34;柳岩\u0026#34;);   //输出集合对象  System.out.println(map);  } }   1.2Map集合的基本功能【应用】   方法介绍\n   方法名 说明     V put(K key,V value) 添加元素   V remove(Object key) 根据键删除键值对元素   void clear() 移除所有的键值对元素   boolean containsKey(Object key) 判断集合是否包含指定的键   boolean containsValue(Object value) 判断集合是否包含指定的值   boolean isEmpty() 判断集合是否为空   int size() 集合的长度，也就是集合中键值对的个数      示例代码","title":"Java集合03"},{"content":"1.Set集合 1.1Set集合概述和特点【应用】  不可以存储重复元素 没有索引,不能使用普通for循环遍历  1.2Set集合的使用【应用】 存储字符串并遍历\npublic class MySet1 {  public static void main(String[] args) {  //创建集合对象  Set\u0026lt;String\u0026gt; set = new TreeSet\u0026lt;\u0026gt;();  //添加元素  set.add(\u0026#34;ccc\u0026#34;);  set.add(\u0026#34;aaa\u0026#34;);  set.add(\u0026#34;aaa\u0026#34;);  set.add(\u0026#34;bbb\u0026#34;);  // for (int i = 0; i \u0026lt; set.size(); i++) { // //Set集合是没有索引的，所以不能使用通过索引获取元素的方法 // }   //遍历集合  Iterator\u0026lt;String\u0026gt; it = set.iterator();  while (it.hasNext()){  String s = it.next();  System.out.println(s);  }  System.out.println(\u0026#34;-----------------------------------\u0026#34;);  for (String s : set) {  System.out.println(s);  }  } } 2.TreeSet集合 2.1TreeSet集合概述和特点【应用】  不可以存储重复元素 没有索引 可以将元素按照规则进行排序  TreeSet()：根据其元素的自然排序进行排序 TreeSet(Comparator comparator) ：根据指定的比较器进行排序    2.2TreeSet集合基本使用【应用】 存储Integer类型的整数并遍历\npublic class TreeSetDemo01 {  public static void main(String[] args) {  //创建集合对象  TreeSet\u0026lt;Integer\u0026gt; ts = new TreeSet\u0026lt;Integer\u0026gt;();   //添加元素  ts.add(10);  ts.add(40);  ts.add(30);  ts.add(50);  ts.add(20);   ts.add(30);   //遍历集合  for(Integer i : ts) {  System.out.println(i);  }  } } 2.3自然排序Comparable的使用【应用】   案例需求\n 存储学生对象并遍历，创建TreeSet集合使用无参构造方法 要求：按照年龄从小到大排序，年龄相同时，按照姓名的字母顺序排序    实现步骤\n 使用空参构造创建TreeSet集合  用TreeSet集合存储自定义对象，无参构造方法使用的是自然排序对元素进行排序的   自定义的Student类实现Comparable接口  自然排序，就是让元素所属的类实现Comparable接口，重写compareTo(T o)方法   重写接口中的compareTo方法  重写方法时，一定要注意排序规则必须按照要求的主要条件和次要条件来写      代码实现\n学生类\npublic class Student implements Comparable\u0026lt;Student\u0026gt;{  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  }   @Override  public int compareTo(Student o) {  //按照对象的年龄进行排序  //主要判断条件: 按照年龄从小到大排序  int result = this.age - o.age;  //次要判断条件: 年龄相同时，按照姓名的字母顺序排序  result = result == 0 ? this.name.compareTo(o.getName()) : result;  return result;  } } 测试类\npublic class MyTreeSet2 {  public static void main(String[] args) {  //创建集合对象  TreeSet\u0026lt;Student\u0026gt; ts = new TreeSet\u0026lt;\u0026gt;(); \t//创建学生对象  Student s1 = new Student(\u0026#34;zhangsan\u0026#34;,28);  Student s2 = new Student(\u0026#34;lisi\u0026#34;,27);  Student s3 = new Student(\u0026#34;wangwu\u0026#34;,29);  Student s4 = new Student(\u0026#34;zhaoliu\u0026#34;,28);  Student s5 = new Student(\u0026#34;qianqi\u0026#34;,30); \t//把学生添加到集合  ts.add(s1);  ts.add(s2);  ts.add(s3);  ts.add(s4);  ts.add(s5); \t//遍历集合  for (Student student : ts) {  System.out.println(student);  }  } }   2.4比较器排序Comparator的使用【应用】   案例需求\n 存储老师对象并遍历，创建TreeSet集合使用带参构造方法 要求：按照年龄从小到大排序，年龄相同时，按照姓名的字母顺序排序    实现步骤\n 用TreeSet集合存储自定义对象，带参构造方法使用的是比较器排序对元素进行排序的 比较器排序，就是让集合构造方法接收Comparator的实现类对象，重写compare(T o1,T o2)方法 重写方法时，一定要注意排序规则必须按照要求的主要条件和次要条件来写    代码实现\n老师类\npublic class Teacher {  private String name;  private int age;   public Teacher() {  }   public Teacher(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Teacher{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } } 测试类\npublic class MyTreeSet4 {  public static void main(String[] args) {  //创建集合对象  TreeSet\u0026lt;Teacher\u0026gt; ts = new TreeSet\u0026lt;\u0026gt;(new Comparator\u0026lt;Teacher\u0026gt;() {  @Override  public int compare(Teacher o1, Teacher o2) {  //o1表示现在要存入的那个元素  //o2表示已经存入到集合中的元素   //主要条件  int result = o1.getAge() - o2.getAge();  //次要条件  result = result == 0 ? o1.getName().compareTo(o2.getName()) : result;  return result;  }  }); \t//创建老师对象  Teacher t1 = new Teacher(\u0026#34;zhangsan\u0026#34;,23);  Teacher t2 = new Teacher(\u0026#34;lisi\u0026#34;,22);  Teacher t3 = new Teacher(\u0026#34;wangwu\u0026#34;,24);  Teacher t4 = new Teacher(\u0026#34;zhaoliu\u0026#34;,24); \t//把老师添加到集合  ts.add(t1);  ts.add(t2);  ts.add(t3);  ts.add(t4); \t//遍历集合  for (Teacher teacher : ts) {  System.out.println(teacher);  }  } }   2.4两种比较方式总结【理解】  两种比较方式小结  自然排序: 自定义类实现Comparable接口,重写compareTo方法,根据返回值进行排序 比较器排序: 创建TreeSet对象的时候传递Comparator的实现类对象,重写compare方法,根据返回值进行排序 在使用的时候,默认使用自然排序,当自然排序不满足现在的需求时,必须使用比较器排序   两种方式中关于返回值的规则  如果返回值为负数，表示当前存入的元素是较小值，存左边 如果返回值为0，表示当前存入的元素跟集合中元素重复了，不存 如果返回值为正数，表示当前存入的元素是较大值，存右边    3.数据结构 3.1二叉树【理解】   二叉树的特点\n 二叉树中,任意一个节点的度要小于等于2  节点: 在树结构中,每一个元素称之为节点 度: 每一个节点的子节点数量称之为度      二叉树结构图\n  3.2二叉查找树【理解】   二叉查找树的特点\n 二叉查找树,又称二叉排序树或者二叉搜索树 每一个节点上最多有两个子节点 左子树上所有节点的值都小于根节点的值 右子树上所有节点的值都大于根节点的值    二叉查找树结构图\n  二叉查找树和二叉树对比结构图\n  二叉查找树添加节点规则\n  小的存左边\n  大的存右边\n  一样的不存\n    3.3平衡二叉树【理解】   平衡二叉树的特点\n 二叉树左右两个子树的高度差不超过1 任意节点的左右两个子树都是一颗平衡二叉树    平衡二叉树旋转\n  旋转触发时机\n 当添加一个节点之后,该树不再是一颗平衡二叉树    左旋\n  就是将根节点的右侧往左拉,原先的右子节点变成新的父节点,并把多余的左子节点出让,给已经降级的根节点当右子节点\n    右旋\n  就是将根节点的左侧往右拉,左子节点变成了新的父节点,并把多余的右子节点出让,给已经降级根节点当左子节点\n      平衡二叉树和二叉查找树对比结构图\n  平衡二叉树旋转的四种情况\n  左左\n  左左: 当根节点左子树的左子树有节点插入,导致二叉树不平衡\n  如何旋转: 直接对整体进行右旋即可\n    左右\n  左右: 当根节点左子树的右子树有节点插入,导致二叉树不平衡\n  如何旋转: 先在左子树对应的节点位置进行左旋,在对整体进行右旋\n    右右\n  右右: 当根节点右子树的右子树有节点插入,导致二叉树不平衡\n  如何旋转: 直接对整体进行左旋即可\n    右左\n  右左:当根节点右子树的左子树有节点插入,导致二叉树不平衡\n  如何旋转: 先在右子树对应的节点位置进行右旋,在对整体进行左旋\n      3.4红黑树【理解】   红黑树的特点\n 平衡二叉B树 每一个节点可以是红或者黑 红黑树不是高度平衡的,它的平衡是通过\u0026quot;自己的红黑规则\u0026quot;进行实现的    红黑树的红黑规则有哪些\n  每一个节点或是红色的,或者是黑色的\n  根节点必须是黑色\n  如果一个节点没有子节点或者父节点,则该节点相应的指针属性值为Nil,这些Nil视为叶节点,每个叶节点(Nil)是黑色的\n  如果某一个节点是红色,那么它的子节点必须是黑色(不能出现两个红色节点相连 的情况)\n  对每一个节点,从该节点到其所有后代叶节点的简单路径上,均包含相同数目的黑色节点\n    红黑树添加节点的默认颜色\n  添加节点时,默认为红色,效率高\n    红黑树添加节点后如何保持红黑规则\n 根节点位置  直接变为黑色   非根节点位置  父节点为黑色  不需要任何操作,默认红色即可   父节点为红色  叔叔节点为红色  将\u0026quot;父节点\u0026quot;设为黑色,将\u0026quot;叔叔节点\u0026quot;设为黑色 将\u0026quot;祖父节点\u0026quot;设为红色 如果\u0026quot;祖父节点\u0026quot;为根节点,则将根节点再次变成黑色   叔叔节点为黑色  将\u0026quot;父节点\u0026quot;设为黑色 将\u0026quot;祖父节点\u0026quot;设为红色 以\u0026quot;祖父节点\u0026quot;为支点进行旋转          3.5成绩排序案例【应用】   案例需求\n 用TreeSet集合存储多个学生信息(姓名,语文成绩,数学成绩,英语成绩),并遍历该集合 要求: 按照总分从高到低出现    代码实现\n学生类\npublic class Student implements Comparable\u0026lt;Student\u0026gt; {  private String name;  private int chinese;  private int math;  private int english;   public Student() {  }   public Student(String name, int chinese, int math, int english) {  this.name = name;  this.chinese = chinese;  this.math = math;  this.english = english;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getChinese() {  return chinese;  }   public void setChinese(int chinese) {  this.chinese = chinese;  }   public int getMath() {  return math;  }   public void setMath(int math) {  this.math = math;  }   public int getEnglish() {  return english;  }   public void setEnglish(int english) {  this.english = english;  }   public int getSum() {  return this.chinese + this.math + this.english;  }   @Override  public int compareTo(Student o) {  // 主要条件: 按照总分进行排序  int result = o.getSum() - this.getSum();  // 次要条件: 如果总分一样,就按照语文成绩排序  result = result == 0 ? o.getChinese() - this.getChinese() : result;  // 如果语文成绩也一样,就按照数学成绩排序  result = result == 0 ? o.getMath() - this.getMath() : result;  // 如果总分一样,各科成绩也都一样,就按照姓名排序  result = result == 0 ? o.getName().compareTo(this.getName()) : result;  return result;  } } 测试类\npublic class TreeSetDemo {  public static void main(String[] args) {  //创建TreeSet集合对象，通过比较器排序进行排序  TreeSet\u0026lt;Student\u0026gt; ts = new TreeSet\u0026lt;Student\u0026gt;();  //创建学生对象  Student s1 = new Student(\u0026#34;jack\u0026#34;, 98, 100, 95);  Student s2 = new Student(\u0026#34;rose\u0026#34;, 95, 95, 95);  Student s3 = new Student(\u0026#34;sam\u0026#34;, 100, 93, 98);  //把学生对象添加到集合  ts.add(s1);  ts.add(s2);  ts.add(s3);   //遍历集合  for (Student s : ts) {  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getChinese() + \u0026#34;,\u0026#34; + s.getMath() + \u0026#34;,\u0026#34; + s.getEnglish() + \u0026#34;,\u0026#34; + s.getSum());  }  } }   4.HashSet集合 4.1HashSet集合概述和特点【应用】  底层数据结构是哈希表 存取无序 不可以存储重复元素 没有索引,不能使用普通for循环遍历  4.2HashSet集合的基本应用【应用】 存储字符串并遍历\npublic class HashSetDemo {  public static void main(String[] args) {  //创建集合对象  HashSet\u0026lt;String\u0026gt; set = new HashSet\u0026lt;String\u0026gt;();   //添加元素  set.add(\u0026#34;hello\u0026#34;);  set.add(\u0026#34;world\u0026#34;);  set.add(\u0026#34;java\u0026#34;);  //不包含重复元素的集合  set.add(\u0026#34;world\u0026#34;);   //遍历  for(String s : set) {  System.out.println(s);  }  } } 4.3哈希值【理解】   哈希值简介\n​\t是JDK根据对象的地址或者字符串或者数字算出来的int类型的数值\n  如何获取哈希值\n​\tObject类中的public int hashCode()：返回对象的哈希码值\n  哈希值的特点\n 同一个对象多次调用hashCode()方法返回的哈希值是相同的 默认情况下，不同对象的哈希值是不同的。而重写hashCode()方法，可以实现让不同对象的哈希值相同    4.4哈希表结构【理解】   JDK1.8以前\n​\t数组 + 链表\n  JDK1.8以后\n  节点个数少于等于8个\n​\t数组 + 链表\n  节点个数多于8个\n​\t数组 + 红黑树\n    4.5HashSet集合存储学生对象并遍历【应用】   案例需求\n 创建一个存储学生对象的集合，存储多个学生对象，使用程序实现在控制台遍历该集合 要求：学生对象的成员变量值相同，我们就认为是同一个对象    代码实现\n学生类\npublic class Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public boolean equals(Object o) {  if (this == o) return true;  if (o == null || getClass() != o.getClass()) return false;   Student student = (Student) o;   if (age != student.age) return false;  return name != null ? name.equals(student.name) : student.name == null;  }   @Override  public int hashCode() {  int result = name != null ? name.hashCode() : 0;  result = 31 * result + age;  return result;  } } 测试类\npublic class HashSetDemo02 {  public static void main(String[] args) {  //创建HashSet集合对象  HashSet\u0026lt;Student\u0026gt; hs = new HashSet\u0026lt;Student\u0026gt;();   //创建学生对象  Student s1 = new Student(\u0026#34;林青霞\u0026#34;, 30);  Student s2 = new Student(\u0026#34;张曼玉\u0026#34;, 35);  Student s3 = new Student(\u0026#34;王祖贤\u0026#34;, 33);   Student s4 = new Student(\u0026#34;王祖贤\u0026#34;, 33);   //把学生添加到集合  hs.add(s1);  hs.add(s2);  hs.add(s3);  hs.add(s4);   //遍历集合(增强for)  for (Student s : hs) {  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge());  }  } }   总结\n​\tHashSet集合存储自定义类型元素,要想实现元素的唯一,要求必须重写hashCode方法和equals方法\n  ","permalink":"https://iblog.zone/archives/java%E9%9B%86%E5%90%8802/","summary":"1.Set集合 1.1Set集合概述和特点【应用】  不可以存储重复元素 没有索引,不能使用普通for循环遍历  1.2Set集合的使用【应用】 存储字符串并遍历\npublic class MySet1 {  public static void main(String[] args) {  //创建集合对象  Set\u0026lt;String\u0026gt; set = new TreeSet\u0026lt;\u0026gt;();  //添加元素  set.add(\u0026#34;ccc\u0026#34;);  set.add(\u0026#34;aaa\u0026#34;);  set.add(\u0026#34;aaa\u0026#34;);  set.add(\u0026#34;bbb\u0026#34;);  // for (int i = 0; i \u0026lt; set.size(); i++) { // //Set集合是没有索引的，所以不能使用通过索引获取元素的方法 // }   //遍历集合  Iterator\u0026lt;String\u0026gt; it = set.iterator();  while (it.hasNext()){  String s = it.next();  System.","title":"Java集合02"},{"content":"1.Collection集合 1.1数组和集合的区别【理解】   相同点\n都是容器,可以存储多个数据\n  不同点\n  数组的长度是不可变的,集合的长度是可变的\n  数组可以存基本数据类型和引用数据类型\n集合只能存引用数据类型,如果要存基本数据类型,需要存对应的包装类\n    1.2集合类体系结构【理解】 1.3Collection 集合概述和使用【应用】   Collection集合概述\n 是单例集合的顶层接口,它表示一组对象,这些对象也称为Collection的元素 JDK 不提供此接口的任何直接实现.它提供更具体的子接口(如Set和List)实现    创建Collection集合的对象\n 多态的方式 具体的实现类ArrayList    Collection集合常用方法\n   方法名 说明     boolean add(E e) 添加元素   boolean remove(Object o) 从集合中移除指定的元素   boolean removeIf(Object o) 根据条件进行移除   void clear() 清空集合中的元素   boolean contains(Object o) 判断集合中是否存在指定的元素   boolean isEmpty() 判断集合是否为空   int size() 集合的长度，也就是集合中元素的个数      1.4Collection集合的遍历【应用】   迭代器介绍\n 迭代器,集合的专用遍历方式 Iterator\u0026lt;E\u0026gt; iterator(): 返回此集合中元素的迭代器,通过集合对象的iterator()方法得到    Iterator中的常用方法\n​\tboolean hasNext(): 判断当前位置是否有元素可以被取出 ​\tE next(): 获取当前位置的元素,将迭代器对象移向下一个索引位置\n  Collection集合的遍历\npublic class IteratorDemo1 {  public static void main(String[] args) {  //创建集合对象  Collection\u0026lt;String\u0026gt; c = new ArrayList\u0026lt;\u0026gt;();   //添加元素  c.add(\u0026#34;hello\u0026#34;);  c.add(\u0026#34;world\u0026#34;);  c.add(\u0026#34;java\u0026#34;);  c.add(\u0026#34;javaee\u0026#34;);   //Iterator\u0026lt;E\u0026gt; iterator()：返回此集合中元素的迭代器，通过集合的iterator()方法得到  Iterator\u0026lt;String\u0026gt; it = c.iterator();   //用while循环改进元素的判断和获取  while (it.hasNext()) {  String s = it.next();  System.out.println(s);  }  } }   迭代器中删除的方法\n​\tvoid remove(): 删除迭代器对象当前指向的元素\npublic class IteratorDemo2 {  public static void main(String[] args) {  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();  list.add(\u0026#34;a\u0026#34;);  list.add(\u0026#34;b\u0026#34;);  list.add(\u0026#34;b\u0026#34;);  list.add(\u0026#34;c\u0026#34;);  list.add(\u0026#34;d\u0026#34;);   Iterator\u0026lt;String\u0026gt; it = list.iterator();  while(it.hasNext()){  String s = it.next();  if(\u0026#34;b\u0026#34;.equals(s)){  //指向谁,那么此时就删除谁.  it.remove();  }  }  System.out.println(list);  } }   1.5增强for循环【应用】   介绍\n 它是JDK5之后出现的,其内部原理是一个Iterator迭代器 实现Iterable接口的类才可以使用迭代器和增强for 简化数组和Collection集合的遍历    格式\n​\tfor(集合/数组中元素的数据类型 变量名 : 集合/数组名) {\n​\t// 已经将当前遍历到的元素封装到变量中了,直接使用变量即可\n​\t}\n  代码\npublic class MyCollectonDemo1 {  public static void main(String[] args) {  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();  list.add(\u0026#34;a\u0026#34;);  list.add(\u0026#34;b\u0026#34;);  list.add(\u0026#34;c\u0026#34;);  list.add(\u0026#34;d\u0026#34;);  list.add(\u0026#34;e\u0026#34;);  list.add(\u0026#34;f\u0026#34;);   //1,数据类型一定是集合或者数组中元素的类型  //2,str仅仅是一个变量名而已,在循环的过程中,依次表示集合或者数组中的每一个元素  //3,list就是要遍历的集合或者数组  for(String str : list){  System.out.println(str);  }  } }   2.List集合 2.1List集合的概述和特点【记忆】  List集合的概述  有序集合,这里的有序指的是存取顺序 用户可以精确控制列表中每个元素的插入位置,用户可以通过整数索引访问元素,并搜索列表中的元素 与Set集合不同,列表通常允许重复的元素   List集合的特点  存取有序 可以重复 有索引    2.2List集合的特有方法【应用】    方法名 描述     void add(int index,E element) 在此集合中的指定位置插入指定的元素   E remove(int index) 删除指定索引处的元素，返回被删除的元素   E set(int index,E element) 修改指定索引处的元素，返回被修改的元素   E get(int index) 返回指定索引处的元素    3.数据结构 3.1数据结构之栈和队列【记忆】   栈结构\n​\t先进后出\n  队列结构\n​\t先进先出\n  3.2数据结构之数组和链表【记忆】   数组结构\n​\t查询快、增删慢\n  队列结构\n​\t查询慢、增删快\n  4.List集合的实现类 4.1List集合子类的特点【记忆】   ArrayList集合\n​\t底层是数组结构实现，查询快、增删慢\n  LinkedList集合\n​\t底层是链表结构实现，查询慢、增删快\n  4.2LinkedList集合的特有功能【应用】  特有方法    方法名 说明     public void addFirst(E e) 在该列表开头插入指定的元素   public void addLast(E e) 将指定的元素追加到此列表的末尾   public E getFirst() 返回此列表中的第一个元素   public E getLast() 返回此列表中的最后一个元素   public E removeFirst() 从此列表中删除并返回第一个元素   public E removeLast() 从此列表中删除并返回最后一个元素      5.泛型 5.1泛型概述【理解】   泛型的介绍\n​\t泛型是JDK5中引入的特性，它提供了编译时类型安全检测机制\n  泛型的好处\n 把运行时期的问题提前到了编译期间 避免了强制类型转换    泛型的定义格式\n \u0026lt;类型\u0026gt;: 指定一种类型的格式.尖括号里面可以任意书写,一般只写一个字母.例如:   \u0026lt;类型1,类型2…\u0026gt;: 指定多种类型的格式,多种类型之间用逗号隔开.例如: \u0026lt;E,T\u0026gt; \u0026lt;K,V\u0026gt;    5.2泛型类【应用】   定义格式\n修饰符 class 类名\u0026lt;类型\u0026gt; { }   示例代码\n  泛型类\npublic class Generic\u0026lt;T\u0026gt; {  private T t;   public T getT() {  return t;  }   public void setT(T t) {  this.t = t;  } }   测试类\npublic class GenericDemo1 {  public static void main(String[] args) {  Generic\u0026lt;String\u0026gt; g1 = new Generic\u0026lt;String\u0026gt;();  g1.setT(\u0026#34;杨幂\u0026#34;);  System.out.println(g1.getT());   Generic\u0026lt;Integer\u0026gt; g2 = new Generic\u0026lt;Integer\u0026gt;();  g2.setT(30);  System.out.println(g2.getT());   Generic\u0026lt;Boolean\u0026gt; g3 = new Generic\u0026lt;Boolean\u0026gt;();  g3.setT(true);  System.out.println(g3.getT());  } }     5.3泛型方法【应用】   定义格式\n修饰符 \u0026lt;类型\u0026gt; 返回值类型 方法名(类型 变量名) { }   示例代码\n  带有泛型方法的类\npublic class Generic {  public \u0026lt;T\u0026gt; void show(T t) {  System.out.println(t);  } }   测试类\npublic class GenericDemo2 {  public static void main(String[] args) { \tGeneric g = new Generic();  g.show(\u0026#34;柳岩\u0026#34;);  g.show(30);  g.show(true);  g.show(12.34);  } }     5.4泛型接口【应用】   定义格式\n修饰符 interface 接口名\u0026lt;类型\u0026gt; { }   示例代码\n  泛型接口\npublic interface Generic\u0026lt;T\u0026gt; {  void show(T t); }   泛型接口实现类1\n​\t定义实现类时,定义和接口相同泛型,创建实现类对象时明确泛型的具体类型\npublic class GenericImpl1\u0026lt;T\u0026gt; implements Generic\u0026lt;T\u0026gt; {  @Override  public void show(T t) {  System.out.println(t);  } }   泛型接口实现类2\n​\t定义实现类时,直接明确泛型的具体类型\npublic class GenericImpl2 implements Generic\u0026lt;Integer\u0026gt;{  @Override  public void show(Integer t) {  System.out.println(t);  } }   测试类\npublic class GenericDemo3 {  public static void main(String[] args) {  GenericImpl1\u0026lt;String\u0026gt; g1 = new GenericImpl\u0026lt;String\u0026gt;();  g1.show(\u0026#34;林青霞\u0026#34;);  GenericImpl1\u0026lt;Integer\u0026gt; g2 = new GenericImpl\u0026lt;Integer\u0026gt;();  g2.show(30);   GenericImpl2 g3 = new GenericImpl2();  g3.show(10);  } }     5.5类型通配符   类型通配符: \n ArrayList: 表示元素类型未知的ArrayList,它的元素可以匹配任何的类型 但是并不能把元素添加到ArrayList中了,获取出来的也是父类类型    类型通配符上限: \u0026lt;? extends 类型\u0026gt;\n ArrayListList \u0026lt;? extends Number\u0026gt;: 它表示的类型是Number或者其子类型    类型通配符下限: \u0026lt;? super 类型\u0026gt;\n ArrayListList \u0026lt;? super Number\u0026gt;: 它表示的类型是Number或者其父类型    泛型通配符的使用\npublic class GenericDemo4 {  public static void main(String[] args) {  ArrayList\u0026lt;Integer\u0026gt; list1 = new ArrayList\u0026lt;\u0026gt;();  ArrayList\u0026lt;String\u0026gt; list2 = new ArrayList\u0026lt;\u0026gt;();  ArrayList\u0026lt;Number\u0026gt; list3 = new ArrayList\u0026lt;\u0026gt;();  ArrayList\u0026lt;Object\u0026gt; list4 = new ArrayList\u0026lt;\u0026gt;();   method(list1);  method(list2);  method(list3);  method(list4);   getElement1(list1);  getElement1(list2);//报错  getElement1(list3);  getElement1(list4);//报错   getElement2(list1);//报错  getElement2(list2);//报错  getElement2(list3);  getElement2(list4);  }   // 泛型通配符: 此时的泛型?,可以是任意类型  public static void method(ArrayList\u0026lt;?\u0026gt; list){}  // 泛型的上限: 此时的泛型?,必须是Number类型或者Number类型的子类  public static void getElement1(ArrayList\u0026lt;? extends Number\u0026gt; list){}  // 泛型的下限: 此时的泛型?,必须是Number类型或者Number类型的父类  public static void getElement2(ArrayList\u0026lt;? super Number\u0026gt; list){}  }   ","permalink":"https://iblog.zone/archives/java%E9%9B%86%E5%90%8801/","summary":"1.Collection集合 1.1数组和集合的区别【理解】   相同点\n都是容器,可以存储多个数据\n  不同点\n  数组的长度是不可变的,集合的长度是可变的\n  数组可以存基本数据类型和引用数据类型\n集合只能存引用数据类型,如果要存基本数据类型,需要存对应的包装类\n    1.2集合类体系结构【理解】 1.3Collection 集合概述和使用【应用】   Collection集合概述\n 是单例集合的顶层接口,它表示一组对象,这些对象也称为Collection的元素 JDK 不提供此接口的任何直接实现.它提供更具体的子接口(如Set和List)实现    创建Collection集合的对象\n 多态的方式 具体的实现类ArrayList    Collection集合常用方法\n   方法名 说明     boolean add(E e) 添加元素   boolean remove(Object o) 从集合中移除指定的元素   boolean removeIf(Object o) 根据条件进行移除   void clear() 清空集合中的元素   boolean contains(Object o) 判断集合中是否存在指定的元素   boolean isEmpty() 判断集合是否为空   int size() 集合的长度，也就是集合中元素的个数      1.","title":"Java集合01"},{"content":"1.时间日期类 1.1 Date类（应用）   计算机中时间原点\n1970年1月1日 00:00:00\n  时间换算单位\n1秒 = 1000毫秒\n  Date类概述\nDate 代表了一个特定的时间，精确到毫秒\n  Date类构造方法\n   方法名 说明     public Date() 分配一个 Date对象，并初始化，以便它代表它被分配的时间，精确到毫秒   public Date(long date) 分配一个 Date对象，并将其初始化为表示从标准基准时间起指定的毫秒数      示例代码\npublic class DateDemo01 {  public static void main(String[] args) {  //public Date()：分配一个 Date对象，并初始化，以便它代表它被分配的时间，精确到毫秒  Date d1 = new Date();  System.out.println(d1);   //public Date(long date)：分配一个 Date对象，并将其初始化为表示从标准基准时间起指定的毫秒数  long date = 1000*60*60;  Date d2 = new Date(date);  System.out.println(d2);  } }   1.2 Date类常用方法（应用）   常用方法\n   方法名 说明     public long getTime() 获取的是日期对象从1970年1月1日 00:00:00到现在的毫秒值   public void setTime(long time) 设置时间，给的是毫秒值      示例代码\npublic class DateDemo02 {  public static void main(String[] args) {  //创建日期对象  Date d = new Date();   //public long getTime():获取的是日期对象从1970年1月1日 00:00:00到现在的毫秒值 // System.out.println(d.getTime()); // System.out.println(d.getTime() * 1.0 / 1000 / 60 / 60 / 24 / 365 + \u0026#34;年\u0026#34;);   //public void setTime(long time):设置时间，给的是毫秒值 // long time = 1000*60*60;  long time = System.currentTimeMillis();  d.setTime(time);   System.out.println(d);  } }   1.3 SimpleDateFormat类（应用）   SimpleDateFormat类概述\n​\tSimpleDateFormat是一个具体的类，用于以区域设置敏感的方式格式化和解析日期。\n​\t我们重点学习日期格式化和解析\n  SimpleDateFormat类构造方法\n   方法名 说明     public SimpleDateFormat() 构造一个SimpleDateFormat，使用默认模式和日期格式   public SimpleDateFormat(String pattern) 构造一个SimpleDateFormat使用给定的模式和默认的日期格式      SimpleDateFormat类的常用方法\n 格式化(从Date到String)  public final String format(Date date)：将日期格式化成日期/时间字符串   解析(从String到Date)  public Date parse(String source)：从给定字符串的开始解析文本以生成日期      示例代码\npublic class SimpleDateFormatDemo {  public static void main(String[] args) throws ParseException {  //格式化：从 Date 到 String  Date d = new Date(); // SimpleDateFormat sdf = new SimpleDateFormat();  SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy年MM月dd日 HH:mm:ss\u0026#34;);  String s = sdf.format(d);  System.out.println(s);  System.out.println(\u0026#34;--------\u0026#34;);   //从 String 到 Date  String ss = \u0026#34;2048-08-09 11:11:11\u0026#34;;  //ParseException  SimpleDateFormat sdf2 = new SimpleDateFormat(\u0026#34;yyyy-MM-dd HH:mm:ss\u0026#34;);  Date dd = sdf2.parse(ss);  System.out.println(dd);  } }   1.4 时间日期类练习 (应用)   需求\n秒杀开始时间是2020年11月11日 00:00:00,结束时间是2020年11月11日 00:10:00,用户小贾下单时间是2020年11月11日 00:03:47,用户小皮下单时间是2020年11月11日 00:10:11,判断用户有没有成功参与秒杀活动\n  实现步骤\n 判断下单时间是否在开始到结束的范围内 把字符串形式的时间变成毫秒值    代码实现\npublic class DateDemo5 {  public static void main(String[] args) throws ParseException {  //开始时间：2020年11月11日 0:0:0  //结束时间：2020年11月11日 0:10:0   //小贾2020年11月11日 0:03:47  //小皮2020年11月11日 0:10:11   //1.判断两位同学的下单时间是否在范围之内就可以了。   //2.要把每一个时间都换算成毫秒值。   String start = \u0026#34;2020年11月11日 0:0:0\u0026#34;;  String end = \u0026#34;2020年11月11日 0:10:0\u0026#34;;   String jia = \u0026#34;2020年11月11日 0:03:47\u0026#34;;  String pi = \u0026#34;2020年11月11日 0:10:11\u0026#34;;   SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy年MM月dd日 HH:mm:ss\u0026#34;);  long startTime = sdf.parse(start).getTime();  long endTime = sdf.parse(end).getTime();  // System.out.println(startTime); // System.out.println(endTime);  long jiaTime = sdf.parse(jia).getTime();  long piTime = sdf.parse(pi).getTime();   if(jiaTime \u0026gt;= startTime \u0026amp;\u0026amp; jiaTime \u0026lt;= endTime){  System.out.println(\u0026#34;小贾同学参加上了秒杀活动\u0026#34;);  }else{  System.out.println(\u0026#34;小贾同学没有参加上秒杀活动\u0026#34;);  }   System.out.println(\u0026#34;------------------------\u0026#34;);   if(piTime \u0026gt;= startTime \u0026amp;\u0026amp; piTime \u0026lt;= endTime){  System.out.println(\u0026#34;小皮同学参加上了秒杀活动\u0026#34;);  }else{  System.out.println(\u0026#34;小皮同学没有参加上秒杀活动\u0026#34;);  }   }  }   2.JDK8时间日期类 2.1 JDK8新增日期类 (理解)  LocalDate 表示日期（年月日） LocalTime 表示时间（时分秒） LocalDateTime 表示时间+ 日期 （年月日时分秒）  2.2 LocalDateTime创建方法 (应用)   方法说明\n   方法名 说明     public static LocalDateTime now() 获取当前系统时间   public static LocalDateTime of (年, 月 , 日, 时, 分, 秒) 使用指定年月日和时分秒初始化一个LocalDateTime对象      示例代码\npublic class JDK8DateDemo2 {  public static void main(String[] args) {  LocalDateTime now = LocalDateTime.now();  System.out.println(now);   LocalDateTime localDateTime = LocalDateTime.of(2020, 11, 11, 11, 11, 11);  System.out.println(localDateTime);  } }   2.3 LocalDateTime获取方法 (应用)   方法说明\n   方法名 说明     public int getYear() 获取年   public int getMonthValue() 获取月份（1-12）   public int getDayOfMonth() 获取月份中的第几天（1-31）   public int getDayOfYear() 获取一年中的第几天（1-366）   public DayOfWeek getDayOfWeek() 获取星期   public int getMinute() 获取分钟   public int getHour() 获取小时      示例代码\npublic class JDK8DateDemo3 {  public static void main(String[] args) {  LocalDateTime localDateTime = LocalDateTime.of(2020, 11, 11, 11, 11, 20);  //public int getYear() 获取年  int year = localDateTime.getYear();  System.out.println(\u0026#34;年为\u0026#34; +year);  //public int getMonthValue() 获取月份（1-12）  int month = localDateTime.getMonthValue();  System.out.println(\u0026#34;月份为\u0026#34; + month);   Month month1 = localDateTime.getMonth(); // System.out.println(month1);   //public int getDayOfMonth() 获取月份中的第几天（1-31）  int day = localDateTime.getDayOfMonth();  System.out.println(\u0026#34;日期为\u0026#34; + day);   //public int getDayOfYear() 获取一年中的第几天（1-366）  int dayOfYear = localDateTime.getDayOfYear();  System.out.println(\u0026#34;这是一年中的第\u0026#34; + dayOfYear + \u0026#34;天\u0026#34;);   //public DayOfWeek getDayOfWeek()获取星期  DayOfWeek dayOfWeek = localDateTime.getDayOfWeek();  System.out.println(\u0026#34;星期为\u0026#34; + dayOfWeek);   //public int getMinute() 获取分钟  int minute = localDateTime.getMinute();  System.out.println(\u0026#34;分钟为\u0026#34; + minute);  //public int getHour() 获取小时   int hour = localDateTime.getHour();  System.out.println(\u0026#34;小时为\u0026#34; + hour);  } }   2.4 LocalDateTime转换方法 (应用)   方法说明\n   方法名 说明     public LocalDate toLocalDate () 转换成为一个LocalDate对象   public LocalTime toLocalTime () 转换成为一个LocalTime对象      示例代码\npublic class JDK8DateDemo4 {  public static void main(String[] args) {  LocalDateTime localDateTime = LocalDateTime.of(2020, 12, 12, 8, 10, 12);  //public LocalDate toLocalDate () 转换成为一个LocalDate对象  LocalDate localDate = localDateTime.toLocalDate();  System.out.println(localDate);   //public LocalTime toLocalTime () 转换成为一个LocalTime对象  LocalTime localTime = localDateTime.toLocalTime();  System.out.println(localTime);  } }   2.5 LocalDateTime格式化和解析 (应用)   方法说明\n   方法名 说明     public String format (指定格式) 把一个LocalDateTime格式化成为一个字符串   public LocalDateTime parse (准备解析的字符串, 解析格式) 把一个日期字符串解析成为一个LocalDateTime对象   public static DateTimeFormatter ofPattern(String pattern) 使用指定的日期模板获取一个日期格式化器DateTimeFormatter对象      示例代码\npublic class JDK8DateDemo5 {  public static void main(String[] args) {  //method1();  //method2();  }   private static void method2() {  //public static LocalDateTime parse (准备解析的字符串, 解析格式) 把一个日期字符串解析成为一个LocalDateTime对象  String s = \u0026#34;2020年11月12日 13:14:15\u0026#34;;  DateTimeFormatter pattern = DateTimeFormatter.ofPattern(\u0026#34;yyyy年MM月dd日 HH:mm:ss\u0026#34;);  LocalDateTime parse = LocalDateTime.parse(s, pattern);  System.out.println(parse);  }   private static void method1() {  LocalDateTime localDateTime = LocalDateTime.of(2020, 11, 12, 13, 14, 15);  System.out.println(localDateTime);  //public String format (指定格式) 把一个LocalDateTime格式化成为一个字符串  DateTimeFormatter pattern = DateTimeFormatter.ofPattern(\u0026#34;yyyy年MM月dd日 HH:mm:ss\u0026#34;);  String s = localDateTime.format(pattern);  System.out.println(s);  } }   2.6 LocalDateTime增加或者减少时间的方法 (应用)   方法说明\n   方法名 说明     public LocalDateTime plusYears (long years) 添加或者减去年   public LocalDateTime plusMonths(long months) 添加或者减去月   public LocalDateTime plusDays(long days) 添加或者减去日   public LocalDateTime plusHours(long hours) 添加或者减去时   public LocalDateTime plusMinutes(long minutes) 添加或者减去分   public LocalDateTime plusSeconds(long seconds) 添加或者减去秒   public LocalDateTime plusWeeks(long weeks) 添加或者减去周      示例代码\n/** * JDK8 时间类添加或者减去时间的方法 */ public class JDK8DateDemo6 {  public static void main(String[] args) {  //public LocalDateTime plusYears (long years) 添加或者减去年   LocalDateTime localDateTime = LocalDateTime.of(2020, 11, 11, 13, 14, 15);  //LocalDateTime newLocalDateTime = localDateTime.plusYears(1);  //System.out.println(newLocalDateTime);   LocalDateTime newLocalDateTime = localDateTime.plusYears(-1);  System.out.println(newLocalDateTime);  } }   2.7 LocalDateTime减少或者增加时间的方法 (应用)   方法说明\n   方法名 说明     public LocalDateTime minusYears (long years) 减去或者添加年   public LocalDateTime minusMonths(long months) 减去或者添加月   public LocalDateTime minusDays(long days) 减去或者添加日   public LocalDateTime minusHours(long hours) 减去或者添加时   public LocalDateTime minusMinutes(long minutes) 减去或者添加分   public LocalDateTime minusSeconds(long seconds) 减去或者添加秒   public LocalDateTime minusWeeks(long weeks) 减去或者添加周      示例代码\n/** * JDK8 时间类减少或者添加时间的方法 */ public class JDK8DateDemo7 {  public static void main(String[] args) {  //public LocalDateTime minusYears (long years) 减去或者添加年  LocalDateTime localDateTime = LocalDateTime.of(2020, 11, 11, 13, 14, 15);  //LocalDateTime newLocalDateTime = localDateTime.minusYears(1);  //System.out.println(newLocalDateTime);   LocalDateTime newLocalDateTime = localDateTime.minusYears(-1);  System.out.println(newLocalDateTime);   } }   2.8 LocalDateTime修改方法 (应用)   方法说明\n   方法名 说明     public LocalDateTime withYear(int year) 直接修改年   public LocalDateTime withMonth(int month) 直接修改月   public LocalDateTime withDayOfMonth(int dayofmonth) 直接修改日期(一个月中的第几天)   public LocalDateTime withDayOfYear(int dayOfYear) 直接修改日期(一年中的第几天)   public LocalDateTime withHour(int hour) 直接修改小时   public LocalDateTime withMinute(int minute) 直接修改分钟   public LocalDateTime withSecond(int second) 直接修改秒      示例代码\n/** * JDK8 时间类修改时间 */ public class JDK8DateDemo8 {  public static void main(String[] args) {  //public LocalDateTime withYear(int year) 修改年  LocalDateTime localDateTime = LocalDateTime.of(2020, 11, 11, 13, 14, 15);  // LocalDateTime newLocalDateTime = localDateTime.withYear(2048);  // System.out.println(newLocalDateTime);   LocalDateTime newLocalDateTime = localDateTime.withMonth(20);  System.out.println(newLocalDateTime);   } }   2.9 Period (应用)   方法说明\n   方法名 说明     public static Period between(开始时间,结束时间) 计算两个“时间\u0026quot;的间隔   public int getYears() 获得这段时间的年数   public int getMonths() 获得此期间的总月数   public int getDays() 获得此期间的天数   public long toTotalMonths() 获取此期间的总月数      示例代码\n/** * 计算两个时间的间隔 */ public class JDK8DateDemo9 {  public static void main(String[] args) {  //public static Period between(开始时间,结束时间) 计算两个\u0026#34;时间\u0026#34;的间隔   LocalDate localDate1 = LocalDate.of(2020, 1, 1);  LocalDate localDate2 = LocalDate.of(2048, 12, 12);  Period period = Period.between(localDate1, localDate2);  System.out.println(period);//P28Y11M11D   //public int getYears() 获得这段时间的年数  System.out.println(period.getYears());//28  //public int getMonths() 获得此期间的月数  System.out.println(period.getMonths());//11  //public int getDays() 获得此期间的天数  System.out.println(period.getDays());//11   //public long toTotalMonths() 获取此期间的总月数  System.out.println(period.toTotalMonths());//347   } }   2.10 Duration (应用)   方法说明\n   方法名 说明     public static Durationbetween(开始时间,结束时间) 计算两个“时间\u0026quot;的间隔   public long toSeconds() 获得此时间间隔的秒   public int toMillis() 获得此时间间隔的毫秒   public int toNanos() 获得此时间间隔的纳秒      示例代码\n/** * 计算两个时间的间隔 */ public class JDK8DateDemo10 {  public static void main(String[] args) {  //public static Duration between(开始时间,结束时间) 计算两个“时间\u0026#34;的间隔   LocalDateTime localDateTime1 = LocalDateTime.of(2020, 1, 1, 13, 14, 15);  LocalDateTime localDateTime2 = LocalDateTime.of(2020, 1, 2, 11, 12, 13);  Duration duration = Duration.between(localDateTime1, localDateTime2);  System.out.println(duration);//PT21H57M58S  //public long toSeconds()\t获得此时间间隔的秒  System.out.println(duration.toSeconds());//79078  //public int toMillis()\t获得此时间间隔的毫秒  System.out.println(duration.toMillis());//79078000  //public int toNanos() 获得此时间间隔的纳秒  System.out.println(duration.toNanos());//79078000000000  } }   3.异常 3.1 异常（记忆）   异常的概述\n​\t异常就是程序出现了不正常的情况\n  异常的体系结构\n  3.2 编译时异常和运行时异常的区别（记忆）   编译时异常\n 都是Exception类及其子类 必须显示处理，否则程序就会发生错误，无法通过编译    运行时异常\n 都是RuntimeException类及其子类 无需显示处理，也可以和编译时异常一样处理    图示\n  3.3 JVM默认处理异常的方式（理解）  如果程序出现了问题，我们没有做任何处理，最终JVM 会做默认的处理，处理方式有如下两个步骤：  把异常的名称，错误原因及异常出现的位置等信息输出在了控制台 程序停止执行    3.4 查看异常信息 (理解) 控制台在打印异常信息时,会打印异常类名,异常出现的原因,异常出现的位置\n我们调bug时,可以根据提示,找到异常出现的位置,分析原因,修改异常代码\n3.5 throws方式处理异常（应用）   定义格式\npublic void 方法() throws 异常类名 {  }   示例代码\npublic class ExceptionDemo {  public static void main(String[] args) throws ParseException{  System.out.println(\u0026#34;开始\u0026#34;); // method();  method2();   System.out.println(\u0026#34;结束\u0026#34;);  }   //编译时异常  public static void method2() throws ParseException {  String s = \u0026#34;2048-08-09\u0026#34;;  SimpleDateFormat sdf = new SimpleDateFormat(\u0026#34;yyyy-MM-dd\u0026#34;);  Date d = sdf.parse(s);  System.out.println(d);  }   //运行时异常  public static void method() throws ArrayIndexOutOfBoundsException {  int[] arr = {1, 2, 3};  System.out.println(arr[3]);  } }   注意事项\n 这个throws格式是跟在方法的括号后面的 编译时异常必须要进行处理，两种处理方案：try\u0026hellip;catch …或者 throws，如果采用 throws 这种方案，在方法上进行显示声明,将来谁调用这个方法谁处理 运行时异常因为在运行时才会发生,所以在方法后面可以不写,运行时出现异常默认交给jvm处理    3.6 throw抛出异常 (应用)   格式\nthrow new 异常();\n  注意\n这个格式是在方法内的，表示当前代码手动抛出一个异常，下面的代码不用再执行了\n  throws和throw的区别\n   throws throw     用在方法声明后面，跟的是异常类名 用在方法体内，跟的是异常对象名   表示声明异常，调用该方法有可能会出现这样的异常 表示手动抛出异常对象，由方法体内的语句处理      示例代码\npublic class ExceptionDemo8 {  public static void main(String[] args) {  //int [] arr = {1,2,3,4,5};  int [] arr = null;  printArr(arr);//就会 接收到一个异常.  //我们还需要自己处理一下异常.  }   private static void printArr(int[] arr) {  if(arr == null){  //调用者知道成功打印了吗?  //System.out.println(\u0026#34;参数不能为null\u0026#34;);  throw new NullPointerException(); //当参数为null的时候  //手动创建了一个异常对象,抛给了调用者,产生了一个异常  }else{  for (int i = 0; i \u0026lt; arr.length; i++) {  System.out.println(arr[i]);  }  }  }  }   3.7 try-catch方式处理异常（应用）   定义格式\ntry { \t可能出现异常的代码; } catch(异常类名 变量名) { \t异常的处理代码; }   执行流程\n 程序从 try 里面的代码开始执行 出现异常，就会跳转到对应的 catch 里面去执行 执行完毕之后，程序还可以继续往下执行    示例代码\npublic class ExceptionDemo01 {  public static void main(String[] args) {  System.out.println(\u0026#34;开始\u0026#34;);  method();  System.out.println(\u0026#34;结束\u0026#34;);  }   public static void method() {  try {  int[] arr = {1, 2, 3};  System.out.println(arr[3]);  System.out.println(\u0026#34;这里能够访问到吗\u0026#34;);  } catch (ArrayIndexOutOfBoundsException e) {  System.out.println(\u0026#34;你访问的数组索引不存在，请回去修改为正确的索引\u0026#34;);  }  } }   注意\n  如果 try 中没有遇到问题，怎么执行？\n会把try中所有的代码全部执行完毕,不会执行catch里面的代码\n  如果 try 中遇到了问题，那么 try 下面的代码还会执行吗？\n那么直接跳转到对应的catch语句中,try下面的代码就不会再执行了 当catch里面的语句全部执行完毕,表示整个体系全部执行完全,继续执行下面的代码\n  如果出现的问题没有被捕获，那么程序如何运行？\n那么try\u0026hellip;catch就相当于没有写.那么也就是自己没有处理. 默认交给虚拟机处理.\n  同时有可能出现多个异常怎么处理？\n出现多个异常,那么就写多个catch就可以了. 注意点:如果多个异常之间存在子父类关系.那么父类一定要写在下面\n    3.8 Throwable成员方法（应用）   常用方法\n   方法名 说明     public String getMessage() 返回此 throwable 的详细消息字符串   public String toString() 返回此可抛出的简短描述   public void printStackTrace() 把异常的错误信息输出在控制台      示例代码\npublic class ExceptionDemo02 {  public static void main(String[] args) {  System.out.println(\u0026#34;开始\u0026#34;);  method();  System.out.println(\u0026#34;结束\u0026#34;);  }   public static void method() {  try {  int[] arr = {1, 2, 3};  System.out.println(arr[3]); //new ArrayIndexOutOfBoundsException();  System.out.println(\u0026#34;这里能够访问到吗\u0026#34;);  } catch (ArrayIndexOutOfBoundsException e) { //new ArrayIndexOutOfBoundsException(); // e.printStackTrace();   //public String getMessage():返回此 throwable 的详细消息字符串 // System.out.println(e.getMessage());  //Index 3 out of bounds for length 3   //public String toString():返回此可抛出的简短描述 // System.out.println(e.toString());  //java.lang.ArrayIndexOutOfBoundsException: Index 3 out of bounds for length 3   //public void printStackTrace():把异常的错误信息输出在控制台  e.printStackTrace(); // java.lang.ArrayIndexOutOfBoundsException: Index 3 out of bounds for length 3 // at com.itheima_02.ExceptionDemo02.method(ExceptionDemo02.java:18) // at com.itheima_02.ExceptionDemo02.main(ExceptionDemo02.java:11)   }  } }   3.9 异常的练习 (应用)   需求\n键盘录入学生的姓名和年龄,其中年龄为18 - 25岁,超出这个范围是异常数据不能赋值.需要重新录入,一直录到正确为止\n  实现步骤\n 创建学生对象 键盘录入姓名和年龄，并赋值给学生对象 如果是非法数据就再次录入    代码实现\n学生类\npublic class Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  if(age \u0026gt;= 18 \u0026amp;\u0026amp; age \u0026lt;= 25){  this.age = age;  }else{  //当年龄不合法时,产生一个异常  throw new RuntimeException(\u0026#34;年龄超出了范围\u0026#34;);  }  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } } 测试类\npublic class ExceptionDemo12 {  public static void main(String[] args) {  // 键盘录入学生的姓名和年龄,其中年龄为 18 - 25岁,  // 超出这个范围是异常数据不能赋值.需要重新录入,一直录到正确为止。   Student s = new Student();   Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入姓名\u0026#34;);  String name = sc.nextLine();  s.setName(name);  while(true){  System.out.println(\u0026#34;请输入年龄\u0026#34;);  String ageStr = sc.nextLine();  try {  int age = Integer.parseInt(ageStr);  s.setAge(age);  break;  } catch (NumberFormatException e) {  System.out.println(\u0026#34;请输入一个整数\u0026#34;);  continue;  } catch (AgeOutOfBoundsException e) {  System.out.println(e.toString());  System.out.println(\u0026#34;请输入一个符合范围的年龄\u0026#34;);  continue;  }  /*if(age \u0026gt;= 18 \u0026amp;\u0026amp; age \u0026lt;=25){ s.setAge(age); break; }else{ System.out.println(\u0026#34;请输入符合要求的年龄\u0026#34;); continue; }*/  }  System.out.println(s);   } }   3.10 自定义异常（应用）   自定义异常概述\n当Java中提供的异常不能满足我们的需求时,我们可以自定义异常\n  实现步骤\n 定义异常类 写继承关系 提供空参构造 提供带参构造    代码实现\n异常类\npublic class AgeOutOfBoundsException extends RuntimeException {  public AgeOutOfBoundsException() {  }   public AgeOutOfBoundsException(String message) {  super(message);  } } 学生类\npublic class Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  if(age \u0026gt;= 18 \u0026amp;\u0026amp; age \u0026lt;= 25){  this.age = age;  }else{  //如果Java中提供的异常不能满足我们的需求,我们可以使用自定义的异常  throw new AgeOutOfBoundsException(\u0026#34;年龄超出了范围\u0026#34;);  }  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } } 测试类\npublic class ExceptionDemo12 {  public static void main(String[] args) {  // 键盘录入学生的姓名和年龄,其中年龄为 18 - 25岁,  // 超出这个范围是异常数据不能赋值.需要重新录入,一直录到正确为止。   Student s = new Student();   Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入姓名\u0026#34;);  String name = sc.nextLine();  s.setName(name);  while(true){  System.out.println(\u0026#34;请输入年龄\u0026#34;);  String ageStr = sc.nextLine();  try {  int age = Integer.parseInt(ageStr);  s.setAge(age);  break;  } catch (NumberFormatException e) {  System.out.println(\u0026#34;请输入一个整数\u0026#34;);  continue;  } catch (AgeOutOfBoundsException e) {  System.out.println(e.toString());  System.out.println(\u0026#34;请输入一个符合范围的年龄\u0026#34;);  continue;  }  /*if(age \u0026gt;= 18 \u0026amp;\u0026amp; age \u0026lt;=25){ s.setAge(age); break; }else{ System.out.println(\u0026#34;请输入符合要求的年龄\u0026#34;); continue; }*/  }  System.out.println(s);   } }   4.Optional 4.1获取对象(应用)   Optional概述\n可能包含或不包含非null值的容器对象\n  方法介绍\n   方法名 说明     static  Optional of(T value) 获取一个Optional对象，封装的是非null值的对象   static  Optional ofNullable(T value) 获取一个Optional对象，Optional封装的值对象可以是null也可以不是null      示例代码\npublic class OptionalDemo1 {  public static void main(String[] args) {  //method1();   //public static \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; ofNullable(T value)  //获取一个Optional对象，Optional封装的值对象可以是null也可以不是null  //Student s = new Student(\u0026#34;zhangsan\u0026#34;,23);  Student s = null;  //ofNullable方法，封装的对象可以是null，也可以不是null。  Optional\u0026lt;Student\u0026gt; optional = Optional.ofNullable(s);   System.out.println(optional);  }   private static void method1() {  //static \u0026lt;T\u0026gt; Optional\u0026lt;T\u0026gt; of(T value) 获取一个Optional对象，封装的是非null值的对象   //Student s = new Student(\u0026#34;zhangsan\u0026#34;,23);  Student s = null;  //Optional可以看做是一个容器，里面装了一个引用数据类型的对象。  //返回值就是Optional的对象  //如果使用of方法，封装的对象如果为空，那么还是会抛出空指针异常  Optional\u0026lt;Student\u0026gt; optional1 = Optional.of(s);  System.out.println(optional1);  } }   4.2常用方法(应用)   方法介绍\n   方法名 说明     T get() 如果存在值,返回值,否则抛出NoSuchElementException   boolean isPresent() 如果存在值,则返回true,否则为false      示例代码\npublic class OptionalDemo2 {  public static void main(String[] args) {  //get() 如果存在值，返回值，否则抛出NoSuchElementException  //public boolean isPresent() 判断Optional所封装的对象是否不为空，如果不为空返回true , 否则返回false   //Student s = new Student(\u0026#34;zhangsan\u0026#34;,23);  Student s = null;  Optional\u0026lt;Student\u0026gt; optional = Optional.ofNullable(s);  //如果封装的是一个null，那么通过get方法再次获取会抛出NoSuchElementException。  if(optional.isPresent()){  Student student = optional.get();  System.out.println(student);  }else{  System.out.println(\u0026#34;Optional封装的对象为空\u0026#34;);  }  } }   4.3处理空指针的方法(应用)   方法介绍\n   方法名 说明     T orElse(T other) 如果不为空,则返回具体的值,否则返回参数中的值   T orElseGet(Supplier\u0026lt;? extends T\u0026gt; supplier) 如果不为空,则返回具体的值,否则返回由括号中函数产生的结果   void ifPresent (Consumer\u0026lt;? super T\u0026gt; action) 如果不为空,则使用该值执行给定的操作,否则不执行任何操作   void ifPresentOrElse(Consumer\u0026lt;? super T\u0026gt; action, Runnable emptyAction) 如果不为空,则使用该值执行给定的操作,否则执行给定的基于空的操作      示例代码\npublic class OptionalDemo3 {  public static void main(String[] args) {  //method1();   //method2();  //method3();  //method4();   }   private static void method4() {  //Student s = new Student(\u0026#34;zhangsan\u0026#34;,23);  Student s = null;  Optional\u0026lt;Student\u0026gt; optional = Optional.ofNullable(s);  //public void ifPresentOrElse(Consumer\u0026lt;? super T\u0026gt; action, Runnable emptyAction)、  //如果不为空，则使用该值执行给定的操作，否则执行给定的基于空的操作。  optional.ifPresentOrElse(student -\u0026gt; System.out.println(student),  ()-\u0026gt;System.out.println(\u0026#34;为空了\u0026#34;));  }   private static void method3() {  //Student s = new Student(\u0026#34;zhangsan\u0026#34;,23);  Student s = null;  Optional\u0026lt;Student\u0026gt; optional = Optional.ofNullable(s);  //ifPresent (Consumer\u0026lt;? super T\u0026gt; action)  //如果不为空，则使用该值执行给定的操作，否则不执行任何操作  optional.ifPresent(student -\u0026gt; System.out.println(student));  }   private static void method2() {  Student s = new Student(\u0026#34;zhangsan\u0026#34;,23);  //Student s = null;  Optional\u0026lt;Student\u0026gt; optional = Optional.ofNullable(s);  //orElseGet(Supplier\u0026lt;? extends T\u0026gt; supplier)  //如果不为空，则返回具体的值，否则返回由括号中函数产生的结果   Student student = optional.orElseGet(()-\u0026gt; new Student(\u0026#34;lisi\u0026#34; , 24));  System.out.println(student);  }   private static void method1() {  //Student s = new Student(\u0026#34;zhangsan\u0026#34;,23);  Student s = null;  Optional\u0026lt;Student\u0026gt; optional = Optional.ofNullable(s);  //orElse(T other) 如果不为空，则返回具体的值，否则返回参数中的值  Student student = optional.orElse(new Student(\u0026#34;lisi\u0026#34;, 24));  System.out.println(student);  } }   ","permalink":"https://iblog.zone/archives/java%E5%B8%B8%E7%94%A8api02/","summary":"1.时间日期类 1.1 Date类（应用）   计算机中时间原点\n1970年1月1日 00:00:00\n  时间换算单位\n1秒 = 1000毫秒\n  Date类概述\nDate 代表了一个特定的时间，精确到毫秒\n  Date类构造方法\n   方法名 说明     public Date() 分配一个 Date对象，并初始化，以便它代表它被分配的时间，精确到毫秒   public Date(long date) 分配一个 Date对象，并将其初始化为表示从标准基准时间起指定的毫秒数      示例代码\npublic class DateDemo01 {  public static void main(String[] args) {  //public Date()：分配一个 Date对象，并初始化，以便它代表它被分配的时间，精确到毫秒  Date d1 = new Date();  System.","title":"Java常用API02"},{"content":"1.API 1.1 API概述【理解】   什么是API\n​\tAPI (Application Programming Interface) ：应用程序编程接口\n  java中的API\n​\t指的就是 JDK 中提供的各种功能的 Java类，这些类将底层的实现封装了起来，我们不需要关心这些类是如何实现的，只需要学习这些类如何使用即可，我们可以通过帮助文档来学习这些API如何使用。\n  1.2 如何使用API帮助文档【应用】   打开帮助文档\n  找到索引选项卡中的输入框\n  在输入框中输入Random\n  看类在哪个包下\n  看类的描述\n  看构造方法\n  看成员方法\n  2.常用API 2.1 Math（应用）   1、Math类概述\n Math 包含执行基本数字运算的方法    2、Math中方法的调用方式\n Math类中无构造方法，但内部的方法都是静态的，则可以通过 类名.进行调用    3、Math类的常用方法\n   方法名 方法名 说明     public static int abs(int a) 返回参数的绝对值   public static double ceil(double a) 返回大于或等于参数的最小double值，等于一个整数   public static double floor(double a) 返回小于或等于参数的最大double值，等于一个整数   public static int round(float a) 按照四舍五入返回最接近参数的int   public static int max(int a,int b) 返回两个int值中的较大值   public static int min(int a,int b) 返回两个int值中的较小值   public static double pow (double a,double b) 返回a的b次幂的值   public static double random() 返回值为double的正值，[0.0,1.0)      2.2 System（应用）   System类的常用方法\n   方法名 说明     public static void exit(int status) 终止当前运行的 Java 虚拟机，非零表示异常终止   public static long currentTimeMillis() 返回当前时间(以毫秒为单位)      示例代码\n 需求：在控制台输出1-10000，计算这段代码执行了多少毫秒  public class SystemDemo {  public static void main(String[] args) {  // 获取开始的时间节点  long start = System.currentTimeMillis();  for (int i = 1; i \u0026lt;= 10000; i++) {  System.out.println(i);  }  // 获取代码运行结束后的时间节点  long end = System.currentTimeMillis();  System.out.println(\u0026#34;共耗时：\u0026#34; + (end - start) + \u0026#34;毫秒\u0026#34;);  } }   2.3 Object类的toString方法（应用）   Object类概述\n Object 是类层次结构的根，每个类都可以将 Object 作为超类。所有类都直接或者间接的继承自该类，换句话说，该类所具备的方法，所有类都会有一份    查看方法源码的方式\n 选中方法，按下Ctrl + B    重写toString方法的方式\n   Alt + Insert 选择toString     在类的空白区域，右键 -\u0026gt; Generate -\u0026gt; 选择toString      toString方法的作用：\n 以良好的格式，更方便的展示对象中的属性值    示例代码：\nclass Student extends Object {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  } } public class ObjectDemo {  public static void main(String[] args) {  Student s = new Student();  s.setName(\u0026#34;林青霞\u0026#34;);  s.setAge(30);  System.out.println(s);  System.out.println(s.toString());  } }   运行结果：\nStudent{name=\u0026#39;林青霞\u0026#39;, age=30} Student{name=\u0026#39;林青霞\u0026#39;, age=30}   2.4 Object类的equals方法（应用）   equals方法的作用\n 用于对象之间的比较，返回true和false的结果 举例：s1.equals(s2); s1和s2是两个对象    重写equals方法的场景\n 不希望比较对象的地址值，想要结合对象属性进行比较的时候。    重写equals方法的方式\n   alt + insert 选择equals() and hashCode()，IntelliJ Default，一路next，finish即可     在类的空白区域，右键 -\u0026gt; Generate -\u0026gt; 选择equals() and hashCode()，后面的同上。      示例代码：\nclass Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public boolean equals(Object o) {  //this -- s1  //o -- s2  if (this == o) return true;  if (o == null || getClass() != o.getClass()) return false;   Student student = (Student) o; //student -- s2   if (age != student.age) return false;  return name != null ? name.equals(student.name) : student.name == null;  } } public class ObjectDemo {  public static void main(String[] args) {  Student s1 = new Student();  s1.setName(\u0026#34;林青霞\u0026#34;);  s1.setAge(30);   Student s2 = new Student();  s2.setName(\u0026#34;林青霞\u0026#34;);  s2.setAge(30);   //需求：比较两个对象的内容是否相同  System.out.println(s1.equals(s2));  } }   面试题\n// 看程序,分析结果 String s = “abc”; StringBuilder sb = new StringBuilder(“abc”); s.equals(sb); sb.equals(s);  public class InterviewTest {  public static void main(String[] args) {  String s1 = \u0026#34;abc\u0026#34;;  StringBuilder sb = new StringBuilder(\u0026#34;abc\u0026#34;);  //1.此时调用的是String类中的equals方法.  //保证参数也是字符串,否则不会比较属性值而直接返回false  //System.out.println(s1.equals(sb)); // false   //StringBuilder类中是没有重写equals方法,用的就是Object类中的.  System.out.println(sb.equals(s1)); // false  } }   2.5 Objects (应用)   常用方法\n   方法名 说明     public static String toString(对象) 返回参数中对象的字符串表示形式。   public static String toString(对象, 默认字符串) 返回对象的字符串表示形式。   public static Boolean isNull(对象) 判断对象是否为空   public static Boolean nonNull(对象) 判断对象是否不为空      示例代码\n学生类\nclass Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   @Override  public String toString() {  return \u0026#34;Student{\u0026#34; +  \u0026#34;name=\u0026#39;\u0026#34; + name + \u0026#39;\\\u0026#39;\u0026#39; +  \u0026#34;, age=\u0026#34; + age +  \u0026#39;}\u0026#39;;  }  } 测试类\npublic class MyObjectsDemo {  public static void main(String[] args) {  // public static String toString(对象): 返回参数中对象的字符串表示形式。  // Student s = new Student(\u0026#34;小罗同学\u0026#34;,50);  // String result = Objects.toString(s);  // System.out.println(result);  // System.out.println(s);   // public static String toString(对象, 默认字符串): 返回对象的字符串表示形式。如果对象为空,那么返回第二个参数.  //Student s = new Student(\u0026#34;小花同学\u0026#34;,23);  // Student s = null;  // String result = Objects.toString(s, \u0026#34;随便写一个\u0026#34;);  // System.out.println(result);   // public static Boolean isNull(对象): 判断对象是否为空  //Student s = null;  // Student s = new Student();  // boolean result = Objects.isNull(s);  // System.out.println(result);   // public static Boolean nonNull(对象): 判断对象是否不为空  //Student s = new Student();  Student s = null;  boolean result = Objects.nonNull(s);  System.out.println(result);  }  }   2.6 BigDecimal (应用)   作用\n可以用来进行精确计算\n  构造方法\n   方法名 说明     BigDecimal(double val) 参数为double   BigDecimal(String val) 参数为String      常用方法\n   方法名 说明     public BigDecimal add(另一个BigDecimal对象) 加法   public BigDecimal subtract (另一个BigDecimal对象) 减法   public BigDecimal multiply (另一个BigDecimal对象) 乘法   public BigDecimal divide (另一个BigDecimal对象) 除法   public BigDecimal divide (另一个BigDecimal对象，精确几位，舍入模式) 除法      总结\n BigDecimal是用来进行精确计算的 创建BigDecimal的对象，构造方法使用参数类型为字符串的。 四则运算中的除法，如果除不尽请使用divide的三个参数的方法。  代码示例：\nBigDecimal divide = bd1.divide(参与运算的对象,小数点后精确到多少位,舍入模式); 参数1 ，表示参与运算的BigDecimal 对象。 参数2 ，表示小数点后面精确到多少位 参数3 ，舍入模式  BigDecimal.ROUND_UP 进一法  BigDecimal.ROUND_FLOOR 去尾法  BigDecimal.ROUND_HALF_UP 四舍五入   3.包装类 3.1 基本类型包装类（记忆）   基本类型包装类的作用\n将基本数据类型封装成对象的好处在于可以在对象中定义更多的功能方法操作该数据\n常用的操作之一：用于基本数据类型与字符串之间的转换\n  基本类型对应的包装类\n   基本数据类型 包装类     byte Byte   short Short   int Integer   long Long   float Float   double Double   char Character   boolean Boolean      3.2 Integer类（应用）   Integer类概述\n包装一个对象中的原始类型 int 的值\n  Integer类构造方法\n   方法名 说明     public Integer(int value) 根据 int 值创建 Integer 对象(过时)   public Integer(String s) 根据 String 值创建 Integer 对象(过时)   public static Integer valueOf(int i) 返回表示指定的 int 值的 Integer 实例   public static Integer valueOf(String s) 返回一个保存指定值的 Integer 对象 String      示例代码\npublic class IntegerDemo {  public static void main(String[] args) {  //public Integer(int value)：根据 int 值创建 Integer 对象(过时)  Integer i1 = new Integer(100);  System.out.println(i1);   //public Integer(String s)：根据 String 值创建 Integer 对象(过时)  Integer i2 = new Integer(\u0026#34;100\u0026#34;); // Integer i2 = new Integer(\u0026#34;abc\u0026#34;); //NumberFormatException  System.out.println(i2);  System.out.println(\u0026#34;--------\u0026#34;);   //public static Integer valueOf(int i)：返回表示指定的 int 值的 Integer 实例  Integer i3 = Integer.valueOf(100);  System.out.println(i3);   //public static Integer valueOf(String s)：返回一个保存指定值的Integer对象 String  Integer i4 = Integer.valueOf(\u0026#34;100\u0026#34;);  System.out.println(i4);  } }   3.3 自动拆箱和自动装箱（理解）   自动装箱\n​\t把基本数据类型转换为对应的包装类类型\n  自动拆箱\n​\t把包装类类型转换为对应的基本数据类型\n  示例代码\nInteger i = 100; // 自动装箱 i += 200; // i = i + 200; i + 200 自动拆箱；i = i + 200; 是自动装箱   3.4 int和String类型的相互转换（记忆）   int转换为String\n  转换方式\n 方式一：直接在数字后加一个空字符串 方式二：通过String类静态方法valueOf()    示例代码\npublic class IntegerDemo {  public static void main(String[] args) {  //int --- String  int number = 100;  //方式1  String s1 = number + \u0026#34;\u0026#34;;  System.out.println(s1);  //方式2  //public static String valueOf(int i)  String s2 = String.valueOf(number);  System.out.println(s2);  System.out.println(\u0026#34;--------\u0026#34;);  } }     String转换为int\n  转换方式\n 方式一：先将字符串数字转成Integer，再调用valueOf()方法 方式二：通过Integer静态方法parseInt()进行转换    示例代码\npublic class IntegerDemo {  public static void main(String[] args) {  //String --- int  String s = \u0026#34;100\u0026#34;;  //方式1：String --- Integer --- int  Integer i = Integer.valueOf(s);  //public int intValue()  int x = i.intValue();  System.out.println(x);  //方式2  //public static int parseInt(String s)  int y = Integer.parseInt(s);  System.out.println(y);  } }     3.5 字符串数据排序案例（应用）   案例需求\n​\t有一个字符串：“91 27 46 38 50”，请写程序实现最终输出结果是：27 38 46 50 91\n  代码实现\npublic class IntegerTest {  public static void main(String[] args) {  //定义一个字符串  String s = \u0026#34;91 27 46 38 50\u0026#34;;   //把字符串中的数字数据存储到一个int类型的数组中  String[] strArray = s.split(\u0026#34; \u0026#34;); // for(int i=0; i\u0026lt;strArray.length; i++) { // System.out.println(strArray[i]); // }   //定义一个int数组，把 String[] 数组中的每一个元素存储到 int 数组中  int[] arr = new int[strArray.length];  for(int i=0; i\u0026lt;arr.length; i++) {  arr[i] = Integer.parseInt(strArray[i]);  }   //对 int 数组进行排序  Arrays.sort(arr);   for(int i=0; i\u0026lt;arr.length; i++){  System.out.print(arr[i] + \u0026#34; \u0026#34;);  } }   4.递归 4.1 递归【应用】   递归的介绍\n 以编程的角度来看，递归指的是方法定义中调用方法本身的现象 把一个复杂的问题层层转化为一个与原问题相似的规模较小的问题来求解 递归策略只需少量的程序就可描述出解题过程所需要的多次重复计算    递归的基本使用\npublic class MyFactorialDemo2 {  public static void main(String[] args) {  int sum = getSum(100);  System.out.println(sum);  }   private static int getSum(int i) {  //1- 100之间的和  //100 + (1-99之间的和)  // 99 + (1- 98之间的和)  //....  //1  //方法的作用: 求 1- i 之间和  if(i == 1){  return 1;  }else{  return i + getSum(i -1);  }  } }   递归的注意事项\n 递归一定要有出口。否则内存溢出 递归虽然有出口，但是递归的次数也不宜过多。否则内存溢出    4.2 递归求阶乘【应用】   案例需求\n​\t用递归求5的阶乘，并把结果在控制台输出\n  代码实现\npublic class DiGuiDemo01 {  public static void main(String[] args) {  //调用方法  int result = jc(5);  //输出结果  System.out.println(\u0026#34;5的阶乘是：\u0026#34; + result);  }   //定义一个方法，用于递归求阶乘，参数为一个int类型的变量  public static int jc(int n) {  //在方法内部判断该变量的值是否是1  if(n == 1) {  //是：返回1  return 1;  } else {  //不是：返回n*(n-1)!  return n*jc(n-1);  }  } }   内存图\n  5.数组的高级操作 5.1 二分查找 (理解)   二分查找概述\n查找指定元素在数组中的位置时,以前的方式是通过遍历,逐个获取每个元素,看是否是要查找的元素,这种方式当数组元素较多时,查找的效率很低\n二分查找也叫折半查找,每次可以去掉一半的查找范围,从而提高查找的效率\n  需求\n在数组{1,2,3,4,5,6,7,8,9,10}中,查找某个元素的位置\n  实现步骤\n 定义两个变量，表示要查找的范围。默认min = 0 ，max = 最大索引 循环查找，但是min \u0026lt;= max 计算出mid的值 判断mid位置的元素是否为要查找的元素，如果是直接返回对应索引 如果要查找的值在mid的左半边，那么min值不变，max = mid -1.继续下次循环查找 如果要查找的值在mid的右半边，那么max值不变，min = mid + 1.继续下次循环查找 当min \u0026gt; max 时，表示要查找的元素在数组中不存在，返回-1.    代码实现\npublic class MyBinarySearchDemo {  public static void main(String[] args) {  int [] arr = {1,2,3,4,5,6,7,8,9,10};  int number = 11;   //1,我现在要干嘛? --- 二分查找  //2.我干这件事情需要什么? --- 数组 元素  //3,我干完了,要不要把结果返回调用者 --- 把索引返回给调用者  int index = binarySearchForIndex(arr,number);  System.out.println(index);  }   private static int binarySearchForIndex(int[] arr, int number) {  //1,定义查找的范围  int min = 0;  int max = arr.length - 1;  //2.循环查找 min \u0026lt;= max  while(min \u0026lt;= max){  //3.计算出中间位置 mid  int mid = (min + max) \u0026gt;\u0026gt; 1;  //mid指向的元素 \u0026gt; number  if(arr[mid] \u0026gt; number){  //表示要查找的元素在左边.  max = mid -1;  }else if(arr[mid] \u0026lt; number){  //mid指向的元素 \u0026lt; number  //表示要查找的元素在右边.  min = mid + 1;  }else{  //mid指向的元素 == number  return mid;  }  }  //如果min大于了max就表示元素不存在,返回-1.  return -1;  }  }   注意事项\n有一个前提条件，数组内的元素一定要按照大小顺序排列，如果没有大小顺序，是不能使用二分查找法的\n  5.2 冒泡排序 (理解)   冒泡排序概述\n一种排序的方式，对要进行排序的数据中相邻的数据进行两两比较，将较大的数据放在后面，依次对所有的数据进行操作，直至所有数据按要求完成排序\n如果有n个数据进行排序，总共需要比较n-1次\n每一次比较完毕，下一次的比较就会少一个数据参与\n  代码实现\npublic class MyBubbleSortDemo2 {  public static void main(String[] args) {  int[] arr = {3, 5, 2, 1, 4};  //1 2 3 4 5  bubbleSort(arr);  }   private static void bubbleSort(int[] arr) {  //外层循环控制的是次数 比数组的长度少一次.  for (int i = 0; i \u0026lt; arr.length -1; i++) {  //内存循环就是实际循环比较的  //-1 是为了让数组不要越界  //-i 每一轮结束之后,我们就会少比一个数字.  for (int j = 0; j \u0026lt; arr.length - 1 - i; j++) {  if (arr[j] \u0026gt; arr[j + 1]) {  int temp = arr[j];  arr[j] = arr[j + 1];  arr[j + 1] = temp;  }  }  }   printArr(arr);  }   private static void printArr(int[] arr) {  for (int i = 0; i \u0026lt; arr.length; i++) {  System.out.print(arr[i] + \u0026#34; \u0026#34;);  }  System.out.println();  }  }   5.3 快速排序 (理解)   快速排序概述\n冒泡排序算法中,一次循环结束,就相当于确定了当前的最大值,也能确定最大值在数组中应存入的位置\n快速排序算法中,每一次递归时以第一个数为基准数,找到数组中所有比基准数小的.再找到所有比基准数大的.小的全部放左边,大的全部放右边,确定基准数的正确位置\n  核心步骤\n 从右开始找比基准数小的 从左开始找比基准数大的 交换两个值的位置 红色继续往左找，蓝色继续往右找，直到两个箭头指向同一个索引为止 基准数归位    代码实现\npublic class MyQuiteSortDemo2 {  public static void main(String[] args) { // 1，从右开始找比基准数小的 // 2，从左开始找比基准数大的 // 3，交换两个值的位置 // 4，红色继续往左找，蓝色继续往右找，直到两个箭头指向同一个索引为止 // 5，基准数归位  int[] arr = {6, 1, 2, 7, 9, 3, 4, 5, 10, 8};   quiteSort(arr,0,arr.length-1);   for (int i = 0; i \u0026lt; arr.length; i++) {  System.out.print(arr[i] + \u0026#34; \u0026#34;);  }  }   private static void quiteSort(int[] arr, int left, int right) {  // 递归结束的条件  if(right \u0026lt; left){  return;  }   int left0 = left;  int right0 = right;   //计算出基准数  int baseNumber = arr[left0];   while(left != right){ // 1，从右开始找比基准数小的  while(arr[right] \u0026gt;= baseNumber \u0026amp;\u0026amp; right \u0026gt; left){  right--;  } // 2，从左开始找比基准数大的  while(arr[left] \u0026lt;= baseNumber \u0026amp;\u0026amp; right \u0026gt; left){  left++;  } // 3，交换两个值的位置  int temp = arr[left];  arr[left] = arr[right];  arr[right] = temp;  }  //基准数归位  int temp = arr[left];  arr[left] = arr[left0];  arr[left0] = temp;  \t// 递归调用自己,将左半部分排好序  quiteSort(arr,left0,left-1);  // 递归调用自己,将右半部分排好序  quiteSort(arr,left +1,right0);   } }   5.4 Arrays (应用)   Arrays的常用方法\n   方法名 说明     public static String toString(int[] a) 返回指定数组的内容的字符串表示形式   public static void sort(int[] a) 按照数字顺序排列指定的数组   public static int binarySearch(int[] a, int key) 利用二分查找返回指定元素的索引      示例代码\npublic class MyArraysDemo {  public static void main(String[] args) {  // public static String toString(int[] a) 返回指定数组的内容的字符串表示形式  // int [] arr = {3,2,4,6,7};  // System.out.println(Arrays.toString(arr));   // public static void sort(int[] a)\t按照数字顺序排列指定的数组  // int [] arr = {3,2,4,6,7};  // Arrays.sort(arr);  // System.out.println(Arrays.toString(arr));   // public static int binarySearch(int[] a, int key) 利用二分查找返回指定元素的索引  int [] arr = {1,2,3,4,5,6,7,8,9,10};  int index = Arrays.binarySearch(arr, 0);  System.out.println(index);  //1,数组必须有序  //2.如果要查找的元素存在,那么返回的是这个元素实际的索引  //3.如果要查找的元素不存在,那么返回的是 (-插入点-1)  //插入点:如果这个元素在数组中,他应该在哪个索引上.  }  }   工具类设计思想\n 构造方法用 private 修饰 成员用 public static 修饰    ","permalink":"https://iblog.zone/archives/java%E5%B8%B8%E7%94%A8api01/","summary":"1.API 1.1 API概述【理解】   什么是API\n​\tAPI (Application Programming Interface) ：应用程序编程接口\n  java中的API\n​\t指的就是 JDK 中提供的各种功能的 Java类，这些类将底层的实现封装了起来，我们不需要关心这些类是如何实现的，只需要学习这些类如何使用即可，我们可以通过帮助文档来学习这些API如何使用。\n  1.2 如何使用API帮助文档【应用】   打开帮助文档\n  找到索引选项卡中的输入框\n  在输入框中输入Random\n  看类在哪个包下\n  看类的描述\n  看构造方法\n  看成员方法\n  2.常用API 2.1 Math（应用）   1、Math类概述\n Math 包含执行基本数字运算的方法    2、Math中方法的调用方式\n Math类中无构造方法，但内部的方法都是静态的，则可以通过 类名.进行调用    3、Math类的常用方法\n   方法名 方法名 说明     public static int abs(int a) 返回参数的绝对值   public static double ceil(double a) 返回大于或等于参数的最小double值，等于一个整数   public static double floor(double a) 返回小于或等于参数的最大double值，等于一个整数   public static int round(float a) 按照四舍五入返回最接近参数的int   public static int max(int a,int b) 返回两个int值中的较大值   public static int min(int a,int b) 返回两个int值中的较小值   public static double pow (double a,double b) 返回a的b次幂的值   public static double random() 返回值为double的正值，[0.","title":"Java常用API01"},{"content":"Helm 介绍 helm 是 kubernetes 的包管理器。它相当于 CentOS 的 yum ，Ubuntu 的 apt 。\n在 helm 中有三大概念：\n Chart：Helm使用的包格式称为 chart。 chart 就是一个描述 Kubernetes 相关资源的文件集合。单个 chart 可以用来部署一些简单的， 类似于 memcache pod，或者某些复杂的 HTTP 服务器以及 web 全栈应用、数据库、缓存等 Repo：chart 的存放仓库，社区的 Helm chart 仓库位于 Artifact Hub，也可以创建运行自己的私有 chart 仓库 Release：运行在 Kubernetes 集群中的 chart 的实例。一个 chart 通常可以在同一个集群中安装多次，而每一次安装都会创建一个新的 release  总结：Helm 安装 charts 到 Kubernetes 集群中，每次安装都会创建一个新的 release 。你可以在 Helm 的 chart repositories 中寻找新的 chart 。\n准备阶段 拥有一个 Kubernetes 集群，如下：\n具体配置：\n   类型 IP地址 系统版本 配置     Master主节点 192.168.19.136 CentOS Linux release 7.9.2009 (Core) 4核4G   Work工作节点1 192.168.19.137 CentOS Linux release 7.9.2009 (Core) 4核4G   Work工作节点2 192.168.19.138 CentOS Linux release 7.9.2009 (Core) 4核4G    选择 Helm 版本    Helm 版本 支持的 Kubernetes 版本     3.5.x 1.20.x - 1.17.x   3.4.x 1.19.x - 1.16.x   3.3.x 1.18.x - 1.15.x   3.2.x 1.18.x - 1.15.x   3.1.x 1.17.x - 1.14.x   3.0.x 1.16.x - 1.13.x   2.16.x 1.16.x - 1.15.x   2.15.x 1.15.x - 1.14.x   2.14.x 1.14.x - 1.13.x   2.13.x 1.13.x - 1.12.x   2.12.x 1.12.x - 1.11.x   2.11.x 1.11.x - 1.10.x   2.10.x 1.10.x - 1.9.x   2.9.x 1.10.x - 1.9.x   2.8.x 1.9.x - 1.8.x   2.7.x 1.8.x - 1.7.x   2.6.x 1.7.x - 1.6.x   2.5.x 1.6.x - 1.5.x   2.4.x 1.6.x - 1.5.x   2.3.x 1.5.x - 1.4.x   2.2.x 1.5.x - 1.4.x   2.1.x 1.5.x - 1.4.x   2.0.x 1.4.x - 1.3.x    注： Helm 2 采用 client/server 架构，分为 Helm 客户端和 Tiller 服务端，而 Helm3 移除了 Tiller。 也就是说 Helm3 只要安装 Helm 就可以了。 关于 Helm 2 和 Helm 3 的区别可以阅读：Helm文档\n本文会选择 Helm 3.4.2 进行安装。\n下载安装 Helm 3.4.2 访问 github.com/helm/helm/r… 选择对应的版本，下载\n拷贝到集群中的 Master 节点\n解压\ntar -zxvf helm-v3.4.2-linux-amd64.tar.gz 移动到 /usr/local/bin\nmv linux-amd64/helm /usr/local/bin/helm 查看是否安装成功\nhelm version 使用 Helm 部署 Consul 集群 Helm 基本用法 在部署consul之前，先来看看 helm 的基本用法。\n查找 Charts ：\nhelm search hub # 从 Artifact Hub 中查找并列出 helm charts。 Artifact Hub中存放了大量不同的仓库。 helm search repo # 从你添加（使用 helm repo add）到本地 helm 客户端中的仓库中进行查找。 添加 HashiCorp Helm 仓库：\nhelm repo add hashicorp https://helm.releases.hashicorp.com # 查看已添加的仓库列表 helm repo list 搜索 consul ：\nhelm search repo consul Consul 所需环境准备 命名空间 创建一个命名空间，后续都会在此命名空间上进行操作：\nkubectl create namespace consul 存储 由于 consul 部署的时候会创建使用 PVC：PersistentVolumeClaim 的 Pod ，Pod 中的应用通过 PVC 进行数据的持久化，而 PVC 使用 PV: PersistentVolume 进行数据的最终持久化处理。所以我们要准备好存储资源供应，否则 consul-server 会因为获取不到存储资源而一直处于 pending 状态，有以下两种方案：\n方案1，手动创建静态 PV 进行存储供应 cat \u0026lt;\u0026lt;EOF | kubectl apply -f - --- kind: PersistentVolume apiVersion: v1 metadata: name: pv-volume-consul-0 namespace: consul labels: type: local spec: storageClassName: \u0026#34;\u0026#34; capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/consul/data\u0026#34; --- kind: PersistentVolume apiVersion: v1 metadata: name: pv-volume-consul-1 namespace: consul labels: type: local spec: storageClassName: \u0026#34;\u0026#34; capacity: storage: 10Gi accessModes: - ReadWriteOnce hostPath: path: \u0026#34;/consul/data\u0026#34; EOF 查看：\nkubectl get pv -n consul -o wide 方案2，通过 StorageClass 实现动态卷供应 在所有节点上安装 nfs-utils：\nyum install -y nfs-utils 选择一台节点，这里选择 work2（192.168.19.138） 节点作为 nfs server：\n# 创建nfs目录 mkdir -p /mnt/nfs # 配置nfs权限 cat\u0026gt;/etc/exports\u0026lt;\u0026lt;EOF /mnt/nfs/ 192.168.19.0/24(insecure,rw,anonuid=0,anongid=0,all_squash,sync) EOF # 启动nfs服务 systemctl start rpcbind.service systemctl start nfs-server.service # 设置开机自启 systemctl enable rpcbind.service systemctl enable nfs-server.service # 配置生效 exportfs -r 在 master 节点使用 helm 安装 nfs-provisioner：\n# 添加仓库源 helm repo add azure http://mirror.azure.cn/kubernetes/charts/ # 搜索nfs-client-provisioner helm search repo nfs-client-provisioner # 安装nfs-client-provisioner helm install nfs-storage azure/nfs-client-provisioner -n consul \\ --set nfs.server=192.168.19.138 \\ --set nfs.path=/mnt/nfs \\ --set storageClass.name=nfs-storage \\ --set storageClass.defaultClass=true # 查看StorageClass kubectl get sc -n consul 至此，当有 PVC 需要申请 PV 时，StorageClass 就会自动为我们创建 PV 了。\n配置文件 创建 config.yaml ：\nglobal:  name: consul # 设置用于 Helm chart 中所有资源的前缀 ui:  service: # 为 Consul UI 配置服务  type: \u0026#39;NodePort\u0026#39; # 服务类型为 NodePort server:  replicas: 2 # 要运行的服务器的数量，即集群数  affinity: \u0026#34;\u0026#34; # 允许每个节点上运行更多的Pod  storage: \u0026#39;10Gi\u0026#39; # 定义用于配置服务器的 StatefulSet 存储的磁盘大小  storageClass: \u0026#34;\u0026#34; # 使用Kubernetes集群的默认 StorageClass  securityContext: # 服务器 Pod 的安全上下文，以 root 用户运行  fsGroup: 2000  runAsGroup: 2000  runAsNonRoot: false  runAsUser: 0  更多配置可以参考：www.consul.io/docs/k8s/he…\n 开始安装 helm install hi-consul hashicorp/consul -n consul -f config.yaml  hi-consul：你命名的 release 名字\nhashicorp/consul：你想安装的 chart 的名称\n-n ：指定命名空间\n-f ：传递配置文件\n 执行安装命令后我们可以监控 pod 的状态：\nkubectl get pods -n consul -o wide -w 等待所有 Pod READY 完毕后，查看 svc 状态：\nkubectl get svc -n consul -o wide 浏览器访问 http://master:30497/ui/ （已设置hosts）或 http://192.168.19.136:30497/ui/\n题外话 # 查看helm已安装列表 helm list -n consul # 卸载 helm uninstall hi-consul -n consul # 更多 helm help ","permalink":"https://iblog.zone/archives/%E4%BD%BF%E7%94%A8helm%E9%83%A8%E7%BD%B2consul%E9%9B%86%E7%BE%A4/","summary":"Helm 介绍 helm 是 kubernetes 的包管理器。它相当于 CentOS 的 yum ，Ubuntu 的 apt 。\n在 helm 中有三大概念：\n Chart：Helm使用的包格式称为 chart。 chart 就是一个描述 Kubernetes 相关资源的文件集合。单个 chart 可以用来部署一些简单的， 类似于 memcache pod，或者某些复杂的 HTTP 服务器以及 web 全栈应用、数据库、缓存等 Repo：chart 的存放仓库，社区的 Helm chart 仓库位于 Artifact Hub，也可以创建运行自己的私有 chart 仓库 Release：运行在 Kubernetes 集群中的 chart 的实例。一个 chart 通常可以在同一个集群中安装多次，而每一次安装都会创建一个新的 release  总结：Helm 安装 charts 到 Kubernetes 集群中，每次安装都会创建一个新的 release 。你可以在 Helm 的 chart repositories 中寻找新的 chart 。\n准备阶段 拥有一个 Kubernetes 集群，如下：\n具体配置：","title":"使用Helm部署Consul集群"},{"content":"一、CephFS介绍  Ceph File System (CephFS) 是与 POSIX 标准兼容的文件系统, 能够提供对 Ceph 存储集群上的文件访问. Jewel 版本 (10.2.0) 是第一个包含稳定 CephFS 的 Ceph 版本. CephFS 需要至少一个元数据服务器 (Metadata Server – MDS) daemon (ceph-mds) 运行, MDS daemon 管理着与存储在 CephFS 上的文件相关的元数据, 并且协调着对 Ceph 存储系统的访问。\n对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。\n二、使用CephFS类型Volume直接挂载  cephfs卷允许将现有的cephfs卷挂载到你的Pod中，与emptyDir类型不同的是，emptyDir会在删除Pod时把数据清除掉，而cephfs卷的数据会被保留下来,仅仅是被卸载，并且cephfs可以被多个设备进行读写。\n1、安装 Ceph 客户端 在部署 kubernetes 之前我们就已经有了 Ceph 集群，因此我们可以直接拿来用。但是 kubernetes 的所有节点（尤其是 master 节点）上依然需要安装 ceph 客户端。\nyum install -y ceph-common  还需要将 ceph 的配置文件 ceph.conf 放在所有节点的 /etc/ceph 目录下  2、创建Ceph secret  注意： ceph_secret.yaml文件中key需要进行base64编码，之后在文件中使用。 以下操作需要在Ceph集群中mon节点操作  # 方式1： [cephu@ceph-node1 ~]$ sudo ceph auth get-key client.admin AQB+dXxfhmh4LRAAM3Ow+ZdP64Py1N5ZvBgKiA==  [cephu@ceph-node1 ~]$ echo -n \u0026#34;AQB+dXxfhmh4LRAAM3Ow+ZdP64Py1N5ZvBgKiA==\u0026#34;| base64 QVFCK2RYeGZobWg0TFJBQU0zT3crWmRQNjRQeTFONVp2QmdLaUE9PQ==  # 方式2： [cephu@ceph-node1 ~]$ sudo ceph auth get-key client.admin | base64 QVFCK2RYeGZobWg0TFJBQU0zT3crWmRQNjRQeTFONVp2QmdLaUE9PQ==  在K8S-Master节点创建secret文件，ceph_secret.yaml文件的定义如下:  #首先创建一个名称空间 [root@k8s-master1 ~]# kubectl create ns dev namespace/dev created  #然后创建secret类型的资源文件 [root@k8s-master1 ~]# vim ceph_secret.yaml apiVersion: v1 kind: Secret metadata:  name: ceph-secret  namespace: dev type: kubernetes.io/rbd data:  key: QVFCK2RYeGZobWg0TFJBQU0zT3crWmRQNjRQeTFONVp2QmdLaUE9PQ==  执行创建ceph_secret资源：  [root@k8s-master1 ~]# kubectl apply -f ceph_secret.yaml  secret/ceph-secret created 3、创建Pod进行挂载  定义的文件如下：  [root@k8s-master1 ~]# vim myapp-pod.yaml apiVersion: v1 kind: Pod metadata:  name: myapp  namespace: dev spec:  containers:  - name: myapp  image: busybox  command: [\u0026#34;sleep\u0026#34;, \u0026#34;60000\u0026#34;]  volumeMounts:  - mountPath: \u0026#34;/mnt/cephfs\u0026#34;  name: cephfs\t#指定使用的挂载卷名称  volumes:  - name: cephfs\t#定义挂载卷名称  cephfs: #挂载类型为cephFS  monitors:  - 192.168.66.201:6789\t#ceph集群mon节点  - 192.168.66.202:6789  - 192.168.66.203:6789  user: admin\t#ceph认证用户  path: /  secretRef:  name: ceph-secret  #调用ceph认证secret  执行创建pod，并挂载cephfs卷：  [root@k8s-master1 ~]# kubectl apply -f myapp-pod.yaml  pod/myapp created  [root@k8s-master1 ~]# kubectl get po -n dev NAME READY STATUS RESTARTS AGE myapp 1/1 Running 0 94s 三、使用PV\u0026amp;PVC方式进行数据卷挂载  1、创建PV  定义的文件如下：  [root@k8s-master1 ~]# vim myapp-pv.yaml apiVersion: v1 kind: PersistentVolume metadata:  name: cephfs-pv  namespace: dev spec:  capacity:  storage: 1Gi  accessModes:  - ReadWriteMany  cephfs:  monitors:  - 192.168.66.201:6789 #ceph集群mon节点地址和端口  - 192.168.66.202:6789  - 192.168.66.203:6789  user: admin\t#ceph的认证用户  path: /  secretRef:  name: ceph-secret\t#此处使用上面创建的secret资源中定义的secret名称  readOnly: false  persistentVolumeReclaimPolicy: Delete  创建PV, 如果PV的状态为Available，则说明PV创建成功，可以提供给PVC使用  [root@k8s-master1 ~]# kubectl apply -f myapp-pv.yaml  persistentvolume \u0026#34;cephfs-pv\u0026#34; created  [root@k8s-master1 ~]# kubectl get pv cephfs-pv -n dev 2、创建PVC  定义的文件如下:  [root@k8s-master1 ~]# vim myapp-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: cephfs-pvc  namespace: dev spec:  accessModes:  - ReadWriteMany  resources:  requests:  storage: 1Gi  **注意：**PVC的访问模式和存储大小必须和PV的匹配才能绑定成功 创建PVC资源:  [root@k8s-master1 ~]# kubectl apply -f myapp-pvc.yaml  persistentvolumeclaim/cephfs-pvc created  [root@k8s-master1 ~]# kubectl get pvc cephfs-pvc -n dev 3、创建Pod挂载该PVC  定义的文件如下：  [root@k8s-master1 ~]# vim myapp-pod-pv.yaml apiVersion: v1 kind: Pod metadata:  labels:  app: cephfs-pv-myapp  name: cephfs-pv-myapp  namespace: dev spec:  containers:  - name: cephfs-pv-myapp  image: busybox  command: [\u0026#34;sleep\u0026#34;, \u0026#34;60000\u0026#34;]  volumeMounts:  - mountPath: \u0026#34;/mnt/cephfs\u0026#34;\t#映射至容器中路径  name: cephfs-myapp  readOnly: false  volumes:  - name: cephfs-myapp  persistentVolumeClaim:  claimName: cephfs-pvc  #挂载的pvc卷名  执行创建Pod，并查看cephfs卷是否挂载上  [root@k8s-master1 ~]# kubectl apply -f myapp-pod-pv.yaml  pod/cephfs-pv-myapp created  [root@k8s-master1 ~]# kubectl get po cephfs-pv-myapp -n dev NAME READY STATUS RESTARTS AGE cephfs-pv-myapp 1/1 Running 0 25s  [root@k8s-master1 ~]# kubectl describe po cephfs-pv-myapp -n dev 四、使用Cephfs provisioner动态分配PV  由于kubernetes官方并没有对cephfs提供类似ceph RBD动态分配PV的storageClass的功能。\n补充: storageClass有一个分配器，用来决定使用哪个卷插件来分配PV。社区提供了一个cephfs-provisioner来实现这个功能，目前该存储库已经不再维护，但是可以使用\nCephfs provisioner已经不再维护，详细信息查看：https://github.com/kubernetes/org/issues/1563\n 接下来我们直接使用cephfs-provisioner来进行试验:  1、创建admin-secret资源 #在ceph-mon节点查看admin用户的权限 [cephu@ceph-node1 ~]$ sudo ceph auth get-key client.admin|base64 QVFCK2RYeGZobWg0TFJBQU0zT3crWmRQNjRQeTFONVp2QmdLaUE9PQ==  #在K8S-Master节点创建名称空间cephfs [root@k8s-master1 ~]# kubectl create ns cephfs  #在K8S-Master1节点上创建cephfs所以定义文件存储目录 [root@k8s-master1 ~]# mkdir ceph-cephfs [root@k8s-master1 ~]# cd ceph-cephfs/  #在K8S集群Master节点上创建secret资源并使用ceph中转换后key [root@k8s-master1 ceph-cephfs]# vim admin-secret.yaml apiVersion: v1 kind: Secret metadata:  name: ceph-secret-admin  namespace: cephfs type: kubernetes.io/rbd data:  key: QVFCK2RYeGZobWg0TFJBQU0zT3crWmRQNjRQeTFONVp2QmdLaUE9PQ==  #执行secret定义文件 [root@k8s-master1 ceph-cephfs]# kubectl apply -f admin-secret.yaml  secret/ceph-secret-admin created  #查看secret资源信息 [root@k8s-master1 ceph-cephfs]# kubectl get secret -n cephfs NAME TYPE DATA AGE ceph-secret-admin kubernetes.io/rbd 1 82s 2、执行CephFS provisioner  创建CephFS provisioner定义文件  [root@k8s-master1 ~]# vim storage-cephfs-provisioner.yaml apiVersion: v1 kind: ServiceAccount metadata:  name: cephfs-provisioner  namespace: cephfs --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: cephfs-provisioner rules:  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;persistentvolumes\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;persistentvolumeclaims\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;]  - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;]  resources: [\u0026#34;storageclasses\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;events\u0026#34;]  verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;endpoints\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;secrets\u0026#34;]  verbs: [\u0026#34;create\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;delete\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: cephfs-provisioner subjects:  - kind: ServiceAccount  name: cephfs-provisioner  namespace: cephfs roleRef:  kind: ClusterRole  name: cephfs-provisioner  apiGroup: rbac.authorization.k8s.io  --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata:  name: cephfs-provisioner  namespace: cephfs rules:  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;secrets\u0026#34;]  verbs: [\u0026#34;create\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;delete\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata:  name: cephfs-provisioner  namespace: cephfs roleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: cephfs-provisioner subjects: - kind: ServiceAccount  name: cephfs-provisioner  namespace: cephfs  --- apiVersion: apps/v1 kind: Deployment metadata:  name: cephfs-provisioner  namespace: cephfs spec:  selector:  matchLabels:  app: cephfs-provisioner  replicas: 1  strategy:  type: Recreate  template:  metadata:  labels:  app: cephfs-provisioner  spec:  containers:  - name: cephfs-provisioner  image: \u0026#34;registry.cn-chengdu.aliyuncs.com/ives/cephfs-provisioner:latest\u0026#34;  env:  - name: PROVISIONER_NAME  value: ceph.com/cephfs  command:  - \u0026#34;/usr/local/bin/cephfs-provisioner\u0026#34;  args:  - \u0026#34;-id=cephfs-provisioner-1\u0026#34;  serviceAccount: cephfs-provisioner  执行定义文件  [root@k8s-master1 ceph-cephfs]# kubectl apply -f storage-cephfs-provisioner.yaml  serviceaccount/cephfs-provisioner created clusterrole.rbac.authorization.k8s.io/cephfs-provisioner created clusterrolebinding.rbac.authorization.k8s.io/cephfs-provisioner created role.rbac.authorization.k8s.io/cephfs-provisioner created rolebinding.rbac.authorization.k8s.io/cephfs-provisioner created deployment.apps/cephfs-provisioner created  检查资源状态信息  [root@k8s-master1 ceph-cephfs]# kubectl get pods -n cephfs |grep cephfs cephfs-provisioner-6d76ff6bd5-pcz5m 1/1 Running 0 103s 3、创建一个Storage Class [root@k8s-master1 ceph-cephfs]# vim storageclass-cephfs.yaml  kind: StorageClass apiVersion: storage.k8s.io/v1 metadata:  name: cephfs\t#注意：该名称后面创建pvc时要与之绑定  namespace: cephfs provisioner: ceph.com/cephfs\t#使用外部cephfs parameters:  monitors: 192.168.66.201:6789,192.168.66.202:6789,192.168.66.203:6789\t#ceph集群mon节点地址和端口  adminId: admin  #cephfs认证用户  adminSecretName: ceph-secret-admin  #secret认证资源NAME名称  adminSecretNamespace: \u0026#34;cephfs\u0026#34;\t#secret所在的名称空间名称  检查cephfs storageClass是否创建成功  [root@k8s-master1 ceph-cephfs]#kubectl apply -f storageclass-cephfs.yaml storageclass.storage.k8s.io/cephfs created  [root@k8s-master1 ceph-cephfs]# kubectl get sc 4、创建PVC并测试动态分配PV [root@k8s-master1 ceph-cephfs]# vim cephfs-pvc-test.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: cephfs-pvc-test  namespace: cephfs  annotations:  volume.beta.kubernetes.io/storage-class: \u0026#34;cephfs\u0026#34; #注意：填写storage资源名称，上面创建的storage资源名称为【cephfs】 spec:  accessModes:  - ReadWriteMany  resources:  requests:  storage: 10Gi  检查ceph-fs-test PVC是否成功的被cephfs storageClass分配PV。如果PVC的状态是Bound则代表绑定成功，如果状态是Pending 或 Failed则表示绑定PVC失败，请describe或者查看日志来确定问题出错在哪。  [root@k8s-master1 ceph-cephfs]# kubectl apply -f cephfs-pvc-test.yaml  persistentvolumeclaim/cephfs-pvc-test created  [root@k8s-master1 ceph-cephfs]#kubectl get pvc cephfs-pvc-test -n cephfs  如上图所示，PVC绑定PV成功，并且cephfs storageClass成功的分配一个匹配的PV给PVC  5、创建一个Pod绑定该PVC  验证是否成功的将cephfs绑定到Pod中 **注意：**PVC与Pod资源必须属于同一名称空间下才可以绑定成功 **注意：**如果PVC删除了，那么关联的Pod数据也会被删除  [root@k8s-master1 ceph-cephfs]# vim cephfs-pvc-pod.yaml  kind: Pod apiVersion: v1 metadata:  labels:  app: cephfs-pvc-pod  name: cephfs-pv-pod  namespace: cephfs spec:  containers:  - name: cephfs-pv-busybox  image: busybox  command: [\u0026#34;sleep\u0026#34;, \u0026#34;60000\u0026#34;]  volumeMounts:  - mountPath: \u0026#34;/mnt/cephfs\u0026#34;  name: cephfs-vol  readOnly: false  volumes:  - name: cephfs-vol  persistentVolumeClaim:  claimName: cephfs-pvc-test\t#Pod关联所要绑定的PVC名称  执行创建Pod，并检查是否挂载cephfs卷成功  [root@k8s-master1 ceph-cephfs]# kubectl apply -f cephfs-pvc-pod.yaml  pod/cephfs-pv-pod created  [root@k8s-master1 ceph-cephfs]# kubectl get po -n cephfs NAME READY STATUS RESTARTS AGE cephfs-pv-pod 1/1 Running 0 43s  [root@k8s-master1 ceph-cephfs]# kubectl get po -n cephfs -o wide 6、创建一个Nginx容器 6.1：创建Pod和PVC\n 创建定义文件  #创建一个名称空间，命名为webapp [root@k8s-master1 ceph-cephfs]# kubectl create ns webapp  #pvc与pod所属名称空间都为webapp #创建PVC [root@k8s-master1 ceph-cephfs]# vim cephfs-pvc-nginx.yaml  kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: cephfs-pvc-nginx  namespace: webapp  annotations:  volume.beta.kubernetes.io/storage-class: \u0026#34;cephfs\u0026#34; #指定绑定的storage名称 spec:  accessModes:  - ReadWriteMany  resources:  requests:  storage: 5Gi  #创建Pod [root@k8s-master1 ceph-cephfs]# vim cephfs-deploy-nginx.yaml kind: Deployment apiVersion: apps/v1 metadata:  labels:  k8s-app: nginx-web  name: nginx-web  namespace: webapp spec:  replicas: 1  revisionHistoryLimit: 10  selector:  matchLabels:  k8s-app: nginx-web  template:  metadata:  labels:  k8s-app: nginx-web  namespace: webapp  name: nginx-web  spec:  containers:  - name: nginx-web  image: nginx  imagePullPolicy: IfNotPresent  ports:  - containerPort: 80  name: web  protocol: TCP  volumeMounts:  - name: cephfs  mountPath: /usr/share/nginx/html #挂载至容器的路径  volumes:  - name: cephfs  persistentVolumeClaim:  claimName: cephfs-pvc-nginx #Pod关联所要绑定的PVC名称  执行定义文件  [root@k8s-master1 ceph-cephfs]# kubectl apply -f cephfs-pvc-nginx.yaml  persistentvolumeclaim/cephfs-pvc-nginx created  [root@k8s-master1 ceph-cephfs]#kubectl apply -f cephfs-deploy-nginx.yaml deployment.apps/nginx-web created  检查PVC绑定信息和Pod运行状态  [root@k8s-master1 ceph-cephfs]# kubectl get pvc -n webapp [root@k8s-master1 ceph-cephfs]# kubectl get po -n webapp 6.2：修改文件内容\n[root@k8s-master1 ~]# kubectl get po -n webapp NAME READY STATUS RESTARTS AGE nginx-web-6898d9dd9b-plwwc 1/1 Running 0 3m57s  [root@k8s-master1 ~]# kubectl exec -it nginx-web-6898d9dd9b-plwwc -n webapp -- /bin/bash -c \u0026#39;echo hello cephfs-nginx \u0026gt; /usr/share/nginx/html/index.html\u0026#39; 6.3：访问Nginx Pod\n[root@k8s-master1 ~]# kubectl get po -o wide -n webapp | grep nginx-web-6898d9dd9b-plwwc | awk \u0026#39;{print $6}\u0026#39;  [root@k8s-master1 ~]# curl 10.68.159.135 hello cephfs-nginx 6.4：删除Pod并重构\n[root@k8s-master1 ~]# cd ceph-cephfs/ [root@k8s-master1 ceph-cephfs]# kubectl delete -f cephfs-deploy-nginx.yaml  deployment.apps \u0026#34;nginx-web\u0026#34; deleted  [root@k8s-master1 ceph-cephfs]# kubectl get po -n webapp No resources found in webapp namespace.  [root@k8s-master1 ceph-cephfs]# kubectl get pvc -n webapp  重新创建一个pod并挂载到上面的PVC上，访问Pod检查是否数据还在  [root@k8s-master1 ceph-cephfs]#kubectl apply -f cephfs-deploy-nginx.yaml deployment.apps/nginx-web created  [root@k8s-master1 ceph-cephfs]# kubectl get po -n webapp NAME READY STATUS RESTARTS AGE nginx-web-6898d9dd9b-lhhjf 1/1 Running 0 16s  [root@k8s-master1 ceph-cephfs]# kubectl get po -o wide -n webapp | grep nginx-web-6898d9dd9b-lhhjf | awk \u0026#39;{print $6}\u0026#39;  [root@k8s-master1 ceph-cephfs]# curl 10.68.159.136 hello cephfs-nginx ","permalink":"https://iblog.zone/archives/k8s%E4%B8%AD%E4%BD%BF%E7%94%A8ceph%E9%9B%86%E7%BE%A4%E5%8A%A8%E6%80%81%E5%92%8C%E9%9D%99%E6%80%81%E6%96%B9%E5%BC%8F%E6%8C%82%E8%BD%BDpv%E4%B8%8Epvc/","summary":"一、CephFS介绍  Ceph File System (CephFS) 是与 POSIX 标准兼容的文件系统, 能够提供对 Ceph 存储集群上的文件访问. Jewel 版本 (10.2.0) 是第一个包含稳定 CephFS 的 Ceph 版本. CephFS 需要至少一个元数据服务器 (Metadata Server – MDS) daemon (ceph-mds) 运行, MDS daemon 管理着与存储在 CephFS 上的文件相关的元数据, 并且协调着对 Ceph 存储系统的访问。\n对象存储的成本比起普通的文件存储还是较高，需要购买专门的对象存储软件以及大容量硬盘。如果对数据量要求不是海量，只是为了做文件共享的时候，直接用文件存储的形式好了，性价比高。\n二、使用CephFS类型Volume直接挂载  cephfs卷允许将现有的cephfs卷挂载到你的Pod中，与emptyDir类型不同的是，emptyDir会在删除Pod时把数据清除掉，而cephfs卷的数据会被保留下来,仅仅是被卸载，并且cephfs可以被多个设备进行读写。\n1、安装 Ceph 客户端 在部署 kubernetes 之前我们就已经有了 Ceph 集群，因此我们可以直接拿来用。但是 kubernetes 的所有节点（尤其是 master 节点）上依然需要安装 ceph 客户端。\nyum install -y ceph-common  还需要将 ceph 的配置文件 ceph.conf 放在所有节点的 /etc/ceph 目录下  2、创建Ceph secret  注意： ceph_secret.","title":"K8S中使用Ceph集群动态和静态方式挂载PV与PVC"},{"content":"一、CephFS介绍  Ceph File System (CephFS) 是与 POSIX 标准兼容的文件系统, 能够提供对 Ceph 存储集群上的文件访问. Jewel 版本 (10.2.0) 是第一个包含稳定 CephFS 的 Ceph 版本. CephFS 需要至少一个元数据服务器 (Metadata Server – MDS) daemon (ceph-mds) 运行, MDS daemon 管理着与存储在 CephFS 上的文件相关的元数据, 并且协调着对 Ceph 存储系统的访问。\n 注意：你集群里必须有MDS，不然无法进行下面的操作  二、CephFS创建   要使用 CephFS， 至少就需要一个 metadata server 进程；在admin节点通过以下命令进行创建  [root@ceph-admin ~]# su - cephu  [cephu@ceph-admin ~]$ cd ~/my-cluster/ [cephu@ceph-admin my-cluster]$ ceph-deploy mds create ceph-node2 #无报错则创建完成 三、CephFS部署  1、部署流程  在一个 Mon 节点上创建 Ceph 文件系统. 若使用 CephX 认证,需要创建一个访问 CephFS 的客户端 挂载 CephFS 到一个专用的节点，以 kernel client 形式挂载 CephFS  2、执行操作 2.1：创建文件系统\n Mon节点我们三台Node上都创建了，我们在任一Mon节点进行创建；这里我在ceph-node1节点创建 CephFS 需要两个 Pools，cephfs-data 和 cephfs-metadata, 分别存储文件数据和文件元数据  [root@ceph-node1 ~]# su - cephu [cephu@ceph-node1 ~]$ sudo ceph osd pool create cephfs-data 128 128 pool \u0026#39;cephfs-data\u0026#39; created  [cephu@ceph-node1 ~]$ sudo ceph osd pool create cephfs-metadata 64 64 pool \u0026#39;cephfs-metadata\u0026#39; created 2.2：创建CephFS\n 创建一个CephFS，名称为cephfs，还是在Mon节点操作  [cephu@ceph-node1 ~]$ sudo ceph fs new cephfs cephfs-metadata cephfs-data  new fs with metadata pool 2 and data pool 1 2.3：检查状态\n[cephu@ceph-node1 ~]$ sudo ceph fs status cephfs 2.4：创建用户\n 在Mon节点创建一个用户，用于访问CephFS；且这个用户有读写的权限  [cephu@ceph-node1 ~]$ sudo ceph auth get-or-create client.cephfs mon \u0026#39;allow r\u0026#39; mds \u0026#39;allow rw\u0026#39; osd \u0026#39;allow rw pool=cephfs-data, allow rw pool=cephfs-metadata\u0026#39;  [client.cephfs] \tkey = AQDnTxVf/k2sFhAANbw0QbPuCa7dLCJBDQbr8A== 2.5：检查Key\n 检查上面步骤生成的Key是否生效  [cephu@ceph-node1 ~]$ sudo ceph auth get client.cephfs  exported keyring for client.cephfs [client.cephfs] \tkey = AQDnTxVf/k2sFhAANbw0QbPuCa7dLCJBDQbr8A== \tcaps mds = \u0026#34;allow rw\u0026#34; \tcaps mon = \u0026#34;allow r\u0026#34; \tcaps osd = \u0026#34;allow rw pool=cephfs-data, allow rw pool=cephfs-metadata\u0026#34; 2.6：检查CephFS和Mds状态\n[cephu@ceph-node1 ~]$ sudo ceph mds stat cephfs-1/1/1 up {0=ceph-node2=up:active}  [cephu@ceph-node1 ~]$ sudo ceph fs ls name: cephfs, metadata pool: cephfs-metadata, data pools: [cephfs-data ]  [cephu@ceph-node1 ~]$ sudo ceph fs status 四、CephFS挂载   Mon默然端口：tcp: 6789 这里我们以 kernel client 的方式进行挂载，在客户端服务器进行挂载  1、创建挂载点 [root@ceph-client ~]# mkdir /cephfs 2、挂载目录 [root@ceph-client ~]# mount -t ceph 192.168.66.201:6789,192.168.66.202:6789,192.168.66.203:6789:/ /cephfs/ -o name=cephfs,secret=AQA+bhVfgv2nFRAAE1oYVGUOdjtqfsXlQS2nEw==  参数说明  name：为使用cephfs存储的用户 secret：为上面在mon节点创建的cephfs对应的用户秘钥    3、自动挂载 [root@ceph-client ~]# echo \u0026#34;192.168.66.201:6789,192.168.66.202:6789,192.168.66.203:6789:/ /cephfs ceph name=cephfs,secretfile=/etc/ceph/cephfs.key,_netdev,noatime 0 0\u0026#34; | sudo tee -a /etc/fstab 4、验证挂载 [root@ceph-client ~]# stat -f /cephfs ","permalink":"https://iblog.zone/archives/ceph%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9Fcephfs%E9%83%A8%E7%BD%B2/","summary":"一、CephFS介绍  Ceph File System (CephFS) 是与 POSIX 标准兼容的文件系统, 能够提供对 Ceph 存储集群上的文件访问. Jewel 版本 (10.2.0) 是第一个包含稳定 CephFS 的 Ceph 版本. CephFS 需要至少一个元数据服务器 (Metadata Server – MDS) daemon (ceph-mds) 运行, MDS daemon 管理着与存储在 CephFS 上的文件相关的元数据, 并且协调着对 Ceph 存储系统的访问。\n 注意：你集群里必须有MDS，不然无法进行下面的操作  二、CephFS创建   要使用 CephFS， 至少就需要一个 metadata server 进程；在admin节点通过以下命令进行创建  [root@ceph-admin ~]# su - cephu  [cephu@ceph-admin ~]$ cd ~/my-cluster/ [cephu@ceph-admin my-cluster]$ ceph-deploy mds create ceph-node2 #无报错则创建完成 三、CephFS部署  1、部署流程  在一个 Mon 节点上创建 Ceph 文件系统.","title":"Ceph文件系统—CephFS部署"},{"content":"一、CEPH 简介 不管你是想为云平台提供Ceph 对象存储或Ceph 块设备，还是想部署一个Ceph 文件系统或者把 Ceph 作为他用，所有Ceph 存储集群的部署都始于部署一个个Ceph 节点、网络和 Ceph 存储集群。 Ceph 存储集群至少需要一个 Ceph Monitor 和两个 OSD 守护进程。而运行 Ceph 文件系统客户端时，则必须要有元数据服务器（ Metadata Server ）。\n **Ceph OSDs：**Ceph OSD 守护进程（ Ceph OSD ）的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD 守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当 Ceph 存储集群设定为有2个副本时，至少需要2个 OSD 守护进程，集群才能达到 active+clean 状态（ Ceph 默认有3个副本，但你可以调整副本数）。 **Monitors：**Ceph Monitor维护着展示集群状态的各种图表，包括监视器图、 OSD 图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors 、 OSD 和 PG上的每一次状态变更的历史信息（称为 epoch ）。 **MDSs：**Ceph 元数据服务器（ MDS ）为 Ceph 文件系统存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。元数据服务器使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令。  Ceph 把客户端数据保存为存储池内的对象。通过使用 CRUSH 算法， Ceph 可以计算出哪个归置组（PG）应该持有指定的对象(Object)，然后进一步计算出哪个 OSD 守护进程持有该归置组。 CRUSH 算法使得 Ceph 存储集群能够动态地伸缩、再均衡和修复。\n 官网文档：http://docs.ceph.org.cn/start/intro/  二、环境描述  硬件推荐：http://docs.ceph.org.cn/start/hardware-recommendations/ 本文测试环境资源规格如下     主机名 IP地址 角色     Ceph-admin 192.168.66.200 Admin，ceph-deploy   Ceph-node1 192.168.66.201 Mon，Mgr，osd   Ceph-node2 192.168.66.202 Mon，Osd   Ceph-node3 192.168.66.203 Mon，Osd   Ceph-Client 192.168.66.204 客户端      **注意：**给三台Node节点单独挂载一块磁盘，大小必须大于5G；生产环境下磁盘建议1TB以上大小的容量，CPU推荐16C以上，内存24G以上，且官方建议ceph集群部署到物理机上；注意磁盘添加后不需要我们手动进行格式化分区等操作，后面通过ceph工具自动创建。\n  **mon：**Monitors, 节点映射管理, 身份验证管理, 需要达到冗余和高可用至少需要3个节点\n  **osd：**object storage daemon, 对象存储服务, 需要达到冗余和高可用至少需要3个节点\n  **mgr：**Manager, 用于跟踪运行指标和集群状态, 性能.\n  **mds：**Metadata Serve, 提供cephfs的元数据存储\n  三、环境准备   第二步中我们将机器准备好，并且三台node节点上除了系统盘之外，单独挂载了一块20G大小的数据盘\n  **注意：**环境准备阶段，除了node节点需要单独挂载磁盘之外，其余节点不需要；所有ceph集群节点都需要执行以下准备阶段的所有步骤，无特殊提示的话则所有节点均需要执行相应的命令。\n  1、安装常用命令  所有集群节点安装我们会经常使用到的一些软件依赖包和命令程序  yum -y install vim lrzsz wget curl rsync git gcc make lsof pcre pcre-devel zlib zlib-devel openssl openssl-devel dos2unix sysstat iotop net-tools httpd-tools 2、更改主机名  所有节点按照第二步中的事先规划好的主机名进行更改，并实现集群主机名之间互相解析  hostnamectl set-hostname ceph-admin hostnamectl set-hostname ceph-node1 hostnamectl set-hostname ceph-node2 hostnamectl set-hostname ceph-node3 hostnamectl set-hostname ceph-client  #所有节点都需要添加到hosts文件中，实现主机名解析 ~]# vim /etc/hosts 192.168.66.200 ceph-admin 192.168.66.201\tceph-node1 192.168.66.202\tceph-node2 192.168.66.203\tceph-node3 192.168.66.204\tceph-client  #配置好后，我们在任意一个节点去ping其他节点的主机名，看是否解析成功 [root@ceph-client ~]# ping ceph-admin PING ceph-admin (192.168.66.200) 56(84) bytes of data. 64 bytes from ceph-admin (192.168.66.200): icmp_seq=1 ttl=64 time=0.211 ms 64 bytes from ceph-admin (192.168.66.200): icmp_seq=2 ttl=64 time=0.854 ms 64 bytes from ceph-admin (192.168.66.200): icmp_seq=3 ttl=64 time=0.806 ms 64 bytes from ceph-admin (192.168.66.200): icmp_seq=4 ttl=64 time=0.985 ms 3、关闭防火墙  关闭防火墙和Selinux  systemctl stop firewalld systemctl disable firewalld  sed -i \u0026#39;s/^SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/sysconfig/selinux sed -i \u0026#39;s/^SELINUX=enforcing/SELINUX=disabled/g\u0026#39; /etc/selinux/config setenforce 0 getenforce 4、安装时间服务  在所有节点执行该操作  yum install ntp ntpdate ntp-doc systemctl start ntpd systemctl enable ntpd 5、创建Ceph用户  官网文档中已经说明，运行ceph必须使用普通用户，并且需要保证该用户有无密码使用 sudo 的权限 各Ceph节点均需创建该用户  useradd cephu echo 123456 | passwd --stdin cephu  echo \u0026#34;cephu ALL=(ALL) NOPASSWD:ALL\u0026#34; | tee /etc/sudoers.d/ceph chmod 0440 /etc/sudoers.d/ceph 6、配置免密认证  实现cephu用户ssh免密登入各ceph节点，在admin节点进行操作  [root@ceph-admin ~]# su - cephu [cephu@ceph-admin ~]$ ssh-keygen -t rsa [cephu@ceph-admin ~]$ ssh-copy-id cephu@ceph-node1 [cephu@ceph-admin ~]$ ssh-copy-id cephu@ceph-node2 [cephu@ceph-admin ~]$ ssh-copy-id cephu@ceph-node3 [cephu@ceph-admin ~]$ ssh-copy-id cephu@ceph-client 7、添加配置文件  在admin节点用登入root用户，并在~/.ssh目录下创建config文件，并将下面的配置信息添加进去  [root@ceph-admin ~]# mkdir ~/.ssh [root@ceph-admin ~]# vim ~/.ssh/config Host ceph-node1 Hostname ceph-node1 User cephu  Host ceph-node2 Hostname ceph-node2 User cephu  Host ceph-node3 Hostname ceph-node3 User cephu 8、添加下载源  在admin节点配置ceph源，并将ce文章来源(Source)：浅时光博客ph源拷贝给所有node节点和客户端节点  [root@ceph-admin ~]# vim /etc/yum.repos.d/ceph.repo [Ceph] name=Ceph packages for $basearch baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/$basearch enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc  [Ceph-noarch] name=Ceph noarch packages baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/noarch/ enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc  [ceph-source] name=Ceph source packages baseurl=https://mirrors.aliyun.com/ceph/rpm-luminous/el7/SRPMS/ enabled=1 gpgcheck=1 type=rpm-md gpgkey=https://download.ceph.com/keys/release.asc   [root@ceph-admin ~]# scp /etc/yum.repos.d/ceph.repo root@192.168.66.201:/etc/yum.repos.d/ [root@ceph-admin ~]# scp /etc/yum.repos.d/ceph.repo root@192.168.66.202:/etc/yum.repos.d/ [root@ceph-admin ~]# scp /etc/yum.repos.d/ceph.repo root@192.168.66.203:/etc/yum.repos.d/ [root@ceph-admin ~]# scp /etc/yum.repos.d/ceph.repo root@192.168.66.204:/etc/yum.repos.d/   #所有节点执行创建缓存 yum clean all yum makecache  在admin节点安装ceph-deploy  [root@ceph-admin ~]# yum -y install ceph-deploy 四、部署ceph集群  注意：如果没有特殊说明，那么接下来的操作则在admin节点上进行操作  1、创建操作目录 [root@ceph-admin ~]# su - cephu [cephu@ceph-admin ~]$ mkdir my-cluster #之后所有的ceph-deploy操作必须在该目录下执行 2、创建ceph集群 2.1：安装distribute包  先下载安装python的distribute包，不然后面部署ceph集群会报错 下载地址：https://pypi.org/project/distribute/#modal-close  [cephu@ceph-admin ~]$ unzip distribute-0.7.3.zip [cephu@ceph-admin ~]$ cd distribute-0.7.3 [cephu@ceph-admin distribute-0.7.3]$ sudo python setup.py install 2.2：进行创建集群  **注意：**new后面跟的是各个节点的主机名,且可以文章来源(Source)：浅时光博客实现admin节点与各node节点主机名之间互相解析  [cephu@ceph-admin ~]$ cd ~/my-cluster/ [cephu@ceph-admin my-cluster]$ ceph-deploy new ceph-node1 ceph-node2 ceph-node3  没有报错表示创建成功  [cephu@ceph-admin my-cluster]$ ls ceph.conf ceph-deploy-ceph.log ceph.mon.keyring  问题：创建集群时提示缺少pkg_resources模块的问题解决  [cephu@ceph-admin my-cluster]$ sudo pip install --upgrade setuptools 2.3：安装luminous  三台node节点下载epel源，注意我们已经在环境准备阶段配置了ceph源，所以这里只需要安装epel源就可以了  yum -y install epel*  分别在三台node节点执行以下命令进行安装软件，注意切换为ceph普通用户  su - cephu  通过以下命令查看当前的最新版本  $ sudo yum --showduplicates list ceph | expand  通过以下命令进行安装ceph  $ sudo yum install ceph ceph-radosgw 2.4：测试安装情况  分别在3台node节点执行下面的命令，来确认我们是否安装成功  $ ceph --version ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) luminous (stable) 3、初始化mon  在admin节点用cephu这个普通用户执行  [cephu@ceph-admin ~]$ cd ~/my-cluster/ [cephu@ceph-admin my-cluster]$ ceph-deploy mon create-initial #没有ERROR报错则安装成功  **注意：**如果之前ceph.conf配置文件中已经存在了内容，则需要添加--overwrite-conf参数进行覆盖，命令如下：  [cephu@ceph-admin my-cluster]$ ceph-deploy --overwrite-conf mon create-initial  授予3个node节点使用命令免用户名权限  [cephu@ceph-admin my-cluster]$ ceph-deploy admin ceph-node1 ceph-node2 ceph-node3 #没有ERROR报错则安装成功 4、安装ceph-mgr  安装在node1节点上，执行安装命令在admin节点上；为安装dashboard做准备  [cephu@ceph-admin my-cluster]$ ceph-deploy mgr create ceph-node1 #没有ERROR报错则安装成功 5、添加OSD  分别为3台node节点添加OSD，注意磁盘名称，我这里为sdb，可通过命令lsblk或者fdisk命令查看磁盘 官网文档：http://docs.ceph.org.cn/rados/deployment/ceph-deploy-osd/ **注意：**我这里只创建data盘，db和wal我这里没单独指定，如果需要单独指定则需要添加参数 --block-db /dev/sdc --block-wal /dev/sdd  #用 create 命令一次完成准备 OSD 、部署到 OSD 节点、并激活它 [cephu@ceph-admin my-cluster]$ ceph-deploy osd create ceph-node1 --data /dev/sdb [cephu@ceph-admin my-cluster]$ ceph-deploy osd create ceph-node2 --data /dev/sdb [cephu@ceph-admin my-cluster]$ ceph-deploy osd create ceph-node3 --data /dev/sdb  通过lsblk -f命令可查看到磁盘分区情况  [cephu@ceph-admin my-cluster]$ ssh ceph-node1 lsblk -f  通过以下命令查看集群状态  [cephu@ceph-admin my-cluster]$ ssh ceph-node1 sudo ceph -s 6、部署Dashboard  在node1节点上部署dashboard  6.1：创建管理域秘钥 [root@ceph-node1 ~]# su - cephu [cephu@ceph-node1 ~]$ sudo ceph auth get-or-create mgr.ceph-node1 mon \u0026#39;allow profile mgr\u0026#39; osd \u0026#39;allow *\u0026#39; mds \u0026#39;allow *\u0026#39;  [mgr.ceph-node1] \tkey = AQDmiQhfDrBDEhAAnfwRTMv5clhbSEuetlrwyw== 6.2：开启mgr管理域 [cephu@ceph-node1 ~]$ sudo ceph-mgr -i ceph-node1 6.3：检查mgr状态  确保mgr的状态为active  [cephu@ceph-node1 ~]$ sudo ceph status 6.4：打开dashboard模块 [cephu@ceph-node1 ~]$ sudo ceph mgr module enable dashboard 6.5：绑定模板mgr节点 [cephu@ceph-node1 ~]$ sudo ceph config-key set \u0026#39;mgr/dashboard/ceph-node1/server_addr\u0026#39; \u0026#39;192.168.66.201\u0026#39; set mgr/dashboard/ceph-node1/server_addr  [cephu@ceph-node1 ~]$ ss -tnlp|grep 7000 LISTEN 0 5 192.168.66.201:7000 *:* 6.6：浏览器访问  http://mgr地址:7000  五、配置客户端  创建客户端使用rdb(块存储)；创建块设备之前需要创建存储池；执行创建存储的命令需要在mon节点上执行，也就是node1节点  1、创建存储池 [root@ceph-node1 ~]# su – cephu [cephu@ceph-node1 ~]$ sudo ceph osd pool create rbd 128 128 pool \u0026#39;rbd\u0026#39; created  参数说明：  128表示如果创建的pool少于5个OSD，那么就是128个pg，5-10为512；10-50为4096    2、初始化存储池 [cephu@ceph-node1 ~]$ sudo rbd pool init rbd 3、准备客户端  这里的客户端就是我们规划的那台，确保客户端是可以跟admin节点实现主机名互通的  3.1：升级内核 官方推荐的客户端服务器内核版本：\n 4.1.4 or later 3.16.3 or later (rbd deadlock regression in 3.16.[0-2]) NOT v3.15.* (rbd deadlock regression) 3.14.* 升级内核版本到4.x以上，接下来在客户端机器上进行操作  [root@ceph-client ~]# uname -r 3.10.0-957.el7.x86_64  导入key  [root@ceph-client ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org 3.2：安装elrepo源 [root@ceph-client ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-2.el7.elrepo.noarch.rpm  查看可用的系统内核包  [root@ceph-client ~]# yum --disablerepo=\u0026#34;*\u0026#34; --enablerepo=\u0026#34;elrepo-kernel\u0026#34; list available 3.3：安装最新内核 [root@ceph-client ~]# yum --enablerepo=elrepo-kernel install kernel-ml-devel kernel-ml -y   如果yum安装很慢的话，通过rpm包的方式进行安装\n  获取rpm包：https://elrepo.org/linux/kernel/el7/x86_64/RPMS/\n  内核选择：\n kernel-lt（lt=long-term）长期有效 kernel-ml（ml=mainline）主流版本    安装\n  [root@ceph-client ~]# rpm -ivh kernel-ml-* 3.4：修改内核启动顺序  查看内核默认启动顺序  [root@ceph-client ~]# awk -F\\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print $2}\u0026#39; /etc/grub2.cfg  CentOS Linux (5.7.8-1.el7.elrepo.x86_64) 7 (Core) CentOS Linux (3.10.0-957.el7.x86_64) 7 (Core) CentOS Linux (0-rescue-7ba72ac2cf764cf39417d13528419374) 7 (Core)  修改启动顺序  [root@ceph-client ~]# grub2-set-default 0  重启服务器  [root@ceph-client ~]# reboot  再次检测系统内核版本  [root@ceph-client ~]# uname -a Linux ceph-client 5.7.8-1.el7.elrepo.x86_64 #1 SMP Tue Jul 7 18:43:16 EDT 2020 x86_64 x86_64 x86_64 GNU/Linux  删除旧的内核  [root@ceph-client ~]# yum remove kernel -y 删除:  kernel.x86_64 0:3.10.0-957.el7 完毕！ 4、客户端安装ceph 4.1：环境检查  先下载安装python的distribute包，不然部署ceph集群会报错；在client节点操作 下载地址：https://pypi.org/project/distribute/#modal-close  [root@ceph-client ~]# su - cephu [cephu@ceph-client ~]$ wget https://files.pythonhosted.org/packages/5f/ad/1fde06877a8d7d5c9b60eff7de2d452f639916ae1d48f0b8f97bf97e570a/distribute-0.7.3.zip [cephu@ceph-client ~]$ unzip distribute-0.7.3.zip [cephu@ceph-client ~]$ cd distribute-0.7.3 [cephu@ceph-client distribute-0.7.3]$ sudo python setup.py install [cephu@ceph-client distribute-0.7.3]$ sudo yum -y install python-setuptools [cephu@ceph-client distribute-0.7.3]$ sudo yum -y install epel* 4.2：安装ceph  确保已经在环境准备阶段时客户端也配置了ceph源  [root@ceph-client ~]# su - cephu  [cephu@ceph-client ~]$ sudo yum install ceph ceph-radosgw [cephu@ceph-client ~]$ ceph --version ceph version 12.2.13 (584a20eb0237c657dc0567da126be145106aa47e) luminous (stable) 4.3：拷贝秘钥  在admin【管理节点】节点上，用 ceph-deploy 把 Ceph 配置文件和 ceph.client.admin.keyring 拷贝到 ceph-client 。  [cephu@ceph-admin ~]$ cd my-cluster/ [cephu@ceph-admin my-cluster]$ ceph-deploy admin ceph-client #ceph-deploy 工具会把密钥环复制到 /etc/ceph 目录，要确保此密钥环文件有读权限（如 sudo chmod +r /etc/ceph/ceph.client.admin.keyring ）  修改client节点该文件的权限  [cephu@ceph-client ~]$ sudo chmod +r /etc/ceph/ceph.client.admin.keyring 4.4：修改配置  修改client节点下的ceph配置文件，为了解决映射镜像时出错问题。  [cephu@ceph-client ~]$ sudo vim /etc/ceph/ceph.conf #最后添加 rbd_default_features = 1 4.5：配置块设备  在 ceph-client 节点上创建一个块设备 image，默认单位为M 语法：rbd create foo –size 4096 [-m {mon-IP}] [-k /path/to/ceph.client.admin.keyring]  [cephu@ceph-client ~]$ rbd create foo --size 4096  在 ceph-client 节点上，把 image 映射为块设备。 语法：sudo rbd map foo –name client.admin [-m {mon-IP}] [-k /path/to/ceph.client.admin.keyring]  [cephu@ceph-client ~]$ sudo rbd map foo --name client.admin /dev/rbd0  在 ceph-client 节点上，创建文件系统后就可以使用块设备了。  [cephu@ceph-client ~]$ sudo mkfs.ext4 -m0 /dev/rbd/rbd/foo  注意：此命令可能耗时较长。  mke2fs 1.42.9 (28-Dec-2013) Discarding device blocks: 完成 文件系统标签= OS type: Linux 块大小=4096 (log=2) 分块大小=4096 (log=2) Stride=16 blocks, Stripe width=16 blocks 262144 inodes, 1048576 blocks 0 blocks (0.00%) reserved for the super user 第一个数据块=0 Maximum filesystem blocks=1073741824 32 block groups 32768 blocks per group, 32768 fragments per group 8192 inodes per group Superblock backups stored on blocks: \t32768, 98304, 163840, 229376, 294912, 819200, 884736  Allocating group tables: 完成 正在写入inode表: 完成 Creating journal (32768 blocks): 完成 Writing superblocks and filesystem accounting information: 完成  在 ceph-client 节点上挂载此文件系统。  [cephu@ceph-client ~]$ sudo mkdir /mnt/ceph-block-device #创建挂载点 [cephu@ceph-client ~]$ sudo mount /dev/rbd/rbd/foo /mnt/ceph-block-device #挂载 [cephu@ceph-client ~]$ cd /mnt/ceph-block-device [cephu@ceph-client ceph-block-device]$ sudo touch dqzboy.txt [cephu@ceph-client ceph-block-device]$ ls dqzboy.txt lost+found ","permalink":"https://iblog.zone/archives/ceph%E5%AD%98%E5%82%A8%E9%9B%86%E7%BE%A4%E9%83%A8%E7%BD%B2/","summary":"一、CEPH 简介 不管你是想为云平台提供Ceph 对象存储或Ceph 块设备，还是想部署一个Ceph 文件系统或者把 Ceph 作为他用，所有Ceph 存储集群的部署都始于部署一个个Ceph 节点、网络和 Ceph 存储集群。 Ceph 存储集群至少需要一个 Ceph Monitor 和两个 OSD 守护进程。而运行 Ceph 文件系统客户端时，则必须要有元数据服务器（ Metadata Server ）。\n **Ceph OSDs：**Ceph OSD 守护进程（ Ceph OSD ）的功能是存储数据，处理数据的复制、恢复、回填、再均衡，并通过检查其他OSD 守护进程的心跳来向 Ceph Monitors 提供一些监控信息。当 Ceph 存储集群设定为有2个副本时，至少需要2个 OSD 守护进程，集群才能达到 active+clean 状态（ Ceph 默认有3个副本，但你可以调整副本数）。 **Monitors：**Ceph Monitor维护着展示集群状态的各种图表，包括监视器图、 OSD 图、归置组（ PG ）图、和 CRUSH 图。 Ceph 保存着发生在Monitors 、 OSD 和 PG上的每一次状态变更的历史信息（称为 epoch ）。 **MDSs：**Ceph 元数据服务器（ MDS ）为 Ceph 文件系统存储元数据（也就是说，Ceph 块设备和 Ceph 对象存储不使用MDS ）。元数据服务器使得 POSIX 文件系统的用户们，可以在不对 Ceph 存储集群造成负担的前提下，执行诸如 ls、find 等基本命令。  Ceph 把客户端数据保存为存储池内的对象。通过使用 CRUSH 算法， Ceph 可以计算出哪个归置组（PG）应该持有指定的对象(Object)，然后进一步计算出哪个 OSD 守护进程持有该归置组。 CRUSH 算法使得 Ceph 存储集群能够动态地伸缩、再均衡和修复。","title":"Ceph存储集群部署"},{"content":"一、概述 分布式文件系统是分布式领域的一个基础应用，其中最著名的毫无疑问是 HDFS/GFS。如今该领域已经趋向于成熟，但了解它的设计要点和思想，对我们将来面临类似场景/问题时，具有借鉴意义。\n并且，分布式文件系统并非只有 HDFS/GFS 这一种形态，在它之外，还有其他形态各异、各有千秋的产品形态，对它们的了解，也对扩展我们的视野有所俾益。\n本文试图分析和思考，在分布式文件系统领域，我们要解决哪些问题、有些什么样的方案、以及各自的选择依据。\n二、过去的样子 在几十年以前，分布式文件系统就已经出现了，以 Sun 在 1984 年开发的“Network File System (NFS)”为代表，那时候解决的主要问题，是网络形态的磁盘，把磁盘从主机中独立出来。\n这样不仅可以获得更大的容量，而且还可以随时切换主机，还可以实现数据共享、备份、容灾等，因为数据是电脑中最重要的资产。\nNFS 的数据通信图如下：\n部署在主机上的客户端，通过 TCP/IP 协议把文件命令转发到远程文件 Server 上执行，整个过程对主机用户透明。\n到了互联网时代，流量和数据快速增长，分布式文件系统所要解决的主要场景变了，开始需要非常大的磁盘空间，这在磁盘体系上垂直扩容是无法达到的，必须要分布式，同时分布式架构下，主机都是可靠性不是非常好的普通服务器，因此容错、高可用、持久化、伸缩性等指标，就成为必须要考量的特性。\n三、对分布式文件系统的要求 对一个分布式文件系统而言，有一些特性是必须要满足的，否则就无法有竞争力。主要如下：\n 应该符合 POSIX 的文件接口标准，使该系统易于使用，同时对于用户的遗留系统也无需改造； 对用户透明，能够像使用本地文件系统那样直接使用； 持久化，保证数据不会丢失； 具有伸缩性，当数据压力逐渐增长时能顺利扩容； 具有可靠的安全机制，保证数据安全； 数据一致性，只要文件内容不发生变化，什么时候去读，得到的内容应该都是一样的。  除此之外，还有些特性是分布式加分项，具体如下：\n 支持的空间越大越好； 支持的并发访问请求越多越好； 性能越快越好； 硬件资源的利用率越高越合理，就越好。  四、架构模型 从业务模型和逻辑架构上，分布式文件系统需要这几类组件：\n 存储组件：负责存储文件数据，它要保证文件的持久化、副本间数据一致、数据块的分配 / 合并等等； 管理组件：负责 meta 信息，即文件数据的元信息，包括文件存放在哪台服务器上、文件大小、权限等，除此之外，还要负责对存储组件的管理，包括存储组件所在的服务器是否正常存活、是否需要数据迁移等； 接口组件：提供接口服务给应用使用，形态包括 SDK(Java/C/C++ 等)、CLI 命令行终端、以及支持 FUSE 挂载机制。  而在部署架构上，有着“中心化”和“无中心化”两种路线分歧，即是否把“管理组件”作为分布式文件系统的中心管理节点。两种路线都有很优秀的产品，下面分别介绍它们的区别。\n1、有中心节点 以 GFS 为代表，中心节点负责文件定位、维护文件 meta 信息、故障检测、数据迁移等管理控制的职能，下图是 GFS 的架构图：\n该图中GFS master 即为 GFS 的中心节点，GF chunkserver 为 GFS 的存储节点。其操作路径如下：\n Client 向中心节点请求“查询某个文件的某部分数据”； 中心节点返回文件所在的位置 (哪台 chunkserver 上的哪个文件) 以及字节区间信息； Client 根据中心节点返回的信息，向对应的 chunk server 直接发送数据读取的请求； chunk server 返回数据。  在这种方案里，一般中心节点并不参与真正的数据读写，而是将文件 meta 信息返回给 Client 之后，即由 Client 与数据节点直接通信。其主要目的是降低中心节点的负载，防止其成为瓶颈。这种有中心节点的方案，在各种存储类系统中得到了广泛应用，因为中心节点易控制、功能强大。\n2、无中心节点 以ceph为代表，每个节点都是自治的、自管理的，整个 ceph 集群只包含一类节点，如下图 (最下层红色的 RADOS 就是 ceph 定义的“同时包含 meta 数据和文件数据”的节点)。\n无中心化的最大优点是解决了中心节点自身的瓶颈，这也就是 ceph 号称可以无限向上扩容的原因。但由 Client 直接和 Server 通信，那么 Client 必须要知道，当对某个文件进行操作时，它该访问集群中的哪个节点。ceph 提供了一个很强大的原创算法来解决这个问题——CRUSH 算法。\n五、持久化 对于文件系统来说，持久化是根本，只要 Client 收到了 Server 保存成功的回应之后，数据就不应该丢失。这主要是通过多副本的方式来解决，但在分布式环境下，多副本有这几个问题要面对。\n 如何保证每个副本的数据是一致的? 如何分散副本，以使灾难发生时，不至于所有副本都被损坏? 怎么检测被损坏或数据过期的副本，以及如何处理? 该返回哪个副本给 Client?  1、如何保证每个副本的数据是一致的？\n同步写入是保证副本数据一致的最直接的办法。当 Client 写入一个文件的时候，Server 会等待所有副本都被成功写入，再返回给 Client。\n这种方式简单、有保障，唯一的缺陷就是性能会受到影响。假设有 3 个副本，如果每个副本需要N秒，则可能会阻塞 Client 3N 秒的时间，有几种方式，可以对其进行优化：\n 并行写：由一个副本作为主副本，并行发送数据给其他副本； 链式写：几个副本组成一个链 (chain)，并不是等内容都接受到了再往后传播，而是像流一样，边接收上游传递过来的数据，一边传递给下游。  还有一种方式是采用 CAP 中所说的 W+R\u0026gt;N 的方式，比如 3 副本 (N=3) 的情况，W＝2，R＝2，即成功写入 2 个就认为成功，读的时候也要从 2 个副本中读。这种方式通过牺牲一定的读成本，来降低写成本，同时增加写入的可用性。这种方式在分布式文件系统中用地比较少。\n2、如何分散副本，以使灾难发生时，不至于所有副本都被损坏？\n这主要避免的是某机房或某城市发生自然环境故障的情况，所以有一个副本应该分配地比较远。它的副作用是会带来这个副本的写入性能可能会有一定的下降，因为它离 Client 最远。所以如果在物理条件上无法保证够用的网络带宽的话，则读写副本的策略上需要做一定考虑。\n可以参考同步写入只写 2 副本、较远副本异步写入的方式，同时为了保证一致性，读取的时候又要注意一些，避免读取到异步写入副本的过时数据。\n3、怎么检测被损坏或数据过期的副本，以及如何处理？\n如果有中心节点，则数据节点定期和中心节点进行通信，汇报自己的数据块的相关信息，中心节点将其与自己维护的信息进行对比。如果某个数据块的 checksum 不对，则表明该数据块被损坏了；如果某个数据块的 version 不对，则表明该数据块过期了。\n如果没有中心节点，以 ceph 为例，它在自己的节点集群中维护了一个比较小的 monitor 集群，数据节点向这个 monitor 集群汇报自己的情况，由其来判定是否被损坏或过期。\n当发现被损坏或过期副本，将它从 meta 信息中移除，再重新创建一份新的副本就好了，移除的副本在随后的回收机制中会被收回。\n4、该返回哪个副本给 Client？\n这里的策略就比较多了，比如 round-robin、速度最快的节点、成功率最高的节点、CPU 资源最空闲的节点、甚至就固定选第一个作为主节点，也可以选择离自己最近的一个，这样对整体的操作完成时间会有一定节约。\n六、伸缩性 1、存储节点的伸缩 当在集群中加入一台新的存储节点，则它主动向中心节点注册，提供自己的信息，当后续有创建文件或者给已有文件增加数据块的时候，中心节点就可以分配到这台新节点了，比较简单。但有一些问题需要考虑。\n 如何尽量使各存储节点的负载相对均衡? 怎样保证新加入的节点，不会因短期负载压力过大而崩塌? 如果需要数据迁移，那如何使其对业务层透明?  1）如何尽量使各存储节点的负载相对均衡？\n首先要有评价存储节点负载的指标。有多种方式，可以从磁盘空间使用率考虑，也可以从磁盘使用率 +CPU 使用情况 + 网络流量情况等做综合判断。一般来说，磁盘使用率是核心指标。\n其次在分配新空间的时候，优先选择资源使用率小的存储节点；而对已存在的存储节点，如果负载已经过载、或者资源使用情况不均衡，则需要做数据迁移。\n2）怎样保证新加入的节点，不会因短期负载压力过大而崩塌？\n当系统发现当前新加入了一台存储节点，显然它的资源使用率是最低的，那么所有的写流量都路由到这台存储节点来，那就可能造成这台新节点短期负载过大。因此，在资源分配的时候，需要有预热时间，在一个时间段内，缓慢地将写压力路由过来，直到达成新的均衡。\n3）如果需要数据迁移，那如何使其对业务层透明?\n在有中心节点的情况下，这个工作比较好做，中心节点就包办了——判断哪台存储节点压力较大，判断把哪些文件迁移到何处，更新自己的 meta 信息，迁移过程中的写入怎么办，发生重命名怎么办。无需上层应用来处理。\n如果没有中心节点，那代价比较大，在系统的整体设计上，也是要考虑到这种情况，比如ceph，它要采取逻辑位置和物理位置两层结构，对Client暴露的是逻辑层 (pool 和 place group)，这个在迁移过程中是不变的，而下层物理层数据块的移动，只是逻辑层所引用的物理块的地址发生了变化，在Client看来，逻辑块的位置并不会发生改变。\n2、中心节点的伸缩 如果有中心节点，还要考虑它的伸缩性。由于中心节点作为控制中心，是主从模式，那么在伸缩性上就受到比较大的限制，是有上限的，不能超过单台物理机的规模。我们可以考虑各种手段，尽量地抬高这个上限。有几种方式可以考虑：\n 以大数据块的形式来存储文件——比如 HDFS 的数据块的大小是 64M，ceph 的的数据块的大小是 4M，都远远超过单机文件系统的 4k。它的意义在于大幅减少 meta data 的数量，使中心节点的单机内存就能够支持足够多的磁盘空间 meta 信息。 中心节点采取多级的方式——顶级中心节点只存储目录的 meta data，其指定某目录的文件去哪台次级总控节点去找，然后再通过该次级总控节点找到文件真正的存储节点； 中心节点共享存储设备——部署多台中心节点，但它们共享同一个存储外设 / 数据库，meta 信息都放在这里，中心节点自身是无状态的。这种模式下，中心节点的请求处理能力大为增强，但性能会受一定影响。iRODS 就是采用这种方式。  七、高可用性 1、中心节点的高可用 中心节点的高可用，不仅要保证自身应用的高可用，还得保证 meta data 的数据高可用。\nmeta data 的高可用主要是数据持久化，并且需要备份机制保证不丢。一般方法是增加一个从节点，主节点的数据实时同步到从节点上。也有采用共享磁盘，通过 raid1 的硬件资源来保障高可用。显然增加从节点的主备方式更易于部署。\nmeta data 的数据持久化策略有以下几种方式：\n 直接保存到存储引擎上，一般是数据库。直接以文件形式保存到磁盘上，也不是不可以，但因为 meta 信息是结构化数据，这样相当于自己研发出一套小型数据库来，复杂化了。 保存日志数据到磁盘文件 (类似 MySQL 的 binlog 或 Redis 的 aof)，系统启动时在内存中重建成结果数据，提供服务。修改时先修改磁盘日志文件，然后更新内存数据。这种方式简单易用。  当前内存服务 + 日志文件持久化是主流方式。一是纯内存操作，效率很高，日志文件的写也是顺序写；二是不依赖外部组件，独立部署。\n为了解决日志文件会随着时间增长越来越大的问题，以让系统能以尽快启动和恢复，需要辅助以内存快照的方式——定期将内存 dump 保存，只保留在 dump 时刻之后的日志文件。这样当恢复时，从最新一次的内存 dump 文件开始，找其对应的 checkpoint 之后的日志文件开始重播。\n2、存储节点的高可用 在前面“持久化”章节，在保证数据副本不丢失的情况下，也就保证了其的高可用性。\n八、性能优化和缓存一致性 这些年随着基础设施的发展，局域网内千兆甚至万兆的带宽已经比较普遍，以万兆计算，每秒传输大约 1250M 字节的数据，而 SATA 磁盘的读写速度这些年基本达到瓶颈，在 300-500M/s 附近，也就是纯读写的话，网络已经超过了磁盘的能力，不再是瓶颈了，像 NAS 网络磁盘这些年也开始普及起来。\n但这并不代表，没有必要对读写进行优化，毕竟网络读写的速度还是远慢于内存的读写。常见的优化方法主要有：\n 内存中缓存文件内容； 预加载数据块，以避免客户端等待； 合并读写请求，也就是将单次请求做些积累，以批量方式发送给 Server 端。  缓存的使用在提高读写性能的同时，也会带来数据不一致的问题：\n 会出现更新丢失的现象。当多个 Client 在一个时间段内，先后写入同一个文件时，先写入的 Client 可能会丢失其写入内容，因为可能会被后写入的 Client 的内容覆盖掉； 数据可见性问题。Client 读取的是自己的缓存，在其过期之前，如果别的 Client 更新了文件内容，它是看不到的；也就是说，在同一时间，不同 Client 读取同一个文件，内容可能不一致。  这类问题有几种方法：\n 文件只读不改：一旦文件被 create 了，就只能读不能修改。这样 Client 端的缓存，就不存在不一致的问题； 通过锁：用锁的话还要考虑不同的粒度。写的时候是否允许其他 Client 读? 读的时候是否允许其他 Client 写? 这是在性能和一致性之间的权衡，作为文件系统来说，由于对业务并没有约束性，所以要做出合理的权衡，比较困难，因此最好是提供不同粒度的锁，由业务端来选择。但这样的副作用是，业务端的使用成本抬高了。  九、安全性 由于分布式文件存储系统，肯定是一个多客户端使用、多租户的一个产品，而它又存储了可能是很重要的信息，所以安全性是它的重要部分。\n主流文件系统的权限模型有以下这么几种：\n DAC：全称是 Discretionary Access Control，就是我们熟悉的 Unix 类权限框架，以 user-group-privilege 为三级体系，其中 user 就是 owner，group 包括 owner 所在 group 和非 owner 所在的 group、privilege 有 read、write 和 execute。这套体系主要是以 owner 为出发点，owner 允许谁对哪些文件具有什么样的权限。 MAC：全称是 Mandatory Access Control，它是从资源的机密程度来划分。比如分为“普通”、“机密”、“绝密”这三层，每个用户可能对应不同的机密阅读权限。这种权限体系起源于安全机构或军队的系统中，会比较常见。它的权限是由管理员来控制和设定的。Linux 中的 SELinux 就是 MAC 的一种实现，为了弥补 DAC 的缺陷和安全风险而提供出来。关于 SELinux 所解决的问题可以参考 What is SELinux? RBAC：全称是 Role Based Access Control，是基于角色 (role) 建立的权限体系。角色拥有什么样的资源权限，用户归到哪个角色，这对应企业 / 公司的组织机构非常合适。RBAC 也可以具体化，就演变成 DAC 或 MAC 的权限模型。  市面上的分布式文件系统有不同的选择，像 ceph 就提供了类似 DAC 但又略有区别的权限体系，Hadoop 自身就是依赖于操作系统的权限框架，同时其生态圈内有 Apache Sentry 提供了基于 RBAC 的权限体系来做补充。\n十、其他 1、空间分配 有连续空间和链表空间两种。连续空间的优势是读写快，按顺序即可，劣势是造成磁盘碎片，更麻烦的是，随着连续的大块磁盘空间被分配满而必须寻找空洞时，连续分配需要提前知道待写入文件的大小，以便找到合适大小的空间，而待写入文件的大小，往往又是无法提前知道的 (比如可编辑的 word 文档，它的内容可以随时增大)；\n而链表空间的优势是磁盘碎片很少，劣势是读写很慢，尤其是随机读，要从链表首个文件块一个一个地往下找。\n为了解决这个问题，出现了索引表——把文件和数据块的对应关系也保存一份，存在索引节点中 (一般称为 i 节点)，操作系统会将 i 节点加载到内存，从而程序随机寻找数据块时，在内存中就可以完成了。通过这种方式来解决磁盘链表的劣势，如果索引节点的内容太大，导致内存无法加载，还有可能形成多级索引结构。\n2、文件删除 实时删除还是延时删除? 实时删除的优势是可以快速释放磁盘空间；延时删除只是在删除动作执行的时候，置个标识位，后续在某个时间点再来批量删除，它的优势是文件仍然可以阶段性地保留，最大程度地避免了误删除，缺点是磁盘空间仍然被占着。在分布式文件系统中，磁盘空间都是比较充裕的资源，因此几乎都采用逻辑删除，以对数据可以进行恢复，同时在一段时间之后 (可能是 2 天或 3 天，这参数一般都可配置)，再对被删除的资源进行回收。\n怎么回收被删除或无用的数据? 可以从文件的 meta 信息出发——如果 meta 信息的“文件 - 数据块”映射表中包含了某个数据块，则它就是有用的；如果不包含，则表明该数据块已经是无效的了。所以，删除文件，其实是删除 meta 中的“文件 - 数据块”映射信息 (如果要保留一段时间，则是把这映射信息移到另外一个地方去)。\n3、面向小文件的分布式文件系统 有很多这样的场景，比如电商——那么多的商品图片、个人头像，比如社交网站——那么多的照片，它们具有的特性，可以简单归纳下：\n 每个文件都不大； 数量特别巨大； 读多写少； 不会修改。  针对这种业务场景，主流的实现方式是仍然是以大数据块的形式存储，小文件以逻辑存储的方式存在，即文件 meta 信息记录其是在哪个大数据块上，以及在该数据块上的 offset 和 length 是多少，形成一个逻辑上的独立文件。这样既复用了大数据块系统的优势和技术积累，又减少了 meta 信息。\n4、文件指纹和去重 文件指纹就是根据文件内容，经过算法，计算出文件的唯一标识。如果两个文件的指纹相同，则文件内容相同。在使用网络云盘的时候，发现有时候上传文件非常地快，就是文件指纹发挥作用。云盘服务商通过判断该文件的指纹，发现之前已经有人上传过了，则不需要真的上传该文件，只要增加一个引用即可。在文件系统中，通过文件指纹可以用来去重、也可以用来判断文件内容是否损坏、或者对比文件副本内容是否一致，是一个基础组件。\n文件指纹的算法也比较多，有熟悉的 md5、sha256、也有 google 专门针对文本领域的 simhash 和 minhash 等。\n十一、总结 分布式文件系统内容庞杂，要考虑的问题远不止上面所说的这些，其具体实现也更为复杂。本文只是尽量从分布式文件系统所要考虑的问题出发，给予一个简要的分析和设计，如果将来遇到类似的场景需要解决，可以想到“有这种解决方案”，然后再来深入研究。\n同时，市面上也是存在多种分布式文件系统的形态，下面就是有研究小组曾经对常见的几种分布式文件系统的设计比较。\n从这里也可以看到，选择其实很多，并不是 GFS 论文中的方式就是最好的。在不同的业务场景中，也可以有更多的选择策略。\n","permalink":"https://iblog.zone/archives/%E4%B8%BB%E6%B5%81%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F%E9%80%89%E5%9E%8B/","summary":"一、概述 分布式文件系统是分布式领域的一个基础应用，其中最著名的毫无疑问是 HDFS/GFS。如今该领域已经趋向于成熟，但了解它的设计要点和思想，对我们将来面临类似场景/问题时，具有借鉴意义。\n并且，分布式文件系统并非只有 HDFS/GFS 这一种形态，在它之外，还有其他形态各异、各有千秋的产品形态，对它们的了解，也对扩展我们的视野有所俾益。\n本文试图分析和思考，在分布式文件系统领域，我们要解决哪些问题、有些什么样的方案、以及各自的选择依据。\n二、过去的样子 在几十年以前，分布式文件系统就已经出现了，以 Sun 在 1984 年开发的“Network File System (NFS)”为代表，那时候解决的主要问题，是网络形态的磁盘，把磁盘从主机中独立出来。\n这样不仅可以获得更大的容量，而且还可以随时切换主机，还可以实现数据共享、备份、容灾等，因为数据是电脑中最重要的资产。\nNFS 的数据通信图如下：\n部署在主机上的客户端，通过 TCP/IP 协议把文件命令转发到远程文件 Server 上执行，整个过程对主机用户透明。\n到了互联网时代，流量和数据快速增长，分布式文件系统所要解决的主要场景变了，开始需要非常大的磁盘空间，这在磁盘体系上垂直扩容是无法达到的，必须要分布式，同时分布式架构下，主机都是可靠性不是非常好的普通服务器，因此容错、高可用、持久化、伸缩性等指标，就成为必须要考量的特性。\n三、对分布式文件系统的要求 对一个分布式文件系统而言，有一些特性是必须要满足的，否则就无法有竞争力。主要如下：\n 应该符合 POSIX 的文件接口标准，使该系统易于使用，同时对于用户的遗留系统也无需改造； 对用户透明，能够像使用本地文件系统那样直接使用； 持久化，保证数据不会丢失； 具有伸缩性，当数据压力逐渐增长时能顺利扩容； 具有可靠的安全机制，保证数据安全； 数据一致性，只要文件内容不发生变化，什么时候去读，得到的内容应该都是一样的。  除此之外，还有些特性是分布式加分项，具体如下：\n 支持的空间越大越好； 支持的并发访问请求越多越好； 性能越快越好； 硬件资源的利用率越高越合理，就越好。  四、架构模型 从业务模型和逻辑架构上，分布式文件系统需要这几类组件：\n 存储组件：负责存储文件数据，它要保证文件的持久化、副本间数据一致、数据块的分配 / 合并等等； 管理组件：负责 meta 信息，即文件数据的元信息，包括文件存放在哪台服务器上、文件大小、权限等，除此之外，还要负责对存储组件的管理，包括存储组件所在的服务器是否正常存活、是否需要数据迁移等； 接口组件：提供接口服务给应用使用，形态包括 SDK(Java/C/C++ 等)、CLI 命令行终端、以及支持 FUSE 挂载机制。  而在部署架构上，有着“中心化”和“无中心化”两种路线分歧，即是否把“管理组件”作为分布式文件系统的中心管理节点。两种路线都有很优秀的产品，下面分别介绍它们的区别。\n1、有中心节点 以 GFS 为代表，中心节点负责文件定位、维护文件 meta 信息、故障检测、数据迁移等管理控制的职能，下图是 GFS 的架构图：\n该图中GFS master 即为 GFS 的中心节点，GF chunkserver 为 GFS 的存储节点。其操作路径如下：","title":"主流分布式文件系统选型"},{"content":"1.接口 1.1黑马信息管理系统集合改进 (应用)   使用数组容器的弊端\n 容器长度是固定的，不能根据添加功能自动增长 没有提供用于赠删改查的方法    优化步骤\n  创建新的StudentDao类，OtherStudentDao\n  创建ArrayList集合容器对象\n  OtherStudentDao中的方法声明，需要跟StudentDao保持一致\n注意：如果不一致，StudentService中的代码就需要进行修改\n  完善方法（添加、删除、修改、查看）\n  替换StudentService中的Dao对象\n    代码实现\nOtherStudentDao类\npublic class OtherStudentDao {  // 集合容器  private static ArrayList\u0026lt;Student\u0026gt; stus = new ArrayList\u0026lt;\u0026gt;();   static {  Student stu1 = new Student(\u0026#34;heima001\u0026#34;,\u0026#34;张三\u0026#34;,\u0026#34;23\u0026#34;,\u0026#34;1999-11-11\u0026#34;);  Student stu2 = new Student(\u0026#34;heima002\u0026#34;,\u0026#34;李四\u0026#34;,\u0026#34;24\u0026#34;,\u0026#34;2000-11-11\u0026#34;);   stus.add(stu1);  stus.add(stu2);  }   // 添加学生方法  public boolean addStudent(Student stu) {  stus.add(stu);  return true;  }   // 查看学生方法  public Student[] findAllStudent() {   Student[] students = new Student[stus.size()];   for (int i = 0; i \u0026lt; students.length; i++) {  students[i] = stus.get(i);  }   return students;  }   public void deleteStudentById(String delId) {  // 1. 查找id在容器中所在的索引位置  int index = getIndex(delId);  stus.remove(index);  }   public int getIndex(String id){  int index = -1;  for (int i = 0; i \u0026lt; stus.size(); i++) {  Student stu = stus.get(i);  if(stu != null \u0026amp;\u0026amp; stu.getId().equals(id)){  index = i;  break;  }  }  return index;  }   public void updateStudent(String updateId, Student newStu) {  // 1. 查找updateId, 在容器中的索引位置  int index = getIndex(updateId);  stus.set(index, newStu);  } } StudentService类\npublic class StudentService {  // 创建StudentDao (库管)  private OtherStudentDao studentDao = new OtherStudentDao();  // 其他方法没有变化,此处省略... }   1.2黑马信息管理系统抽取Dao (应用)   优化步骤\n 将方法向上抽取，抽取出一个父类 （ BaseStudentDao ） 方法的功能实现在父类中无法给出具体明确，定义为抽象方法 让两个类分别继承 BaseStudentDao ，重写内部抽象方法    代码实现\nBaseStudentDao类\npublic abstract class BaseStudentDao {  // 添加学生方法  public abstract boolean addStudent(Student stu);  // 查看学生方法  public abstract Student[] findAllStudent();  // 删除学生方法  public abstract void deleteStudentById(String delId);  // 根据id找索引方法  public abstract int getIndex(String id);  // 修改学生方法  public abstract void updateStudent(String updateId, Student newStu); } StudentDao类\npublic class StudentDao extends BaseStudentDao {  // 其他内容不变,此处省略 } OtherStudentDao类\npublic class OtherStudentDao extends BaseStudentDao {  // 其他内容不变,此处省略 }   1.3接口的概述（理解）  接口就是一种公共的规范标准，只要符合规范标准，大家都可以通用。 Java中接口存在的两个意义  用来定义规范 用来做功能的拓展    1.4接口的特点（记忆）   接口用关键字interface修饰\npublic interface 接口名 {}   类实现接口用implements表示\npublic class 类名 implements 接口名 {}   接口不能实例化\n​\t我们可以创建接口的实现类对象使用\n  接口的子类\n​\t要么重写接口中的所有抽象方法\n​\t要么子类也是抽象类\n  1.5接口的成员特点（记忆）   成员特点\n  成员变量\n​\t只能是常量 ​\t默认修饰符：public static final\n  构造方法\n​\t没有，因为接口主要是扩展功能的，而没有具体存在\n  成员方法\n​\t只能是抽象方法\n​\t默认修饰符：public abstract\n​\t关于接口中的方法，JDK8和JDK9中有一些新特性，后面再讲解\n    代码演示\n 接口  public interface Inter {  public static final int NUM = 10;   public abstract void show(); }  实现类  class InterImpl implements Inter{   public void method(){  // NUM = 20;  System.out.println(NUM);  }   public void show(){   } }  测试类  public class TestInterface {  /* 成员变量: 只能是常量 系统会默认加入三个关键字 public static final 构造方法: 没有 成员方法: 只能是抽象方法, 系统会默认加入两个关键字 public abstract */  public static void main(String[] args) {  System.out.println(Inter.NUM);  }  }   1.6类和接口的关系（记忆）   类与类的关系\n​\t继承关系，只能单继承，但是可以多层继承\n  类与接口的关系\n​\t实现关系，可以单实现，也可以多实现，还可以在继承一个类的同时实现多个接口\n  接口与接口的关系\n​\t继承关系，可以单继承，也可以多继承\n  1.7黑马信息管理系统使用接口改进 (应用)   实现步骤\n 将 BaseStudentDao 改进为一个接口 让 StudentDao 和 OtherStudentDao 去实现这个接口    代码实现\nBaseStudentDao接口\npublic interface BaseStudentDao {  // 添加学生方法  public abstract boolean addStudent(Student stu);  // 查看学生方法  public abstract Student[] findAllStudent();  // 删除学生方法  public abstract void deleteStudentById(String delId);  // 根据id找索引方法  public abstract int getIndex(String id);  // 修改学生方法  public abstract void updateStudent(String updateId, Student newStu); } StudentDao类\npublic class StudentDao implements BaseStudentDao {  // 其他内容不变,此处省略 } OtherStudentDao类\npublic class OtherStudentDao implements BaseStudentDao {  // 其他内容不变,此处省略 }   1.8黑马信息管理系统解耦合改进 (应用)   实现步骤\n 创建factory包，创建 StudentDaoFactory（工厂类） 提供 static 修改的 getStudentDao 方法，该方法用于创建StudentDao对象并返回    代码实现\nStudentDaoFactory类\npublic class StudentDaoFactory {  public static OtherStudentDao getStudentDao(){  return new OtherStudentDao();  } } StudentService类\npublic class StudentService {  // 创建StudentDao (库管)  // private OtherStudentDao studentDao = new OtherStudentDao();   // 通过学生库管工厂类, 获取库管对象  private OtherStudentDao studentDao = StudentDaoFactory.getStudentDao(); }   2.接口组成更新 2.1接口组成更新概述【理解】   常量\npublic static final\n  抽象方法\npublic abstract\n  默认方法(Java 8)\n  静态方法(Java 8)\n  私有方法(Java 9)\n  2.2接口中默认方法【应用】   格式\npublic default 返回值类型 方法名(参数列表) { }\n  作用\n解决接口升级的问题\n  范例\npublic default void show3() { }   注意事项\n 默认方法不是抽象方法，所以不强制被重写。但是可以被重写，重写的时候去掉default关键字 public可以省略，default不能省略 如果实现了多个接口，多个接口中存在相同的方法声明，子类就必须对该方法进行重写    2.3接口中静态方法【应用】   格式\npublic static 返回值类型 方法名(参数列表) { }\n  范例\npublic static void show() { }   注意事项\n 静态方法只能通过接口名调用，不能通过实现类名或者对象名调用 public可以省略，static不能省略    2.4接口中私有方法【应用】   私有方法产生原因\nJava 9中新增了带方法体的私有方法，这其实在Java 8中就埋下了伏笔：Java 8允许在接口中定义带方法体的默认方法和静态方法。这样可能就会引发一个问题：当两个默认方法或者静态方法中包含一段相同的代码实现时，程序必然考虑将这段实现代码抽取成一个共性方法，而这个共性方法是不需要让别人使用的，因此用私有给隐藏起来，这就是Java 9增加私有方法的必然性\n  定义格式\n  格式1\nprivate 返回值类型 方法名(参数列表) { }\n  范例1\nprivate void show() { }   格式2\nprivate static 返回值类型 方法名(参数列表) { }\n  范例2\nprivate static void method() { }     注意事项\n 默认方法可以调用私有的静态方法和非静态方法 静态方法只能调用私有的静态方法    3.多态 3.1多态的概述（记忆）   什么是多态\n​\t同一个对象，在不同时刻表现出来的不同形态\n  多态的前提\n 要有继承或实现关系 要有方法的重写 要有父类引用指向子类对象    代码演示\nclass Animal {  public void eat(){  System.out.println(\u0026#34;动物吃饭\u0026#34;);  } }  class Cat extends Animal {  @Override  public void eat() {  System.out.println(\u0026#34;猫吃鱼\u0026#34;);  } }  public class Test1Polymorphic {  /* 多态的前提: 1. 要有(继承 \\ 实现)关系 2. 要有方法重写 3. 要有父类引用, 指向子类对象 */  public static void main(String[] args) {  // 当前事物, 是一只猫  Cat c = new Cat();  // 当前事物, 是一只动物  Animal a = new Cat();  a.eat();   } }   3.2多态中的成员访问特点（记忆）   成员访问特点\n  成员变量\n​\t编译看父类，运行看父类\n  成员方法\n​\t编译看父类，运行看子类\n    代码演示\nclass Fu {  int num = 10;   public void method(){  System.out.println(\u0026#34;Fu.. method\u0026#34;);  } }  class Zi extends Fu {  int num = 20;   public void method(){  System.out.println(\u0026#34;Zi.. method\u0026#34;);  } }  public class Test2Polymorpic {  /* 多态的成员访问特点: 成员变量: 编译看左边 (父类), 运行看左边 (父类) 成员方法: 编译看左边 (父类), 运行看右边 (子类) */  public static void main(String[] args) {  Fu f = new Zi();  System.out.println(f.num);  f.method();  } }   3.3多态的好处和弊端（记忆）   好处\n​\t提高程序的扩展性。定义方法时候，使用父类型作为参数，在使用的时候，使用具体的子类型参与操作\n  弊端\n​\t不能使用子类的特有成员\n  3.4多态中的转型（应用）   向上转型\n​\t父类引用指向子类对象就是向上转型\n  向下转型\n​\t格式：子类型 对象名 = (子类型)父类引用;\n  代码演示\nclass Fu {  public void show(){  System.out.println(\u0026#34;Fu..show...\u0026#34;);  } }  class Zi extends Fu {  @Override  public void show() {  System.out.println(\u0026#34;Zi..show...\u0026#34;);  }   public void method(){  System.out.println(\u0026#34;我是子类特有的方法, method\u0026#34;);  } }  public class Test3Polymorpic {  public static void main(String[] args) {  // 1. 向上转型 : 父类引用指向子类对象  Fu f = new Zi();  f.show();  // 多态的弊端: 不能调用子类特有的成员  // f.method();   // A: 直接创建子类对象  // B: 向下转型   // 2. 向下转型 : 从父类类型, 转换回子类类型  Zi z = (Zi) f;  z.method();  } }   3.5多态中转型存在的风险和解决方案 (应用)   风险\n如果被转的引用类型变量,对应的实际类型和目标类型不是同一种类型,那么在转换的时候就会出现ClassCastException\n  解决方案\n  关键字\ninstanceof\n  使用格式\n变量名 instanceof 类型\n通俗的理解：判断关键字左边的变量，是否是右边的类型，返回boolean类型结果\n    代码演示\nabstract class Animal {  public abstract void eat(); }  class Dog extends Animal {  public void eat() {  System.out.println(\u0026#34;狗吃肉\u0026#34;);  }   public void watchHome(){  System.out.println(\u0026#34;看家\u0026#34;);  } }  class Cat extends Animal {  public void eat() {  System.out.println(\u0026#34;猫吃鱼\u0026#34;);  } }  public class Test4Polymorpic {  public static void main(String[] args) {  useAnimal(new Dog());  useAnimal(new Cat());  }   public static void useAnimal(Animal a){ // Animal a = new Dog();  // Animal a = new Cat();  a.eat();  //a.watchHome();  // Dog dog = (Dog) a; // dog.watchHome(); // ClassCastException 类型转换异常   // 判断a变量记录的类型, 是否是Dog  if(a instanceof Dog){  Dog dog = (Dog) a;  dog.watchHome();  }  }  }   3.6黑马信息管理系统多态改进 (应用)   实现步骤\n StudentDaoFactory类中方法的返回值定义成父类类型BaseStudentDao StudentService中接收方法返回值的类型定义成父类类型BaseStudentDao    代码实现\nStudentDaoFactory类\npublic class StudentDaoFactory {  public static BaseStudentDao getStudentDao(){  return new OtherStudentDao();  } } StudentService类\npublic class StudentService {  // 创建StudentDao (库管)  // private OtherStudentDao studentDao = new OtherStudentDao();   // 通过学生库管工厂类, 获取库管对象  private BaseStudentDao studentDao = StudentDaoFactory.getStudentDao(); }   4.内部类 4.1 内部类的基本使用（理解）   内部类概念\n 在一个类中定义一个类。举例：在一个类A的内部定义一个类B，类B就被称为内部类    内部类定义格式\n  格式\u0026amp;举例：\n/* 格式： class 外部类名{ 修饰符 class 内部类名{ } } */  class Outer {  public class Inner {   } }     内部类的访问特点\n 内部类可以直接访问外部类的成员，包括私有 外部类要访问内部类的成员，必须创建对象    示例代码：\n/* 内部类访问特点： 内部类可以直接访问外部类的成员，包括私有 外部类要访问内部类的成员，必须创建对象 */ public class Outer {  private int num = 10;  public class Inner {  public void show() {  System.out.println(num);  }  }  public void method() {  Inner i = new Inner();  i.show();  } }   2.2 成员内部类（理解）   成员内部类的定义位置\n 在类中方法，跟成员变量是一个位置    外界创建成员内部类格式\n 格式：外部类名.内部类名 对象名 = 外部类对象.内部类对象; 举例：Outer.Inner oi = new Outer().new Inner();    私有成员内部类\n  将一个类，设计为内部类的目的，大多数都是不想让外界去访问，所以内部类的定义应该私有化，私有化之后，再提供一个可以让外界调用的方法，方法内部创建内部类对象并调用。\n  示例代码：\nclass Outer {  private int num = 10;  private class Inner {  public void show() {  System.out.println(num);  }  }  public void method() {  Inner i = new Inner();  i.show();  } } public class InnerDemo {  public static void main(String[] args) { \t//Outer.Inner oi = new Outer().new Inner(); \t//oi.show();  Outer o = new Outer();  o.method();  } }     静态成员内部类\n  静态成员内部类访问格式：外部类名.内部类名 对象名 = new 外部类名.内部类名();\n  静态成员内部类中的静态方法：外部类名.内部类名.方法名();\n  示例代码\nclass Outer {  static class Inner {  public void show(){  System.out.println(\u0026#34;inner..show\u0026#34;);  }   public static void method(){  System.out.println(\u0026#34;inner..method\u0026#34;);  }  } }  public class Test3Innerclass {  /* 静态成员内部类演示 */  public static void main(String[] args) {  // 外部类名.内部类名 对象名 = new 外部类名.内部类名();  Outer.Inner oi = new Outer.Inner();  oi.show();   Outer.Inner.method();  } }     2.3 局部内部类（理解）   局部内部类定义位置\n 局部内部类是在方法中定义的类    局部内部类方式方式\n 局部内部类，外界是无法直接使用，需要在方法内部创建对象并使用 该类可以直接访问外部类的成员，也可以访问方法内的局部变量    示例代码\nclass Outer {  private int num = 10;  public void method() {  int num2 = 20;  class Inner {  public void show() {  System.out.println(num);  System.out.println(num2);  }  }  Inner i = new Inner();  i.show();  } } public class OuterDemo {  public static void main(String[] args) {  Outer o = new Outer();  o.method();  } }   2.4 匿名内部类（应用）   匿名内部类的前提\n 存在一个类或者接口，这里的类可以是具体类也可以是抽象类    匿名内部类的格式\n  格式：new 类名 ( ) { 重写方法 } new 接口名 ( ) { 重写方法 }\n  举例：\nnew Inter(){  @Override  public void method(){} }     匿名内部类的本质\n 本质：是一个继承了该类或者实现了该接口的子类匿名对象    匿名内部类的细节\n  匿名内部类可以通过多态的形式接受\nInter i = new Inter(){  @Override  public void method(){   } }     匿名内部类直接调用方法\ninterface Inter{  void method(); }  class Test{  public static void main(String[] args){  new Inter(){  @Override  public void method(){  System.out.println(\u0026#34;我是匿名内部类\u0026#34;);  }  }.method();\t// 直接调用方法  } }   2.4 匿名内部类在开发中的使用（应用）   匿名内部类在开发中的使用\n 当发现某个方法需要，接口或抽象类的子类对象，我们就可以传递一个匿名内部类过去，来简化传统的代码    示例代码：\n/* 游泳接口 */ interface Swimming {  void swim(); }  public class TestSwimming {  public static void main(String[] args) {  goSwimming(new Swimming() {  @Override  public void swim() {  System.out.println(\u0026#34;铁汁, 我们去游泳吧\u0026#34;);  }  });  }   /** * 使用接口的方法 */  public static void goSwimming(Swimming swimming){  /* Swimming swim = new Swimming() { @Override public void swim() { System.out.println(\u0026#34;铁汁, 我们去游泳吧\u0026#34;); } } */  swimming.swim();  } }   5.Lambda表达式 5.1体验Lambda表达式【理解】   代码演示\n/* 游泳接口 */ interface Swimming {  void swim(); }  public class TestSwimming {  public static void main(String[] args) {  // 通过匿名内部类实现  goSwimming(new Swimming() {  @Override  public void swim() {  System.out.println(\u0026#34;铁汁, 我们去游泳吧\u0026#34;);  }  });   /* 通过Lambda表达式实现 理解: 对于Lambda表达式, 对匿名内部类进行了优化 */  goSwimming(() -\u0026gt; System.out.println(\u0026#34;铁汁, 我们去游泳吧\u0026#34;));  }   /** * 使用接口的方法 */  public static void goSwimming(Swimming swimming) {  swimming.swim();  } }   函数式编程思想概述\n在数学中，函数就是有输入量、输出量的一套计算方案，也就是“拿数据做操作”\n面向对象思想强调“必须通过对象的形式来做事情”\n函数式思想则尽量忽略面向对象的复杂语法：“强调做什么，而不是以什么形式去做”\n而我们要学习的Lambda表达式就是函数式思想的体现\n  5.2Lambda表达式的标准格式【理解】   格式：\n​\t(形式参数) -\u0026gt; {代码块}\n 形式参数：如果有多个参数，参数之间用逗号隔开；如果没有参数，留空即可 -\u0026gt;：由英文中画线和大于符号组成，固定写法。代表指向动作 代码块：是我们具体要做的事情，也就是以前我们写的方法体内容    组成Lambda表达式的三要素：\n 形式参数，箭头，代码块    5.3Lambda表达式练习1【应用】   Lambda表达式的使用前提\n 有一个接口 接口中有且仅有一个抽象方法    练习描述\n​\t无参无返回值抽象方法的练习\n  操作步骤\n 定义一个接口(Eatable)，里面定义一个抽象方法：void eat(); 定义一个测试类(EatableDemo)，在测试类中提供两个方法  一个方法是：useEatable(Eatable e) 一个方法是主方法，在主方法中调用useEatable方法      示例代码\n//接口 public interface Eatable {  void eat(); } //实现类 public class EatableImpl implements Eatable {  @Override  public void eat() {  System.out.println(\u0026#34;一天一苹果，医生远离我\u0026#34;);  } } //测试类 public class EatableDemo {  public static void main(String[] args) {  //在主方法中调用useEatable方法  Eatable e = new EatableImpl();  useEatable(e);   //匿名内部类  useEatable(new Eatable() {  @Override  public void eat() {  System.out.println(\u0026#34;一天一苹果，医生远离我\u0026#34;);  }  });   //Lambda表达式  useEatable(() -\u0026gt; {  System.out.println(\u0026#34;一天一苹果，医生远离我\u0026#34;);  });  }   private static void useEatable(Eatable e) {  e.eat();  } }   5.4Lambda表达式练习2【应用】   练习描述\n有参无返回值抽象方法的练习\n  操作步骤\n 定义一个接口(Flyable)，里面定义一个抽象方法：void fly(String s); 定义一个测试类(FlyableDemo)，在测试类中提供两个方法  一个方法是：useFlyable(Flyable f) 一个方法是主方法，在主方法中调用useFlyable方法      示例代码\npublic interface Flyable {  void fly(String s); }  public class FlyableDemo {  public static void main(String[] args) {  //在主方法中调用useFlyable方法  //匿名内部类  useFlyable(new Flyable() {  @Override  public void fly(String s) {  System.out.println(s);  System.out.println(\u0026#34;飞机自驾游\u0026#34;);  }  });  System.out.println(\u0026#34;--------\u0026#34;);   //Lambda  useFlyable((String s) -\u0026gt; {  System.out.println(s);  System.out.println(\u0026#34;飞机自驾游\u0026#34;);  });   }   private static void useFlyable(Flyable f) {  f.fly(\u0026#34;风和日丽，晴空万里\u0026#34;);  } }   5.5Lambda表达式练习3【应用】   练习描述\n有参有返回值抽象方法的练习\n  操作步骤\n 定义一个接口(Addable)，里面定义一个抽象方法：int add(int x,int y); 定义一个测试类(AddableDemo)，在测试类中提供两个方法  一个方法是：useAddable(Addable a) 一个方法是主方法，在主方法中调用useAddable方法      示例代码\npublic interface Addable {  int add(int x,int y); }  public class AddableDemo {  public static void main(String[] args) {  //在主方法中调用useAddable方法  useAddable((int x,int y) -\u0026gt; {  return x + y;  });   }   private static void useAddable(Addable a) {  int sum = a.add(10, 20);  System.out.println(sum);  } }   5.6Lambda表达式的省略模式【应用】   省略的规则\n 参数类型可以省略。但是有多个参数的情况下，不能只省略一个 如果参数有且仅有一个，那么小括号可以省略 如果代码块的语句只有一条，可以省略大括号和分号，和return关键字    代码演示\npublic interface Addable {  int add(int x, int y); }  public interface Flyable {  void fly(String s); }  public class LambdaDemo {  public static void main(String[] args) { // useAddable((int x,int y) -\u0026gt; { // return x + y; // });  //参数的类型可以省略  useAddable((x, y) -\u0026gt; {  return x + y;  });  // useFlyable((String s) -\u0026gt; { // System.out.println(s); // });  //如果参数有且仅有一个，那么小括号可以省略 // useFlyable(s -\u0026gt; { // System.out.println(s); // });   //如果代码块的语句只有一条，可以省略大括号和分号  useFlyable(s -\u0026gt; System.out.println(s));   //如果代码块的语句只有一条，可以省略大括号和分号，如果有return，return也要省略掉  useAddable((x, y) -\u0026gt; x + y);  }   private static void useFlyable(Flyable f) {  f.fly(\u0026#34;风和日丽，晴空万里\u0026#34;);  }   private static void useAddable(Addable a) {  int sum = a.add(10, 20);  System.out.println(sum);  } }   5.7Lambda表达式的使用前提【理解】  使用Lambda必须要有接口 并且要求接口中有且仅有一个抽象方法  5.8Lambda表达式和匿名内部类的区别【理解】  所需类型不同  匿名内部类：可以是接口，也可以是抽象类，还可以是具体类 Lambda表达式：只能是接口   使用限制不同  如果接口中有且仅有一个抽象方法，可以使用Lambda表达式，也可以使用匿名内部类 如果接口中多于一个抽象方法，只能使用匿名内部类，而不能使用Lambda表达式   实现原理不同  匿名内部类：编译之后，产生一个单独的.class字节码文件 Lambda表达式：编译之后，没有一个单独的.class字节码文件。对应的字节码会在运行的时候动态生成    ","permalink":"https://iblog.zone/archives/java%E6%8E%A5%E5%8F%A3%E5%92%8C%E5%86%85%E9%83%A8%E7%B1%BB/","summary":"1.接口 1.1黑马信息管理系统集合改进 (应用)   使用数组容器的弊端\n 容器长度是固定的，不能根据添加功能自动增长 没有提供用于赠删改查的方法    优化步骤\n  创建新的StudentDao类，OtherStudentDao\n  创建ArrayList集合容器对象\n  OtherStudentDao中的方法声明，需要跟StudentDao保持一致\n注意：如果不一致，StudentService中的代码就需要进行修改\n  完善方法（添加、删除、修改、查看）\n  替换StudentService中的Dao对象\n    代码实现\nOtherStudentDao类\npublic class OtherStudentDao {  // 集合容器  private static ArrayList\u0026lt;Student\u0026gt; stus = new ArrayList\u0026lt;\u0026gt;();   static {  Student stu1 = new Student(\u0026#34;heima001\u0026#34;,\u0026#34;张三\u0026#34;,\u0026#34;23\u0026#34;,\u0026#34;1999-11-11\u0026#34;);  Student stu2 = new Student(\u0026#34;heima002\u0026#34;,\u0026#34;李四\u0026#34;,\u0026#34;24\u0026#34;,\u0026#34;2000-11-11\u0026#34;);   stus.add(stu1);  stus.","title":"Java接口和内部类"},{"content":"1. 继承 1.1 继承的实现（掌握）   继承的概念\n 继承是面向对象三大特征之一，可以使得子类具有父类的属性和方法，还可以在子类中重新定义，以及追加属性和方法    实现继承的格式\n 继承通过extends实现 格式：class 子类 extends 父类 { }  举例：class Dog extends Animal { }      继承带来的好处\n 继承可以让类与类之间产生关系，子父类关系，产生子父类后，子类则可以使用父类中非私有的成员。    示例代码\npublic class Fu {  public void show() {  System.out.println(\u0026#34;show方法被调用\u0026#34;);  } } public class Zi extends Fu {  public void method() {  System.out.println(\u0026#34;method方法被调用\u0026#34;);  } } public class Demo {  public static void main(String[] args) {  //创建对象，调用方法  Fu f = new Fu();  f.show();   Zi z = new Zi();  z.method();  z.show();  } }   1.2 继承的好处和弊端（理解）  继承好处  提高了代码的复用性(多个类相同的成员可以放到同一个类中) 提高了代码的维护性(如果方法的代码需要修改，修改一处即可)   继承弊端  继承让类与类之间产生了关系，类的耦合性增强了，当父类发生变化时子类实现也不得不跟着变化，削弱了子类的独立性   继承的应用场景：  使用继承，需要考虑类与类之间是否存在is..a的关系，不能盲目使用继承  is..a的关系：谁是谁的一种，例如：老师和学生是人的一种，那人就是父类，学生和老师就是子类      1.3. Java中继承的特点（掌握）   Java中继承的特点\n Java中类只支持单继承，不支持多继承  错误范例：class A extends B, C { }   Java中类支持多层继承    多层继承示例代码：\npublic class Granddad {   public void drink() {  System.out.println(\u0026#34;爷爷爱喝酒\u0026#34;);  }  }  public class Father extends Granddad {   public void smoke() {  System.out.println(\u0026#34;爸爸爱抽烟\u0026#34;);  }  }  public class Mother {   public void dance() {  System.out.println(\u0026#34;妈妈爱跳舞\u0026#34;);  }  } public class Son extends Father { \t// 此时，Son类中就同时拥有drink方法以及smoke方法 }   2. 继承中的成员访问特点 2.1 继承中变量的访问特点（掌握） 在子类方法中访问一个变量，采用的是就近原则。\n 子类局部范围找 子类成员范围找 父类成员范围找 如果都没有就报错(不考虑父亲的父亲…)    示例代码\nclass Fu {  int num = 10; } class Zi {  int num = 20;  public void show(){  int num = 30;  System.out.println(num);  } } public class Demo1 {  public static void main(String[] args) {  Zi z = new Zi();  z.show();\t// 输出show方法中的局部变量30  } }   2.2 super（掌握）  this\u0026amp;super关键字：  this：代表本类对象的引用 super：代表父类存储空间的标识(可以理解为父类对象引用)   this和super的使用分别  成员变量：  this.成员变量 - 访问本类成员变量 super.成员变量 - 访问父类成员变量   成员方法：  this.成员方法 - 访问本类成员方法 super.成员方法 - 访问父类成员方法     构造方法：  this(…) - 访问本类构造方法 super(…) - 访问父类构造方法    2.3 继承中构造方法的访问特点（理解） 注意：子类中所有的构造方法默认都会访问父类中无参的构造方法\n​\t子类会继承父类中的数据，可能还会使用父类的数据。所以，子类初始化之前，一定要先完成父类数据的初始化，原因在于，每一个子类构造方法的第一条语句默认都是：super()\n问题：如果父类中没有无参构造方法，只有带参构造方法，该怎么办呢？\n1. 通过使用super关键字去显示的调用父类的带参构造方法 2. 子类通过this去调用本类的其他构造方法,本类其他构造方法再通过super去手动调用父类的带参的构造方法 注意: this(…)super(…) 必须放在构造方法的第一行有效语句，并且二者不能共存 2.4 继承中成员方法的访问特点（掌握） 通过子类对象访问一个方法\n 子类成员范围找 父类成员范围找 如果都没有就报错(不考虑父亲的父亲…)  2.5 super内存图（理解）   对象在堆内存中，会单独存在一块super区域，用来存放父类的数据\n  2.6 方法重写（掌握）  1、方法重写概念  子类出现了和父类中一模一样的方法声明（方法名一样，参数列表也必须一样）   2、方法重写的应用场景  当子类需要父类的功能，而功能主体子类有自己特有内容时，可以重写父类中的方法，这样，即沿袭了父类的功能，又定义了子类特有的内容   3、Override注解  用来检测当前的方法，是否是重写的方法，起到【校验】的作用    2.7 方法重写的注意事项（掌握）  方法重写的注意事项   私有方法不能被重写(父类私有成员子类是不能继承的) 子类方法访问权限不能更低(public \u0026gt; 默认 \u0026gt; 私有) 静态方法不能被重写,如果子类也有相同的方法,并不是重写的父类的方法   示例代码  public class Fu {  private void show() {  System.out.println(\u0026#34;Fu中show()方法被调用\u0026#34;);  }   void method() {  System.out.println(\u0026#34;Fu中method()方法被调用\u0026#34;);  } }  public class Zi extends Fu {   /* 编译【出错】，子类不能重写父类私有的方法*/  @Override  private void show() {  System.out.println(\u0026#34;Zi中show()方法被调用\u0026#34;);  }   /* 编译【出错】，子类重写父类方法的时候，访问权限需要大于等于父类 */  @Override  private void method() {  System.out.println(\u0026#34;Zi中method()方法被调用\u0026#34;);  }   /* 编译【通过】，子类重写父类方法的时候，访问权限需要大于等于父类 */  @Override  public void method() {  System.out.println(\u0026#34;Zi中method()方法被调用\u0026#34;);  } } 2.8 权限修饰符 (理解) 2.9 黑马信息管理系统使用继承改进 (掌握)   需求\n把学生类和老师类共性的内容向上抽取,抽取到出一个 Person 父类,让学生类和老师类继承 Person 类\n  实现步骤\n  抽取Person类\n  优化StudentController类中，inputStudentInfo方法，将setXxx赋值方式，改进为构造方法初始化\n注意：直接修改这种操作方式，不符合我们开发中的一个原则\n​\t开闭原则 ( 对扩展开放对修改关闭 ) : 尽量在不更改原有代码的前提下以完成需求\n解决：重新创建一个OtherStudentController类\n编写新的inputStudentInfo方法\n  根据StudentController类、OtherStudentController类，向上抽取出BaseStudentController类 再让StudentController类、OtherStudentController类，继承BaseStudentController类\n    代码实现\nPerson类及学生类和老师类\npublic class Person {  private String id;  private String name;  private String age;  private String birthday;   public Person() {  }   public Person(String id, String name, String age, String birthday) {  this.id = id;  this.name = name;  this.age = age;  this.birthday = birthday;  }   public String getId() {  return id;  }   public void setId(String id) {  this.id = id;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public String getAge() {  return age;  }   public void setAge(String age) {  this.age = age;  }   public String getBirthday() {  return birthday;  }   public void setBirthday(String birthday) {  this.birthday = birthday;  } } // Student类 public class Student extends Person {  public Student() {  }   public Student(String id, String name, String age, String birthday) {  super(id, name, age, birthday);  } } // Teacher类 public class Teacher extends Person {  public Teacher() {  }   public Teacher(String id, String name, String age, String birthday) {  super(id, name, age, birthday);  } } BaseStudentController类\npublic abstract class BaseStudentController {  // 业务员对象  private StudentService studentService = new StudentService();   private Scanner sc = new Scanner(System.in);   // 开启学生管理系统, 并展示学生管理系统菜单  public void start() {  //Scanner sc = new Scanner(System.in);  studentLoop:  while (true) {  System.out.println(\u0026#34;--------欢迎来到 \u0026lt;学生\u0026gt; 管理系统--------\u0026#34;);  System.out.println(\u0026#34;请输入您的选择: 1.添加学生 2.删除学生 3.修改学生 4.查看学生 5.退出\u0026#34;);  String choice = sc.next();  switch (choice) {  case \u0026#34;1\u0026#34;:  // System.out.println(\u0026#34;添加\u0026#34;);  addStudent();  break;  case \u0026#34;2\u0026#34;:  // System.out.println(\u0026#34;删除\u0026#34;);  deleteStudentById();  break;  case \u0026#34;3\u0026#34;:  // System.out.println(\u0026#34;修改\u0026#34;);  updateStudent();  break;  case \u0026#34;4\u0026#34;:  // System.out.println(\u0026#34;查询\u0026#34;);  findAllStudent();  break;  case \u0026#34;5\u0026#34;:  System.out.println(\u0026#34;感谢您使用学生管理系统, 再见!\u0026#34;);  break studentLoop;  default:  System.out.println(\u0026#34;您的输入有误, 请重新输入\u0026#34;);  break;  }  }  }   // 修改学生方法  public void updateStudent() {  String updateId = inputStudentId();  Student newStu = inputStudentInfo(updateId);  studentService.updateStudent(updateId, newStu);   System.out.println(\u0026#34;修改成功!\u0026#34;);  }   // 删除学生方法  public void deleteStudentById() {  String delId = inputStudentId();  // 3. 调用业务员中的deleteStudentById根据id, 删除学生  studentService.deleteStudentById(delId);  // 4. 提示删除成功  System.out.println(\u0026#34;删除成功!\u0026#34;);  }   // 查看学生方法  public void findAllStudent() {  // 1. 调用业务员中的获取方法, 得到学生的对象数组  Student[] stus = studentService.findAllStudent();  // 2. 判断数组的内存地址, 是否为null  if (stus == null) {  System.out.println(\u0026#34;查无信息, 请添加后重试\u0026#34;);  return;  }  // 3. 遍历数组, 获取学生信息并打印在控制台  System.out.println(\u0026#34;学号\\t\\t姓名\\t年龄\\t生日\u0026#34;);  for (int i = 0; i \u0026lt; stus.length; i++) {  Student stu = stus[i];  if (stu != null) {  System.out.println(stu.getId() + \u0026#34;\\t\u0026#34; + stu.getName() + \u0026#34;\\t\u0026#34; + stu.getAge() + \u0026#34;\\t\\t\u0026#34; + stu.getBirthday());  }  }  }   // 添加学生方法  public void addStudent() {  // StudentService studentService = new StudentService();  // 1. 键盘接收学生信息  String id;  while (true) {  System.out.println(\u0026#34;请输入学生id:\u0026#34;);  id = sc.next();  boolean flag = studentService.isExists(id);  if (flag) {  System.out.println(\u0026#34;学号已被占用, 请重新输入\u0026#34;);  } else {  break;  }  }   Student stu = inputStudentInfo(id);   // 3. 将学生对象,传递给StudentService(业务员)中的addStudent方法  boolean result = studentService.addStudent(stu);  // 4. 根据返回的boolean类型结果, 在控制台打印成功\\失败  if (result) {  System.out.println(\u0026#34;添加成功\u0026#34;);  } else {  System.out.println(\u0026#34;添加失败\u0026#34;);  }  }   // 键盘录入学生id  public String inputStudentId() {  String id;  while (true) {  System.out.println(\u0026#34;请输入学生id:\u0026#34;);  id = sc.next();  boolean exists = studentService.isExists(id);  if (!exists) {  System.out.println(\u0026#34;您输入的id不存在, 请重新输入:\u0026#34;);  } else {  break;  }  }  return id;  }   // 键盘录入学生信息  // 开闭原则: 对扩展内容开放, 对修改内容关闭  public Student inputStudentInfo(String id){  return null;  } } StudentController类\npublic class StudentController extends BaseStudentController {   private Scanner sc = new Scanner(System.in);   // 键盘录入学生信息  // 开闭原则: 对扩展内容开放, 对修改内容关闭  @Override  public Student inputStudentInfo(String id) {  System.out.println(\u0026#34;请输入学生姓名:\u0026#34;);  String name = sc.next();  System.out.println(\u0026#34;请输入学生年龄:\u0026#34;);  String age = sc.next();  System.out.println(\u0026#34;请输入学生生日:\u0026#34;);  String birthday = sc.next();  Student stu = new Student();  stu.setId(id);  stu.setName(name);  stu.setAge(age);  stu.setBirthday(birthday);  return stu;  } } OtherStudentController类\npublic class OtherStudentController extends BaseStudentController {   private Scanner sc = new Scanner(System.in);   // 键盘录入学生信息  // 开闭原则: 对扩展内容开放, 对修改内容关闭  @Override  public Student inputStudentInfo(String id) {  System.out.println(\u0026#34;请输入学生姓名:\u0026#34;);  String name = sc.next();  System.out.println(\u0026#34;请输入学生年龄:\u0026#34;);  String age = sc.next();  System.out.println(\u0026#34;请输入学生生日:\u0026#34;);  String birthday = sc.next();  Student stu = new Student(id,name,age,birthday);  return stu;  } }   3.抽象类 3.1抽象类的概述（理解） ​\t当我们在做子类共性功能抽取时，有些方法在父类中并没有具体的体现，这个时候就需要抽象类了！\n​\t在Java中，一个没有方法体的方法应该定义为抽象方法，而类中如果有抽象方法，该类必须定义为抽象类！\n3.2抽象类的特点（记忆）   抽象类和抽象方法必须使用 abstract 关键字修饰\n//抽象类的定义 public abstract class 类名 {}  //抽象方法的定义 public abstract void eat();   抽象类中不一定有抽象方法，有抽象方法的类一定是抽象类\n  抽象类不能实例化\n  抽象类可以有构造方法\n  抽象类的子类\n​\t要么重写抽象类中的所有抽象方法\n​\t要么是抽象类\n  3.3抽象类的案例（应用）   案例需求\n​\t定义猫类(Cat)和狗类(Dog)\n​\t猫类成员方法：eat（猫吃鱼）drink（喝水…）\n​\t狗类成员方法：eat（狗吃肉）drink（喝水…）\n  实现步骤\n 猫类和狗类中存在共性内容，应向上抽取出一个动物类（Animal） 父类Animal中，无法将 eat 方法具体实现描述清楚，所以定义为抽象方法 抽象方法需要存活在抽象类中，将Animal定义为抽象类 让 Cat 和 Dog 分别继承 Animal，重写eat方法 测试类中创建 Cat 和 Dog 对象，调用方法测试    代码实现\n 动物类  public abstract class Animal {  public void drink(){  System.out.println(\u0026#34;喝水\u0026#34;);  }   public Animal(){   }   public abstract void eat(); }  猫类  public class Cat extends Animal {  @Override  public void eat() {  System.out.println(\u0026#34;猫吃鱼\u0026#34;);  } }  狗类  public class Dog extends Animal {  @Override  public void eat() {  System.out.println(\u0026#34;狗吃肉\u0026#34;);  } }  测试类  public static void main(String[] args) {  Dog d = new Dog();  d.eat();  d.drink();   Cat c = new Cat();  c.drink();  c.eat();   //Animal a = new Animal();  //a.eat();  }   3.4模板设计模式   设计模式\n设计模式（Design pattern）是一套被反复使用、多数人知晓的、经过分类编目的、代码设计经验的总结。 使用设计模式是为了可重用代码、让代码更容易被他人理解、保证代码可靠性、程序的重用性。\n  模板设计模式\n把抽象类整体就可以看做成一个模板，模板中不能决定的东西定义成抽象方法 让使用模板的类（继承抽象类的类）去重写抽象方法实现需求\n  模板设计模式的优势\n模板已经定义了通用结构，使用者只需要关心自己需要实现的功能即可\n  示例代码\n模板类\n/* 作文模板类 */ public abstract class CompositionTemplate {   public final void write(){  System.out.println(\u0026#34;\u0026lt;\u0026lt;我的爸爸\u0026gt;\u0026gt;\u0026#34;);   body();   System.out.println(\u0026#34;啊~ 这就是我的爸爸\u0026#34;);   }   public abstract void body(); } 实现类A\npublic class Tom extends CompositionTemplate {   @Override  public void body() {  System.out.println(\u0026#34;那是一个秋天, 风儿那么缠绵,记忆中, \u0026#34; +  \u0026#34;那天爸爸骑车接我放学回家,我的脚卡在了自行车链当中, 爸爸蹬不动,他就站起来蹬...\u0026#34;);  } } 实现类B\npublic class Tony extends CompositionTemplate {  @Override  public void body() {   }   /*public void write(){ }*/ } 测试类\npublic class Test {  public static void main(String[] args) {  Tom t = new Tom();  t.write();  } }   3.5final（应用）   fianl关键字的作用\n final代表最终的意思，可以修饰成员方法，成员变量，类    final修饰类、方法、变量的效果\n  fianl修饰类：该类不能被继承（不能有子类，但是可以有父类）\n  final修饰方法：该方法不能被重写\n  final修饰变量：表明该变量是一个常量，不能再次赋值\n  变量是基本类型,不能改变的是值\n  变量是引用类型,不能改变的是地址值,但地址里面的内容是可以改变的\n  举例\npublic static void main(String[] args){  final Student s = new Student(23);  s = new Student(24); // 错误  s.setAge(24); // 正确 }       3.6黑马信息管理系统使用抽象类改进 (应用)   需求\n 使用抽象类的思想，将BaseStudentController 中的 inputStudentInfo 方法，定义为抽象方法 将不希望子类重写的方法，使用 final 进行修饰    代码实现\nBaseStudentController类\npublic abstract class BaseStudentController {  // 业务员对象  private StudentService studentService = new StudentService();   private Scanner sc = new Scanner(System.in);   // 开启学生管理系统, 并展示学生管理系统菜单  public final void start() {  //Scanner sc = new Scanner(System.in);  studentLoop:  while (true) {  System.out.println(\u0026#34;--------欢迎来到 \u0026lt;学生\u0026gt; 管理系统--------\u0026#34;);  System.out.println(\u0026#34;请输入您的选择: 1.添加学生 2.删除学生 3.修改学生 4.查看学生 5.退出\u0026#34;);  String choice = sc.next();  switch (choice) {  case \u0026#34;1\u0026#34;:  // System.out.println(\u0026#34;添加\u0026#34;);  addStudent();  break;  case \u0026#34;2\u0026#34;:  // System.out.println(\u0026#34;删除\u0026#34;);  deleteStudentById();  break;  case \u0026#34;3\u0026#34;:  // System.out.println(\u0026#34;修改\u0026#34;);  updateStudent();  break;  case \u0026#34;4\u0026#34;:  // System.out.println(\u0026#34;查询\u0026#34;);  findAllStudent();  break;  case \u0026#34;5\u0026#34;:  System.out.println(\u0026#34;感谢您使用学生管理系统, 再见!\u0026#34;);  break studentLoop;  default:  System.out.println(\u0026#34;您的输入有误, 请重新输入\u0026#34;);  break;  }  }  }   // 修改学生方法  public final void updateStudent() {  String updateId = inputStudentId();  Student newStu = inputStudentInfo(updateId);  studentService.updateStudent(updateId, newStu);   System.out.println(\u0026#34;修改成功!\u0026#34;);  }   // 删除学生方法  public final void deleteStudentById() {  String delId = inputStudentId();  // 3. 调用业务员中的deleteStudentById根据id, 删除学生  studentService.deleteStudentById(delId);  // 4. 提示删除成功  System.out.println(\u0026#34;删除成功!\u0026#34;);  }   // 查看学生方法  public final void findAllStudent() {  // 1. 调用业务员中的获取方法, 得到学生的对象数组  Student[] stus = studentService.findAllStudent();  // 2. 判断数组的内存地址, 是否为null  if (stus == null) {  System.out.println(\u0026#34;查无信息, 请添加后重试\u0026#34;);  return;  }  // 3. 遍历数组, 获取学生信息并打印在控制台  System.out.println(\u0026#34;学号\\t\\t姓名\\t年龄\\t生日\u0026#34;);  for (int i = 0; i \u0026lt; stus.length; i++) {  Student stu = stus[i];  if (stu != null) {  System.out.println(stu.getId() + \u0026#34;\\t\u0026#34; + stu.getName() + \u0026#34;\\t\u0026#34; + stu.getAge() + \u0026#34;\\t\\t\u0026#34; + stu.getBirthday());  }  }  }   // 添加学生方法  public final void addStudent() {  // StudentService studentService = new StudentService();  // 1. 键盘接收学生信息  String id;  while (true) {  System.out.println(\u0026#34;请输入学生id:\u0026#34;);  id = sc.next();  boolean flag = studentService.isExists(id);  if (flag) {  System.out.println(\u0026#34;学号已被占用, 请重新输入\u0026#34;);  } else {  break;  }  }   Student stu = inputStudentInfo(id);   // 3. 将学生对象,传递给StudentService(业务员)中的addStudent方法  boolean result = studentService.addStudent(stu);  // 4. 根据返回的boolean类型结果, 在控制台打印成功\\失败  if (result) {  System.out.println(\u0026#34;添加成功\u0026#34;);  } else {  System.out.println(\u0026#34;添加失败\u0026#34;);  }  }   // 键盘录入学生id  public String inputStudentId() {  String id;  while (true) {  System.out.println(\u0026#34;请输入学生id:\u0026#34;);  id = sc.next();  boolean exists = studentService.isExists(id);  if (!exists) {  System.out.println(\u0026#34;您输入的id不存在, 请重新输入:\u0026#34;);  } else {  break;  }  }  return id;  }   // 键盘录入学生信息  // 开闭原则: 对扩展内容开放, 对修改内容关闭  public abstract Student inputStudentInfo(String id); }   4.代码块 4.1代码块概述 (理解) 在Java中，使用 { } 括起来的代码被称为代码块\n4.2代码块分类 (理解)   局部代码块\n  位置: 方法中定义\n  作用: 限定变量的生命周期，及早释放，提高内存利用率\n  示例代码\npublic class Test {  /* 局部代码块 位置：方法中定义 作用：限定变量的生命周期，及早释放，提高内存利用率 */  public static void main(String[] args) {  {  int a = 10;  System.out.println(a);  }   // System.out.println(a);  } }     构造代码块\n  位置: 类中方法外定义\n  特点: 每次构造方法执行的时，都会执行该代码块中的代码，并且在构造方法执行前执行\n  作用: 将多个构造方法中相同的代码，抽取到构造代码块中，提高代码的复用性\n  示例代码\npublic class Test {  /* 构造代码块: 位置：类中方法外定义 特点：每次构造方法执行的时，都会执行该代码块中的代码，并且在构造方法执行前执行 作用：将多个构造方法中相同的代码，抽取到构造代码块中，提高代码的复用性 */  public static void main(String[] args) {  Student stu1 = new Student();  Student stu2 = new Student(10);  } }  class Student {   {  System.out.println(\u0026#34;好好学习\u0026#34;);  }   public Student(){  System.out.println(\u0026#34;空参数构造方法\u0026#34;);  }   public Student(int a){  System.out.println(\u0026#34;带参数构造方法...........\u0026#34;);  } }     静态代码块\n  位置: 类中方法外定义\n  特点: 需要通过static关键字修饰，随着类的加载而加载，并且只执行一次\n  作用: 在类加载的时候做一些数据初始化的操作\n  示例代码\npublic class Test {  /* 静态代码块: 位置：类中方法外定义 特点：需要通过static关键字修饰，随着类的加载而加载，并且只执行一次 作用：在类加载的时候做一些数据初始化的操作 */  public static void main(String[] args) {  Person p1 = new Person();  Person p2 = new Person(10);  } }  class Person {  static {  System.out.println(\u0026#34;我是静态代码块, 我执行了\u0026#34;);  }   public Person(){  System.out.println(\u0026#34;我是Person类的空参数构造方法\u0026#34;);  }   public Person(int a){  System.out.println(\u0026#34;我是Person类的带...........参数构造方法\u0026#34;);  } }     4.3黑马信息管理系统使用代码块改进 (应用)   需求\n使用静态代码块，初始化一些学生数据\n  实现步骤\n 在StudentDao类中定义一个静态代码块，用来初始化一些学生数据 将初始化好的学生数据存储到学生数组中    示例代码\nStudentDao类\npublic class StudentDao {  // 创建学生对象数组  private static Student[] stus = new Student[5];   static {  Student stu1 = new Student(\u0026#34;heima001\u0026#34;,\u0026#34;张三\u0026#34;,\u0026#34;23\u0026#34;,\u0026#34;1999-11-11\u0026#34;);  Student stu2 = new Student(\u0026#34;heima002\u0026#34;,\u0026#34;李四\u0026#34;,\u0026#34;24\u0026#34;,\u0026#34;2000-11-11\u0026#34;);   stus[0] = stu1;  stus[1] = stu2;  }   // 添加学生方法  public boolean addStudent(Student stu) {   // 2. 添加学生到数组  //2.1 定义变量index为-1，假设数组已经全部存满，没有null的元素  int index = -1;  //2.2 遍历数组取出每一个元素，判断是否是null  for (int i = 0; i \u0026lt; stus.length; i++) {  Student student = stus[i];  if(student == null){  index = i;  //2.3 如果为null，让index变量记录当前索引位置，并使用break结束循环遍历  break;  }  }   // 3. 返回是否添加成功的boolean类型状态  if(index == -1){  // 装满了  return false;  }else{  // 没有装满, 正常添加, 返回true  stus[index] = stu;  return true;  }  }  // 查看学生方法  public Student[] findAllStudent() {  return stus;  }   public void deleteStudentById(String delId) {  // 1. 查找id在容器中所在的索引位置  int index = getIndex(delId);  // 2. 将该索引位置,使用null元素进行覆盖  stus[index] = null;  }   public int getIndex(String id){  int index = -1;  for (int i = 0; i \u0026lt; stus.length; i++) {  Student stu = stus[i];  if(stu != null \u0026amp;\u0026amp; stu.getId().equals(id)){  index = i;  break;  }  }  return index;  }   public void updateStudent(String updateId, Student newStu) {  // 1. 查找updateId, 在容器中的索引位置  int index = getIndex(updateId);  // 2. 将该索引位置, 使用新的学生对象替换  stus[index] = newStu;  } }   ","permalink":"https://iblog.zone/archives/java%E7%BB%A7%E6%89%BF/","summary":"1. 继承 1.1 继承的实现（掌握）   继承的概念\n 继承是面向对象三大特征之一，可以使得子类具有父类的属性和方法，还可以在子类中重新定义，以及追加属性和方法    实现继承的格式\n 继承通过extends实现 格式：class 子类 extends 父类 { }  举例：class Dog extends Animal { }      继承带来的好处\n 继承可以让类与类之间产生关系，子父类关系，产生子父类后，子类则可以使用父类中非私有的成员。    示例代码\npublic class Fu {  public void show() {  System.out.println(\u0026#34;show方法被调用\u0026#34;);  } } public class Zi extends Fu {  public void method() {  System.out.println(\u0026#34;method方法被调用\u0026#34;);  } } public class Demo {  public static void main(String[] args) {  //创建对象，调用方法  Fu f = new Fu();  f.","title":"Java继承"},{"content":"1.案例驱动模式 1.1案例驱动模式概述 (理解) 通过我们已掌握的知识点,先实现一个案例,然后找出这个案例中,存在的一些问题,在通过新知识点解决问题\n1.2案例驱动模式的好处 (理解)  解决重复代码过多的冗余,提高代码的复用性 解决业务逻辑聚集紧密导致的可读性差,提高代码的可读性 解决代码可维护性差,提高代码的维护性  2.分类思想 2.1分类思想概述 (理解) 分工协作,专人干专事\n2.2黑马信息管理系统 (理解)   Student类 标准学生类,封装键盘录入的学生信息(id , name , age , birthday)\n  StudentDao类 Dao : (Data Access Object 缩写) 用于访问存储数据的数组或集合\n  StudentService类 用来进行业务逻辑的处理(例如: 判断录入的id是否存在)\n  StudentController类 和用户打交道(接收用户需求,采集用户信息,打印数据到控制台)\n  3.分包思想 3.1分包思想概述 (理解) 如果将所有的类文件都放在同一个包下,不利于管理和后期维护,所以,对于不同功能的类文件,可以放在不同的包下进行管理\n3.2包的概述 (记忆)   包\n本质上就是文件夹\n  创建包\n多级包之间使用 \u0026quot; . \u0026quot; 进行分割 多级包的定义规范：公司的网站地址翻转(去掉www) 比如：黑马程序员的网站址为www.itheima.com 后期我们所定义的包的结构就是：com.itheima.其他的包名\n  包的命名规则\n字母都是小写\n  3.3包的注意事项 (理解)  package语句必须是程序的第一条可执行的代码 package语句在一个java文件中只能有一个 如果没有package,默认表示无包名  3.4类与类之间的访问 (理解)   同一个包下的访问\n不需要导包，直接使用即可\n  不同包下的访问\n1.import 导包后访问\n2.通过全类名（包名 + 类名）访问\n  注意：import 、package 、class 三个关键字的摆放位置存在顺序关系\npackage 必须是程序的第一条可执行的代码\nimport 需要写在 package 下面\nclass 需要在 import 下面\n  4.黑马信息管理系统 4.1系统介绍 (理解) 4.2学生管理系统 (应用) 4.2.1需求说明   添加学生: 键盘录入学生信息(id，name，age，birthday)\n使用数组存储学生信息,要求学生的id不能重复\n  删除学生: 键盘录入要删除学生的id值,将该学生从数组中移除,如果录入的id在数组中不存在,需要重新录入\n  修改学生: 键盘录入要修改学生的id值和修改后的学生信息\n将数组中该学生的信息修改,如果录入的id在数组中不存在,需要重新录入\n  查询学生: 将数组中存储的所有学生的信息输出到控制台\n  4.2.2实现步骤   环境搭建实现步骤\n   包 存储的类 作用     com.itheima.edu.info.manager.domain Student.java 封装学生信息   com.itheima.edu.info.manager.dao StudentDao.java 访问存储数据的数组，进行赠删改查（库管）   com.itheima.edu.info.manager.service StudentService.java 业务的逻辑处理（业务员）   com.itheima.edu.info.manager.controller StudentController.java 和用户打交道（客服接待）   com.itheima.edu.info.manager.entry InfoManagerEntry.java 程序的入口类，提供一个main方法      菜单搭建实现步骤\n 需求  黑马管理系统菜单搭建 学生管理系统菜单搭建   实现步骤  展示欢迎页面,用输出语句完成主界面的编写 获取用户的选择,用Scanner实现键盘录入数据 根据用户的选择执行对应的操作,用switch语句完成操作的选择      添加功能实现步骤\n  添加功能优化:判断id是否存在\n  查询功能实现步骤\n  删除功能实现步骤\n  修改功能实现步骤\n  系统优化\n  把updateStudent和deleteStudentById中录入学生id代码抽取到一个方法(inputStudentId)中 该方法的主要作用就是录入学生的id，方法的返回值为String类型\n  把addStudent和updateStudent中录入学生信息的代码抽取到一个方法(inputStudentInfo)中 该方法的主要作用就是录入学生的信息，并封装为学生对象，方法的返回值为Student类型\n    4.2.3代码实现 学生类\npublic class Student {  private String id;  private String name;  private String age;  private String birthday;  String address;   public Student() {  }   public Student(String id, String name, String age, String birthday) {  this.id = id;  this.name = name;  this.age = age;  this.birthday = birthday;  }   public String getId() {  return id;  }   public void setId(String id) {  this.id = id;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public String getAge() {  return age;  }   public void setAge(String age) {  this.age = age;  }   public String getBirthday() {  return birthday;  }   public void setBirthday(String birthday) {  this.birthday = birthday;  }  } 程序入口InfoManagerEntry类\npublic class InfoManagerEntry {  public static void main(String[] args) {  Scanner sc = new Scanner(System.in);  while (true) {  // 主菜单搭建  System.out.println(\u0026#34;--------欢迎来到黑马信息管理系统--------\u0026#34;);  System.out.println(\u0026#34;请输入您的选择: 1.学生管理 2.老师管理 3.退出\u0026#34;);  String choice = sc.next();  switch (choice) {  case \u0026#34;1\u0026#34;:  // System.out.println(\u0026#34;学生管理\u0026#34;);  // 开启学生管理系统  StudentController studentController = new StudentController();  studentController.start();  break;  case \u0026#34;2\u0026#34;:  System.out.println(\u0026#34;老师管理\u0026#34;);  TeacherController teacherController = new TeacherController();  teacherController.start();  break;  case \u0026#34;3\u0026#34;:  System.out.println(\u0026#34;感谢您的使用\u0026#34;);  // 退出当前正在运行的JVM虚拟机  System.exit(0);  break;  default:  System.out.println(\u0026#34;您的输入有误, 请重新输入\u0026#34;);  break;  }  }  } } StudentController类\npublic class StudentController {  // 业务员对象  private StudentService studentService = new StudentService();   private Scanner sc = new Scanner(System.in);   // 开启学生管理系统, 并展示学生管理系统菜单  public void start() {  //Scanner sc = new Scanner(System.in);  studentLoop:  while (true) {  System.out.println(\u0026#34;--------欢迎来到 \u0026lt;学生\u0026gt; 管理系统--------\u0026#34;);  System.out.println(\u0026#34;请输入您的选择: 1.添加学生 2.删除学生 3.修改学生 4.查看学生 5.退出\u0026#34;);  String choice = sc.next();  switch (choice) {  case \u0026#34;1\u0026#34;:  // System.out.println(\u0026#34;添加\u0026#34;);  addStudent();  break;  case \u0026#34;2\u0026#34;:  // System.out.println(\u0026#34;删除\u0026#34;);  deleteStudentById();  break;  case \u0026#34;3\u0026#34;:  // System.out.println(\u0026#34;修改\u0026#34;);  updateStudent();  break;  case \u0026#34;4\u0026#34;:  // System.out.println(\u0026#34;查询\u0026#34;);  findAllStudent();  break;  case \u0026#34;5\u0026#34;:  System.out.println(\u0026#34;感谢您使用学生管理系统, 再见!\u0026#34;);  break studentLoop;  default:  System.out.println(\u0026#34;您的输入有误, 请重新输入\u0026#34;);  break;  }  }  }   // 修改学生方法  public void updateStudent() {  String updateId = inputStudentId();  Student newStu = inputStudentInfo(updateId);  studentService.updateStudent(updateId, newStu);   System.out.println(\u0026#34;修改成功!\u0026#34;);  }   // 删除学生方法  public void deleteStudentById() {  String delId = inputStudentId();  // 3. 调用业务员中的deleteStudentById根据id, 删除学生  studentService.deleteStudentById(delId);  // 4. 提示删除成功  System.out.println(\u0026#34;删除成功!\u0026#34;);  }   // 查看学生方法  public void findAllStudent() {  // 1. 调用业务员中的获取方法, 得到学生的对象数组  Student[] stus = studentService.findAllStudent();  // 2. 判断数组的内存地址, 是否为null  if (stus == null) {  System.out.println(\u0026#34;查无信息, 请添加后重试\u0026#34;);  return;  }  // 3. 遍历数组, 获取学生信息并打印在控制台  System.out.println(\u0026#34;学号\\t\\t姓名\\t年龄\\t生日\u0026#34;);  for (int i = 0; i \u0026lt; stus.length; i++) {  Student stu = stus[i];  if (stu != null) {  System.out.println(stu.getId() + \u0026#34;\\t\u0026#34; + stu.getName() + \u0026#34;\\t\u0026#34; + stu.getAge() + \u0026#34;\\t\\t\u0026#34; + stu.getBirthday());  }  }  }   // 添加学生方法  public void addStudent() {  // StudentService studentService = new StudentService();  // 1. 键盘接收学生信息  String id;  while (true) {  System.out.println(\u0026#34;请输入学生id:\u0026#34;);  id = sc.next();  boolean flag = studentService.isExists(id);  if (flag) {  System.out.println(\u0026#34;学号已被占用, 请重新输入\u0026#34;);  } else {  break;  }  }   Student stu = inputStudentInfo(id);   // 3. 将学生对象,传递给StudentService(业务员)中的addStudent方法  boolean result = studentService.addStudent(stu);  // 4. 根据返回的boolean类型结果, 在控制台打印成功\\失败  if (result) {  System.out.println(\u0026#34;添加成功\u0026#34;);  } else {  System.out.println(\u0026#34;添加失败\u0026#34;);  }  }   // 键盘录入学生id  public String inputStudentId() {  String id;  while (true) {  System.out.println(\u0026#34;请输入学生id:\u0026#34;);  id = sc.next();  boolean exists = studentService.isExists(id);  if (!exists) {  System.out.println(\u0026#34;您输入的id不存在, 请重新输入:\u0026#34;);  } else {  break;  }  }  return id;  }   // 键盘录入学生信息  public Student inputStudentInfo(String id) {  System.out.println(\u0026#34;请输入学生姓名:\u0026#34;);  String name = sc.next();  System.out.println(\u0026#34;请输入学生年龄:\u0026#34;);  String age = sc.next();  System.out.println(\u0026#34;请输入学生生日:\u0026#34;);  String birthday = sc.next();  Student stu = new Student();  stu.setId(id);  stu.setName(name);  stu.setAge(age);  stu.setBirthday(birthday);  return stu;  } } StudentService类\npublic class StudentService {  // 创建StudentDao (库管)  private StudentDao studentDao = new StudentDao();  // 添加学生方法  public boolean addStudent(Student stu) {  // 2. 将学生对象, 传递给StudentDao 库管中的addStudent方法  // 3. 将返回的boolean类型结果, 返还给StudentController  return studentDao.addStudent(stu);  }  // 判断学号是否存在方法  public boolean isExists(String id) {  Student[] stus = studentDao.findAllStudent();  // 假设id在数组中不存在  boolean exists = false;  // 遍历数组, 获取每一个学生对象, 准备进行判断  for (int i = 0; i \u0026lt; stus.length; i++) {  Student student = stus[i];  if(student != null \u0026amp;\u0026amp; student.getId().equals(id)){  exists = true;  break;  }  }   return exists;  }  // 查看学生方法  public Student[] findAllStudent() {  // 1. 调用库管对象的findAllStudent获取学生对象数组  Student[] allStudent = studentDao.findAllStudent();  // 2. 判断数组中是否有学生信息 (有: 返回地址, 没有: 返回null)  // 思路: 数组中只要存在一个不是null的元素, 那就代表有学生信息  boolean flag = false;  for (int i = 0; i \u0026lt; allStudent.length; i++) {  Student stu = allStudent[i];  if(stu != null){  flag = true;  break;  }  }   if(flag){  // 有信息  return allStudent;  }else{  // 没有信息  return null;  }   }   public void deleteStudentById(String delId) {  studentDao.deleteStudentById(delId);  }   public void updateStudent(String updateId, Student newStu) {  studentDao.updateStudent(updateId, newStu);  } } StudentDao类\npublic class StudentDao {  // 创建学生对象数组  private static Student[] stus = new Student[5];  // 添加学生方法  public boolean addStudent(Student stu) {   // 2. 添加学生到数组  //2.1 定义变量index为-1，假设数组已经全部存满，没有null的元素  int index = -1;  //2.2 遍历数组取出每一个元素，判断是否是null  for (int i = 0; i \u0026lt; stus.length; i++) {  Student student = stus[i];  if(student == null){  index = i;  //2.3 如果为null，让index变量记录当前索引位置，并使用break结束循环遍历  break;  }  }   // 3. 返回是否添加成功的boolean类型状态  if(index == -1){  // 装满了  return false;  }else{  // 没有装满, 正常添加, 返回true  stus[index] = stu;  return true;  }  }  // 查看学生方法  public Student[] findAllStudent() {  return stus;  }   public void deleteStudentById(String delId) {  // 1. 查找id在容器中所在的索引位置  int index = getIndex(delId);  // 2. 将该索引位置,使用null元素进行覆盖  stus[index] = null;  }   public int getIndex(String id){  int index = -1;  for (int i = 0; i \u0026lt; stus.length; i++) {  Student stu = stus[i];  if(stu != null \u0026amp;\u0026amp; stu.getId().equals(id)){  index = i;  break;  }  }  return index;  }   public void updateStudent(String updateId, Student newStu) {  // 1. 查找updateId, 在容器中的索引位置  int index = getIndex(updateId);  // 2. 将该索引位置, 使用新的学生对象替换  stus[index] = newStu;  } } 4.3老师管理系统 (应用) 4.3.1需求说明   添加老师: 通过键盘录入老师信息(id，name，age，birthday)\n使用数组存储老师信息,要求老师的id不能重复\n  删除老师: 通过键盘录入要删除老师的id值,将该老师从数组中移除,如果录入的id在数组中不存在,需要重新录入\n  修改老师: 通过键盘录入要修改老师的id值和修改后的老师信息\n将数组中该老师的信息修改,如果录入的id在数组中不存在,需要重新录入\n  查询老师: 将数组中存储的所有老师的信息输出到控制台\n  4.3.2实现步骤   环境搭建实现步骤\n   包 存储的类 作用     com.itheima.edu.info.manager.domain Student.java Teacher.java 封装学生信息 封装老师信息   com.itheima.edu.info.manager.dao StudentDao.java TeacherDao.java 访问存储数据的数组,进行赠删改查（库管）   com.itheima.edu.info.manager.service StudentService.java TeacherService.java 业务的逻辑处理（业务员）   com.itheima.edu.info.manager.controller StudentController.java TeacherController.java 和用户打交道（客服接待）   com.itheima.edu.info.manager.entry InfoManagerEntry.java 程序的入口类,提供一个main方法      菜单搭建实现步骤\n 展示欢迎页面,用输出语句完成主界面的编写 获取用户的选择,用Scanner实现键盘录入数据 根据用户的选择执行对应的操作,用switch语句完成操作的选择    添加功能实现步骤\n  查询功能实现步骤\n  删除功能实现步骤\n  修改功能实现步骤\n  系统优化\n 把updateTeacher和deleteTeacherById中录入老师id代码抽取到一个方法(inputTeacherId)中 该方法的主要作用就是录入老师的id,方法的返回值为String类型 把addTeacher和updateTeacher中录入老师信息的代码抽取到一个方法(inputTeacherInfo)中 该方法的主要作用就是录入老师的信息,并封装为老师对象,方法的返回值为Teacher类型    4.3.3代码实现 老师类\npublic class Teacher extends Person{ \tprivate String id;  private String name;  private String age;  private String birthday;  String address;   public Teacher() {  }   public Teacher(String id, String name, String age, String birthday) {  this.id = id;  this.name = name;  this.age = age;  this.birthday = birthday;  }   public String getId() {  return id;  }   public void setId(String id) {  this.id = id;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public String getAge() {  return age;  }   public void setAge(String age) {  this.age = age;  }   public String getBirthday() {  return birthday;  }   public void setBirthday(String birthday) {  this.birthday = birthday;  } } TeacherController类\npublic class TeacherController {   private Scanner sc = new Scanner(System.in);  private TeacherService teacherService = new TeacherService();   public void start() {   teacherLoop:  while (true) {  System.out.println(\u0026#34;--------欢迎来到 \u0026lt;老师\u0026gt; 管理系统--------\u0026#34;);  System.out.println(\u0026#34;请输入您的选择: 1.添加老师 2.删除老师 3.修改老师 4.查看老师 5.退出\u0026#34;);  String choice = sc.next();  switch (choice) {  case \u0026#34;1\u0026#34;:  // System.out.println(\u0026#34;添加老师\u0026#34;);  addTeacher();  break;  case \u0026#34;2\u0026#34;:  // System.out.println(\u0026#34;删除老师\u0026#34;);  deleteTeacherById();  break;  case \u0026#34;3\u0026#34;:  // System.out.println(\u0026#34;修改老师\u0026#34;);  updateTeacher();  break;  case \u0026#34;4\u0026#34;:  // System.out.println(\u0026#34;查看老师\u0026#34;);  findAllTeacher();  break;  case \u0026#34;5\u0026#34;:  System.out.println(\u0026#34;感谢您使用老师管理系统, 再见!\u0026#34;);  break teacherLoop;  default:  System.out.println(\u0026#34;您的输入有误, 请重新输入\u0026#34;);  break;  }  }   }   public void updateTeacher() {  String id = inputTeacherId();   Teacher newTeacher = inputTeacherInfo(id);   // 调用业务员的修改方法  teacherService.updateTeacher(id,newTeacher);  System.out.println(\u0026#34;修改成功\u0026#34;);  }   public void deleteTeacherById() {   String id = inputTeacherId();   // 2. 调用业务员中的删除方法, 根据id, 删除老师  teacherService.deleteTeacherById(id);   // 3. 提示删除成功  System.out.println(\u0026#34;删除成功\u0026#34;);    }   public void findAllTeacher() {  // 1. 从业务员中, 获取老师对象数组  Teacher[] teachers = teacherService.findAllTeacher();   // 2. 判断数组中是否有元素  if (teachers == null) {  System.out.println(\u0026#34;查无信息, 请添加后重试\u0026#34;);  return;  }   // 3. 遍历数组, 取出元素, 并打印在控制台  System.out.println(\u0026#34;学号\\t\\t姓名\\t年龄\\t生日\u0026#34;);  for (int i = 0; i \u0026lt; teachers.length; i++) {  Teacher t = teachers[i];  if (t != null) {  System.out.println(t.getId() + \u0026#34;\\t\u0026#34; + t.getName() + \u0026#34;\\t\u0026#34; + t.getAge() + \u0026#34;\\t\\t\u0026#34; + t.getBirthday());  }  }  }   public void addTeacher() {  String id;  while (true) {  // 1. 接收不存在的老师id  System.out.println(\u0026#34;请输入老师id:\u0026#34;);  id = sc.next();  // 2. 判断id是否存在  boolean exists = teacherService.isExists(id);   if (exists) {  System.out.println(\u0026#34;id已被占用, 请重新输入:\u0026#34;);  } else {  break;  }  }   Teacher t = inputTeacherInfo(id);   // 5. 将封装好的老师对象, 传递给TeacherService继续完成添加操作  boolean result = teacherService.addTeacher(t);   if (result) {  System.out.println(\u0026#34;添加成功\u0026#34;);  } else {  System.out.println(\u0026#34;添加失败\u0026#34;);  }  }   // 录入老师id  public String inputTeacherId(){  String id;  while(true){  System.out.println(\u0026#34;请输入id\u0026#34;);  id = sc.next();  boolean exists = teacherService.isExists(id);  if(!exists){  System.out.println(\u0026#34;您输入的id不存在, 请重新输入:\u0026#34;);  }else{  break;  }  }  return id;  }   // 录入老师信息, 封装为老师对象  public Teacher inputTeacherInfo(String id){  System.out.println(\u0026#34;请输入老师姓名:\u0026#34;);  String name = sc.next();  System.out.println(\u0026#34;请输入老师年龄:\u0026#34;);  String age = sc.next();  System.out.println(\u0026#34;请输入老师生日:\u0026#34;);  String birthday = sc.next();   Teacher t = new Teacher();  t.setId(id);  t.setName(name);  t.setAge(age);  t.setBirthday(birthday);   return t;  } } TeacherService类\npublic class TeacherService {   private TeacherDao teacherDao = new TeacherDao();   public boolean addTeacher(Teacher t) {  return teacherDao.addTeacher(t);  }   public boolean isExists(String id) {  // 1. 获取库管对象中的数组  Teacher[] teachers = teacherDao.findAllTeacher();   boolean exists = false;   // 2. 遍历数组, 取出每一个元素, 进行判断  for (int i = 0; i \u0026lt; teachers.length; i++) {  Teacher teacher = teachers[i];  if(teacher != null \u0026amp;\u0026amp; teacher.getId().equals(id)){  exists = true;  break;  }  }   return exists;  }   public Teacher[] findAllTeacher() {  Teacher[] allTeacher = teacherDao.findAllTeacher();   boolean flag = false;   for (int i = 0; i \u0026lt; allTeacher.length; i++) {  Teacher t = allTeacher[i];  if(t != null){  flag = true;  break;  }  }   if(flag){  return allTeacher;  }else{  return null;  }   }   public void deleteTeacherById(String id) {  teacherDao.deleteTeacherById(id);  }   public void updateTeacher(String id, Teacher newTeacher) {  teacherDao.updateTeacher(id,newTeacher);  } } TeacherDao类\npublic class TeacherDao {   private static Teacher[] teachers = new Teacher[5];   public boolean addTeacher(Teacher t) {  int index = -1;  for (int i = 0; i \u0026lt; teachers.length; i++) {  Teacher teacher = teachers[i];  if(teacher == null){  index = i;  break;  }  }   if(index == -1){  return false;  }else{  teachers[index] = t;  return true;  }   }   public Teacher[] findAllTeacher() {  return teachers;  }   public void deleteTeacherById(String id) {  // 1. 查询id在数组中的索引位置  int index = getIndex(id);  // 2. 将该索引位置的元素, 使用null进行替换  teachers[index] = null;  }   public int getIndex(String id){  int index = -1;  for (int i = 0; i \u0026lt; teachers.length; i++) {  Teacher t = teachers[i];  if(t != null \u0026amp;\u0026amp; t.getId().equals(id)){  index = i;  break;  }  }   return index;  }   public void updateTeacher(String id, Teacher newTeacher) {  int index = getIndex(id);  teachers[index] = newTeacher;  } } 5.static关键字 5.1static关键字概述 (理解) static 关键字是静态的意思,是Java中的一个修饰符,可以修饰成员方法,成员变量\n5.2static修饰的特点 (记忆)   被类的所有对象共享\n是我们判断是否使用静态关键字的条件\n  随着类的加载而加载，优先于对象存在\n对象需要类被加载后，才能创建\n  可以通过类名调用\n也可以通过对象名调用\n  5.3static关键字注意事项 (理解)  静态方法只能访问静态的成员 非静态方法可以访问静态的成员，也可以访问非静态的成员 静态方法中是没有this关键字  ","permalink":"https://iblog.zone/archives/java%E5%88%86%E7%B1%BB%E5%92%8Cstatic/","summary":"1.案例驱动模式 1.1案例驱动模式概述 (理解) 通过我们已掌握的知识点,先实现一个案例,然后找出这个案例中,存在的一些问题,在通过新知识点解决问题\n1.2案例驱动模式的好处 (理解)  解决重复代码过多的冗余,提高代码的复用性 解决业务逻辑聚集紧密导致的可读性差,提高代码的可读性 解决代码可维护性差,提高代码的维护性  2.分类思想 2.1分类思想概述 (理解) 分工协作,专人干专事\n2.2黑马信息管理系统 (理解)   Student类 标准学生类,封装键盘录入的学生信息(id , name , age , birthday)\n  StudentDao类 Dao : (Data Access Object 缩写) 用于访问存储数据的数组或集合\n  StudentService类 用来进行业务逻辑的处理(例如: 判断录入的id是否存在)\n  StudentController类 和用户打交道(接收用户需求,采集用户信息,打印数据到控制台)\n  3.分包思想 3.1分包思想概述 (理解) 如果将所有的类文件都放在同一个包下,不利于管理和后期维护,所以,对于不同功能的类文件,可以放在不同的包下进行管理\n3.2包的概述 (记忆)   包\n本质上就是文件夹\n  创建包\n多级包之间使用 \u0026quot; . \u0026quot; 进行分割 多级包的定义规范：公司的网站地址翻转(去掉www) 比如：黑马程序员的网站址为www.itheima.com 后期我们所定义的包的结构就是：com.itheima.其他的包名\n  包的命名规则","title":"Java分类和static"},{"content":"1.Git介绍 1.1版本控制(理解) 无论是代码编写，还是文档编写，我们都会遇到对文档内容反复修改的情况\n1.2开发中存在的问题(理解)  程序员小明负责的模块就要完成了，就在即将提交发布之前的一瞬间，电脑突然蓝屏，硬盘光荣下岗！  几个月来的努力付之东流\n  老王需要在项目中加入一个很复杂的功能，一边尝试，一边修改代码，就这样摸索了一个星期。 可是这被改得面目全非的代码已经回不到从前了。\n  小明和老王先后从文件服务器上下载了同一个文件\n  因项目中Bug过多，导致项目进度拖延，项目经理老徐因此被骂，但不清楚Bug是手下哪一个程序员写的\n  开发中要解决的问题\n 代码备份 版本控制 协同工作 责任追溯    1.3SVN版本控制(理解) SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而开发人员工作的时候，用的都是自己的电脑， 所以首先要从中央服务器下载最新的版本，然后开发，开发完后，需要把自己开发的代码提交到中央服务器。\n  服务器单点故障\n将会导致所有人员无法工作\n  而服务器硬盘损坏\n这意味着，你可能失去了该项目的所有历史记录，这是毁灭性的。\n  1.4Git版本控制(理解) Git是在2005年，Linux系统的创建者Linus Torvalds,为了帮助全球的开发者，维护Linux系统内核的开发 而开发了自己的开源分布式版本控制工具,分为两种类型的仓库：本地仓库和远程仓库。\n  每一个客户端都保存了完整的历史记录\n服务器的故障，都可以通过客户端的记录得以恢复。\n  2.Git下载和安装 2.1Git的下载(应用) 官网下载地址：https://git-scm.com/downloads\n2.2Git的安装(应用)   双击安装包，进入安装向导界面\n  指定安装目录\n  一路next下一步\n  等待安装\n  安装完成\n  安装完成后在电脑桌面（也可以是其他目录）点击右键，如果能够看到如下两个菜单则说明Git安装成功。\n  运行Git命令客户端，使用git \u0026ndash;version 命令，可以查看git版本\n  2.3TortoiseGit的安装(应用)   双击安装包，进入安装向导界面\n  一路next下一步\n  指定安装目录\n  安装\n  配置\n  安装TortoiseGit中文语言包,一路next即可\n  配置TortoiseGit中文语言\n  3.Git操作入门 3.1Git基本工作流程(理解) 本地仓库\n3.2Git命令行操作(应用)   git常用命令\n   命令 作用     git init 初始化，创建 git 仓库   git status 查看 git 状态 （文件是否进行了添加、提交操作）   git add 文件名 添加，将指定文件添加到暂存区   git commit -m \u0026lsquo;提交信息\u0026rsquo; 提交，将暂存区文件提交到历史仓库   git log 查看日志（ git 提交的历史日志）      操作步骤\n  创建工作目录、初始化本地 git 仓库\n  新建一个 test.txt 文件（暂不执行添加操作）\n  使用 status 命令，查看状态\n  使用 add 命令添加，并查看状态\n  使用 commit 命令，提交到本地历史仓库\n  使用 log 命令，查看日志\n  修改 test.txt 文件\n  添加并提交，查看日志\n    3.3Git图形化工具操作(理解)   创建工作目录、初始化本地 git 仓库\n  新建一个 test.txt 文件（暂不执行添加操作）\n  选中文件右键，选择TortoiseGit，之后选择添加\n  空白处右键,Git提交,提交到本地历史仓库\n  空白处右键,TortoiseGit,显示日志,可以产看日志信息\n  修改 test.txt 文件\n  添加并提交，查看日志\n  4.Git版本管理 4.1历史版本切换(理解)   准备动作\n 查看 my_project 的 log 日志 git reflog ：可以查看所有分支的所有操作记录（包括已经被删除的 commit 记录的操作） 增加一次新的修改记录    需求: 将代码切换到第二次修改的版本\n指令：git reset \u0026ndash;hard 版本唯一索引值\n  4.2分支管理介绍(理解)   分支\n 由每次提交的代码，串成的一条时间线 使用分支意味着你可以把你的工作从开发主线上分离开来,以免影响开发主线    分支的使用场景\n 周期较长的模块开发 假设你准备开发一个新功能，但是需要一个月才能完成 第一周写了20%的代码，突然发现原来已经写好的功能出现了一个严重的Bug 那现在就需要放下手中的新功能，去修复Bug 但这20%的代码不能舍弃，并且也担心丢失，这就需要开启一个新的版本控制。 尝试性的模块开发 业务人员给我们提出了一个需求，经过我们的思考和分析 该需求应该可以使用技术手段进行实现。 但是我们还不敢确定，我们就可以去创建一个分支基于分支进行尝试性开发。    分支工作流程\n  Master: 指向提交的代码版本\n  Header: 指向当前所使用的的分支\n    4.3分支管理操作(应用)   创建和切换\n创建命令：git branch 分支名 切换命令：git checkout 分支名\n  新分支添加文件\n查看文件命令：ls\n总结：不同分支之间的关系是平行的关系，不会相互影响\n  合并分支\n合并命令：git merge 分支名\n  删除分支\n删除命令：git branch -d 分支名\n  查看分支列表\n查看命令：git branch\n  5.远程仓库 5.1远程仓库工作流程(理解) 5.2远程仓库平台介绍(理解)   GitHub\n域名：https://github.com 介绍：GitHub是全球最大的开源项目托管平台，俗称大型程序员社区化交友网站\n​\t各类好玩有趣的开源项目，只有想不到，没有找不到。\n  码云\n域名：https://gitee.com 介绍：码云是全国最大的开源项目托管平台，良心平台，速度快，提供免费私有库\n  5.3码云的注册(应用) 5.4先有本地项目,远程为空(应用)   步骤\n 创建本地仓库 创建或修改文件，添加（add）文件到暂存区，提交（commit）到本地仓库 创建远程仓库 推送到远程仓库    创建远程仓库\n  生成SSH公钥\n  推送代码之前，需要先配置SSH公钥\n  生成SSH公钥步骤\n  设置Git账户\n  git config user.name（查看git账户）\n  git config user.email（查看git邮箱）\n  git config \u0026ndash;global user.name “账户名”（设置全局账户名）\n  git config \u0026ndash;global user.email “邮箱”（设置全局邮箱）\n  cd ~/.ssh（查看是否生成过SSH公钥）\n    生成SSH公钥\n  生成命令: ssh-keygen –t rsa –C “邮箱” ( 注意：这里需要敲3次回车)\n  查看命令: cat ~/.ssh/id-rsa.pub\n    设置账户公钥\n  公钥测试\n  命令: ssh -T git@gitee.com\n        推送到远程仓库\n  步骤\n 为远程仓库的URL（网址），自定义仓库名称 推送    命令 git remote add 远程名称 远程仓库URL git push -u 仓库名称 分支名\n    5.5先有远程仓库,本地为空(应用)  步骤  将远程仓库的代码，克隆到本地仓库 克隆命令：git clone 仓库地址 创建新文件，添加并提交到本地仓库 推送至远程仓库 项目拉取更新 拉取命令：git pull 远程仓库名 分支名    5.6代码冲突(应用)   产生原因:\n两个程序员操作同一个文件,其中一个程序员在修改文件后,push到远程仓库,另一个程序员应该先pull将最新的代码更新到本地仓库后,在修改代码,之后push到远程仓库,结果他没有先pull将最新的代码更新到本地仓库,而是直接将自己的代码push到远程仓库,这样就可能会导致代码冲突\n    如何解决冲突\n\u0026laquo;\u0026laquo;\u0026laquo;\u0026lt;和\u0026raquo;\u0026raquo;\u0026raquo;\u0026gt;中间的内容,就是冲突部分\n 修改冲突行，保存，即可解决冲突。 重新add冲突文件并commit到本地仓库，重新push到远程    6.IDEA集成Git 6.1IDEA中配置Git(应用)   File -\u0026gt; Settings\n  Version Control -\u0026gt; Git -\u0026gt; 指定git.exe存放目录\n  点击Test测试\n  6.2创建本地仓库(应用)   VCS-\u0026gt;Import into Version Control-\u0026gt;Create Git Repository\n  选择工程所在的目录,这样就创建好本地仓库了\n  点击git后边的对勾,将当前项目代码提交到本地仓库\n注意: 项目中的配置文件不需要提交到本地仓库中,提交时,忽略掉即可\n  6.3版本切换(应用)   方式一: 控制台Version Control-\u0026gt;Log-\u0026gt;Reset Current Branch\u0026hellip;-\u0026gt;Reset\n这种切换的特点是会抛弃原来的提交记录\n  方式二:控制台Version Control-\u0026gt;Log-\u0026gt;Revert Commit-\u0026gt;Merge-\u0026gt;处理代码-\u0026gt;commit\n这种切换的特点是会当成一个新的提交记录,之前的提交记录也都保留\n  6.4分支管理(应用)   创建分支\nVCS-\u0026gt;Git-\u0026gt;Branches-\u0026gt;New Branch-\u0026gt;给分支起名字-\u0026gt;ok\n  切换分支\nidea右下角Git-\u0026gt;选择要切换的分支-\u0026gt;checkout\n  合并分支\nVCS-\u0026gt;Git-\u0026gt;Merge changes-\u0026gt;选择要合并的分支-\u0026gt;merge\n处理分支中的代码\n  删除分支\nidea右下角-\u0026gt;选中要删除的分支-\u0026gt;Delete\n  6.5本地仓库推送到远程仓库(应用)   VCS-\u0026gt;Git-\u0026gt;Push-\u0026gt;点击master Define remote\n  将远程仓库的路径复制过来-\u0026gt;Push\n  6.6远程仓库克隆到本地仓库(应用) File-\u0026gt;Close Project-\u0026gt;Checkout from Version Control-\u0026gt;Git-\u0026gt;指定远程仓库的路径-\u0026gt;指定本地存放的路径-\u0026gt;clone\n","permalink":"https://iblog.zone/archives/git%E7%9A%84%E5%AE%89%E8%A3%85%E5%8F%8A%E4%BD%BF%E7%94%A8/","summary":"1.Git介绍 1.1版本控制(理解) 无论是代码编写，还是文档编写，我们都会遇到对文档内容反复修改的情况\n1.2开发中存在的问题(理解)  程序员小明负责的模块就要完成了，就在即将提交发布之前的一瞬间，电脑突然蓝屏，硬盘光荣下岗！  几个月来的努力付之东流\n  老王需要在项目中加入一个很复杂的功能，一边尝试，一边修改代码，就这样摸索了一个星期。 可是这被改得面目全非的代码已经回不到从前了。\n  小明和老王先后从文件服务器上下载了同一个文件\n  因项目中Bug过多，导致项目进度拖延，项目经理老徐因此被骂，但不清楚Bug是手下哪一个程序员写的\n  开发中要解决的问题\n 代码备份 版本控制 协同工作 责任追溯    1.3SVN版本控制(理解) SVN是集中式版本控制系统，版本库是集中放在中央服务器的，而开发人员工作的时候，用的都是自己的电脑， 所以首先要从中央服务器下载最新的版本，然后开发，开发完后，需要把自己开发的代码提交到中央服务器。\n  服务器单点故障\n将会导致所有人员无法工作\n  而服务器硬盘损坏\n这意味着，你可能失去了该项目的所有历史记录，这是毁灭性的。\n  1.4Git版本控制(理解) Git是在2005年，Linux系统的创建者Linus Torvalds,为了帮助全球的开发者，维护Linux系统内核的开发 而开发了自己的开源分布式版本控制工具,分为两种类型的仓库：本地仓库和远程仓库。\n  每一个客户端都保存了完整的历史记录\n服务器的故障，都可以通过客户端的记录得以恢复。\n  2.Git下载和安装 2.1Git的下载(应用) 官网下载地址：https://git-scm.com/downloads\n2.2Git的安装(应用)   双击安装包，进入安装向导界面\n  指定安装目录\n  一路next下一步\n  等待安装\n  安装完成","title":"Git的安装及使用"},{"content":"1.ArrayList 集合和数组的区别 :\n​\t共同点：都是存储数据的容器\n​\t不同点：数组的容量是固定的，集合的容量是可变的\n1.1 -ArrayList的构造方法和添加方法    public ArrayList() 创建一个空的集合对象     public boolean add(E e) 将指定的元素追加到此集合的末尾   public void add(int index,E element) 在此集合中的指定位置插入指定的元素    ArrayList\u0026lt;E\u0026gt; ：\n​\t可调整大小的数组实现\n​\t\u0026lt;E\u0026gt; : 是一种特殊的数据类型，泛型。\n怎么用呢 ?\n​\t在出现E的地方我们使用引用数据类型替换即可\n​\t举例：ArrayList\u0026lt;String\u0026gt;, ArrayList\u0026lt;Student\u0026gt;\n1.2ArrayList类常用方法【应用】 **成员方法 : **\n   public boolean remove(Object o) 删除指定的元素，返回删除是否成功     public E remove(int index) 删除指定索引处的元素，返回被删除的元素   public E set(int index,E element) 修改指定索引处的元素，返回被修改的元素   public E get(int index) 返回指定索引处的元素   public int size() 返回集合中的元素的个数    示例代码 :\npublic class ArrayListDemo02 {  public static void main(String[] args) {  //创建集合  ArrayList\u0026lt;String\u0026gt; array = new ArrayList\u0026lt;String\u0026gt;();   //添加元素  array.add(\u0026#34;hello\u0026#34;);  array.add(\u0026#34;world\u0026#34;);  array.add(\u0026#34;java\u0026#34;);   //public boolean remove(Object o)：删除指定的元素，返回删除是否成功 // System.out.println(array.remove(\u0026#34;world\u0026#34;)); // System.out.println(array.remove(\u0026#34;javaee\u0026#34;));   //public E remove(int index)：删除指定索引处的元素，返回被删除的元素 // System.out.println(array.remove(1));   //IndexOutOfBoundsException // System.out.println(array.remove(3));   //public E set(int index,E element)：修改指定索引处的元素，返回被修改的元素 // System.out.println(array.set(1,\u0026#34;javaee\u0026#34;));   //IndexOutOfBoundsException // System.out.println(array.set(3,\u0026#34;javaee\u0026#34;));   //public E get(int index)：返回指定索引处的元素 // System.out.println(array.get(0)); // System.out.println(array.get(1)); // System.out.println(array.get(2));  //System.out.println(array.get(3)); //？？？？？？ 自己测试   //public int size()：返回集合中的元素的个数  System.out.println(array.size());   //输出集合  System.out.println(\u0026#34;array:\u0026#34; + array);  } } 1.3 ArrayList存储字符串并遍历 案例需求 :\n​\t创建一个存储字符串的集合，存储3个字符串元素，使用程序实现在控制台遍历该集合\n实现步骤 :\n1:创建集合对象 2:往集合中添加字符串对象 3:遍历集合，首先要能够获取到集合中的每一个元素，这个通过get(int index)方法实现 4:遍历集合，其次要能够获取到集合的长度，这个通过size()方法实现 5:遍历集合的通用格式 代码实现 :\n/* 思路： 1:创建集合对象 2:往集合中添加字符串对象 3:遍历集合，首先要能够获取到集合中的每一个元素，这个通过get(int index)方法实现 4:遍历集合，其次要能够获取到集合的长度，这个通过size()方法实现 5:遍历集合的通用格式 */ public class ArrayListTest01 {  public static void main(String[] args) {  //创建集合对象  ArrayList\u0026lt;String\u0026gt; array = new ArrayList\u0026lt;String\u0026gt;();   //往集合中添加字符串对象  array.add(\u0026#34;刘正风\u0026#34;);  array.add(\u0026#34;左冷禅\u0026#34;);  array.add(\u0026#34;风清扬\u0026#34;);   //遍历集合，其次要能够获取到集合的长度，这个通过size()方法实现 // System.out.println(array.size());   //遍历集合的通用格式  for(int i=0; i\u0026lt;array.size(); i++) {  String s = array.get(i);  System.out.println(s);  }  } } 1.4 ArrayList存储学生对象并遍历 案例需求 :\n​\t创建一个存储学生对象的集合，存储3个学生对象，使用程序实现在控制台遍历该集合\n**实现步骤 : **\n1:定义学生类 2:创建集合对象 3:创建学生对象 4:添加学生对象到集合中 5:遍历集合，采用通用遍历格式实现 代码实现 :\n/* 思路： 1:定义学生类 2:创建集合对象 3:创建学生对象 4:添加学生对象到集合中 5:遍历集合，采用通用遍历格式实现 */ public class ArrayListTest02 {  public static void main(String[] args) {  //创建集合对象  ArrayList\u0026lt;Student\u0026gt; array = new ArrayList\u0026lt;\u0026gt;();   //创建学生对象  Student s1 = new Student(\u0026#34;林青霞\u0026#34;, 30);  Student s2 = new Student(\u0026#34;风清扬\u0026#34;, 33);  Student s3 = new Student(\u0026#34;张曼玉\u0026#34;, 18);   //添加学生对象到集合中  array.add(s1);  array.add(s2);  array.add(s3);   //遍历集合，采用通用遍历格式实现  for (int i = 0; i \u0026lt; array.size(); i++) {  Student s = array.get(i);  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge());  }  } } 1.5 键盘录入学生信息到集合 案例需求 :\n​\t创建一个存储学生对象的集合，存储3个学生对象，使用程序实现在控制台遍历该集合\n 学生的姓名和年龄来自于键盘录入\n 实现步骤 :\n1:定义学生类，为了键盘录入数据方便，把学生类中的成员变量都定义为String类型 2:创建集合对象 3:键盘录入学生对象所需要的数据 4:创建学生对象，把键盘录入的数据赋值给学生对象的成员变量 5:往集合中添加学生对象 6:遍历集合，采用通用遍历格式实现 代码实现 :\n/* 思路： 1:定义学生类，为了键盘录入数据方便，把学生类中的成员变量都定义为String类型 2:创建集合对象 3:键盘录入学生对象所需要的数据 4:创建学生对象，把键盘录入的数据赋值给学生对象的成员变量 5:往集合中添加学生对象 6:遍历集合，采用通用遍历格式实现 */ public class ArrayListTest {  public static void main(String[] args) {  //创建集合对象  ArrayList\u0026lt;Student\u0026gt; array = new ArrayList\u0026lt;Student\u0026gt;();   //为了提高代码的复用性，我们用方法来改进程序  addStudent(array);  addStudent(array);  addStudent(array);   //遍历集合，采用通用遍历格式实现  for (int i = 0; i \u0026lt; array.size(); i++) {  Student s = array.get(i);  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge());  }  }   /* 两个明确： 返回值类型：void 参数：ArrayList\u0026lt;Student\u0026gt; array */  public static void addStudent(ArrayList\u0026lt;Student\u0026gt; array) {  //键盘录入学生对象所需要的数据  Scanner sc = new Scanner(System.in);   System.out.println(\u0026#34;请输入学生姓名:\u0026#34;);  String name = sc.nextLine();   System.out.println(\u0026#34;请输入学生年龄:\u0026#34;);  String age = sc.nextLine();   //创建学生对象，把键盘录入的数据赋值给学生对象的成员变量  Student s = new Student();  s.setName(name);  s.setAge(age);   //往集合中添加学生对象  array.add(s);  } } 2. 学生管理系统 2.1 学生管理系统实现步骤   案例需求\n​\t针对目前我们的所学内容，完成一个综合案例：学生管理系统！该系统主要功能如下：\n​\t添加学生：通过键盘录入学生信息，添加到集合中\n​\t删除学生：通过键盘录入要删除学生的学号，将该学生对象从集合中删除\n​\t修改学生：通过键盘录入要修改学生的学号，将该学生对象其他信息进行修改\n​\t查看学生：将集合中的学生对象信息进行展示\n​\t退出系统：结束程序\n  实现步骤\n  定义学生类，包含以下成员变量\n学生类：\tStudent成员变量：\n​\t学号：sid\n​\t姓名：name\n​\t年龄：age\n​\t生日：birthday\n​\t构造方法：\n​\t无参构造\n​\t带四个参数的构造成员方法：\n​\t每个成员变量对应给出get/set方法\n  学生管理系统主界面的搭建步骤\n2.1 用输出语句完成主界面的编写\n2.2 用Scanner实现键盘录入数据\n2.3 用switch语句完成操作的选择\n2.4 用循环完成再次回到主界面\n  学生管理系统的添加学生功能实现步骤\n3.1 用键盘录入选择添加学生\n3.2 定义一个方法，用于添加学生\n​\t显示提示信息，提示要输入何种信息\n​\t键盘录入学生对象所需要的数据\n​\t创建学生对象，把键盘录入的数据赋值给学生对象的成员变量\n​\t将学生对象添加到集合中（保存）\n​\t给出添加成功提示\n3.3 调用方法\n  学生管理系统的查看学生功能实现步骤\n4.1 用键盘录入选择查看所有学生信息\n4.2 定义一个方法，用于查看学生信息\n​\t显示表头信息\n​\t将集合中数据取出按照对应格式显示学生信息，年龄显示补充“岁”\n4.3 调用方法\n  学生管理系统的删除学生功能实现步骤\n5.1 用键盘录入选择删除学生信息\n5.2 定义一个方法，用于删除学生信息\n​\t显示提示信息\n​\t键盘录入要删除的学生学号\n​\t调用getIndex方法，查找该学号在集合的索引\n​\t如果索引为-1，提示信息不存在\n​\t如果索引不是-1，调用remove方法删除并提示删除成功\n5.3 调用方法\n  学生管理系统的修改学生功能实现步骤\n6.1 用键盘录入选择修改学生信息\n6.2 定义一个方法，用于修改学生信息\n​\t显示提示信息\n​\t键盘录入要修改的学生学号\n​\t调用getIndex方法，查找该学号在集合的索引\n​\t如果索引为-1，提示信息不存在\n​\t如果索引不是-1，键盘录入要修改的学生信息\n​\t集合修改对应的学生信息\n​\t给出修改成功提示\n6.3 调用方法\n  退出系统\n使用System.exit(0);退出JVM\n    2.2 学生类的定义 package com.itheima.domain;  public class Student {  private String sid; // 学号  private String name; // 姓名  private int age; // 年龄  private String birthday; // 生日   public Student() {  }   public Student(String sid, String name, int age, String birthday) {  this.sid = sid;  this.name = name;  this.age = age;  this.birthday = birthday;  }   public String getSid() {  return sid;  }   public void setSid(String sid) {  this.sid = sid;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   public String getBirthday() {  return birthday;  }   public void setBirthday(String birthday) {  this.birthday = birthday;  } } 2.3 测试类的定义 package com.itheima.test;  import com.itheima.domain.Student;  import java.util.ArrayList; import java.util.Scanner;  public class StudentManager {  public static void main(String[] args) {   Scanner sc = new Scanner(System.in);   // 创建集合容器对象  ArrayList\u0026lt;Student\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();   lo:  while (true) {  // 1. 搭建主界面菜单  System.out.println(\u0026#34;--------欢迎来到学生管理系统--------\u0026#34;);  System.out.println(\u0026#34;1 添加学生\u0026#34;);  System.out.println(\u0026#34;2 删除学生\u0026#34;);  System.out.println(\u0026#34;3 修改学生\u0026#34;);  System.out.println(\u0026#34;4 查看学生\u0026#34;);  System.out.println(\u0026#34;5 退出\u0026#34;);  System.out.println(\u0026#34;请输入您的选择:\u0026#34;);   String choice = sc.next();   switch (choice) {  case \u0026#34;1\u0026#34;:  //System.out.println(\u0026#34;添加学生\u0026#34;);  addStudent(list);  break;  case \u0026#34;2\u0026#34;:  //System.out.println(\u0026#34;删除学生\u0026#34;);  deleteStudent(list);  break;  case \u0026#34;3\u0026#34;:  //System.out.println(\u0026#34;修改学生\u0026#34;);  updateStudent(list);  break;  case \u0026#34;4\u0026#34;:  // System.out.println(\u0026#34;查看学生\u0026#34;);  queryStudents(list);  break;  case \u0026#34;5\u0026#34;:  System.out.println(\u0026#34;感谢您的使用\u0026#34;);  break lo;  default:  System.out.println(\u0026#34;您的输入有误\u0026#34;);  break;  }  }    }   // 修改学生的方法  public static void updateStudent(ArrayList\u0026lt;Student\u0026gt; list) {  System.out.println(\u0026#34;请输入您要修改的学生学号:\u0026#34;);  Scanner sc = new Scanner(System.in);  String updateSid = sc.next();  // 3. 调用getIndex方法, 查找该学号在集合中出现的索引位置  int index = getIndex(list,updateSid);  // 4. 根据索引判断, 学号在集合中是否存在  if(index == -1){  // 不存在: 给出提示  System.out.println(\u0026#34;查无信息, 请重新输入\u0026#34;);  }else{  // 存在: 接收新的学生信息  System.out.println(\u0026#34;请输入新的学生姓名:\u0026#34;);  String name = sc.next();  System.out.println(\u0026#34;请输入新的学生年龄:\u0026#34;);  int age = sc.nextInt();  System.out.println(\u0026#34;请输入新的学生生日:\u0026#34;);  String birthday = sc.next();  // 封装为新的学生对象  Student stu = new Student(updateSid, name, age, birthday);  // 调用集合的set方法, 完成修改  list.set(index, stu);  System.out.println(\u0026#34;修改成功!\u0026#34;);  }  }   // 删除学生的方法  public static void deleteStudent(ArrayList\u0026lt;Student\u0026gt; list) {  // 1. 给出提示信息 (请输入您要删除的学号)  System.out.println(\u0026#34;请输入您要删除的学生学号:\u0026#34;);  // 2. 键盘接收要删除的学号  Scanner sc = new Scanner(System.in);  String deleteSid = sc.next();  // 3. 调用getIndex方法, 查找该学号在集合中出现的索引位置  int index = getIndex(list,deleteSid);  // 4. 根据索引判断, 学号在集合中是否存在  if(index == -1){  // 不存在: 给出提示  System.out.println(\u0026#34;查无信息, 请重新输入\u0026#34;);  }else{  // 存在:删除  list.remove(index);  System.out.println(\u0026#34;删除成功!\u0026#34;);  }  }   // 查看学生的方法  public static void queryStudents(ArrayList\u0026lt;Student\u0026gt; list) {  // 1. 判断集合中是否存在数据, 如果不存在直接给出提示  if(list.size() == 0){  System.out.println(\u0026#34;无信息, 请添加后重新查询\u0026#34;);  return;  }  // 2. 存在: 展示表头数据  System.out.println(\u0026#34;学号\\t\\t姓名\\t年龄\\t生日\u0026#34;);  // 3. 遍历集合, 获取每一个学生对象的信息, 打印在控制台  for (int i = 0; i \u0026lt; list.size(); i++) {  Student stu = list.get(i);  System.out.println(stu.getSid() + \u0026#34;\\t\u0026#34; + stu.getName() + \u0026#34;\\t\u0026#34; + stu.getAge() + \u0026#34;\\t\\t\u0026#34; + stu.getBirthday());  }  }   // 添加学生的方法  public static void addStudent(ArrayList\u0026lt;Student\u0026gt; list) {  Scanner sc = new Scanner(System.in);  // 1. 给出录入的提示信息   String sid;   while(true){  System.out.println(\u0026#34;请输入学号:\u0026#34;);  sid = sc.next();   int index = getIndex(list, sid);   if(index == -1){  // sid不存在, 学号可以使用  break;  }  }   System.out.println(\u0026#34;请输入姓名:\u0026#34;);  String name = sc.next();  System.out.println(\u0026#34;请输入年龄:\u0026#34;);  int age = sc.nextInt();  System.out.println(\u0026#34;请输入生日:\u0026#34;);  String birthday = sc.next();  // 2. 将键盘录入的信息封装为学生对象  Student stu = new Student(sid,name,age,birthday);  // 3. 将封装好的学生对象, 添加到集合容器当中  list.add(stu);  // 4. 给出添加成功的提示信息  System.out.println(\u0026#34;添加成功!\u0026#34;);  }   /* getIndex : 接收一个集合对象, 接收一个学生学号 查找这个学号, 在集合中出现的索引位置 */  public static int getIndex(ArrayList\u0026lt;Student\u0026gt; list, String sid){  // 1. 假设传入的学号, 在集合中不存在  int index = -1;  // 2. 遍历集合, 获取每一个学生对象, 准备进行查找  for (int i = 0; i \u0026lt; list.size(); i++) {  Student stu = list.get(i);  // 3. 获取每一个学生对象的学号  String id = stu.getSid();  // 4. 使用获取出的学生学号, 和传入的学号(查找的学号)进行比对  if(id.equals(sid)){  // 存在: 让index变量记录正确的索引位置  index = i;  }  }   return index;  } } ","permalink":"https://iblog.zone/archives/java-arraylist%E9%9B%86%E5%90%88%E5%AD%A6%E7%94%9F%E7%AE%A1%E7%90%86%E7%B3%BB%E7%BB%9F/","summary":"1.ArrayList 集合和数组的区别 :\n​\t共同点：都是存储数据的容器\n​\t不同点：数组的容量是固定的，集合的容量是可变的\n1.1 -ArrayList的构造方法和添加方法    public ArrayList() 创建一个空的集合对象     public boolean add(E e) 将指定的元素追加到此集合的末尾   public void add(int index,E element) 在此集合中的指定位置插入指定的元素    ArrayList\u0026lt;E\u0026gt; ：\n​\t可调整大小的数组实现\n​\t\u0026lt;E\u0026gt; : 是一种特殊的数据类型，泛型。\n怎么用呢 ?\n​\t在出现E的地方我们使用引用数据类型替换即可\n​\t举例：ArrayList\u0026lt;String\u0026gt;, ArrayList\u0026lt;Student\u0026gt;\n1.2ArrayList类常用方法【应用】 **成员方法 : **\n   public boolean remove(Object o) 删除指定的元素，返回删除是否成功     public E remove(int index) 删除指定索引处的元素，返回被删除的元素   public E set(int index,E element) 修改指定索引处的元素，返回被修改的元素   public E get(int index) 返回指定索引处的元素   public int size() 返回集合中的元素的个数    示例代码 :","title":"Java ArrayList集合\u0026学生管理系统"},{"content":"前言 因资源成本问题，本Harbor高可用架构为最小开销方案，如果资源充足，可以将PG、Redis全部使用使用云厂商集群模式。\n同时为了配置简单，并没有使用keepalived与heartbeat等高可用开源组件。\n准备工作    阿里云SLB 阿里云ECS 共享存储 Redis     最小实例SLB 2c4g 2台 阿里云NFS 阿里云Redis    操作系统为Ubuntu18.04，在2台ECS上搭建主从PG，如果不想用阿里云redis，也可以使用ECS搭建Redis。\n安装Harbor，用于导出基础harbor数据，恢复到PG集群中.   安装docker-compose\ncurl -L \u0026#34;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo add-apt-repository \u0026#34;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs)stable\u0026#34; # 添加国内阿里云 curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - #更新 sudo apt-get update [[查看docker]]版本 apt-cache madison docker-ce #安装最新版 sudo apt-get install -y docker-ce [[安装5]]:19.03.6~3-0~ubuntu-bionic版 sudo apt-get install -y docker-ce=5:19.03.6~3-0~ubuntu-bionic   Docker配置镜像加速与国内docker-cn源\nsudo tee /etc/docker/daemon.json \u0026lt;\u0026lt;-\u0026#39;EOF\u0026#39; { \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://8sab4djv.mirror.aliyuncs.com\u0026#34;], \u0026#34;registry-mirrors\u0026#34;: [\u0026#34;https://registry.docker-cn.com\u0026#34;], \u0026#34;insecure-registries\u0026#34;: [\u0026#34;https://harbor.unixsre.com\u0026#34;] } EOF sudo systemctl daemon-reload sudo systemctl restart docker   安装Harbor2.3\n# 下载Harbor wget -P /usr/local wget https://github.com/goharbor/harbor/releases/download/v2.3.2/harbor-online-installer-v2.3.2.tgz  tar zxf /usr/local/harbor-online-installer-v2.3.2.tgz -C /data/harbor  # 修改配置文件，根据自己的需求进行修改 cd /data/harbor cp harbor.yml.tmpl harbor.yml # harbor.yml中按需修改或添加如下内容 # Configuration file of Harbor  # The IP address or hostname to access admin UI and registry service. # DO NOT use localhost or 127.0.0.1, because Harbor needs to be accessed by external clients. hostname: harbor.unixsre.com  # http related config http:  # port for http, default is 80. If https enabled, this port will redirect to https port  port: 80  # https related config https:  # https port for harbor, default is 443  port: 443  # The path of cert and key files for nginx  certificate: /data/harbor/ssl/unixsre.com.cer  private_key: /data/harbor/ssl/unixsre.com.key  # # Uncomment following will enable tls communication between all harbor components # internal_tls: # # set enabled to true means internal tls is enabled # enabled: true # # put your cert and key files on dir # dir: /etc/harbor/tls/internal  # Uncomment external_url if you want to enable external proxy # And when it enabled the hostname will no longer used # external_url: https://reg.mydomain.com:8433  # The initial password of Harbor admin # It only works in first time to install harbor # Remember Change the admin password from UI after launching Harbor. # 初始密码，可以修改成自己需要的，然后后续在WEBUI上自行修改。 harbor_admin_password: 1234567 ## 添加禁止用户自注册 self_registration: off ## 设置只有管理员可以创建项目 project_creation_restriction: adminonly # The default data volume data_volume: /data/harbor # 执行安装命令 bash /data/harbor/install.sh # 如果对配置文件harbor.yml，需要使用./prepare脚本重新生成 ./prepare # 重启 docker-compose restart   常用命令示例\n# 登录 docker login https://harbor.unixsre.com # 拉取 docker pull busybox # 打包 docker build -t busybox:v1 . docker build -t busybox:v1 -f Dockerfile . # 打TAG docker tag busybox:latest harbor.unixsre.com/ops/busybox:latest # 上传 docker push harbor.unixsre.com/library/busybox:latest # k3s pull k3s crictl pull harbor.unixsre.com/library/busybox   备份Harbor使用到的数据库，并且导出用于恢复.\n# 进入容器备份 docker container exec -it harbor-db /bin/bash # 执行pg备份 pg_dump -U postgres registry \u0026gt; /tmp/registry.sql pg_dump -U postgres notarysigner \u0026gt; /tmp/notarysigner.sql pg_dump -U postgres notaryserver \u0026gt; /tmp/notaryserver.sql # 复制到本地宿主机 docker container cp harbor-db:/tmp/registry.sql /data/harbor/backup_sql/ docker container cp harbor-db:/tmp/notarysigner.sql /data/harbor/backup_sql/ docker container cp harbor-db:/tmp/notaryserver.sql /data/harbor/backup_sql/   安装PG主从集群 PostgreSql主从复制是一种高可用解决方案，可以实现读写分离，实时备份，PG的主从复制是基于xlog来实现的，主库开启日志功能，从库根据主库xlog来完成数据的同步。\nPG主从复制注意事项：\n 启动从库之前: 不能执行初始化，若已经初始化了需要删掉对应的目录中的数据文件。 启动从库之前: 需要通过base_backup从主服务器上同步配置与数据。 启动从库之前: 需要对同步之后的配置文件（standby.signal）进行修改。 从库只能读，不能写。    分别在每个ECS安装postgresql-13\n# 添加PG apt源 sh -c \u0026#39;echo \u0026#34;deb http://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\u0026#34; \u0026gt; /etc/apt/sources.list.d/pgdg.list\u0026#39; wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add - # 更新源 apt-get update # 安装PG13 apt -y install postgresql-13 postgresql-client-13 postgresql-contrib # 验证服务是否启动成功 systemctl status postgresql@13-main.service # 登录验证修改密码 sudo -i -u postgres psql -p 5432 ALTER USER postgres WITH PASSWORD \u0026#39;1234567.com\u0026#39;; # 登录验证 psql -h localhost -p 5432 -U postgres   创建PG数据目录，分别在每个机器上创建.\n#创建数据目录 mkdir -p /data/harbor_nas/pgsql/data \u0026amp;\u0026amp; chown postgres:postgres /data/harbor_nas/pgsql/data #创建归档目录 mkdir -p /data/harbor_nas/pgsql/pg_archive \u0026amp;\u0026amp; chown postgres:postgres /data/harbor_nas/pgsql/pg_archive #给目录赋权 chmod 700 /data/harbor_nas/pgsql/pg_archive/ \u0026amp;\u0026amp; chmod 700 /data/harbor_nas/pgsql/data/   添加systemd启动配置文件中的数据目录环境变量.\nvim /lib/systemd/system/postgresql@.service Environment=PGDATA=/data/harbor_nas/pgsql/data # 重载 systemctl daemon-reload # 删除默认集群 pg_dropcluster --stop 13 main # 在新目录创建集群 pg_createcluster -d /data/harbor_nas/pgsql/data 13 main # 重启服务 systemctl restart postgresql@13-main.service # 配置开机启动 systemctl enable postgresql@13-main.service #开启外部访问配置 vim /etc/postgresql/13/main/pg_hba.conf local all postgres peer  # TYPE DATABASE USER ADDRESS METHOD  # \u0026#34;local\u0026#34; is for Unix domain socket connections only local all all peer # IPv4 local connections: host all all 0.0.0.0/0 md5 # IPv6 local connections: host all all ::1/128 md5 # Allow replication connections from localhost, by a user with the # replication privilege. local replication all peer host replication all 127.0.0.1/32 md5 host replication all ::1/128 md5 # 修改集群监听地址 vim /etc/postgresql/13/main/postgresql.conf listen_addresses = \u0026#39;*\u0026#39; # 重启服务 systemctl restart postgresql@13-main.service   主服务器配置\n# 创建具有复制流操作权限的的用户：replica CREATE ROLE replica login replication encrypted password \u0026#39;Deniss_12PRO@@@\u0026#39;; # 添加从服务器免密登录,replica为用户,172.19.48.254X为从节点的内网IP，md5为允许密码验证, trust为免密。 vim /etc/postgresql/13/main/pg_hba.conf host replication replica 172.19.48.254/20 trust # 添加主服务器postgresql.conf配置 vim /etc/postgresql/13/main/postgresql.conf listen_addresses = \u0026#39;*\u0026#39; max_connections = 100 archive_mode = on archive_command = \u0026#39;test ! -f /data/harbor_nas/pgsql/pg_archive/%f \u0026amp;\u0026amp; cp %p /data/harbor_nas/pgsql/pg_archive/%f\u0026#39; wal_level = replica # 重启服务 systemctl restart postgresql@13-main.service   从服务器配置\n# 如果前面已经在从服务器执行过了这个操作，直接可以进入postgres用户家目录清理、复制数据。 #创建数据目录 mkdir -p /data/harbor_nas/pgsql_replica/data \u0026amp;\u0026amp; chown postgres:postgres /data/harbor_nas/pgsql_replica/data #创建归档目录 mkdir -p /data/harbor_nas/pgsql_replica/pg_archive \u0026amp;\u0026amp; chown postgres:postgres /data/harbor_nas/pgsql_replica/pg_archive #给目录赋权 chmod 700 /data/harbor_nas/pgsql_replica/pg_archive/ \u0026amp;\u0026amp; chmod 700 /data/harbor_nas/pgsql_replica/data/ # 添加如下配置 vim /lib/systemd/system/postgresql@.service Environment=PGDATA=/data/harbor_nas/pgsql_replica/data/ # 重载配置 systemctl daemon-reload #删除默认目录的集群 pg_dropcluster --stop 13 main #在新目录创建集群 pg_createcluster -d /data/harbor_nas/pgsql_replica/data 13 main #重启服务 systemctl restart postgresql@13-main.service # 进入postgres用户清理初始化的数据，从主服务器复制数据。 su - postgres rm -rf /data/harbor_nas/pgsql_replica/data/* pg_basebackup -h 172.19.48.253 -p 5432 -U replica -Fp -Xs -Pv -R -D /data/harbor_nas/pgsql_replica/data echo \u0026#34;standby_mode = \u0026#39;on\u0026#39;\u0026#34; \u0026gt; /data/harbor_nas/pgsql_replica/data/standby.signal # 修改从服务器配置 vim /etc/postgresql/13/main/postgresql.conf primary_conninfo = \u0026#39;host=172.19.48.253 port=5432 user=replica password=Deniss_12PRO@@@\u0026#39; recovery_target_timeline = latest max_connections = 100 hot_standby = on max_standby_streaming_delay = 30s wal_receiver_status_interval = 10s hot_standby_feedback = on # 启动从节点PG数据库 systemctl start postgresql@13-main.service # 登录主节点数据库查看装 psql -h 172.19.48.253 -p 5432 -U postgres postgres=# select client_addr,sync_state from pg_stat_replication;  client_addr | sync_state ---------------+------------  172.19.48.254 | async # 至此，PG主从复制安装完成。   配置Horbor为PG主节点   登录主节点创建harbor用户与harbor需要的DB，并且将数据恢复到当前数据.\n# 新建Harbor用户 CREATE USER harbor LOGIN PASSWORD \u0026#39;Deniss1112s\u0026#39;; CREATE SCHEMA harbor; GRANT harbor TO postgres;GRANT USAGE ON SCHEMA harbor TO postgres; ALTER SCHEMA harbor OWNER TO postgres; # 创建数据库 CREATE DATABASE registry OWNER harbor; CREATE DATABASE notarysigner OWNER harbor; CREATE DATABASE notaryserver OWNER harbor; # 授权 GRANT ALL PRIVILEGES ON DATABASE registry TO harbor; GRANT ALL PRIVILEGES ON DATABASE notarysigner TO harbor; GRANT ALL PRIVILEGES ON DATABASE notaryserver TO harbor; # 恢复数据库 psql -h localhost -U harbor registry \u0026lt; /data/harbor/backup_sql/registry.sql psql -h localhost -U harbor notarysigner \u0026lt; /data/harbor/backup_sql/notarysigner.sql psql -h localhost -U harbor notaryserver \u0026lt; /data/harbor/backup_sql/notaryserver.sql   对2个ECS的harbor.yml进行调整，开启外部PG、Redis配置，注释掉默认PG数据库配置，注意：2个ECS中Harbor链接的配置的必须为同样的Redis与PG数据库。\nhostname: harbor.unixsre.com  http:  port: 80  https:  port: 443  certificate: /data/harbor/ssl/unixsre.com.cer  private_key: /data/harbor/ssl/unixsre.com.key  harbor_admin_password: 1234567  data_volume: /data/harbor_nas/harbor_data  trivy:  ignore_unfixed: false  skip_update: false  insecure: false  jobservice:  max_job_workers: 10  notification:  webhook_job_max_retry: 10  chart:  absolute_url: disabled  log:  level: info  local:  rotate_count: 50  rotate_size: 200M  location: /var/log/harbor  _version: 2.3.0  external_database:  harbor:  host: 172.19.48.253  port: 5432  db_name: registry  username: harbor  password: Deniss1112s  ssl_mode: disable  max_idle_conns: 2  max_open_conns: 0  notary_signer:  host: 172.19.48.253  port: 5432  db_name: notarysigner  username: harbor  password: Deniss1112s  ssl_mode: disable  notary_server:  host: 172.19.48.253  port: 5432  db_name: notaryserver  username: harbor  password: Deniss1112s  ssl_mode: disable  external_redis:  host: 172.19.48.253:6379  password: Deniss1589s  registry_db_index: 1  jobservice_db_index: 2  chartmuseum_db_index: 3  trivy_db_index: 5  idle_timeout_seconds: 30  proxy:  http_proxy:  https_proxy:  no_proxy:  components:  - core  - jobservice  - trivy   harbor重新生成配置，并且重启容器.\ncd /data/harbor/ ./prepare docker-compose down \u0026amp;\u0026amp; docker-compose up -d   在阿里云创建传统SLB，使用TCP四层添加443端口监听。\n  将域名绑定在新建的SLB上，这个SLB不一定非要是阿里云的，任何云的SLB都可以，比如AWS、微软云、GCP都可以。\n  PG主从故障切换 假设主库宕机或者主节点宕机，因为我们的Redis在阿里云，而Harbor的镜像数据在阿里云的NFS，要保证服务的可用性，这个时候，只需要快速的将从节点切换为主库，并且修改Harbor的配置文件，重启Harbor的服务下即可。\n下面为手动操作，建议调整为脚本执行快速切换。\n  模拟当前主节点库挂掉,\n# 停止主数据库的PG服务. service postgresql@13-main stop   激活备库为主库.\npsql -h 172.19.48.254 -p 5432 -U postgres postgres=# select pg_promote(true,60); # 验证是否升级为主库 /usr/lib/postgresql/13/bin/pg_controldata -D /data/harbor_nas/pgsql_replica/data/ |grep cluster Database cluster state: in production   修改Harbor配置，重启所有Harbor服务\n#  sed -i \u0026#39;s/172.19.48.253/172.19.48.254/\u0026#39; ./prepare docker-compose down \u0026amp;\u0026amp; docker-compose up -d   访问域名，验证harbor服务的可用性。\n  快速恢复主节点，将主节点的PG库设置为从库。\n# 修改253从库免密配置，可以提前设置好，不需要此处配置了 /etc/postgresql/13/main/pg_hba.conf host replication replica 172.19.48.253/20 trust # 切换用户 su - postgres # 清理数据 rm -rf /data/harbor_nas/pgsql/data/* # 同步254数据到253 pg_basebackup -h 172.19.48.254 -p 5432 -U replica -Fp -Xs -Pv -R -D /data/harbor_nas/pgsql/data/ echo \u0026#34;standby_mode = \u0026#39;on\u0026#39;\u0026#34; \u0026gt; /data/harbor_nas/pgsql/data/standby.signal # 修改253配置 vim /etc/postgresql/13/main/postgresql.conf primary_conninfo = \u0026#39;host=172.19.48.254 port=5432 user=replica password=Deniss_12PRO@@@\u0026#39; recovery_target_timeline = latest max_connections = 100 hot_standby = on max_standby_streaming_delay = 30s wal_receiver_status_interval = 10s hot_standby_feedback = on # 启动253PG服务 systemctl start postgresql@13-main.service   在当前主节点254登录验证集群复制是否正常.\n# 登录节点验证当前同步是否正常 psql -h localhost -p 5432 -U postgres postgres=# select client_addr,sync_state from pg_stat_replication;  client_addr | sync_state ---------------+------------  172.19.48.253 | async   如果想将原来的库基本恢复成主库，只需要清理掉standby.signal文件，在原来的从库上的数据目录中新建standby.signal文件，并且将standby_mode = 'on'配置好，重启PG服务即可。\n  灾难性故障恢复 对于不可抗拒因素是比较极端的情况，任何人都无法预料，包括当前的各种云厂商，我们只把能想到的，能做到的全部做好，我这边已经做了PG数据库的全备上传到了OSS上，Harbor的镜像数据阿里云NFS一份，OSS一份，想要灾难性恢复必须保证如下俩个前提：\n PG数据库全备可用（注意：必须可以承受丢失全备时间起止到故障时间的数据）。 阿里云NFS或者OSS中的Harbor镜像数据文件可用。  恢复步骤：搭建一个单节点PG，全备导入，Harbor中的配置使用单节点PG，Redis本地或者harbor启动的都可以，然后使用docker-compose启动即可，具体操作步骤不在叙述。\n但是这样并不是最快的方法，还有没有更好的方案呢？当然有了，使用云服务，一切都交给云，但是就算是云也不可能保证100%的可用性，此处的灾难性故障恢复，仅做抛砖引玉，并不是最终的解决方案，只是给大家提供一个可以展开思考的思路，如果大家有更完美完善的方案，欢迎一起交流。\n","permalink":"https://iblog.zone/archives/%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E9%AB%98%E5%8F%AF%E7%94%A8harbor/","summary":"前言 因资源成本问题，本Harbor高可用架构为最小开销方案，如果资源充足，可以将PG、Redis全部使用使用云厂商集群模式。\n同时为了配置简单，并没有使用keepalived与heartbeat等高可用开源组件。\n准备工作    阿里云SLB 阿里云ECS 共享存储 Redis     最小实例SLB 2c4g 2台 阿里云NFS 阿里云Redis    操作系统为Ubuntu18.04，在2台ECS上搭建主从PG，如果不想用阿里云redis，也可以使用ECS搭建Redis。\n安装Harbor，用于导出基础harbor数据，恢复到PG集群中.   安装docker-compose\ncurl -L \u0026#34;https://github.com/docker/compose/releases/download/1.24.0/docker-compose-$(uname -s)-$(uname -m)\u0026#34; -o /usr/local/bin/docker-compose sudo chmod +x /usr/local/bin/docker-compose sudo add-apt-repository \u0026#34;deb [arch=amd64] http://mirrors.aliyun.com/docker-ce/linux/ubuntu $(lsb_release -cs)stable\u0026#34; # 添加国内阿里云 curl -fsSL http://mirrors.aliyun.com/docker-ce/linux/ubuntu/gpg | sudo apt-key add - #更新 sudo apt-get update [[查看docker]]版本 apt-cache madison docker-ce #安装最新版 sudo apt-get install -y docker-ce [[安装5]]:19.","title":"生产环境搭建高可用Harbor"},{"content":"背景 容器技术的一个最佳实践是构建尽可能精简的容器镜像。但这一实践却会给排查问题带来麻烦：精简后的容器中普遍缺失常用的排障工具，部分容器里甚至没有 shell (比如 FROM scratch ）。 在这种状况下，我们只能通过日志或者到宿主机上通过 docker-cli 或 nsenter 来排查问题，效率很低，在K8s环境部署应用后，经常遇到需要进入pod进行排错。除了查看pod logs和describe方式之外，传统的解决方式是在业务pod基础镜像中提前安装好procps、net-tools、tcpdump、vim等工具。但这样既不符合最小化镜像原则，又徒增Pod安全漏洞风险。\n今天为大家推荐一款K8s pod诊断工具，kubectl-debug是一个简单、易用、强大的 kubectl 插件, 能够帮助你便捷地进行 Kubernetes 上的 Pod 排障诊断。它通过启动一个排错工具容器，并将其加入到目标业务容器的pid, network, user 以及 ipc namespace 中，这时我们就可以在新容器中直接用 netstat, tcpdump 这些熟悉的工具来解决问题了, 而业务容器可以保持最小化, 不需要预装任何额外的排障工具。 kubectl-debug 主要包含以下两部分:\n kubectl-debug：命令行工具 debug-agent：部署在K8s的node上，用于启动关联排错工具容器  工作原理 我们知道，容器本质上是带有 cgroup 资源限制和 namespace 隔离的一组进程。因此，我们只要启动一个进程，并且让这个进程加入到目标容器的各种 namespace 中，这个进程就能 “进入容器内部”（注意引号），与容器中的进程”看到”相同的根文件系统、虚拟网卡、进程空间了——这也正是 docker exec 和 kubectl exec 等命令的运行方式。\n现在的状况是，我们不仅要 “进入容器内部”，还希望带一套工具集进去帮忙排查问题。那么，想要高效管理一套工具集，又要可以跨平台，最好的办法就是把工具本身都打包在一个容器镜像当中。 接下来，我们只需要通过这个”工具镜像”启动容器，再指定这个容器加入目标容器的的各种 namespace，自然就实现了 “携带一套工具集进入容器内部”。事实上，使用 docker-cli 就可以实现这个操作：\nexport TARGET_ID=666666666 # 加入目标容器的 network, pid 以及 ipc namespace docker run -it --network=container:$TARGET_ID --pid=container:$TARGET_ID --ipc=container:$TARGET_ID busybox 这就是 kubectl-debug 的出发点： 用工具容器来诊断业务容器 。背后的设计思路和 sidecar 等模式是一致的：每个容器只做一件事情。\n具体到实现上，一条 kubectl debug命令背后逻辑流程是这样的：\n步骤分别是:\n 插件查询 ApiServer：demo-pod 是否存在，所在节点是什么 ApiServer 返回 demo-pod 所在所在节点 插件请求在目标节点上创建 Debug Agent Pod Kubelet 创建 Debug Agent Pod 插件发现 Debug Agent 已经 Ready，发起 debug 请求（长连接） Debug Agent 收到 debug 请求，创建 Debug 容器并加入目标容器的各个 Namespace 中，创建完成后，与 Debug 容器的 tty 建立连接  接下来，客户端就可以开始通过 5，6 这两个连接开始 debug 操作。操作结束后，Debug Agent 清理 Debug 容器，插件清理 Debug Agent，一次 Debug 完成\n安装 github地址：https://github.com/aylei/kubectl-debug\n Mac 可以直接使用 brew 安装  brew install aylei/tap/kubectl-debug  通过下载二进制文件安装  export PLUGIN_VERSION=0.1.1 # linux x86_64 curl -Lo kubectl-debug.tar.gz https://github.com/aylei/kubectl-debug/releases/download/v${PLUGIN_VERSION}/kubectl-debug_${PLUGIN_VERSION}_linux_amd64.tar.gz # macos curl -Lo kubectl-debug.tar.gz https://github.com/aylei/kubectl-debug/releases/download/v${PLUGIN_VERSION}/kubectl-debug_${PLUGIN_VERSION}_darwin_amd64.tar.gz  tar -zxvf kubectl-debug.tar.gz kubectl-debug sudo mv kubectl-debug /usr/local/bin/ Windows 用户可以在Release 页面选择进行下载windows版本，加入环境变量使用\n其中github上有提供debug agent以DaemonSet的方式安装在集群中，但是daemonset模式，agent pod预先部署在所有node上，会始终占用资源，对于排错调试频率不高的环境造成资源浪费\n日常用法说明 简单使用 1、kubectl 1.12.0 或更高的版本, 可以直接使用\n#查看常用命令参数 kubectl debug -h kubectl 从 1.12 版本之后开始支持从 PATH 中自动发现插件。1.12 版本之前的 kubectl 不支持这种插件机制，但也可以通过命令名 kubectl-debug 直接调用。\n2、假如安装了 debug-agent 的 daemonset, 可以略去 \u0026ndash;agentless 来加快启动速度，之后的命令里会略去 \u0026ndash;agentless\nkubectl debug POD_NAME --daemonset-ns=default --daemonset-name=debug-agent 其中github上有提供debug agent以DaemonSet的方式安装在集群中，但是daemonset模式，agent pod预先部署在所有node上，会始终占用资源，对于排错调试频率不高的环境造成资源浪费，部署方式：kubectl apply -f https://raw.githubusercontent.com/aylei/kubectl-debug/master/scripts/agent_daemonset.yml\n3、agentless模式，kubectl-debug执行命令后，才创建agent pod和排错工具容器，并在退出后删除工具容器和agent pod。由于每次执行都要重新拉起agent，启动会比daemon-set模式稍慢。使用-a, \u0026ndash;agentless开启agentless模式：\nkubectl debug POD_NAME --agentless --port-forward 4、假如 Node 没有公网 IP 或无法直接访问(防火墙等原因), 请使用 port-forward 模式\nkubectl debug POD_NAME --agentless --port-forward 进阶使用 1、排错init-container\nkubectl debug POD_NAME --container=init-pod 2、假如 Pod 处于 CrashLookBackoff 状态无法连接, 可以复制一个完全相同的 Pod 来进行诊断\nkubectl debug POD_NAME --fork 自定义镜像配置 --image：可自定义排错工具容器镜像，改为私有镜像仓库，默认为nicolaka/netshoot:latest --agent-image：在agentless模式下，自定义debug-agent镜像，默认为aylei/debug-agent:latest。在daemon-set模式下，直接将debug-agent daemonset pod template修改为私有仓库镜像即可 配置文件 ~/.kube/debug-config，通过配置文件修改默认参数，免去使用命令时设置flag。\n# debug agent listening port(outside container) default to 10027 agentPort: 10027 whether using agentless mode default to false agentless: true namespace of debug-agent pod, used in agentless mode default to \u0026#39;default\u0026#39; agentPodNamespace: default prefix of debug-agent pod, used in agentless mode default to \u0026#39;debug-agent-pod\u0026#39; agentPodNamePrefix: debug-agent-pod image of debug-agent pod, used in agentless mode default to \u0026#39;aylei/debug-agent:latest\u0026#39; agentImage: aylei/debug-agent:latest daemonset name of the debug-agent, used in port-forward default to \u0026#39;debug-agent\u0026#39; debugAgentDaemonset: debug-agent daemonset namespace of the debug-agent, used in port-forwad default to \u0026#39;default\u0026#39; debugAgentNamespace: kube-system whether using port-forward when connecting debug-agent default false portForward: true image of the debug container default as showed image: nicolaka/netshoot:latest start command of the debug container default [\u0026#39;bash\u0026#39;] command: - \u0026#39;/bin/bash\u0026#39; - \u0026#39;-l\u0026#39; 典型案例 使用 iftop查看pod的网络流量 比如查看POD_NAME是kube-flannel-ds-amd64-2xwqp的网络流量：\n ~  kubectl debug kube-flannel-ds-amd64-2xwqp -n kube-system Agent Pod info: [Name:debug-agent-pod-b14bd868-61a9-11ec-bc72-acbc328370f3, Namespace:default, Image:registry.cn-hangzhou.aliyuncs.com/querycapimages/kubectl-debug-agent:latest, HostPort:10027, ContainerPort:10027] Waiting for pod debug-agent-pod-b14bd868-61a9-11ec-bc72-acbc328370f3 to run... Forwarding from 127.0.0.1:10027 -\u0026gt; 10027 Forwarding from [::1]:10027 -\u0026gt; 10027 Handling connection for 10027  set container procfs correct false .. pulling image registry.cn-hangzhou.aliyuncs.com/querycapimages/netshoot:latest, skip TLS false... latest: Pulling from querycapimages/netshoot Digest: sha256:f0eba49c9bf66600788d58779e57c2d7334708e12cb292ff8ccc9414c1b6730c Status: Image is up to date for registry.cn-hangzhou.aliyuncs.com/querycapimages/netshoot:latest starting debug container... container created, open tty... bash-5.0# iftop -i eth0 interface: eth0 IP address is: 172.17.3.3 MAC address is: 52:54:be:83:3a:e4 使用 drill 诊断 DNS 解析 比如查看POD_NAME是kube-flannel-ds-amd64-2xwqp的网络流量：\nbash-5.0# drill any www.baidu.com ;; -\u0026gt;\u0026gt;HEADER\u0026lt;\u0026lt;- opcode: QUERY, rcode: NOERROR, id: 3214 ;; flags: qr rd ra ; QUERY: 1, ANSWER: 1, AUTHORITY: 5, ADDITIONAL: 3 ;; QUESTION SECTION: ;; www.baidu.com. IN ANY  ;; ANSWER SECTION: www.baidu.com. 803 IN CNAME www.a.shifen.com.  ;; AUTHORITY SECTION: baidu.com. 38993 IN NS ns4.baidu.com. baidu.com. 38993 IN NS ns3.baidu.com. baidu.com. 38993 IN NS ns7.baidu.com. baidu.com. 38993 IN NS dns.baidu.com. baidu.com. 38993 IN NS ns2.baidu.com.  ;; ADDITIONAL SECTION: ns2.baidu.com. 19348 IN A 220.181.33.31 ns3.baidu.com. 23022 IN A 112.80.248.64 ns7.baidu.com. 20697 IN A 180.76.76.92  ;; Query time: 1 msec ;; SERVER: 100.64.9.5 ;; WHEN: Mon Dec 20 15:37:35 2021 ;; MSG SIZE rcvd: 196 drill 命令详解：https://commandnotfound.cn/linux/1/533/drill-%E5%91%BD%E4%BB%A4\n使用 tcpdump 抓包 比如查看POD_NAME是kube-flannel-ds-amd64-2xwqp的网络流量：\nbash-5.0# tcpdump -i eth0 -c 1 -Xvv tcpdump: listening on eth0, link-type EN10MB (Ethernet), capture size 262144 bytes 15:39:27.577342 IP (tos 0x0, ttl 63, id 41476, offset 0, flags [DF], proto TCP (6), length 89)  198.19.116.60.16710 \u0026gt; 172.17.3.3.6443: Flags [P.], cksum 0xf831 (correct), seq 677521811:677521848, ack 1388710574, win 1037, options [nop,nop,TS val 2849535414 ecr 1924260089], length 37  0x0000: 4500 0059 a204 4000 3f06 b036 c613 743c E..Y..@.?..6..t\u0026lt;  0x0010: ac11 0303 4146 192b 2862 2993 52c6 0aae ....AF.+(b).R...  0x0020: 8018 040d f831 0000 0101 080a a9d8 75b6 .....1........u.  0x0030: 72b1 e0f9 1703 0300 2047 49f1 8fbb 2835 r........GI...(5  0x0040: 059a 5e82 0746 afaf bd2d 5af3 c797 16b5 ..^..F...-Z.....  0x0050: 8709 4666 7e61 6f5a 0b ..Ff~aoZ. 1 packet captured 18 packets received by filter 0 packets dropped by kernel bash-5.0# tcpdump -n -vvv -w /tmp/kube-flannel-ds-amd64-2xwqp.pcap tcpdump: listening on veth19416cac, link-type EN10MB (Ethernet), capture size 262144 bytes 50 packets captured 50 packets received by filter 0 packets dropped by kernel 这里需要注意，如果是想拿到-w抓包保存的文件用wireshark工具分析，则需要去POD_NAME对应的宿主机上拷贝出来进行分析\n[root@k8s-demo-master-01-2 ~]# docker ps |grep netshoot 58b918b67b3f registry.cn-hangzhou.aliyuncs.com/querycapimages/netshoot:latest \u0026#34;bash\u0026#34; 15 minutes ago Up 15 minutes unruffled_fermat [root@k8s-demo-master-01-2 ~]# docker cp 58b918b67b3f:/tmp/kube-flannel-ds-amd64-2xwqp.pcap . [root@k8s-demo-master-01-2 ~]# ll |grep kube-flannel-ds-amd64-2xwqp.pcap -rw-r--r-- 1 root root 5404 12月 20 23:41 kube-flannel-ds-amd64-2xwqp.pcap 诊断 CrashLoopBackoff 排查 CrashLoopBackoff 是一个很麻烦的问题，Pod 可能会不断重启， kubectl exec 和 kubectl debug 都没法稳定进行排查问题，基本上只能寄希望于 Pod 的日志中打印出了有用的信息。 为了让针对 CrashLoopBackoff 的排查更方便， kubectl-debug 参考 oc debug 命令，添加了一个 \u0026ndash;fork 参数。当指定 \u0026ndash;fork 时，插件会复制当前的 Pod Spec，做一些小修改， 再创建一个新 Pod：\n 新 Pod 的所有 Labels 会被删掉，避免 Service 将流量导到 fork 出的 Pod 上 新 Pod 的 ReadinessProbe 和 LivnessProbe 也会被移除，避免 kubelet 杀死 Pod 新 Pod 中目标容器（待排障的容器）的启动命令会被改写，避免新 Pod 继续 Crash  接下来，我们就可以在新 Pod 中尝试复现旧 Pod 中导致 Crash 的问题，示例pod_name为srv-es-driver-7445f6cf48-ff7bq的go服务。为了保证操作的一致性，可以先 chroot 到目标容器的根文件系统中：\n ~  kubectl-debug srv-es-driver-7445f6cf48-ff7bq -n devops --agentless --port-forward Agent Pod info: [Name:debug-agent-pod-177482f4-61ad-11ec-b297-acbc328370f3, Namespace:default, Image:registry.cn-hangzhou.aliyuncs.com/querycapimages/kubectl-debug-agent:latest, HostPort:10027, ContainerPort:10027] Waiting for pod debug-agent-pod-177482f4-61ad-11ec-b297-acbc328370f3 to run... Forwarding from 127.0.0.1:10027 -\u0026gt; 10027 Forwarding from [::1]:10027 -\u0026gt; 10027 Handling connection for 10027  set container procfs correct false .. pulling image registry.cn-hangzhou.aliyuncs.com/querycapimages/netshoot:latest, skip TLS false... latest: Pulling from querycapimages/netshoot Digest: sha256:f0eba49c9bf66600788d58779e57c2d7334708e12cb292ff8ccc9414c1b6730c Status: Image is up to date for registry.cn-hangzhou.aliyuncs.com/querycapimages/netshoot:latest starting debug container... container created, open tty... bash-5.0# ls bin mnt sys dev opt termshark_2.1.1_linux_x64 etc proc tmp home root usr lib run var lib64 sbin media srv bash-5.0# chroot /proc/1/root root@srv-es-driver-7445f6cf48-ff7bq:/# ls bin dev go lib media opt root sbin sys usr boot etc home lib64 mnt proc run srv tmp var root@srv-es-driver-7445f6cf48-ff7bq:/# cd /go/bin/ root@srv-es-driver-7445f6cf48-ff7bq:/go/bin# ls openapi.json srv-es-driver root@srv-es-driver-7445f6cf48-ff7bq:/go/bin# ./srv-es-driver  # 观察执行启动脚本时的信息并根据信息进一步排障 自定义image作为sidercar安装命令行调试 对于没有安装yum,apt-get 的镜像可以挂载 centos或者ubuntu的sidercar镜像, 再进行操作, 如安装 redis 命令, 再使用redis-cli 命令\n ~  kubectl-debug srv-es-driver-7445f6cf48-ff7bq -n devops --agentless --port-forward --image centos Agent Pod info: [Name:debug-agent-pod-f5077b08-61ad-11ec-8728-acbc328370f3, Namespace:default, Image:registry.cn-hangzhou.aliyuncs.com/querycapimages/kubectl-debug-agent:latest, HostPort:10027, ContainerPort:10027] Waiting for pod debug-agent-pod-f5077b08-61ad-11ec-8728-acbc328370f3 to run... Forwarding from 127.0.0.1:10027 -\u0026gt; 10027 Forwarding from [::1]:10027 -\u0026gt; 10027 Handling connection for 10027  set container procfs correct false .. pulling image centos, skip TLS false... latest: Pulling from library/centos a1d0c7532777: Pull complete Digest: sha256:a27fd8080b517143cbbbab9dfb7c8571c40d67d534bbdee55bd6c473f432b177 Status: Downloaded newer image for centos:latest starting debug container... container created, open tty... [root@srv-es-driver-7445f6cf48-ff7bq /]# yum install -y redis  参考链接：\nhttps://aleiwu.com/post/kubectl-debug-intro/\n","permalink":"https://iblog.zone/archives/%E8%B6%85%E5%A5%BD%E7%94%A8%E7%9A%84k8s%E4%B8%ADpod%E8%AF%8A%E6%96%AD%E5%B7%A5%E5%85%B7kubectl-debug/","summary":"背景 容器技术的一个最佳实践是构建尽可能精简的容器镜像。但这一实践却会给排查问题带来麻烦：精简后的容器中普遍缺失常用的排障工具，部分容器里甚至没有 shell (比如 FROM scratch ）。 在这种状况下，我们只能通过日志或者到宿主机上通过 docker-cli 或 nsenter 来排查问题，效率很低，在K8s环境部署应用后，经常遇到需要进入pod进行排错。除了查看pod logs和describe方式之外，传统的解决方式是在业务pod基础镜像中提前安装好procps、net-tools、tcpdump、vim等工具。但这样既不符合最小化镜像原则，又徒增Pod安全漏洞风险。\n今天为大家推荐一款K8s pod诊断工具，kubectl-debug是一个简单、易用、强大的 kubectl 插件, 能够帮助你便捷地进行 Kubernetes 上的 Pod 排障诊断。它通过启动一个排错工具容器，并将其加入到目标业务容器的pid, network, user 以及 ipc namespace 中，这时我们就可以在新容器中直接用 netstat, tcpdump 这些熟悉的工具来解决问题了, 而业务容器可以保持最小化, 不需要预装任何额外的排障工具。 kubectl-debug 主要包含以下两部分:\n kubectl-debug：命令行工具 debug-agent：部署在K8s的node上，用于启动关联排错工具容器  工作原理 我们知道，容器本质上是带有 cgroup 资源限制和 namespace 隔离的一组进程。因此，我们只要启动一个进程，并且让这个进程加入到目标容器的各种 namespace 中，这个进程就能 “进入容器内部”（注意引号），与容器中的进程”看到”相同的根文件系统、虚拟网卡、进程空间了——这也正是 docker exec 和 kubectl exec 等命令的运行方式。\n现在的状况是，我们不仅要 “进入容器内部”，还希望带一套工具集进去帮忙排查问题。那么，想要高效管理一套工具集，又要可以跨平台，最好的办法就是把工具本身都打包在一个容器镜像当中。 接下来，我们只需要通过这个”工具镜像”启动容器，再指定这个容器加入目标容器的的各种 namespace，自然就实现了 “携带一套工具集进入容器内部”。事实上，使用 docker-cli 就可以实现这个操作：\nexport TARGET_ID=666666666 # 加入目标容器的 network, pid 以及 ipc namespace docker run -it --network=container:$TARGET_ID --pid=container:$TARGET_ID --ipc=container:$TARGET_ID busybox 这就是 kubectl-debug 的出发点： 用工具容器来诊断业务容器 。背后的设计思路和 sidecar 等模式是一致的：每个容器只做一件事情。","title":"超好用的k8s中pod诊断工具：kubectl-debug"},{"content":"1.API 1.1 API概述-帮助文档的使用   什么是API\n​\tAPI (Application Programming Interface) ：应用程序编程接口\n  java中的API\n​\t指的就是 JDK 中提供的各种功能的 Java类，这些类将底层的实现封装了起来，我们不需要关心这些类是如何实现的，只需要学习这些类如何使用即可，我们可以通过帮助文档来学习这些API如何使用。\n  如何使用API帮助文档 :\n  打开帮助文档\n  找到索引选项卡中的输入框\n  在输入框中输入Random\n  看类在哪个包下\n  看类的描述\n  看构造方法\n  看成员方法\n  1.2 键盘录入字符串 Scanner类 :\n​\tnext() : 遇到了空格, 就不再录入数据了 , 结束标记: 空格, tab键\n​\tnextLine() : 可以将数据完整的接收过来 , 结束标记: 回车换行符\n代码实现 :\npackage com.itheima.api;  import java.util.Scanner;  public class Demo1Scanner {  /* next() : 遇到了空格, 就不再录入数据了 结束标记: 空格, tab键 nextLine() : 可以将数据完整的接收过来 结束标记: 回车换行符 */  public static void main(String[] args) {  // 1. 创建Scanner对象  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入:\u0026#34;);  // 2. 调用nextLine方法接收字符串  // ctrl + alt + v : 快速生成方法的返回值  String s = sc.nextLine();   System.out.println(s);  } } package com.itheima.api;  import java.util.Scanner;  public class Demo2Scanner {  /* nextInt和nextLine方法配合使用的时候, nextLine方法就没有键盘录入的机会了 建议: 今后键盘录入数据的时候, 如果是字符串和整数一起接受, 建议使用next方法接受字符串. */  public static void main(String[] args) {  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入整数:\u0026#34;);  int num = sc.nextInt(); // 10 + 回车换行  System.out.println(\u0026#34;请输入字符串:\u0026#34;);  String s = sc.nextLine();    System.out.println(num);  System.out.println(s);  } } 2. String类 2.1 String概述 ​\t1 String 类在 java.lang 包下，所以使用的时候不需要导包\n​\t2 String 类代表字符串，Java 程序中的所有字符串文字（例如“abc”）都被实现为此类的实例也就是说，Java 程序中所有的双引号字符串，都是 String 类的对象\n​\t3 字符串不可变，它们的值在创建后不能被更改\n2.2 String类的构造方法 常用的构造方法\n示例代码\npackage com.itheima.string;  public class Demo2StringConstructor {  /* String类常见构造方法: public String() : 创建一个空白字符串对象，不含有任何内容 public String(char[] chs) : 根据字符数组的内容，来创建字符串对象 public String(String original) : 根据传入的字符串内容，来创建字符串对象 String s = “abc”; 直接赋值的方式创建字符串对象，内容就是abc 注意: String这个类比较特殊, 打印其对象名的时候, 不会出现内存地址 而是该对象所记录的真实内容. 面向对象-继承, Object类 */  public static void main(String[] args) {  // public String() : 创建一个空白字符串对象，不含有任何内容  String s1 = new String();  System.out.println(s1);   // public String(char[] chs) : 根据字符数组的内容，来创建字符串对象  char[] chs = {\u0026#39;a\u0026#39;,\u0026#39;b\u0026#39;,\u0026#39;c\u0026#39;};  String s2 = new String(chs);  System.out.println(s2);   // public String(String original) : 根据传入的字符串内容，来创建字符串对象  String s3 = new String(\u0026#34;123\u0026#34;);  System.out.println(s3);  } } 2.4 创建字符串对象的区别对比   通过构造方法创建\n​\t通过 new 创建的字符串对象，每一次 new 都会申请一个内存空间，虽然内容相同，但是地址值不同\n  直接赋值方式创建\n​\t以“”方式给出的字符串，只要字符序列相同(顺序和大小写)，无论在程序代码中出现几次，JVM 都只会建立一个 String 对象，并在字符串池中维护\n  2.5 字符串的比较 2.5.1 字符串的比较  == 比较基本数据类型：比较的是具体的值 == 比较引用数据类型：比较的是对象地址值  String类 : public boolean equals(String s) 比较两个字符串内容是否相同、区分大小写\n代码 :\npackage com.itheima.stringmethod;  public class Demo1Equals {  public static void main(String[] args) {  String s1 = \u0026#34;abc\u0026#34;;  String s2 = \u0026#34;ABC\u0026#34;;  String s3 = \u0026#34;abc\u0026#34;;   // equals : 比较字符串内容, 区分大小写  System.out.println(s1.equals(s2));  System.out.println(s1.equals(s3));   // equalsIgnoreCase : 比较字符串内容, 忽略大小写  System.out.println(s1.equalsIgnoreCase(s2));  } } 2.6 用户登录案例【应用】 案例需求 :\n​\t已知用户名和密码，请用程序实现模拟用户登录。总共给三次机会，登录之后，给出相应的提示\n**实现步骤 : **\n 已知用户名和密码，定义两个字符串表示即可 键盘录入要登录的用户名和密码，用 Scanner 实现 拿键盘录入的用户名、密码和已知的用户名、密码进行比较，给出相应的提示。 字符串的内容比较，用equals() 方法实现 用循环实现多次机会，这里的次数明确，采用for循环实现，并在登录成功的时候，使用break结束循  代码实现 :\npackage com.itheima.test;  import java.util.Scanner;  public class Test1 {  /* 需求：已知用户名和密码，请用程序实现模拟用户登录。 总共给三次机会，登录之后，给出相应的提示 思路： 1. 已知用户名和密码，定义两个字符串表示即可 2. 键盘录入要登录的用户名和密码，用 Scanner 实现 3. 拿键盘录入的用户名、密码和已知的用户名、密码进行比较，给出相应的提示。 字符串的内容比较，用equals() 方法实现 4. 用循环实现多次机会，这里的次数明确，采用for循环实现，并在登录成功的时候，使用break结束循环 */  public static void main(String[] args) {  // 1. 已知用户名和密码，定义两个字符串表示即可  String username = \u0026#34;admin\u0026#34;;  String password = \u0026#34;123456\u0026#34;;  // 2. 键盘录入要登录的用户名和密码，用 Scanner 实现  Scanner sc = new Scanner(System.in);  // 4. 用循环实现多次机会，这里的次数明确，采用for循环实现  for(int i = 1; i \u0026lt;= 3; i++){  System.out.println(\u0026#34;请输入用户名:\u0026#34;);  String scUsername = sc.nextLine();  System.out.println(\u0026#34;请输入密码:\u0026#34;);  String scPassword = sc.nextLine();  // 3. 拿键盘录入的用户名、密码和已知的用户名、密码进行比较，给出相应的提示。  if(username.equals(scUsername) \u0026amp;\u0026amp; password.equals(scPassword)){  System.out.println(\u0026#34;登录成功\u0026#34;);  break;  }else{  if(i == 3){  System.out.println(\u0026#34;您的登录次数已达到今日上限, 请明天再来\u0026#34;);  }else{  System.out.println(\u0026#34;登录失败,您还剩余\u0026#34; + (3-i) +\u0026#34;次机会\u0026#34;);  }   }  }   } } 2.7 遍历字符串案例【应用】 案例需求 :\n​\t键盘录入一个字符串，使用程序实现在控制台遍历该字符串\n实现步骤 :\n 键盘录入一个字符串，用 Scanner 实现 遍历字符串，首先要能够获取到字符串中的每一个字符, public char charAt(int index)：返回指定索引处的char值，字符串的索引也是从0开始的 遍历字符串，其次要能够获取到字符串的长度, public int length()：返回此字符串的长度 遍历打印  代码实现 :\npackage com.itheima.test;  import java.util.Scanner;  public class Test2 {  /* 需求：键盘录入一个字符串，使用程序实现在控制台遍历该字符串 思路： 1. 键盘录入一个字符串，用 Scanner 实现 2. 遍历字符串，首先要能够获取到字符串中的每一个字符 public char charAt(int index)：返回指定索引处的char值，字符串的索引也是从0开始的 3. 遍历字符串，其次要能够获取到字符串的长度 public int length()：返回此字符串的长度 4. 遍历打印 9 */  public static void main(String[] args) {  // 1. 键盘录入一个字符串，用 Scanner 实现  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入:\u0026#34;);  String s = sc.nextLine();  // 2. 遍历字符串，首先要能够获取到字符串中的每一个字符  for(int i = 0; i \u0026lt; s.length(); i++){  // i : 字符串的每一个索引  char c = s.charAt(i);  System.out.println(c);  }  } } 2.8 统计字符次数案例【应用】 案例需求 :\n​\t键盘录入一个字符串，使用程序实现在控制台遍历该字符串\n实现步骤 :\n 键盘录入一个字符串，用 Scanner 实现 将字符串拆分为字符数组 , public char[] toCharArray( )：将当前字符串拆分为字符数组并返回 遍历字符数  代码实现 :\npackage com.itheima.test;  import java.util.Scanner;  public class Test3 {  /* 需求：键盘录入一个字符串，使用程序实现在控制台遍历该字符串 思路： 1. 键盘录入一个字符串，用 Scanner 实现 2. 将字符串拆分为字符数组 public char[] toCharArray( )：将当前字符串拆分为字符数组并返回 3. 遍历字符数组 */  public static void main(String[] args) {  // 1. 键盘录入一个字符串，用 Scanner 实现  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入:\u0026#34;);  String s = sc.nextLine();  // 2. 将字符串拆分为字符数组  char[] chars = s.toCharArray();  // 3. 遍历字符数组  for (int i = 0; i \u0026lt; chars.length; i++) {  System.out.println(chars[i]);  }  } } 2.9 手机号屏蔽-字符串截取 案例需求 :\n​\t以字符串的形式从键盘接受一个手机号，将中间四位号码屏蔽 ​ 最终效果为：156****1234\n实现步骤 :\n 键盘录入一个字符串，用 Scanner 实现 截取字符串前三位 截取字符串后四位 将截取后的两个字符串，中间加上****进行拼接，输出结果  代码实现 :\npackage com.itheima.test;  import java.util.Scanner;  public class Test5 {  /* 需求：以字符串的形式从键盘接受一个手机号，将中间四位号码屏蔽 最终效果为：156****1234 思路： 1. 键盘录入一个字符串，用 Scanner 实现 2. 截取字符串前三位 3. 截取字符串后四位 4. 将截取后的两个字符串，中间加上****进行拼接，输出结果 */  public static void main(String[] args) {  // 1. 键盘录入一个字符串，用 Scanner 实现  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入手机号:\u0026#34;);  String telString = sc.nextLine();  // 2. 截取字符串前三位  String start = telString.substring(0,3);  // 3. 截取字符串后四位  String end = telString.substring(7);  // 4. 将截取后的两个字符串，中间加上****进行拼接，输出结果  System.out.println(start + \u0026#34;****\u0026#34; + end);  } } 2.10 敏感词替换-字符串替换 案例需求 :\n​\t键盘录入一个 字符串，如果字符串中包含（TMD），则使用***替换\n实现步骤 :\n  键盘录入一个字符串，用 Scanner 实现\n  替换敏感词\nString replace(CharSequence target, CharSequence replacement)\n将当前字符串中的target内容，使用replacement进行替换，返回新的字符串\n  输出结果\n  代码实现 :\npackage com.itheima.test;  import java.util.Scanner;  public class Test6 {  /* 需求：键盘录入一个 字符串，如果字符串中包含（TMD），则使用***替换 思路： 1. 键盘录入一个字符串，用 Scanner 实现 2. 替换敏感词 String replace(CharSequence target, CharSequence replacement) 将当前字符串中的target内容，使用replacement进行替换，返回新的字符串 3. 输出结果 */  public static void main(String[] args) {  // 1. 键盘录入一个字符串，用 Scanner 实现  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入:\u0026#34;);  String s = sc.nextLine();  // 2. 替换敏感词  String result = s.replace(\u0026#34;TMD\u0026#34;,\u0026#34;***\u0026#34;);  // 3. 输出结果  System.out.println(result);  } } 2.11 切割字符串 案例需求 :\n​\t以字符串的形式从键盘录入学生信息，例如：“张三 , 23”\n​\t从该字符串中切割出有效数据,封装为Student学生对象\n实现步骤 :\n  编写Student类，用于封装数据\n  键盘录入一个字符串，用 Scanner 实现\n  根据逗号切割字符串，得到（张三）（23）\nString[] split(String regex) ：根据传入的字符串作为规则进行切割 将切割后的内容存入字符串数组中，并将字符串数组返回\n  从得到的字符串数组中取出元素内容，通过Student类的有参构造方法封装为对象\n  调用对象getXxx方法，取出数据并打印。\n  代码实现 :\npackage com.itheima.test;  import com.itheima.domain.Student;  import java.util.Scanner;  public class Test7 {  /* 需求：以字符串的形式从键盘录入学生信息，例如：“张三 , 23” 从该字符串中切割出有效数据,封装为Student学生对象 思路： 1. 编写Student类，用于封装数据 2. 键盘录入一个字符串，用 Scanner 实现 3. 根据逗号切割字符串，得到（张三）（23） String[] split(String regex) ：根据传入的字符串作为规则进行切割 将切割后的内容存入字符串数组中，并将字符串数组返回 4. 从得到的字符串数组中取出元素内容，通过Student类的有参构造方法封装为对象 5. 调用对象getXxx方法，取出数据并打印。 */  public static void main(String[] args) {  // 2. 键盘录入一个字符串，用 Scanner 实现  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入学生信息:\u0026#34;);  String stuInfo = sc.nextLine();  // stuInfo = \u0026#34;张三,23\u0026#34;;  // 3. 根据逗号切割字符串，得到（张三）（23）  String[] sArr = stuInfo.split(\u0026#34;,\u0026#34;);  // System.out.println(sArr[0]); // System.out.println(sArr[1]);   // 4. 从得到的字符串数组中取出元素内容，通过Student类的有参构造方法封装为对象  Student stu = new Student(sArr[0],sArr[1]);   // 5. 调用对象getXxx方法，取出数据并打印。  System.out.println(stu.getName() + \u0026#34;...\u0026#34; + stu.getAge());  } } 2.12 String方法小结 String类的常用方法 :\n​\tpublic boolean equals(Object anObject) 比较字符串的内容，严格区分大小写\n​\tpublic boolean equalsIgnoreCase(String anotherString) 比较字符串的内容，忽略大小写\n​\tpublic int length() 返回此字符串的长度\n​\tpublic char charAt(int index) 返回指定索引处的 char 值\n​\tpublic char[] toCharArray() 将字符串拆分为字符数组后返回\n​\tpublic String substring(int beginIndex, int endIndex) 根据开始和结束索引进行截取，得到新的字符串（包含头，不包含尾）\n​\tpublic String substring(int beginIndex) 从传入的索引处截取，截取到末尾，得到新的字符串\n​\tpublic String replace(CharSequence target, CharSequence replacement) 使用新值，将字符串中的旧值替换，得到新的字符串\n​\tpublic String[] split(String regex) 根据传入的规则切割字符串，得到字符串数组\n3 StringBuilder类 3.1 StringBuilder类概述 ​\t概述 : StringBuilder 是一个可变的字符串类，我们可以把它看成是一个容器，这里的可变指的是 StringBuilder 对象中的内容是可变的\n3.2 StringBuilder类和String类的区别  **String类：**内容是不可变的 **StringBuilder类：**内容是可变的  3.3StringBuilder类的构造方法 常用的构造方法\n   方法名 说明     public StringBuilder() 创建一个空白可变字符串对象，不含有任何内容   public StringBuilder(String str) 根据字符串的内容，来创建可变字符串对象    示例代码\npublic class StringBuilderDemo01 {  public static void main(String[] args) {  //public StringBuilder()：创建一个空白可变字符串对象，不含有任何内容  StringBuilder sb = new StringBuilder();  System.out.println(\u0026#34;sb:\u0026#34; + sb);  System.out.println(\u0026#34;sb.length():\u0026#34; + sb.length());   //public StringBuilder(String str)：根据字符串的内容，来创建可变字符串对象  StringBuilder sb2 = new StringBuilder(\u0026#34;hello\u0026#34;);  System.out.println(\u0026#34;sb2:\u0026#34; + sb2);  System.out.println(\u0026#34;sb2.length():\u0026#34; + sb2.length());  } } 3.4 StringBuilder常用的成员方法   添加和反转方法\n   方法名 说明     public StringBuilder append(任意类型) 添加数据，并返回对象本身   public StringBuilder reverse() 返回相反的字符序列      示例代码\n  public class StringBuilderDemo01 {  public static void main(String[] args) {  //创建对象  StringBuilder sb = new StringBuilder();   //public StringBuilder append(任意类型)：添加数据，并返回对象本身 // StringBuilder sb2 = sb.append(\u0026#34;hello\u0026#34;); // // System.out.println(\u0026#34;sb:\u0026#34; + sb); // System.out.println(\u0026#34;sb2:\u0026#34; + sb2); // System.out.println(sb == sb2);  // sb.append(\u0026#34;hello\u0026#34;); // sb.append(\u0026#34;world\u0026#34;); // sb.append(\u0026#34;java\u0026#34;); // sb.append(100);   //链式编程  sb.append(\u0026#34;hello\u0026#34;).append(\u0026#34;world\u0026#34;).append(\u0026#34;java\u0026#34;).append(100);   System.out.println(\u0026#34;sb:\u0026#34; + sb);   //public StringBuilder reverse()：返回相反的字符序列  sb.reverse();  System.out.println(\u0026#34;sb:\u0026#34; + sb);  } } 3.5StringBuilder和String相互转换【应用】   StringBuilder转换为String\n​ public String toString()：通过 toString() 就可以实现把 StringBuilder 转换为 String\n  String转换为StringBuilder\n​ public StringBuilder(String s)：通过构造方法就可以实现把 String 转换为 StringBuilder\n  示例代码\n  public class StringBuilderDemo02 {  public static void main(String[] args) {  /* //StringBuilder 转换为 String StringBuilder sb = new StringBuilder(); sb.append(\u0026#34;hello\u0026#34;); //String s = sb; //这个是错误的做法 //public String toString()：通过 toString() 就可以实现把 StringBuilder 转换为 String String s = sb.toString(); System.out.println(s); */   //String 转换为 StringBuilder  String s = \u0026#34;hello\u0026#34;;   //StringBuilder sb = s; //这个是错误的做法   //public StringBuilder(String s)：通过构造方法就可以实现把 String 转换为 StringBuilder  StringBuilder sb = new StringBuilder(s);   System.out.println(sb);  } } 3.6 StringBuilder拼接字符串案例 案例需求 :\n​\t定义一个方法，把 int 数组中的数据按照指定的格式拼接成一个字符串返回，调用该方法，\n​\t并在控制台输出结果。例如，数组为int[] arr = {1,2,3}; ，执行方法后的输出结果为：[1, 2, 3]\n实现步骤 :\n  定义一个 int 类型的数组，用静态初始化完成数组元素的初始化\n  定义一个方法，用于把 int 数组中的数据按照指定格式拼接成一个字符串返回。\n返回值类型 String，参数列表 int[] arr\n  在方法中用 StringBuilder 按照要求进行拼接，并把结果转成 String 返回\n  调用方法，用一个变量接收结果\n  输出结果\n  代码实现 :\n/* 思路： 1:定义一个 int 类型的数组，用静态初始化完成数组元素的初始化 2:定义一个方法，用于把 int 数组中的数据按照指定格式拼接成一个字符串返回。 返回值类型 String，参数列表 int[] arr 3:在方法中用 StringBuilder 按照要求进行拼接，并把结果转成 String 返回 4:调用方法，用一个变量接收结果 5:输出结果 */ public class StringBuilderTest01 {  public static void main(String[] args) {  //定义一个 int 类型的数组，用静态初始化完成数组元素的初始化  int[] arr = {1, 2, 3};   //调用方法，用一个变量接收结果  String s = arrayToString(arr);   //输出结果  System.out.println(\u0026#34;s:\u0026#34; + s);   }   //定义一个方法，用于把 int 数组中的数据按照指定格式拼接成一个字符串返回  /* 两个明确： 返回值类型：String 参数：int[] arr */  public static String arrayToString(int[] arr) {  //在方法中用 StringBuilder 按照要求进行拼接，并把结果转成 String 返回  StringBuilder sb = new StringBuilder();   sb.append(\u0026#34;[\u0026#34;);   for(int i=0; i\u0026lt;arr.length; i++) {  if(i == arr.length-1) {  sb.append(arr[i]);  } else {  sb.append(arr[i]).append(\u0026#34;, \u0026#34;);  }  }   sb.append(\u0026#34;]\u0026#34;);   String s = sb.toString();   return s;  } } ","permalink":"https://iblog.zone/archives/java%E5%B8%B8%E7%94%A8api/","summary":"1.API 1.1 API概述-帮助文档的使用   什么是API\n​\tAPI (Application Programming Interface) ：应用程序编程接口\n  java中的API\n​\t指的就是 JDK 中提供的各种功能的 Java类，这些类将底层的实现封装了起来，我们不需要关心这些类是如何实现的，只需要学习这些类如何使用即可，我们可以通过帮助文档来学习这些API如何使用。\n  如何使用API帮助文档 :\n  打开帮助文档\n  找到索引选项卡中的输入框\n  在输入框中输入Random\n  看类在哪个包下\n  看类的描述\n  看构造方法\n  看成员方法\n  1.2 键盘录入字符串 Scanner类 :\n​\tnext() : 遇到了空格, 就不再录入数据了 , 结束标记: 空格, tab键\n​\tnextLine() : 可以将数据完整的接收过来 , 结束标记: 回车换行符\n代码实现 :\npackage com.","title":"Java常用API"},{"content":"1、进入oracle官网 https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html\n下载：\noracle-instantclient-basic-21.3.0.0.0-1.el8.x86_64.rpm oracle-instantclient-sqlplus-21.3.0.0.0-1.el8.x86_64.rpm oracle-instantclient-devel-21.3.0.0.0-1.el8.x86_64.rpm 依次安装：\nrpm -ivh oracle-instantclient-basic-21.3.0.0.0-1.el8.x86_64.rpm rpm -ivh oracle-instantclient-sqlplus-21.3.0.0.0-1.el8.x86_64.rpm rpm -ivh oracle-instantclient-devel-21.3.0.0.0-1.el8.x86_64.rpm 安装的文件默认放在两个位置： 头文件：/usr/include/oracle/21/client64 下，如果在使用时报错找不到头文件，记得看路径是否是这个。 包文件：/usr/lib/oracle/21/client64下，包含{bin、lib}两个文件夹；\n2、创建监听文件，并添加内容\ncd /usr/lib/oracle/21/client64/lib/network/admin\nvi tnsnames.ora\nORCL =  (DESCRIPTION =  (ADDRESS = (PROTOCOL = TCP)(HOST = 192.169.1.109)(PORT = 1521))  (CONNECT_DATA =  (SERVER = DEDICATED)  (SERVICE_NAME = orcl)  )  ) Note:host是远程数据库的ip地址，service_name为远程数据库的sid\n配置环境变量 vi /etc/profile，添加\n#配置ORACLE环境变量 vi /etc/profile export ORACLE_BASE=/usr/lib/oracle/21 export ORACLE_VERSION=21 export ORACLE_HOME=/usr/lib/oracle/21/client64 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH export SQLPATH=$ORACLE_HOME/lib/network/admin export TNS_ADMIN=$ORACLE_HOME/lib/network/admin export NLS_LANG=AMERICAN_AMERICA.AL32UTF8 #export NLS_LANG=\u0026#34;Simplified Chinese_china\u0026#34;.ZHS16GBK export PATH=$PATH:$HOME/bin:$ORACLE_HOME/bin 配置保存后 source /etc/profile 即可使配置即时生效 之后连接数据库测试 sqlplus username/password@ORCL\n","permalink":"https://iblog.zone/archives/centos8%E5%AE%89%E8%A3%85oracle%E5%AE%A2%E6%88%B7%E7%AB%AF/","summary":"1、进入oracle官网 https://www.oracle.com/database/technologies/instant-client/linux-x86-64-downloads.html\n下载：\noracle-instantclient-basic-21.3.0.0.0-1.el8.x86_64.rpm oracle-instantclient-sqlplus-21.3.0.0.0-1.el8.x86_64.rpm oracle-instantclient-devel-21.3.0.0.0-1.el8.x86_64.rpm 依次安装：\nrpm -ivh oracle-instantclient-basic-21.3.0.0.0-1.el8.x86_64.rpm rpm -ivh oracle-instantclient-sqlplus-21.3.0.0.0-1.el8.x86_64.rpm rpm -ivh oracle-instantclient-devel-21.3.0.0.0-1.el8.x86_64.rpm 安装的文件默认放在两个位置： 头文件：/usr/include/oracle/21/client64 下，如果在使用时报错找不到头文件，记得看路径是否是这个。 包文件：/usr/lib/oracle/21/client64下，包含{bin、lib}两个文件夹；\n2、创建监听文件，并添加内容\ncd /usr/lib/oracle/21/client64/lib/network/admin\nvi tnsnames.ora\nORCL =  (DESCRIPTION =  (ADDRESS = (PROTOCOL = TCP)(HOST = 192.169.1.109)(PORT = 1521))  (CONNECT_DATA =  (SERVER = DEDICATED)  (SERVICE_NAME = orcl)  )  ) Note:host是远程数据库的ip地址，service_name为远程数据库的sid\n配置环境变量 vi /etc/profile，添加\n#配置ORACLE环境变量 vi /etc/profile export ORACLE_BASE=/usr/lib/oracle/21 export ORACLE_VERSION=21 export ORACLE_HOME=/usr/lib/oracle/21/client64 export LD_LIBRARY_PATH=$ORACLE_HOME/lib:$LD_LIBRARY_PATH export SQLPATH=$ORACLE_HOME/lib/network/admin export TNS_ADMIN=$ORACLE_HOME/lib/network/admin export NLS_LANG=AMERICAN_AMERICA.","title":"CentOS8安装oracle客户端"},{"content":"RabbitMQ安装步骤   *虚拟机：VMware workstation 12.0*\n  Linux系统：CentOS 7.0\n安装Erlang环境 由于RabbitMQ是采用Erlang编写的，首先需要安装该语言库，以便运行代理服务器，可以参考Erlang官方文档。\n  erlang-solution配置信息安装\n  wget http://packages.erlang-solutions.com/erlang-solutions-1.0-1.noarch.rpm sudo rpm -Uvh erlang-solutions-1.0-1.noarch.rpm rpm --import http://packages.erlang-solutions.com/rpm/erlang_solutions.asc  第三方yum源依赖  wget http://packages.sw.be/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm rpm –import http://apt.sw.be/RPM-GPG-KEY.dag.txt sudo rpm -i rpmforge-release-0.5.2-2.el6.rf.*.rpm  安装erlang  sudo yum install erlang  运行*erl*命令进行测试  安装RabbitMQ  首先下载最新版的RabbitMQ  wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.1/rabbitmq-server-3.6.1-1.noarch.rpm  使用rpm和yum进行安装  rpm --import http://www.rabbitmq.com/rabbitmq-signing-key-public.asc yum install rabbitmq-server-3.6.1-1.noarch.rpm  启动RabbitMQ管理插件，用于web界面管理  rabbitmq-plugins enable rabbitmq_management service rabbitmq-server restart  测试安装完成的RabbitMQ  rabbitmqctl status 具体内容可以参考RabbitMQ安装官方文档。\n **注意：**在Ubuntu下安装RabbitMQ非常简单，系统已经默认安装Erlang环境，使用apt-get install rabbitmq-server即可安装。\n RabbitMQ使用和管理 后台操作命令管理RabbitMQ rabbitmqctl是RabbitMQ中间件的一个命令行管理工具，原理就是通过连接一个中间件的节点执行所有的动作，本地节点默认为“rabbit”，rabbitmqctl来指定RabbitMQ中间件在本地节点rabbit@localhost进行管理操作。\n 注意：在使用rabbitmqctl命令时，可以用-n标志来明确指定的节点，比如rabbitmqctl -n rabbit@localhost …，在使用默认节点的情况下，这个可以省略。\n 常用命令列举  应用管理  rabbitmqctl status //显示RabbitMQ中间件的所有信息 rabbitmqctl stop //停止RabbitMQ应用，关闭节点 rabbitmqctl stop_app //停止RabbitMQ应用 rabbitmqctl start_app //启动RabbitMQ应用 rabbitmqctl restart //重置RabbitMQ节点 rabbitmqctl force_restart //强制重置RabbitMQ节点  用户管理  rabbitmqctl add_user username password //添加用户 rabbitmqctl delete_user username //删除用户 rabbitmqctl change_password username newpassword //修改密码 rabbitmqctl list_users //列出所有用户  权限控制管理   rabbitmqctl add_vhost vhostpath //创建虚拟主机  rabbitmqctl delete_vhost vhostpath //删除虚拟主机  rabbitmqctl list_vhosts //列出所有虚拟主机  rabbitmqctl set_permissions [-p vhostpath] username \u0026lt;conf\u0026gt; \u0026lt;write\u0026gt; \u0026lt;read\u0026gt; //设置用户权限  rabbitmqctl clear_permissions [-p vhostpath] username //删除用户权限  rabbitmqctl list_permissions [-p vhostpath] //列出虚拟机上的所有权限  rabbitmqctl list_user_permissions username //列出用户权限  集群管理  rabbitmqctl cluster_status //获得集群配置信息 rabbitmqctl join_cluster rabbit@localhost --ram | --disc //加入到rabbit节点中，使用内存模式或者磁盘模式 rabbitmqctl change_cluster_node_type disc | ram //修改存储模式 rabbitmqctl set_cluster_name newname //修改名字  查看管理   rabbitmqctl list_queues [-p \u0026lt;vhostpath\u0026gt;] //查看所有队列  rabbitmqctl list_exchanges [-p \u0026lt;vhostpath\u0026gt;] //查看所有交换机  rabbitmqctl list_bindings [-p \u0026lt;vhostpath\u0026gt;] //查看所有绑定  rabbitmqctl list_connections //查看所有连接  rabbitmqctl list_channels //查看所有信道  rabbitmqctl list_consumers //查看所有消费者信息 Web界面管理RabbitMQ RabbitMQ通过使用RabbitMQ Management 插件的Web界面来管理用户、队列和交换器。\nWeb界面包含的内容\n 服务器数据统计概览 导入/导出服务器配置 监控服务器连接 信道列表 交换器列表、添加交换器 队列列表、添加队列 修改队列绑定 用户列表、添加用户 查看vhost、添加vhost   注意：使用rabbitmq-plugins enable rabbitmq_management*来启动Management插件。 默认是可以本地登录localhost:15672***，用户名：guest；密码：guest；端口默认15672。\n CLI管理 在web界面上还有两个选项，HTTP API和CLT。\n HTTP API：提供了一个关于REST接口的文档界面，Web界面可以完成的功能，都可以通过使用curl并调用API命令来完成。比如需要列出服务器上的vhost的话，在终端执行下列代码即可：  curl -i -u guest:guest http://localhost:15672/api/vhosts  CLI：主要是Python脚本，相比于REST的API好处是，不需要手工编写请求，rabbitmqadmin会包装REST API，使用干净的接口与其交互，举例来说：  curl -i -u guest:guest http://localhost:15672/api/queues //使用REST API ./rabbitmqadmin list queues //使用CLI rabbitmqadmin脚本安装 wget http://localhost:15672/cli/rabbitmqadmin chmod +x rabbitmqadmin 总结 三种管理方式各有特点：\n Web UI对于日常的开发更加简单，可以通过视图查看服务器的状态，方便观察。 REST API可以自动化这些任务，并通过curl来调用，得到JSON对象后就可以集成到当前的工具和语言中。 rabbitmqadmin脚本不需要手工构造通过curl发送的HTTP请求，获得更加简介易懂的格式化输出，帮助管理和监控RabbitMQ。  ","permalink":"https://iblog.zone/archives/rabbitmq%E5%AE%89%E8%A3%85%E5%92%8C%E4%BD%BF%E7%94%A8/","summary":"RabbitMQ安装步骤   *虚拟机：VMware workstation 12.0*\n  Linux系统：CentOS 7.0\n安装Erlang环境 由于RabbitMQ是采用Erlang编写的，首先需要安装该语言库，以便运行代理服务器，可以参考Erlang官方文档。\n  erlang-solution配置信息安装\n  wget http://packages.erlang-solutions.com/erlang-solutions-1.0-1.noarch.rpm sudo rpm -Uvh erlang-solutions-1.0-1.noarch.rpm rpm --import http://packages.erlang-solutions.com/rpm/erlang_solutions.asc  第三方yum源依赖  wget http://packages.sw.be/rpmforge-release/rpmforge-release-0.5.2-2.el6.rf.x86_64.rpm rpm –import http://apt.sw.be/RPM-GPG-KEY.dag.txt sudo rpm -i rpmforge-release-0.5.2-2.el6.rf.*.rpm  安装erlang  sudo yum install erlang  运行*erl*命令进行测试  安装RabbitMQ  首先下载最新版的RabbitMQ  wget http://www.rabbitmq.com/releases/rabbitmq-server/v3.6.1/rabbitmq-server-3.6.1-1.noarch.rpm  使用rpm和yum进行安装  rpm --import http://www.rabbitmq.com/rabbitmq-signing-key-public.asc yum install rabbitmq-server-3.6.1-1.noarch.rpm  启动RabbitMQ管理插件，用于web界面管理  rabbitmq-plugins enable rabbitmq_management service rabbitmq-server restart  测试安装完成的RabbitMQ  rabbitmqctl status 具体内容可以参考RabbitMQ安装官方文档。","title":"RabbitMQ安装和使用"},{"content":"仓库 # 在当前目录新建一个Git代码库 $ git init  # 新建一个目录，将其初始化为Git代码库 $ git init [project-name]  # 下载一个项目和它的整个代码历史 $ git clone [url] 配置 # 显示当前的Git配置 $ git config --list  # 编辑Git配置文件 $ git config -e [--global]  # 设置提交代码时的用户信息 $ git config [--global] user.name \u0026#34;[name]\u0026#34; $ git config [--global] user.email \u0026#34;[email address]\u0026#34; 增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ...  # 添加指定目录到暂存区，包括子目录 $ git add [dir]  # 添加当前目录的所有文件到暂存区 $ git add .  # 添加每个变化前，都会要求确认 # 对于同一个文件的多处变化，可以实现分次提交 $ git add -p  # 删除工作区文件，并且将这次删除放入暂存区 $ git rm [file1] [file2] ...  # 停止追踪指定文件，但该文件会保留在工作区 $ git rm --cached [file]  # 改名文件，并且将这个改名放入暂存区 $ git mv [file-original] [file-renamed] 代码提交 # 提交暂存区到仓库区 $ git commit -m [message]  # 提交暂存区的指定文件到仓库区 $ git commit [file1] [file2] ... -m [message]  # 提交工作区自上次commit之后的变化，直接到仓库区 $ git commit -a  # 提交时显示所有diff信息 $ git commit -v  # 使用一次新的commit，替代上一次提交 # 如果代码没有任何新变化，则用来改写上一次commit的提交信息 $ git commit --amend -m [message]  # 重做上一次commit，并包括指定文件的新变化 $ git commit --amend [file1] [file2] ... 分支 # 列出所有本地分支 $ git branch  # 列出所有远程分支 $ git branch -r  # 列出所有本地分支和远程分支 $ git branch -a  # 新建一个分支，但依然停留在当前分支 $ git branch [branch-name]  # 新建一个分支，并切换到该分支 $ git checkout -b [branch]  # 新建一个分支，指向指定commit $ git branch [branch] [commit]  # 新建一个分支，与指定的远程分支建立追踪关系 $ git branch --track [branch] [remote-branch]  # 切换到指定分支，并更新工作区 $ git checkout [branch-name]  # 切换到上一个分支 $ git checkout -  # 建立追踪关系，在现有分支与指定的远程分支之间 $ git branch --set-upstream [branch] [remote-branch]  # 合并指定分支到当前分支 $ git merge [branch]  # 选择一个commit，合并进当前分支 $ git cherry-pick [commit]  # 删除分支 $ git branch -d [branch-name]  # 删除远程分支 $ git push origin --delete [branch-name] $ git branch -dr [remote/branch] 标签 # 列出所有tag $ git tag  # 新建一个tag在当前commit $ git tag [tag]  # 新建一个tag在指定commit $ git tag [tag] [commit]  # 删除本地tag $ git tag -d [tag]  # 删除远程tag $ git push origin :refs/tags/[tagName]  # 查看tag信息 $ git show [tag]  # 提交指定tag $ git push [remote] [tag]  # 提交所有tag $ git push [remote] --tags  # 新建一个分支，指向某个tag $ git checkout -b [branch] [tag] 查看信息 # 显示有变更的文件 $ git status  # 显示当前分支的版本历史 $ git log  # 显示commit历史，以及每次commit发生变更的文件 $ git log --stat  # 搜索提交历史，根据关键词 $ git log -S [keyword]  # 显示某个commit之后的所有变动，每个commit占据一行 $ git log [tag] HEAD --pretty=format:%s  # 显示某个commit之后的所有变动，其\u0026#34;提交说明\u0026#34;必须符合搜索条件 $ git log [tag] HEAD --grep feature  # 显示某个文件的版本历史，包括文件改名 $ git log --follow [file] $ git whatchanged [file]  # 显示指定文件相关的每一次diff $ git log -p [file]  # 显示过去5次提交 $ git log -5 --pretty --oneline  # 显示所有提交过的用户，按提交次数排序 $ git shortlog -sn  # 显示指定文件是什么人在什么时间修改过 $ git blame [file]  # 显示暂存区和工作区的差异 $ git diff  # 显示暂存区和上一个commit的差异 $ git diff --cached [file]  # 显示工作区与当前分支最新commit之间的差异 $ git diff HEAD  # 显示两次提交之间的差异 $ git diff [first-branch]...[second-branch]  # 显示今天你写了多少行代码 $ git diff --shortstat \u0026#34;@{0 day ago}\u0026#34;  # 显示某次提交的元数据和内容变化 $ git show [commit]  # 显示某次提交发生变化的文件 $ git show --name-only [commit]  # 显示某次提交时，某个文件的内容 $ git show [commit]:[filename]  # 显示当前分支的最近几次提交 $ git reflog 远程同步 # 下载远程仓库的所有变动 $ git fetch [remote]  # 显示所有远程仓库 $ git remote -v  # 显示某个远程仓库的信息 $ git remote show [remote]  # 增加一个新的远程仓库，并命名 $ git remote add [shortname] [url]  # 取回远程仓库的变化，并与本地分支合并 $ git pull [remote] [branch]  # 上传本地指定分支到远程仓库 $ git push [remote] [branch]  # 强行推送当前分支到远程仓库，即使有冲突 $ git push [remote] --force  # 推送所有分支到远程仓库 $ git push [remote] --all 撤销 # 恢复暂存区的指定文件到工作区 $ git checkout [file]  # 恢复某个commit的指定文件到暂存区和工作区 $ git checkout [commit] [file]  # 恢复暂存区的所有文件到工作区 $ git checkout .  # 重置暂存区的指定文件，与上一次commit保持一致，但工作区不变 $ git reset [file]  # 重置暂存区与工作区，与上一次commit保持一致 $ git reset --hard  # 重置当前分支的指针为指定commit，同时重置暂存区，但工作区不变 $ git reset [commit]  # 重置当前分支的HEAD为指定commit，同时重置暂存区和工作区，与指定commit一致 $ git reset --hard [commit]  # 重置当前HEAD为指定commit，但保持暂存区和工作区不变 $ git reset --keep [commit]  # 新建一个commit，用来撤销指定commit # 后者的所有变化都将被前者抵消，并且应用到当前分支 $ git revert [commit]  暂时将未提交的变化移除，稍后再移入 $ git stash $ git stash pop 其他 # 生成一个可供发布的压缩包 $ git archive ","permalink":"https://iblog.zone/archives/%E5%B8%B8%E7%94%A8-git-%E5%91%BD%E4%BB%A4%E6%B8%85%E5%8D%95/","summary":"仓库 # 在当前目录新建一个Git代码库 $ git init  # 新建一个目录，将其初始化为Git代码库 $ git init [project-name]  # 下载一个项目和它的整个代码历史 $ git clone [url] 配置 # 显示当前的Git配置 $ git config --list  # 编辑Git配置文件 $ git config -e [--global]  # 设置提交代码时的用户信息 $ git config [--global] user.name \u0026#34;[name]\u0026#34; $ git config [--global] user.email \u0026#34;[email address]\u0026#34; 增加/删除文件 # 添加指定文件到暂存区 $ git add [file1] [file2] ...  # 添加指定目录到暂存区，包括子目录 $ git add [dir]  # 添加当前目录的所有文件到暂存区 $ git add .","title":"常用 Git 命令清单"},{"content":"一、Nginx安装 1、去官网http://nginx.org/下载对应的nginx包，推荐使用稳定版本 2、上传nginx到linux系统 3、安装依赖环境 (1)安装gcc环境\nyum install gcc-c++ (2)安装PCRE库，用于解析正则表达式\nyum install -y pcre pcre-devel (3)zlib压缩和解压缩依赖\nyum install -y zlib zlib-devel (4)SSL 安全的加密的套接字协议层，用于HTTP安全传输，也就是https\nyum install -y openssl openssl-devel 4、解压，需要注意，解压后得到的是源码，源码需要编译后才能安装 tar -zxvf nginx-1.16.1.tar.gz 5、编译之前，先创建nginx临时目录，如果不创建，在启动nginx的过程中会报错 mkdir /var/temp/nginx -p 6、在nginx目录，输入如下命令进行配置，目的是为了创建makefile文件 ./configure \\  --prefix=/usr/local/nginx \\  --pid-path=/var/run/nginx/nginx.pid \\  --lock-path=/var/lock/nginx.lock \\  --error-log-path=/var/log/nginx/error.log \\  --http-log-path=/var/log/nginx/access.log \\  --with-http_gzip_static_module \\  --http-client-body-temp-path=/var/temp/nginx/client \\  --http-proxy-temp-path=/var/temp/nginx/proxy \\  --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\  --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\  --http-scgi-temp-path=/var/temp/nginx/scgi 注：代表在命令行中换行，用于提高可读性配置命令：\n7、make编译\u0026amp;安装 make make install 8、进入sbin目录启动nginx 启动：nginx 停止：./nginx -s stop 重新加载：./nginx -s reload 二、配置反向代理 1、配置upstream\nupstream [proxyName] {  server 192.168.1.173:8080;  server 192.168.1.174:8080;  server 192.168.1.175:8080; } 2、配置server\nserver {  listem 80;  server_name www.tomcats.com;   location / {  proxy_pass http://tomcats;  } } 三、配置负载均衡 nginx默认采用轮训的方式进行负载均衡\n1、使用加权轮询\nupstream [proxyName] {  server 192.168.1.173:8080 weight=1;  server 192.168.1.174:8080 weight=5;  server 192.168.1.175:8080 weight=2; } 2、hash负载均衡\nupstream [proxyName] {  ip_hash   server 192.168.1.173:8080;  server 192.168.1.174:8080;  server 192.168.1.175:8080; } hash算法实际上只会计算 192.168.1这段做哈希\n使用ip_hash的注意点：\n 不能把后台服务器直接移除，只能标记down.  3、url hash负载均衡\nupstream [proxyName] {  hash $request_url;   server 192.168.1.173:8080;  server 192.168.1.174:8080;  server 192.168.1.175:8080; } 4、最小连接负载均衡\nupstream [proxyName] {  least_conn;   server 192.168.1.173:8080;  server 192.168.1.174:8080;  server 192.168.1.175:8080; } 四、upstream指令参数  max_conns：限制最大同时连接数 1.11.5之前只能用于商业版 slow_start：单位秒，权重在指定时间内从1上升到指定值，不适用与hash负载均衡、随机负载均衡 如果在 upstream 中只有一台 server，则该参数失效（商业版才有） down：禁止访问 backup：备用机 只有在其他服务器无法访问的时候才能访问到 不适用与hash负载均衡、随机负载均衡 max_fails：表示失败几次，则标记server已宕机，剔出上游服务 默认值1 fail_timeout：表示失败的重试时间 默认值10  1、keepalived\nupstream [proxyName] {  server 192.168.1.173:8080 weight=1;  server 192.168.1.174:8080 weight=5;  server 192.168.1.175:8080 weight=2;   keepalive 32; #保持的连接数 }  server {  listem 80;  server_name www.tomcats.com;   location / {  proxy_pass http://tomcats;  proxy_http_version 1.1; #连接的协议版本  proxy_set_header Connection \u0026#34;\u0026#34;; 清空连接请求头  } } 2、控制浏览器缓存\nserver {  listem 80;  server_name www.tomcats.com;   location / {  proxy_pass http://tomcats;  expires 10s; #浏览器缓存10秒钟  #expires @22h30m #在晚上10点30的时候过期  #expires -1h #缓存在一小时前时效  #expires epoch #不设置缓存  #expires off #缓存关闭，浏览器自己控制缓存  #expires max #最大过期时间  } } 3、反向代理缓存\nupstream [proxyName] {  server 192.168.1.173:8080 weight=1;  server 192.168.1.174:8080 weight=5;  server 192.168.1.175:8080 weight=2; }  #proxy_cache_path 设置缓存保存的目录的位置 #keys_zone设置共享内以及占用的空间大小 #mas_size 设置缓存最大空间 #inactive 缓存过期时间，错过此时间自动清理 #use_temp_path 关闭零时目录 proxy_cache_path /usr/local/nginx/upsteam_cache keys_zone=mycache:5m max_size=1g inactive=8h use_temp_path=off;  server {  listem 80;  server_name www.tomcats.com;  #开启并使用缓存  proxy_cache mycache;  #针对200和304响应码的缓存过期时间  proxy_cache_valid 200 304 8h;   location / {  proxy_pass http://tomcats;  } } 五、配置ssl证书提供https访问 1. 安装SSL模块 要在nginx中配置https，就必须安装ssl模块，也就是: http_ssl_module。\n进入到nginx的解压目录：/home/software/nginx-1.16.1\n新增ssl模块(原来的那些模块需要保留)\n./configure \\ --prefix=/usr/local/nginx \\ --pid-path=/var/run/nginx/nginx.pid \\ --lock-path=/var/lock/nginx.lock \\ --error-log-path=/var/log/nginx/error.log \\ --http-log-path=/var/log/nginx/access.log \\ --with-http_gzip_static_module \\ --http-client-body-temp-path=/var/temp/nginx/client \\ --http-proxy-temp-path=/var/temp/nginx/proxy \\ --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\ --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\ --http-scgi-temp-path=/var/temp/nginx/scgi \\ --with-http_ssl_module 编译和安装\nmake make install 2、配置HTTPS 把ssl证书 *.crt 和 私钥 *.key 拷贝到/usr/local/nginx/conf目录中。\n新增 server 监听 443 端口：\nserver {  listen 443;  server_name www.imoocdsp.com;  # 开启ssl  ssl on;  # 配置ssl证书  ssl_certificate 1_www.imoocdsp.com_bundle.crt;  # 配置证书秘钥  ssl_certificate_key 2_www.imoocdsp.com.key;  # ssl会话cache  ssl_session_cache shared:SSL:1m;  # ssl会话超时时间  ssl_session_timeout 5m;  # 配置加密套件，写法遵循 openssl 标准  ssl_protocols TLSv1 TLSv1.1 TLSv1.2;  ssl_ciphers ECDHE-RSA-AES128-GCM-SHA256:HIGH:!aNULL:!MD5:!RC4:!DHE;  ssl_prefer_server_ciphers on;   location / {  proxy_pass http://tomcats/;  index index.html index.htm;  } } 六、配置ha nginx 1、安装keepalived (1)下载\nhttps://www.keepalived.org/download.html (2)解压\ntar -zxvf keepalived-2.0.18.tar.gz (3)使用configure命令配置安装目录与核心配置文件所在位置：\n./configure --prefix=/usr/local/keepalived --sysconf=/etc  prefix：keepalived安装的位置sysconf：keepalived核心配置文件所在位置，固定位置，改成其他位置则keepalived启动不了，/var/log/messages中会报错 sysconf：keepalived核心配置文件所在位置，固定位置，改成其他位置则keepalived启动不了，/var/log/messages中会报错  配置过程中可能会出现警告信息，如下所示：\n*** WARNING - this build will not support IPVS with IPv6. Please install libnl/libnl-3 dev libraries to support IPv6 with IPVS.  # 安装libnl/libnl-3依赖 yum -y install libnl libnl-devel (4)安装keepalived\nmake \u0026amp;\u0026amp; make install (5)配置文件 在/etc/keepalived/keepalived.conf\n(6)忘记安装配置的目录，则通过如下命令找到：\nwhereis keepalived (7)启动keepalived\n进入sbin目录\n./keepalived 2、配置keepalived 主机 (1)通过命令 vim keepalived.conf 打开配置文件\nglobal_defs {  # 路由id：当前安装keepalived的节点主机标识符，保证全局唯一   router_id keep_171 }  vrrp_instance VI_1 {  # 表示状态是MASTER主机还是备用机BACKUP   state MASTER  # 该实例绑定的网卡   interface ens33  # 保证主备节点一致即可   virtual_router_id 51  # 权重，master权重一般高于backup，如果有多个，那就是选举，谁的权重高，谁就当选   priority 100  # 主备之间同步检查时间间隔，单位秒   advert_int 2  # 认证权限密码，防止非法节点进入   authentication {  auth_type PASS  auth_pass 1111  }  # 虚拟出来的ip，可以有多个（vip）   virtual_ipaddress {  192.168.1.161  } } 附：查看网卡信息命令\nip addr (2)启动keepalived\n(3)查看进程\nps -ef|grep keepalived (4)查看vip(虚拟ip)\n在网卡ens33下，多了一个192.168.1.161，这个就是虚拟ip\n3、把keepalived注册为系统服务 (1)拷贝配置文件\n 将keepalived目录下etc/init.d/keepalived拷贝到/etc/init.d/下 将keepalived目录下etc/sysconfig/keepalived拷贝到/etc/sysconfig/下  (2)刷新systemctl\nsystemctl daemon-reload (3)启动、停止、重启keepalived\n#启动 systemctl start keepalived.service #停止 systemctl stop keepalived.service #重启 systemctl restart keepalived.service 4、实现双机主备高可用 (1)修改备机配置\nglobal_defs {  router_id keep_172 } vrrp_instance VI_1 {  # 备用机设置为BACKUP   state BACKUP  interface ens33  virtual_router_id 51  # 权重低于MASTER   priority 80  advert_int 2  authentication {  auth_type PASS auth_pass 1111  }  virtual_ipaddress {  # 注意：主备两台的vip都是一样的，绑定到同一个vip   192.168.1.161  } } (2) 启动 Keepalived\n(3) 访问vip即可访问主机，当主机失效时访问vip就会访问到备机\n5、keepalived配置nginx自动重启 (1)编写脚本\n在/etc/keepalived/下创建脚本check_nginx_alive_or_not\n#!/bin/bash  A=`ps -C nginx --no-header |wc -l` # 判断nginx是否宕机，如果宕机了，尝试重启  if [ $A -eq 0 ];then  /usr/local/nginx/sbin/nginx  # 等待一小会再次检查nginx，如果没有启动成功，则停止keepalived，使其启动备用机   sleep 3  if [ `ps -C nginx --no-header |wc -l` -eq 0 ];then  killall keepalived  fi fi (2)添加运行权限\nchmod +x /etc/keepalived/check_nginx_alive_or_not.sh (3)配置keepalived监听nginx脚本\nvrrp_script check_nginx_alive {  script \u0026#34;/etc/keepalived/check_nginx_alive_or_not.sh\u0026#34;  interval 2 # 每隔两秒运行上一行脚本   weight 10 # 如果脚本运行失败，则升级权重+10  } (4)在vrrp_instance中新增监控的脚本\ntrack_script {  check_nginx_alive # 追踪 nginx 脚本 } (5)重启Keepalived使得配置文件生效\nsystemctl restart keepalived 6、keepalived双主热备 (1)配置DNS轮询\n在同一个域名下配置两个ip，自行百度\n(2)配置第一台主机\nglobal_defs {  router_id keep_171 } vrrp_instance VI_1 {  state MASTER i  nterface ens33  virtual_router_id 51  priority 100  advert_int 1  authentication {  auth_type PASS  auth_pass 1111  }  virtual_ipaddress {  192.168.1.161  } }  vrrp_instance VI_2 {  state BACKUP  interface ens33  virtual_router_id 52  priority 80  advert_int 1  authentication {  auth_type PASS  auth_pass 1111  }  virtual_ipaddress {  192.168.1.162  } } (3)配置第二台主机\nglobal_defs {  router_id keep_172 } vrrp_instance VI_1 {  state BACKUP  interface ens33  virtual_router_id 51  priority 80  advert_int 1  authentication {  auth_type PASS  auth_pass 1111  }  virtual_ipaddress {  192.168.1.161  } }  vrrp_instance VI_2 {  state MASTER  interface ens33  virtual_router_id 52  priority 100  advert_int 1  authentication {  auth_type PASS  auth_pass 1111  }  virtual_ipaddress {  192.168.1.162  } } (4)重启两台Keepalived\nsystemctl restart keepalived 七、LVS（Linux Virtual Server）实现高可用负载均衡 1、为什么要使用LVS+Nginx  lvs基于四层负载均衡，工作效率较Nginx的七层负载更高，使用LVS搭建Nginx集群，可以提高性能 四层负载均衡无法对信息处理，只能通过ip+端口的形式转发，所以需要七成负载进行数据的处理 Nginx接收请求来回，LVS可以只接受不响应  2、LVS的三种模式 (1)NAT模式\n 客户端将请求发往LVS，LVS会选择一台服务器响应请求，服务器将结果返回给LVS，LVS再返回给客户端。 在NAT模式中，服务器的网关必须指向LVS，否则报文无法送达客户端 NAT 技术将请求的报文和响应的报文都需要通过LVS进行地址改写，因此网站访问量比较大的时候负载均衡调度器有比较大的瓶颈，一般要求最多之能 10-20 台节点 NAT 模式支持对 IP 地址和端口进行转换。即用户请求的端口和真实服务器的端口可以不一致  (2)TUN模式\n 客户端将请求发往LVS，LVS会选择一台服务器响应请求，在客户端与服务器之间建立隧道，返回结果的时候直接由服务器返回响应，不在经过LVS。 TUN模式必须所有的服务器上都绑定VIP的IP地址，所有的服务器都必须有网卡。 TUN模式走隧道运维难度大，并且会直接暴露服务器地址 服务器将应答包直接发给用户。所以，减少了负载均衡器的大量数据流动，负载均衡器不再是系统的瓶颈，就能处理很巨大的请求量，这种方式，一台负载均衡器能够为很多服务器进行分发。而且跑在公网上就能进行不同地域的分发  (3)DR模式\n 客户端将请求发往LVS，LVS会选择一台服务器响应请求，返回结果的时候通过统一的路由进行返回，不在经过LVS。 和TUN模式一样，LVS只是分发请求，应答包通过单独的路由返回给客户端，与TUN相比这种方式不需要隧道结构，可以兼容大多数的操作系统，同时统一路由可以隐藏真实的物理服务器。DR模式效率更高，但配置更复杂. 所有服务器节点和LVS只能在一个局域网里面。  3、搭建LVS-DR模式 先关闭掉服务器上网络配置管理器，避免网络接口冲突\nsystemctl stop NetworkManager systemctl disable NetworkManager (1)创建子接口（创建LVS的虚拟ip）\n进入网卡配置目录/etc/sysconfig/network-scripts/,找到网卡配置文件，这里以ifcfg-ens33为例，拷贝并创建子接口\ncp ifcfg-ens33 ifcfg-ens33:1 修改子接口配置如下\n 配置中的 192.168.1.150 就是vip，是提供给外网用户访问的ip地址  DEVICE=\u0026#34;ens33:1\u0026#34; ONBOOT=\u0026#34;yes\u0026#34; IPADDR=192.168.1.150 NETMASK=255.255.255.0 BOOTPROTO=static  重启网络服务  service network restart 重启成功后，ip addr 查看一下，你会发现多了一个ip，也就是虚拟ip（vip）\n 注意：阿里云不支持配置网卡，需要购买相应的负载均衡服务，腾讯云支持配置网卡，但需要购买网卡支持，一个网卡支持10个虚拟ip配置\n (2)安装ipvsadm\n如今的centos都集成了LVS，所以ipvs是自带的，我们只需要安装ipvsadm即可（ipvsadm是管理集群的工具，通过ipvs可以管理集群，查看集群等操作）\nyum install ipvsadm (3)配置服务器（RS）的虚拟ip\n进入网卡配置目录/etc/sysconfig/network-scripts/,找到ifcfg-lo，拷贝并创建子接口\ncp ifcfg-lo ifcfg-lo:1 修改子接口配置如下\nDEVICE=\u0026#34;lo:1\u0026#34; IPADDR=192.168.1.150 NETMASK=255.255.255.255 NETWORK=127.0.0.0 BROADCAST=127.255.255.255 ONBOOT=\u0026#34;yes\u0026#34; NAME=loopback 重启网络服务成功后，ip addr 查看一下，你会发现多了一个ip，也就是虚拟ip（vip）\n(4)为服务器（RS）配置arp\nARP响应级别与通告行为参数说明\narp-ignore：ARP响应级别（处理请求）  0：只要本机配置了ip，就能响应请求  1：请求的目标地址到达对应的网络接口，才会响应请求 arp-announce：ARP通告行为（返回响应）  0：本机上任何网络接口都向外通告，所有的网卡都能接受到通告  1：尽可能避免本网卡与不匹配的目标进行通告2：只在本网卡通告 打开sysctl.conf:\nvim /etc/sysctl.conf 配置所有网卡、默认网卡以及虚拟网卡的arp响应级别和通告行为，分别对应：all，default，lo\n# configration for lvs  net.ipv4.conf.all.arp_ignore = 1 net.ipv4.conf.default.arp_ignore = 1 net.ipv4.conf.lo.arp_ignore = 1  net.ipv4.conf.all.arp_announce = 2 net.ipv4.conf.default.arp_announce = 2 net.ipv4.conf.lo.arp_announce = 2 刷新配置文件\nsysctl -p 增加一个网关，用于接收数据报文，当有请求到本机后，会交给lo去处理\nroute add -host 192.168.1.150 dev lo:1 将网关添加至开机启动\necho \u0026#34;route add -host 192.168.1.150 dev lo:1\u0026#34; \u0026gt;\u0026gt; /etc/rc.local (4)使用ipvsadm配置集群规则\n创建LVS节点，用户访问的集群调度者\nipvsadm -A -t 192.168.1.150:80 -s rr -p 5  -A：添加集群 -t：tcp协议ip地址：设定集群的访问 ip：也就是LVS的虚拟ip -s：设置负载均衡的算法， rr：表示轮询 -p：设置连接持久化的时间,在指定时间内同一个用户的请求会访问到同一个服务器中  创建多台RS真实服务器\nipvsadm -a -t 192.168.1.150:80 -r 192.168.1.171:80 -g ipvsadm -a -t 192.168.1.150:80 -r 192.168.1.172:80 -g  -a：添加真实服务器 -t：tcp协议 -r：真实服务器的ip地址 -g：设定DR模式  保存到规则库，否则重启失效\nipvsadm -S 检查集群\n#查看集群列表 ipvsadm -Ln #查看集群状态 ipvsadm -Ln --stats 一些其他命令\n # 重启ipvsadm，重启后需要重新配置   service ipvsadm restart  # 查看持久化连接   ipvsadm -Ln --persistent-conn  # 查看连接请求过期时间以及请求源ip和目标ip   ipvsadm -Lnc  # 设置tcp tcpfin udp 的过期时间（一般保持默认）   ipvsadm --set 1 1 1  # 查看过期时间   ipvsadm -Ln --timeout (5)访问虚拟ip，完成LVS搭建\n附：LVS的负载均衡算法 (1)静态算法 静态：根据LVS本身自由的固定的算法分发用户请求。\n 轮询（Round Robin 简写’rr’）：轮询算法假设所有的服务器处理请求的能力都一样的，调度器会把所有的请求平均分配给每个真实服务器。（同Nginx的轮询） 加权轮询（Weight Round Robin 简写’wrr’）：安装权重比例分配用户请求。权重越高，被分配到处理的请求越多。（同Nginx的权重） 源地址散列（Source Hash 简写’sh’）：同一个用户ip的请求，会由同一个RS来处理。（同Nginx的ip_hash） 目标地址散列（Destination Hash 简写’dh’）：根据url的不同，请求到不同的RS。（同Nginx的url_hash）  (2)动态算法 动态：会根据流量的不同，或者服务器的压力不同来分配用户请求，这是动态计算的。\n  最小连接数（Least Connections 简写’lc’）：把新的连接请求分配到当前连接数最小的服务器。\n  加权最少连接数（Weight Least Connections 简写’wlc’）：服务器的处理性能用数值来代表，权重越大处理的请求越多。Real Server 有可能会存在性能上的差异，wlc动态获取不同服务器的负载状况，把请求分发到性能好并且比较空闲的服务器。\n  最短期望延迟（Shortest Expected Delay 简写’sed’）：特殊的wlc算法。举例阐述，假设有ABC三台服务器，权重分别为1、2、3 。如果使用wlc算法的话，当一个新请求进来，它可能会分给ABC中的任意一个。使用sed算法后会进行如下运算：\n   A：（1+1）/1=2 B：（1+2）/2=3/2 C：（1+3）/3=4/3    最终结果，会把这个请求交给得出运算结果最小的服务器。最少队列调度（Never Queue 简写’nq’）：永不使用队列。如果有Real Server的连接数等于0，则直接把这个请求分配过去，不需要在排队等待运算了（sed运算）。\n八、搭建Keepalived+Lvs+Nginx高可用集群负载均衡 如果原先服务器上配置了LVS+nginx需要清空ipvsadm中的配置\nipvsadm -C 如果配置了Keepalived+Nginx双主集群也需要去除掉Keepalived中原先的配置，按照的后文进行配置\n(1)使用keepalived配置Master LVS 在LVS的机器上安装keepalived，安装过程参考上文\n(1)修改keepalived的配置\nglobal_defs {  router_id keep_151 } vrrp_instance VI_1 {  state MASTER  interface ens33  virtual_router_id 41  priority 100  advert_int 1  authentication {  auth_type PASS  auth_pass 1111  }  virtual_ipaddress {  192.168.1.150  } }  #配置集群访问的ip+端口，端口和nginx保持一致 virtual_server 192.168.1.150 80{  #健康检查的时间，单位：秒  delay_loop 6  #配置负载均衡的算法，默认的轮询  lb_algo rr  #设置LVS的模式 NAT|TUN|DR  lb-kind DR  #设置会话持久化的时间  persistence_timeout 5  #协议  protocol TCP   #配置负载均衡的真实服务器，也就是nginx节点的具体的ip地址  real_server 192.168.1.171 80{  #轮询权重配比  weight 1  #设置健康检查  TCP_CHECK {  #检查80端口  connect_port 80  #超时时间  connect_timeout 2  #重试次数  nb_get_retry 2  #重试间隔时间  delay_before_retry 3  }  }  real_server 192.168.1.171 80{  weight 1  TCP_CHECK {  connect_port 80  connect_timeout 2  nb_get_retry 2  delay_before_retry 3  }  } } (2)启动/重启keepalived\nsystemctl restart keepalived (2)使用keepalived配置Backup LVS 配置在备用机上\nglobal_defs {  router_id keep_152 } vrrp_instance VI_1 {  state BACKUP  interface ens33  virtual_router_id 41  priority 50  advert_int 1  authentication {  auth_type PASS  auth_pass 1111  }  virtual_ipaddress {  192.168.1.150  } }  #配置集群访问的ip+端口，端口和nginx保持一致 virtual_server 192.168.1.150 80{  #健康检查的时间，单位：秒  delay_loop 6  #配置负载均衡的算法，默认的轮询  lb_algo rr  #设置LVS的模式 NAT|TUN|DR  lb-kind DR  #设置会话持久化的时间  persistence_timeout 5  #协议  protocol TCP   #配置负载均衡的真实服务器，也就是nginx节点的具体的ip地址  real_server 192.168.1.171 80{  #轮询权重配比  weight 1  #设置健康检查  TCP_CHECK {  #检查80端口  connect_port 80  #超时时间  connect_timeout 2  #重试次数  nb_get_retry 2  #重试间隔时间  delay_before_retry 3  }  }  real_server 192.168.1.171 80{  weight 1  TCP_CHECK {  connect_port 80  connect_timeout 2  nb_get_retry 2  delay_before_retry 3  }  } } ","permalink":"https://iblog.zone/archives/nginx%E5%AE%89%E8%A3%85%E5%92%8C%E9%AB%98%E5%8F%AF%E7%94%A8/","summary":"一、Nginx安装 1、去官网http://nginx.org/下载对应的nginx包，推荐使用稳定版本 2、上传nginx到linux系统 3、安装依赖环境 (1)安装gcc环境\nyum install gcc-c++ (2)安装PCRE库，用于解析正则表达式\nyum install -y pcre pcre-devel (3)zlib压缩和解压缩依赖\nyum install -y zlib zlib-devel (4)SSL 安全的加密的套接字协议层，用于HTTP安全传输，也就是https\nyum install -y openssl openssl-devel 4、解压，需要注意，解压后得到的是源码，源码需要编译后才能安装 tar -zxvf nginx-1.16.1.tar.gz 5、编译之前，先创建nginx临时目录，如果不创建，在启动nginx的过程中会报错 mkdir /var/temp/nginx -p 6、在nginx目录，输入如下命令进行配置，目的是为了创建makefile文件 ./configure \\  --prefix=/usr/local/nginx \\  --pid-path=/var/run/nginx/nginx.pid \\  --lock-path=/var/lock/nginx.lock \\  --error-log-path=/var/log/nginx/error.log \\  --http-log-path=/var/log/nginx/access.log \\  --with-http_gzip_static_module \\  --http-client-body-temp-path=/var/temp/nginx/client \\  --http-proxy-temp-path=/var/temp/nginx/proxy \\  --http-fastcgi-temp-path=/var/temp/nginx/fastcgi \\  --http-uwsgi-temp-path=/var/temp/nginx/uwsgi \\  --http-scgi-temp-path=/var/temp/nginx/scgi 注：代表在命令行中换行，用于提高可读性配置命令：","title":"Nginx安装和高可用"},{"content":"  查看防火墙某个端口是否开放\nfirewall-cmd \u0026ndash;query-port=3306/tcp\n  开放防火墙端口3306\nfirewall-cmd \u0026ndash;zone=public \u0026ndash;add-port=3306/tcp \u0026ndash;permanent\n注意：开放端口后要重启防火墙生效\n  重启防火墙\nsystemctl restart firewalld\n  关闭防火墙端口\nfirewall-cmd \u0026ndash;remove-port=3306/tcp \u0026ndash;permanent\n  查看防火墙状态\nsystemctl status firewalld\n  关闭防火墙\nsystemctl stop firewalld\n  打开防火墙\nsystemctl start firewalld\n  开放一段端口\nfirewall-cmd \u0026ndash;zone=public \u0026ndash;add-port=40000-45000/tcp \u0026ndash;permanent\n  查看开放的端口列表\nfirewall-cmd \u0026ndash;zone=public \u0026ndash;list-ports\n  ","permalink":"https://iblog.zone/archives/firewall%E9%98%B2%E7%81%AB%E5%A2%99%E5%B8%B8%E7%94%A8%E6%93%8D%E4%BD%9C/","summary":"  查看防火墙某个端口是否开放\nfirewall-cmd \u0026ndash;query-port=3306/tcp\n  开放防火墙端口3306\nfirewall-cmd \u0026ndash;zone=public \u0026ndash;add-port=3306/tcp \u0026ndash;permanent\n注意：开放端口后要重启防火墙生效\n  重启防火墙\nsystemctl restart firewalld\n  关闭防火墙端口\nfirewall-cmd \u0026ndash;remove-port=3306/tcp \u0026ndash;permanent\n  查看防火墙状态\nsystemctl status firewalld\n  关闭防火墙\nsystemctl stop firewalld\n  打开防火墙\nsystemctl start firewalld\n  开放一段端口\nfirewall-cmd \u0026ndash;zone=public \u0026ndash;add-port=40000-45000/tcp \u0026ndash;permanent\n  查看开放的端口列表\nfirewall-cmd \u0026ndash;zone=public \u0026ndash;list-ports\n  ","title":"Firewall防火墙常用操作"},{"content":"如果你经常遇到 Java 线上性能问题束手无策，看着线上服务 CPU 飙升一筹莫展，发现内存不断泄露满脸茫然。别慌，这里有一款低开销、自带火焰图、让你大呼好用的 Java 性能分析工具 - async-profiler。\n最近 Arthas 性能分析工具上线了火焰图分析功能，Arthas 使用 async-profiler 生成 CPU/内存火焰图进行性能分析，弥补了之前内存分析的不足。在 Arthas 上使用还是比较方便的，使用方式可以看官方文档。这篇文章介绍 async-profiler 相关内容。\nArthas 火焰图官方文档：alibaba.github.io/arthas/prof…\n如果你想了解更多 Arthas 信息，可以参考：Arthas - Java 线上问题定位处理的终极利器\nasync-profiler 介绍 async-profiler 是一款开源的 Java 性能分析工具，原理是基于 HotSpot 的 API，以微乎其微的性能开销收集程序运行中的堆栈信息、内存分配等信息进行分析。\n使用 async-profiler 可以做下面几个方面的分析。\n CPU cycles Hardware and Software performance counters like cache misses, branch misses, page faults, context switches etc. Allocations in Java Heap Contented lock attempts, including both Java object monitors and ReentrantLocks  我们常用的是 CPU 性能分析和 Heap 内存分配分析。在进行 CPU 性能分析时，仅需要非常低的性能开销就可以进行分析，这也是这个工具的优点之一。\n在进行 Heap 分配分析时，async-profiler 工具会收集内存分配信息，而不是去检测占用 CPU 的代码。async-profiler 不使用侵入性的技术，例如字节码检测工具或者探针检测等，这也说明 async-profiler 的内存分配分析像 CPU 性能分析一样，不会产生太大的性能开销，同时也不用写出庞大的堆栈文件再去进行进一步处理，。\nasync-profile 目前支持 Linux 和 macOS 平台（macOS 下只能分析用户空间的代码）。\n Linux / x64 / x86 / ARM / AArch64 macOS / x64  async-profiler 工具在采样后可以生成采样结果的日志报告，也可以生成 SVG 格式的火焰图，在之前生成火焰图要使用 FlameGraph 工具。现在已经不需要了，从 1.2 版本开始，就已经内置了开箱即用的 SVG 文件生成功能。\n其他信息可以看官方文档：github.com/jvm-profili…\nasync-profiler 安装 下载 async-profiler 工具可以在官方的 Github 上直接下载编译好的文件，如果你就是想体验手动挡的感觉，也可以克隆项目，手动编译一下，不得不说这个工具十分的易用，我在手动编译的过程十分顺滑，没有出现任何问题。\n如果你想下载编译好的，可以到这里下载。\ngithub.com/jvm-profili…\n如果想体验手动挡的感觉，可以克隆整个项目，进项项目编译。\n手动编译的环境要求。\n JDK GCC  下面是手动安装的操作命令。\ngit clone https://github.com/jvm-profiling-tools/async-profiler cd async-profiler make 执行 make 命令编译后会在项目的目录下生成一个 build 文件夹，里面存放着编译的结果。下面是我手动编译的过程输出。\n➜ develop git clone https://github.com/jvm-profiling-tools/async-profiler Cloning into \u0026#39;async-profiler\u0026#39;... remote: Enumerating objects: 69, done. remote: Counting objects: 100% (69/69), done. remote: Compressing objects: 100% (54/54), done. remote: Total 1805 (delta 34), reused 32 (delta 15), pack-reused 1736 Receiving objects: 100% (1805/1805), 590.78 KiB | 23.00 KiB/s, done. Resolving deltas: 100% (1288/1288), done. ➜ develop cd async-profiler ➜ async-profiler git:(master) make mkdir -p build g++ -O2 -D_XOPEN_SOURCE -D_DARWIN_C_SOURCE -DPROFILER_VERSION=\\\u0026#34;1.6\\\u0026#34; -I/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/include -I/Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/include/darwin -fPIC -shared -o build/libasyncProfiler.so src/*.cpp -ldl -lpthread gcc -O2 -DJATTACH_VERSION=\\\u0026#34;1.5\\\u0026#34; -o build/jattach src/jattach/jattach.c mkdir -p build/classes /Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/bin/javac -source 6 -target 6 -d build/classes src/java/one/profiler/AsyncProfiler.java src/java/one/profiler/AsyncProfilerMXBean.java src/java/one/profiler/Counter.java src/java/one/profiler/Events.java 警告: [options] 未与 -source 1.6 一起设置引导类路径 1 个警告 /Library/Java/JavaVirtualMachines/jdk1.8.0_181.jdk/Contents/Home/bin/jar cvf build/async-profiler.jar -C build/classes . 已添加清单 正在添加: one/(输入 = 0) (输出 = 0)(存储了 0%) 正在添加: one/profiler/(输入 = 0) (输出 = 0)(存储了 0%) 正在添加: one/profiler/AsyncProfiler.class(输入 = 1885) (输出 = 908)(压缩了 51%) 正在添加: one/profiler/Events.class(输入 = 405) (输出 = 286)(压缩了 29%) 正在添加: one/profiler/Counter.class(输入 = 845) (输出 = 473)(压缩了 44%) 正在添加: one/profiler/AsyncProfilerMXBean.class(输入 = 631) (输出 = 344)(压缩了 45%) rm -rf build/classes ➜ async-profiler git:(master) async-profiler 使用 运行项目里的 profiler.sh 可以看到 async-profiler 的使用帮助文档。\n➜ async-profiler git:(master) ./profiler.sh Usage: ./profiler.sh [action] [options] \u0026lt;pid\u0026gt; Actions:  start start profiling and return immediately  resume resume profiling without resetting collected data  stop stop profiling  status print profiling status  list list profiling events supported by the target JVM  collect collect profile for the specified period of time  and then stop (default action) Options:  -e event profiling event: cpu|alloc|lock|cache-misses etc.  -d duration run profiling for \u0026lt;duration\u0026gt; seconds  -f filename dump output to \u0026lt;filename\u0026gt;  -i interval sampling interval in nanoseconds  -j jstackdepth maximum Java stack depth  -b bufsize frame buffer size  -t profile different threads separately  -s simple class names instead of FQN  -g print method signatures  -a annotate Java method names  -o fmt output format: summary|traces|flat|collapsed|svg|tree|jfr  -v, --version display version string   --title string SVG title  --width px SVG width  --height px SVG frame height  --minwidth px skip frames smaller than px  --reverse generate stack-reversed FlameGraph / Call tree   --all-kernel only include kernel-mode events  --all-user only include user-mode events  --sync-walk use synchronous JVMTI stack walker (dangerous!)  \u0026lt;pid\u0026gt; is a numeric process ID of the target JVM  or \u0026#39;jps\u0026#39; keyword to find running JVM automatically  Example: ./profiler.sh -d 30 -f profile.svg 3456  ./profiler.sh start -i 999000 jps  ./profiler.sh stop -o summary,flat jps 可以看到使用的方式是：Usage: ./profiler.sh [action] [options] ，也就是 命令+操作+参数+PID。\n常用的使用的几个步骤：\n 查看 java 进程的 PID（可以使用 jps ）。 使用 ./profiler.sh start 开始采样。 使用 ./profiler.sh status 查看已经采样的时间。 使用 ./profiler.sh stop 停止采样，输出结果。  这种方式使用起来多费劲啊，而且最后输出的是文本结果，看起来更是费劲，为了不那么费劲，可以使用帮助里给的采样后生成 SVG 文件例子。\n./profiler.sh -d 30 -f profile.svg 3456 这个命令的意思是，对 PID 为 3456 的 java 进程采样 30 秒，然后生成 profile.svg 结果文件。\n默认情况下是分析 CPU 性能，如果要进行其他分析，可以使用 -e 参数。\n-e event profiling event: cpu|alloc|lock|cache-misses etc. 可以看到支持的分析事件有 CPU、Alloc、Lock、Cache-misses 。\nasync-profiler 案例 上面说完了 async-profiler 工具的作用和使用方式，既然能进行 CPU 性能分析和 Heap 内存分配分析，那么我们就写几个不一般的方法分析试试看。看看是不是有像上面介绍的那么好用。\nJava 案例编码 很简单的几个方法，hotmethod 方法写了几个常见操作，三个方法中很明显 hotmethod3 方法里的生成 UUID 和 replace（需要正则匹配）操作消耗的 CPU 性能会较多。allocate 方法里因为要不断的创建长度为 6万的数组，消耗的内存空间一定是最多的。\nimport java.util.ArrayList; import java.util.Random; import java.util.UUID;  /** * \u0026lt;p\u0026gt; * 模拟热点代码 * * @Author niujinpeng */ public class HotCode {   private static volatile int value;   private static Object array;   public static void main(String[] args) {  while (true) {  hotmethod1();  hotmethod2();  hotmethod3();  allocate();  }  }   /** * 生成 6万长度的数组 */  private static void allocate() {  array = new int[6 * 1000];  array = new Integer[6 * 1000];  }   /** * 生成一个UUID */  private static void hotmethod3() {  ArrayList\u0026lt;String\u0026gt; list = new ArrayList\u0026lt;\u0026gt;();  UUID uuid = UUID.randomUUID();  String str = uuid.toString().replace(\u0026#34;-\u0026#34;, \u0026#34;\u0026#34;);  list.add(str);  }   /** * 数字累加 */  private static void hotmethod2() {  value++;  }   /** * 生成一个随机数 */  private static void hotmethod1() {  Random random = new Random();  int anInt = random.nextInt();  } } CPU 性能分析 运行上面的程序，然后使用 JPS 命令查看 PID 信息。\n➜ develop jps 2800 Jps 2449 HotCode 2450 Launcher 805 RemoteMavenServer36 470 NutstoreGUI 699 ➜ develop 上面运行的类名是 HotCode，可以看到对应的 PID 是 2449。\n使用 ./profiler.sh -d 20 -f 2449.svg 2449 命令对 2449 号进程采样20秒，然后得到生成的 2449.svg 文件，然后我们使用浏览器打开这个文件，可以看到 CPU 的使用火焰图。\n关于火焰图怎么看，一言以蔽之：火焰图里，横条越长，代表使用的越多，从下到上是调用堆栈信息。在这个图里可以看到 main 方法上面的调用中 hotmethod3 方法的 CPU 使用是最多的，点击这个方法。还可能看到更详细的信息。\n可以看到 replace 方法占用的 CPU 最多，也是程序中性能问题所在，是需要注意的地方。\nHeap 内存分析 还是上面运行的程序，进程 PID 还是 2449，这次使用 -e 参数分析内存使用情况。\n命令：./profiler.sh -d 20 -e alloc -f 2449-alloc.svg 2449\n命令的意思是收集进程号是 2449 的进程的内存信息 20 秒，然后输出为 2449-alloc.svg 文件。20秒后得到 svg 文件使用浏览器打开，可以看到内存分配情况。\n依旧是横条越长，代表使用的越多，从下到上是调用堆栈信息。从图里可以看出来 main 方法调用的 allocate 方法使用的内存最多，这个方法里的 Integer 类型数组占用的内存又最多，为 71%。\n文中测试代码已经上传到 Github：github.com/niumoo/lab-…\n","permalink":"https://iblog.zone/archives/java%E6%80%A7%E8%83%BD%E5%88%86%E6%9E%90%E5%B7%A5%E5%85%B7async-profiler%E7%81%AB%E7%84%B0%E5%9B%BE%E4%BD%BF%E7%94%A8/","summary":"如果你经常遇到 Java 线上性能问题束手无策，看着线上服务 CPU 飙升一筹莫展，发现内存不断泄露满脸茫然。别慌，这里有一款低开销、自带火焰图、让你大呼好用的 Java 性能分析工具 - async-profiler。\n最近 Arthas 性能分析工具上线了火焰图分析功能，Arthas 使用 async-profiler 生成 CPU/内存火焰图进行性能分析，弥补了之前内存分析的不足。在 Arthas 上使用还是比较方便的，使用方式可以看官方文档。这篇文章介绍 async-profiler 相关内容。\nArthas 火焰图官方文档：alibaba.github.io/arthas/prof…\n如果你想了解更多 Arthas 信息，可以参考：Arthas - Java 线上问题定位处理的终极利器\nasync-profiler 介绍 async-profiler 是一款开源的 Java 性能分析工具，原理是基于 HotSpot 的 API，以微乎其微的性能开销收集程序运行中的堆栈信息、内存分配等信息进行分析。\n使用 async-profiler 可以做下面几个方面的分析。\n CPU cycles Hardware and Software performance counters like cache misses, branch misses, page faults, context switches etc. Allocations in Java Heap Contented lock attempts, including both Java object monitors and ReentrantLocks  我们常用的是 CPU 性能分析和 Heap 内存分配分析。在进行 CPU 性能分析时，仅需要非常低的性能开销就可以进行分析，这也是这个工具的优点之一。","title":"Java性能分析工具Async-profiler(火焰图)使用"},{"content":"一、简介 redis-shake是阿里云Redis\u0026amp;MongoDB团队开源的用于redis数据同步的工具。\n1.1基本功能 它支持解析、恢复、备份、同步四个功能。以下主要介绍同步sync。\n 恢复restore：将RDB文件恢复到目的redis数据库。 备份dump：将源redis的全量数据通过RDB文件备份起来。 解析decode：对RDB文件进行读取，并以json格式解析存储。 同步sync：支持源redis和目的redis的数据同步，支持全量和增量数据的迁移。 同步rump：支持源redis和目的redis的数据同步，仅支持全量的迁移。采用scan和restore命令进行迁移。  1.2 基本原理 redis-shake的基本原理就是模拟一个从节点加入源redis集群，首先进行全量拉取并回放，然后进行增量的拉取（通过psync命令）。如下图所示：\n如果源端是集群模式，只需要启动一个redis-shake进行拉取，同时不能开启源端的move slot操作。如果目的端是集群模式，可以写入到一个结点，然后再进行slot的迁移，当然也可以多对多写入。 目前，redis-shake到目的端采用单链路实现，对于正常情况下，这不会成为瓶颈，但对于极端情况，qps比较大的时候，此部分性能可能成为瓶颈，后续我们可能会计划对此进行优化。另外，redis-shake到目的端的数据同步采用异步的方式，读写分离在2个线程操作，降低因为网络时延带来的同步性能下降。\n1.3 高效性 全量同步阶段并发执行，增量同步阶段异步执行，能够达到毫秒级别延迟（取决于网络延迟）。同时，我们还对大key同步进行分批拉取，优化同步性能。\n1.4 监控 用户可以通过我们提供的restful拉取metric来对redis-shake进行实时监控：curl 127.0.0.1:9320/metric。\n1.5 校验 可采用redis-full-check来校验同步的正确性。\n二、安装redis-shake 2.1 下载 wget https://github.com/alibaba/RedisShake/releases/download/release-v2.0.3-20200724/redis-shake-v2.0.3.tar.gz; tar zxvf redis-shake-v2.0.3.tar.gz; cd redis-shake-v2.0.3; 2.2 修改配置文件 vim redis-shake.conf - source.type: 源redis的类型，支持一下4种类型： standalone: 单db节点/主从版模式。 sentinel: sentinel模式。 cluster: 集群模式。 proxy: proxy模式。如果是阿里云redis的集群版，从proxy拉取/写入请选择proxy - source.address: 源redis的地址，不同的类型对应不同的地址： standalone模式下，需填写单个db节点的地址，主从版需输入master或者slave的地址。 sentinel模式下，例如：master:master@127.0.0.1:26379;127.0.0.1:26380 cluster模式下，需填写集群地址，以分号（;）分割。例如：10.1.1.1:20331;10.1.1.2:20441 proxy模式下，需要填写单个proxy的地址，此模式目前仅用于rump。 - source.password_raw：源redis的密码。 - target.type: 目的redis的类型 - target.address：目的redis的地址。 sentinel模式，例如：mymaster@127.0.0.1:26379;127.0.0.1:26380 cluster模式，参见source.address。 proxy模式下，填写proxy的地址，如果是多个proxy，则round-robin循环负载均衡连接，保证一个源端db连接只会对应一个proxy。 - target.password_raw：目的redis的密码。\n2.3 配置示例 2.3.1 单个节点到单个节点配置举例 source.type: standalone source.address: 192.168.118.41:6379 source.password_raw: 12345 target.type: standalone target.address: 192.168.118.51:6379 target.password_raw: 12345 2.3.2 集群版cluster到集群版cluster配置举例 source.type: cluster #源集群可只写从节点 source.address: 192.168.118.41:6380;192.168.118.42:6380;192.168.118.43:6380 source.password_raw: 12345 target.type: cluster target.address: 192.168.118.51:6379;192.168.118.52:6379;192.168.118.53:6379 target.password_raw: 12345 2.3.3 主从版/单节点到cluster配置举例 source.type: standalone source.address: 192.168.121.221:6380;192.168.121.222:6380;192.168.121.223:6380 source.password_raw: 12345 target.type: cluster target.address: 192.168.121.224:6379;192.168.121.225:6379;192.168.121.226:6379 target.password_raw: 12345 2.4 测试数据 大概建立了230w的测试数据\n#!/bin/bash for ((i=1;i\u0026lt;2300000;i++)) do echo -en \u0026#34;helloworld$i\u0026#34; | redis-cli -c -x set name$i \u0026gt;\u0026gt;redis.log done 2.5 配置源集群从节点，目标集群主节点 source.type = cluster source.address = 192.168.121.221:6380;192.168.121.222:6380;192.168.121.223:6380 source.password_raw = target.type = cluster target.address = 192.168.121.224:6379;192.168.121.225:6379;192.168.121.226:6379 target.password_raw = 2.6 启动 ./redis-shake.linux -conf=redis-shake.conf -type=sync   开始同步\n  全量同步阶段，显示百分比：\n  增量同步，出现字样sync rdb done后，当前dbSyncer进入增量同步:\n  其中forwardCommands表示发送的命令个数，filterCommands表示过滤的命令个数，比如opinfo或者指定了filter都会被过滤，writeBytes表示发送的字节数。\n同步完查看源集群和目标集群keys  同步过程中源集群监控  同步过程中目标集群监控  2.7 监控 http://192.168.121.226:9320/metric 三、 校验 3.1下载 wget https://github.com/alibaba/RedisFullCheck/releases/download/release-v1.4.8-20200212/redis-full-check-1.4.8.tar.gz tar zxvf redis-full-check-1.4.8.tar.gz cd redis-full-check-1.4.8 参数：\n-s, --source=SOURCE 源redis库地址（ip:port）， -p, --sourcepassword=Password 源redis库密码  --sourcedbtype= 源库的类别，0：db(单节点、主从)，1: cluster（集群版），2: 阿里云 -t, --target=TARGET 目的redis库地址（ip:port） -a, --targetpassword=Password 目的redis库密码  --targetdbtype= 参考sourcedbtype -d, --db=Sqlite3-DB-FILE 对于差异的key存储的sqlite3 db的位置，默认result.db  --comparetimes=COUNT 比较轮数 -m, --comparemode= 比较模式，1表示全量比较，2表示只对比value的长度，3只对比key是否存在，4全量比较的情况下，忽略大key的比较  --id= 用于打metric -q, --qps= qps限速阈值  --interval=Second 每轮之间的时间间隔  --batchcount=COUNT 批量聚合的数量  --parallel=COUNT 比较的并发协程数，默认5  --log=FILE log文件  --result=FILE 不一致结果记录到result文件中，格式：\u0026#39;db diff-type key field\u0026#39; 3.2开始校验 ./redis-full-check -s \u0026#34;192.168.121.221:6380;192.168.121.222:6380;192.168.121.223:6380\u0026#34; -t \u0026#34;192.168.121.224:6379;192.168.121.225:6379;192.168.121.226:6379\u0026#34; --comparemode=1 --comparetimes=3 --sourcedbtype=1 --targetdbtype=1 注：\u0026ndash;comparemode=1 比较模式：全量比较 \u0026ndash;comparetimes=3 比较轮数：3轮\n3.3 校验日志分析   初始化校验\n  开始scan比较\n  开始比较第二轮\n  开始比较第三轮\n  校验结果验证\n  查看异常表数据\n  结果会保存在sqlite3 db file中，不指定的话，就是当前目录的 result.db 文件：比如有3轮比较，那么会有result.db.1，result.db.2，result.db.3 3个文件, - 表key：保存不一致的key - 表field：保存hash,set,zset,list不一致的field, list 存的是下标值 - 表feild的key_id字段关联表key的id字段 - 表key_和field_：保存第N轮比较后的结果，即中间结果\n","permalink":"https://iblog.zone/archives/redis-shake%E6%95%B0%E6%8D%AE%E5%90%8C%E6%AD%A5%E8%BF%81%E7%A7%BB%E5%B7%A5%E5%85%B7/","summary":"一、简介 redis-shake是阿里云Redis\u0026amp;MongoDB团队开源的用于redis数据同步的工具。\n1.1基本功能 它支持解析、恢复、备份、同步四个功能。以下主要介绍同步sync。\n 恢复restore：将RDB文件恢复到目的redis数据库。 备份dump：将源redis的全量数据通过RDB文件备份起来。 解析decode：对RDB文件进行读取，并以json格式解析存储。 同步sync：支持源redis和目的redis的数据同步，支持全量和增量数据的迁移。 同步rump：支持源redis和目的redis的数据同步，仅支持全量的迁移。采用scan和restore命令进行迁移。  1.2 基本原理 redis-shake的基本原理就是模拟一个从节点加入源redis集群，首先进行全量拉取并回放，然后进行增量的拉取（通过psync命令）。如下图所示：\n如果源端是集群模式，只需要启动一个redis-shake进行拉取，同时不能开启源端的move slot操作。如果目的端是集群模式，可以写入到一个结点，然后再进行slot的迁移，当然也可以多对多写入。 目前，redis-shake到目的端采用单链路实现，对于正常情况下，这不会成为瓶颈，但对于极端情况，qps比较大的时候，此部分性能可能成为瓶颈，后续我们可能会计划对此进行优化。另外，redis-shake到目的端的数据同步采用异步的方式，读写分离在2个线程操作，降低因为网络时延带来的同步性能下降。\n1.3 高效性 全量同步阶段并发执行，增量同步阶段异步执行，能够达到毫秒级别延迟（取决于网络延迟）。同时，我们还对大key同步进行分批拉取，优化同步性能。\n1.4 监控 用户可以通过我们提供的restful拉取metric来对redis-shake进行实时监控：curl 127.0.0.1:9320/metric。\n1.5 校验 可采用redis-full-check来校验同步的正确性。\n二、安装redis-shake 2.1 下载 wget https://github.com/alibaba/RedisShake/releases/download/release-v2.0.3-20200724/redis-shake-v2.0.3.tar.gz; tar zxvf redis-shake-v2.0.3.tar.gz; cd redis-shake-v2.0.3; 2.2 修改配置文件 vim redis-shake.conf - source.type: 源redis的类型，支持一下4种类型： standalone: 单db节点/主从版模式。 sentinel: sentinel模式。 cluster: 集群模式。 proxy: proxy模式。如果是阿里云redis的集群版，从proxy拉取/写入请选择proxy - source.address: 源redis的地址，不同的类型对应不同的地址： standalone模式下，需填写单个db节点的地址，主从版需输入master或者slave的地址。 sentinel模式下，例如：master:master@127.0.0.1:26379;127.0.0.1:26380 cluster模式下，需填写集群地址，以分号（;）分割。例如：10.1.1.1:20331;10.1.1.2:20441 proxy模式下，需要填写单个proxy的地址，此模式目前仅用于rump。 - source.password_raw：源redis的密码。 - target.type: 目的redis的类型 - target.address：目的redis的地址。 sentinel模式，例如：mymaster@127.0.0.1:26379;127.0.0.1:26380 cluster模式，参见source.address。 proxy模式下，填写proxy的地址，如果是多个proxy，则round-robin循环负载均衡连接，保证一个源端db连接只会对应一个proxy。 - target.","title":"redis-shake数据同步\u0026迁移工具"},{"content":"1. 类和对象 面向对象和面向过程的思想对比 :\n​\t**面向过程 ：**是一种以过程为中心的编程思想，实现功能的每一步，都是自己实现的\n​\t**面向对象 ：**是一种以对象为中心的编程思想，通过指挥对象实现具体的功能\n1.1 类和对象的关系 客观存在的事物皆为对象 ，所以我们也常常说万物皆对象。\n 类  类的理解  类是对现实生活中一类具有共同属性和行为的事物的抽象 类是对象的数据类型，类是具有相同属性和行为的一组对象的集合 简单理解：类就是对现实事物的一种描述   类的组成  属性：指事物的特征，例如：手机事物（品牌，价格，尺寸） 行为：指事物能执行的操作，例如：手机事物（打电话，发短信）     类和对象的关系  类：类是对现实生活中一类具有共同属性和行为的事物的抽象 对象：是能够看得到摸的着的真实存在的实体 简单理解：类是对事物的一种描述，对象则为具体存在的事物    1.2 类的定义【应用】 类的组成是由属性和行为两部分组成\n **属性：**在类中通过成员变量来体现（类中方法外的变量） **行为：**在类中通过成员方法来体现（和前面的方法相比去掉static关键字即可）  类的定义步骤：\n​\t① 定义类\n​\t② 编写类的成员变量\n​\t③ 编写类的成员方法\npublic class Student {  // 属性 : 姓名, 年龄  // 成员变量: 跟之前定义变量的格式一样, 只不过位置发生了改变, 类中方法外  String name;  int age;   // 行为 : 学习  // 成员方法: 跟之前定义方法的格式一样, 只不过去掉了static关键字.  public void study(){  System.out.println(\u0026#34;学习\u0026#34;);  } } 1.3 对象的创建和使用  创建对象的格式：  类名 对象名 = new 类名();   调用成员的格式：  对象名.成员变量 对象名.成员方法();   示例代码 :  package com.itheima.object1;  public class TestStudent {  /* 创建对象的格式: 类名 对象名 = new 类名(); 调用成员变量的格式: 对象名.变量名 调用成员方法的格式: 对象名.方法名(); */  public static void main(String[] args) {  // 类名 对象名 = new 类名();  Student stu = new Student();  // 对象名.变量名  // 默认初始化值  System.out.println(stu.name); // null  System.out.println(stu.age); // 0   stu.name = \u0026#34;张三\u0026#34;;  stu.age = 23;   System.out.println(stu.name); // 张三  System.out.println(stu.age); // 23   // 对象名.方法名();  stu.study();  // com.itheima.object1.Student@b4c966a  // 全类名(包名 + 类名)  System.out.println(stu);  } } 1.4 案例-手机类的创建和使用 **需求 ：**首先定义一个手机类，然后定义一个手机测试类，在手机测试类中通过对象完成成员变量和成员方法的使用\n分析 ：\n 成员变量：品牌, 价格 成员方法：打电话, 发短信 示例代码：  package com.itheima.test1;  public class Phone {  // 品牌, 价格  String brand;  int price;   // 打电话, 发短信  public void call(String name){  System.out.println(\u0026#34;给\u0026#34;+name+\u0026#34;打电话\u0026#34;);  }   public void sendMessage(){  System.out.println(\u0026#34;群发短信\u0026#34;);  } } package com.itheima.test1;  public class TestPhone {  public static void main(String[] args) {  // 1. 创建对象  Phone p = new Phone();  // 2. 给成员变量进行赋值  p.brand = \u0026#34;大米\u0026#34;;  p.price = 2999;  // 3. 打印赋值后的成员变量  System.out.println(p.brand + \u0026#34;...\u0026#34; + p.price);  // 4. 调用成员方法  p.call(\u0026#34;阿强\u0026#34;);  p.sendMessage();  } } 2. 对象内存图 2.1 单个对象内存图【理解】 2.2 多个对象内存图【理解】   总结：\n多个对象在堆内存中，都有不同的内存划分，成员变量存储在各自的内存区域中，成员方法多个对象共用的一份\n  2.3 多个对象指向相同内存图【理解】   总结 :\n当多个对象的引用指向同一个内存空间（变量所记录的地址值是一样的）\n只要有任何一个对象修改了内存中的数据，随后，无论使用哪一个对象进行数据获取，都是修改后的数据。\n  3. 成员变量和局部变量 3.1 成员变量和局部变量的区别  **类中位置不同：**成员变量（类中方法外）局部变量（方法内部或方法声明上） **内存中位置不同：**成员变量（堆内存）局部变量（栈内存） **生命周期不同：**成员变量（随着对象的存在而存在，随着对象的消失而消失）局部变量（随着方法的调用而存在，醉着方法的调用完毕而消失） **初始化值不同：**成员变量（有默认初始化值）局部变量（没有默认初始化值，必须先定义，赋值才能使用）  4. 封装 4.1 private关键字 ​\t概述 : private是一个修饰符，可以用来修饰成员（成员变量，成员方法）\n​\t特点 : 被private修饰的成员，只能在本类进行访问，针对private修饰的成员变量，如果需要被其他类使用，\t提供相应的操作\n​\t提供“get变量名()”方法，用于获取成员变量的值，方法用public修饰\n​\t提供“set变量名(参数)”方法，用于设置成员变量的值，方法用public修饰\n​\t示例代码：\n/* 学生类 */ class Student {  //成员变量  String name;  private int age;   //提供get/set方法  public void setAge(int a) {  if(a\u0026lt;0 || a\u0026gt;120) {  System.out.println(\u0026#34;你给的年龄有误\u0026#34;);  } else {  age = a;  }  }   public int getAge() {  return age;  }   //成员方法  public void show() {  System.out.println(name + \u0026#34;,\u0026#34; + age);  } } /* 学生测试类 */ public class StudentDemo {  public static void main(String[] args) {  //创建对象  Student s = new Student();  //给成员变量赋值  s.name = \u0026#34;林青霞\u0026#34;;  s.setAge(30);  //调用show方法  s.show();  } } 4.2 private关键字的使用   需求：\n 定义标准的学生类，要求name和age使用private修饰 并提供set和get方法以及便于显示数据的show方法 测试类中创建对象并使用，最终控制台输出 林青霞，30    示例代码：\n/* 学生类 */ class Student {  //成员变量  private String name;  private int age;   //get/set方法  public void setName(String n) {  name = n;  }   public String getName() {  return name;  }   public void setAge(int a) {  age = a;  }   public int getAge() {  return age;  }   public void show() {  System.out.println(name + \u0026#34;,\u0026#34; + age);  } } /* 学生测试类 */ public class StudentDemo {  public static void main(String[] args) {  //创建对象  Student s = new Student();   //使用set方法给成员变量赋值  s.setName(\u0026#34;林青霞\u0026#34;);  s.setAge(30);   s.show();   //使用get方法获取成员变量的值  System.out.println(s.getName() + \u0026#34;---\u0026#34; + s.getAge());  System.out.println(s.getName() + \u0026#34;,\u0026#34; + s.getAge());   } }   4.3 this关键字【应用】 概述 : this修饰的变量用于指代成员变量，其主要作用是（区分局部变量和成员变量的重名问题）\n 方法的形参如果与成员变量同名，不带this修饰的变量指的是形参，而不是成员变量 方法的形参没有与成员变量同名，不带this修饰的变量指的是成员变量  代码实现 :\npublic class Student {  private String name;  private int age;   public void setName(String name) {  this.name = name;  }   public String getName() {  return name;  }   public void setAge(int age) {  this.age = age;  }   public int getAge() {  return age;  }   public void show() {  System.out.println(name + \u0026#34;,\u0026#34; + age);  } } 4.4 this内存原理【理解】  注意 : this代表当前调用方法的引用，哪个对象调用的方法，this就代表哪一个对象 图解 ：    4.5 封装思想  封装概述 是面向对象三大特征之一（封装，继承，多态） 是面向对象编程语言对客观世界的模拟，客观世界里成员变量都是隐藏在对象内部的，外界是无法直接操作的 封装原则 将类的某些信息隐藏在类内部，不允许外部程序直接访问，而是通过该类提供的方法来实现对隐藏信息的操作和访问 成员变量private，提供对应的getXxx()/setXxx()方法 封装好处 通过方法来控制成员变量的操作，提高了代码的安全性 把代码用方法进行封装，提高了代码的复用性  5. 构造方法 5.1 构造方法的格式和执行时机  格式注意 :  方法名与类名相同，大小写也要一致 没有返回值类型，连void都没有 没有具体的返回值（不能由retrun带回结果数据）   执行时机 ：  创建对象的时候调用，每创建一次对象，就会执行一次构造方法 不能手动调用构造方法   示例代码：  class Student {  private String name;  private int age;   //构造方法  public Student() {  System.out.println(\u0026#34;无参构造方法\u0026#34;);  }   public void show() {  System.out.println(name + \u0026#34;,\u0026#34; + age);  } } /* 测试类 */ public class StudentDemo {  public static void main(String[] args) {  //创建对象  Student s = new Student();  s.show();  } } 5.2 构造方法的作用  用于给对象的数据（属性）进行初始化  package com.itheima.constructor;  public class Student {  /* 格式: 1. 方法名需要跟类名相同, 大小写也要一致 2. 没有返回值类型, 连void都没有 3. 没有具体的返回值(不能由return带回具体的结果) */   private String name;  private int age;   // 1. 如果一个类中没有编写任何构造方法, 系统将会提供一个默认的无参数构造方法  public Student(){}   // 2. 如果手动编写了构造方法, 系统就不会再提供默认的无参数构造方法了  public Student(String name, int age){  this.name = name;  this.age = age;  System.out.println(\u0026#34;我是Student类的构造方法\u0026#34;);  }   public void show(){  System.out.println(name + \u0026#34;...\u0026#34; + age);  } } package com.itheima.constructor;  public class TestStudent {  public static void main(String[] args) {  Student stu1 = new Student(\u0026#34;张三\u0026#34;,23);  stu1.show();   Student stu2 = new Student();  } } 5.3 构造方法的注意事项 构造方法的创建 :\n​\t如果没有定义构造方法，系统将给出一个默认的无参数构造方法\n​\t如果定义了构造方法，系统将不再提供默认的构造方法\n构造方法的创建 :\n​\t如果没有定义构造方法，系统将给出一个默认的无参数构造方法如果定义了构造方法，系统将不再提供默认的构造方法\n推荐的使用方式 :\n​\t无论是否使用，都手动书写无参数构造方法，和带参数构造方法\n5.4 标准类的代码编写和使用 代码 :\npackage com.itheima.test3;  /* JavaBean类: 封装数据 */ public class Student {  private String name;  private int age;   public Student() {  }   public Student(String name, int age) {  this.name = name;  this.age = age;  }   public String getName() {  return name;  }   public void setName(String name) {  this.name = name;  }   public int getAge() {  return age;  }   public void setAge(int age) {  this.age = age;  }   public void show(){  System.out.println(name + \u0026#34;...\u0026#34; + age);  } } package com.itheima.test3;  public class TestStudent {  public static void main(String[] args) {  // 1. 无参数构造方法创建对象, 通过setXxx方法给成员变量进行赋值  Student stu1 = new Student();  stu1.setName(\u0026#34;张三\u0026#34;);  stu1.setAge(23);  stu1.show();   // 2. 通过带参数构造方法, 直接给属性进行赋值  Student stu2 = new Student(\u0026#34;李四\u0026#34;,24);  stu2.show();  } } ","permalink":"https://iblog.zone/archives/java%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1/","summary":"1. 类和对象 面向对象和面向过程的思想对比 :\n​\t**面向过程 ：**是一种以过程为中心的编程思想，实现功能的每一步，都是自己实现的\n​\t**面向对象 ：**是一种以对象为中心的编程思想，通过指挥对象实现具体的功能\n1.1 类和对象的关系 客观存在的事物皆为对象 ，所以我们也常常说万物皆对象。\n 类  类的理解  类是对现实生活中一类具有共同属性和行为的事物的抽象 类是对象的数据类型，类是具有相同属性和行为的一组对象的集合 简单理解：类就是对现实事物的一种描述   类的组成  属性：指事物的特征，例如：手机事物（品牌，价格，尺寸） 行为：指事物能执行的操作，例如：手机事物（打电话，发短信）     类和对象的关系  类：类是对现实生活中一类具有共同属性和行为的事物的抽象 对象：是能够看得到摸的着的真实存在的实体 简单理解：类是对事物的一种描述，对象则为具体存在的事物    1.2 类的定义【应用】 类的组成是由属性和行为两部分组成\n **属性：**在类中通过成员变量来体现（类中方法外的变量） **行为：**在类中通过成员方法来体现（和前面的方法相比去掉static关键字即可）  类的定义步骤：\n​\t① 定义类\n​\t② 编写类的成员变量\n​\t③ 编写类的成员方法\npublic class Student {  // 属性 : 姓名, 年龄  // 成员变量: 跟之前定义变量的格式一样, 只不过位置发生了改变, 类中方法外  String name;  int age;   // 行为 : 学习  // 成员方法: 跟之前定义方法的格式一样, 只不过去掉了static关键字.","title":"Java面向对象"},{"content":"1.Debug模式 1.1 什么是Debug模式 是供程序员使用的程序调试工具，它可以用于查看程序的执行流程，也可以用于追踪程序执行过程来调试程序。\n1.2 Debug介绍与操作流程   如何加断点\n 选择要设置断点的代码行，在行号的区域后面单击鼠标左键即可    如何运行加了断点的程序\n 在代码区域右键Debug执行    看哪里\n  看Debugger窗口\n  看Console窗口\n    点哪里\n 点Step Into (F7)这个箭头，也可以直接按F7    如何删除断点\n  选择要删除的断点，单击鼠标左键即可\n  如果是多个断点，可以每一个再点击一次。也可以一次性全部删除\n    2. 进制的介绍与书写格式 2.1 进制的介绍与书写格式 代码 :\npublic class Demo1 {  /* 十进制：Java中，数值默认都是10进制，不需要加任何修饰。 二进制：数值前面以0b开头，b大小写都可以。 八进制：数值前面以0开头。 十六进制：数值前面以0x开头，x大小写都可以。 注意: 书写的时候, 虽然加入了进制的标识, 但打印在控制台展示的都是十进制数据. */  public static void main(String[] args) {  System.out.println(10);  System.out.println(\u0026#34;二进制数据0b10的十进制表示为:\u0026#34; + 0b10);  System.out.println(\u0026#34;八进制数据010的十进制表示为:\u0026#34; + 010);  System.out.println(\u0026#34;十六进制数据0x10的十进制表示为:\u0026#34; + 0x10);  } } 2.2 任意进制到十进制的转换 2.3 进制转换-十进制到任意进制转换 ​\t2.3.1 : 十进制到二进制的转换\n​\t公式：除基取余使用源数据，不断的除以基数（几进制，基数就是几）得到余数，直到商为0，再将余数倒着拼起来即可。\n​\t需求：将十进制数字11，转换为2进制。\n​\t实现方式：源数据为11，使用11不断的除以基数，也就是2，直到商为0。\n​\t2.3.2 : 十进制到十六进制的转换\n​\t公式：除基取余使用源数据，不断的除以基数（几进制，基数就是几）得到余数，直到商为0，再将余数倒着拼起来即可。\n​\t需求：将十进制数字60，转换为16进制。\n​\t实现方式：源数据为60，使用60不断的除以基数，也就是16，直到商为0。\n​\t结论：十进制到任意进制的转换\n​\t公式：除基取余使用源数据，不断的除以基数（几进制，基数就是几）得到余数，直到商为0，再将余数倒着\t拼起来即可\n2.4 快速进制转换法 ​\t8421码：\n​\t8421码又称BCD码，是BCD代码中最常用的一种BCD： (Binary-Coded Decimal‎) 二进制码十进制数在这种编码方式中，每一位二进制值的1都是代表一个固定数值，把每一位的1代表的十进制数加起来得到的结果就是它所代表的十进制数。\n2.5 原码反码补码 前言 : 计算机中的数据，都是以二进制补码的形式在运算，而补码则是通过反码和原码推算出来的\n**原码 **:（可直观看出数据大小）\n就是二进制定点表示法，即最高位为符号位，【0】表示正，【1】表示负，其余位表示数值的大小。\n通过一个字节表示+7和-7，代码：byte b1 = 7; byte b2 = -7;一个字节等于8个比特位，也就是8个二进制位\n0(符号位)\t0000111\n1(符号位)\t0000111\n反码 : 正数的反码与其原码相同；负数的反码是对其原码逐位取反，但符号位除外。\n补码 : （数据以该状态进行运算）正数的补码与其原码相同；负数的补码是在其反码的末位加1。\n2.6 位运算-基本位运算符 package com.itheima.demo;  public class Demo2 {  /* 位运算: 位运算符指的是二进制位的运算，先将十进制数转成二进制后再进行运算。 在二进制位运算中，1表示true，0表示false。 \u0026amp; 位与 : 遇false则false, 遇0则0 00000000 00000000 00000000 00000110 // 6的二进制 \u0026amp; 00000000 00000000 00000000 00000010 // 2的二进制 ----------------------------------------- 00000000 00000000 00000000 00000010 // 结果: 2 | 位或 : 遇true则true, 遇1则1 ^ 位异或 : 相同为false, 不同为true ~ 取反 : 全部取反, 0变1, 1变0 (也包括符号位) 00000000 00000000 00000000 00000110 // 6的二进制补码 ~ 11111111 11111111 11111111 11111001 - 1 // -1求反码 ------------------------------------ 11111111 11111111 11111111 11111000 // 反码推原码 10000000 00000000 00000000 00000111 // -7 */  public static void main(String[] args) {  System.out.println(6 \u0026amp; 2);  System.out.println(~6);  } } 2.7 位运算-位移运算符 位运算概述 : 位运算符指的是二进制位的运算，先将十进制数转成二进制后再进行运算。在二进制位运算中，1表示true，0表示false。\n位运算符介绍 :\n代码 :\npackage com.itheima.demo;  public class Demo3 {  /* 位移运算符: \u0026lt;\u0026lt; 有符号左移运算，二进制位向左移动, 左边符号位丢弃, 右边补齐0 运算规律: 向左移动几位, 就是乘以2的几次幂 12 \u0026lt;\u0026lt; 2 (0)0000000 00000000 00000000 000011000 // 12的二进制 ----------------------------------------------------------------------------- \u0026gt;\u0026gt; 有符号右移运算，二进制位向右移动, 使用符号位进行补位 运算规律: 向右移动几位, 就是除以2的几次幂 000000000 00000000 00000000 0000001(1) // 3的二进制 ----------------------------------------------------------------------------- \u0026gt;\u0026gt;\u0026gt; 无符号右移运算符, 无论符号位是0还是1，都补0 010000000 00000000 00000000 00000110 // -6的二进制 */  public static void main(String[] args) {  System.out.println(12 \u0026lt;\u0026lt; 1); // 24  System.out.println(12 \u0026lt;\u0026lt; 2); // 48   } } package com.itheima.demo;  public class Demo4 {  /* ^ 运算符的特点 一个数, 被另外一个数, 异或两次, 该数本身不变 */  public static void main(String[] args) {  System.out.println(10 ^ 5 ^ 10);  } } 3.基础练习 3.1 数据交换 案例需求\n​\t已知两个整数变量a = 10，b = 20，使用程序实现这两个变量的数据交换 ​ 最终输出a = 20，b = 10;\n代码实现\npackage com.itheima.test;  public class Test1 {  /* 需求：已知两个整数变量a = 10，b = 20，使用程序实现这两个变量的数据交换 最终输出a = 20，b = 10; 思路： 1. 定义一个三方变量temp，将a原本记录的值，交给temp记录 （a的值，不会丢了） 2. 使用 a 变量记录 b 的值，（第一步交换完毕，b的值也丢不了了） 3. 使用 b 变量记录 temp的值，也就是a原本的值 （交换完毕） 4. 输出 a 和 b 变量即可 */  /* 动态初始化格式： 数据类型[][] 变量名 = new 数据类型[m][n]; m表示这个二维数组，可以存放多少个一维数组 n表示每一个一维数组，可以存放多少个元素 */  public static void main(String[] args) {  int a = 10;  int b = 20;   // 将a原本记录的值，交给temp记录 （a的值，不会丢了）  int temp = a;  // 用 a 变量记录 b 的值，（第一步交换完毕，b的值也丢不了了）  a = b;  // 使用 b 变量记录 temp的值，也就是a原本的值 （交换完毕）  b = temp;   // 输出 a 和 b 变量即可  System.out.println(\u0026#34;a=\u0026#34; + a);  System.out.println(\u0026#34;b=\u0026#34; + b);  } } 3.2 数组反转【应用】 案例需求 :\n​\t已知一个数组 arr = {19, 28, 37, 46, 50}; 用程序实现把数组中的元素值交换，\n​\t交换后的数组 arr = {50, 46, 37, 28, 19}; 并在控制台输出交换后的数组元素\n实现步骤 :\n  定义两个变量, start和end来表示开始和结束的指针.\n  确定交换条件, start \u0026lt; end 允许交换\n  循环中编写交换逻辑代码\n  每一次交换完成, 改变两个指针所指向的索引 start++, end\u0026ndash;\n  循环结束后, 遍历数组并打印, 查看反转后的数组\n  代码实现 :\npackage com.itheima.test;  public class Test2 {  /* 需求：已知一个数组 arr = {19, 28, 37, 46, 50}; 用程序实现把数组中的元素值交换， 交换后的数组 arr = {50, 46, 37, 28, 19}; 并在控制台输出交换后的数组元素。 步骤: 1. 定义两个变量, start和end来表示开始和结束的指针. 2. 确定交换条件, start \u0026lt; end 允许交换 3. 循环中编写交换逻辑代码 4. 每一次交换完成, 改变两个指针所指向的索引 start++, end-- 5. 循环结束后, 遍历数组并打印, 查看反转后的数组 */  public static void main(String[] args) {  int[] arr = {19, 28, 37, 46, 50};  // 1. 定义两个变量, start和end来表示开始和结束的指针.  int start = 0;  int end = arr.length -1;  // 2. 确定交换条件, start \u0026lt; end 允许交换  // 4. 每一次交换完成, 改变两个指针所指向的索引 start++, end--  // for(int start = 0, end = arr.length -1; start \u0026lt; end; start++, end--)  for( ; start \u0026lt; end; start++, end--){  // 3. 循环中编写交换逻辑代码  int temp = arr[start];  arr[start] = arr[end];  arr[end] = temp;  }   for (int i = 0; i \u0026lt; arr.length; i++) {  System.out.println(arr[i]);  }  } } 3.3 二维数组概述 ​\t概述 : 二维数组也是一种容器，不同于一维数组，该容器存储的都是一维数组容器\n3.4 二维数组动态初始化 动态初始化格式：  数据类型[][] 变量名 = new 数据类型[m][n]; m表示这个二维数组，可以存放多少个一维数组 n表示每一个一维数组，可以存放多少个元素 package com.itheima.demo;  public class Demo1Array {  /* 动态初始化格式： 数据类型[][] 变量名 = new 数据类型[m][n]; m表示这个二维数组，可以存放多少个一维数组 n表示每一个一维数组，可以存放多少个元素 */  public static void main(String[] args) {  // 数据类型[][] 变量名 = new 数据类型[m][n];  int[][] arr = new int[3][3];  /* [[I@10f87f48 @ : 分隔符 10f87f48 : 十六进制内存地址 I : 数组中存储的数据类型 [[ : 几个中括号就代表的是几维数组 */  System.out.println(arr);   /* 二维数组存储一维数组的时候, 存储的是一维数组的内存地址 */  System.out.println(arr[0]);  System.out.println(arr[1]);  System.out.println(arr[2]);   System.out.println(arr[0][0]);  System.out.println(arr[1][1]);  System.out.println(arr[2][2]);   // 向二维数组中存储元素  arr[0][0] = 11;  arr[0][1] = 22;  arr[0][2] = 33;   arr[1][0] = 11;  arr[1][1] = 22;  arr[1][2] = 33;   arr[2][0] = 11;  arr[2][1] = 22;  arr[2][2] = 33;   // 从二维数组中取出元素并打印  System.out.println(arr[0][0]);  System.out.println(arr[0][1]);  System.out.println(arr[0][2]);  System.out.println(arr[1][0]);  System.out.println(arr[1][1]);  System.out.println(arr[1][2]);  System.out.println(arr[2][0]);  System.out.println(arr[2][1]);  System.out.println(arr[2][2]);  } } 3.5 二维数组访问元素的细节问题 问题 : 二维数组中存储的是一维数组, 那能不能存入 [提前创建好的一维数组] 呢 ?\n答 : 可以的\n代码实现 package com.itheima.demo;  public class Demo2Array {  /* 问题: 二维数组中存储的是一维数组, 那能不能存入 [提前创建好的一维数组] 呢 ? 答 : 可以的 */  public static void main(String[] args) {  int[] arr1 = {11,22,33};  int[] arr2 = {44,55,66};  int[] arr3 = {77,88,99,100};   int[][] arr = new int[3][3];   arr[2][3] = 100;   arr[0] = arr1;  arr[1] = arr2;  arr[2] = arr3;   System.out.println(arr[1][2]);  System.out.println(arr[2][3]);  } } 3.6 二维数组静态初始化 **完整格式 :** 数据类型[][] 变量名 = new 数据类型[][]{ {元素1, 元素2...} , {元素1, 元素2...} **简化格式 :** 数据类型[][] 变量名 = { {元素1, 元素2...} , {元素1, 元素2...} ...};  **代码实现 : **\npackage com.itheima.demo;  public class Demo3Array {  /* 完整格式：数据类型[][] 变量名 = new 数据类型[][]{ {元素1, 元素2...} , {元素1, 元素2...} ...}; 简化格式: 数据类型[][] 变量名 = { {元素1, 元素2...} , {元素1, 元素2...} ...}; */  public static void main(String[] args) {  int[] arr1 = {11,22,33};  int[] arr2 = {44,55,66};   int[][] arr = {{11,22,33}, {44,55,66}};  System.out.println(arr[0][2]);   int[][] array = {arr1,arr2};  System.out.println(array[0][2]);  } } 3.7 二维数组遍历 需求 :\n​\t已知一个二维数组 arr = {{11, 22, 33},{33, 44, 55}};\n​\t遍历该数组，取出所有元素并打印\n步骤 :\n1. 遍历二维数组，取出里面每一个一维数组 2. 在遍历的过程中，对每一个一维数组继续完成遍历，获取内部存储的每一个元素  代码实现 :\npackage com.itheima.test;  public class Test1 {  /* 需求: 已知一个二维数组 arr = {{11, 22, 33}, {33, 44, 55}}; 遍历该数组，取出所有元素并打印 步骤: 1. 遍历二维数组，取出里面每一个一维数组 2. 在遍历的过程中，对每一个一维数组继续完成遍历，获取内部存储的每一个元素 */  public static void main(String[] args) {  int[][] arr = {{11, 22, 33}, {33, 44, 55}};   // 1. 遍历二维数组，取出里面每一个一维数组  for (int i = 0; i \u0026lt; arr.length; i++) {  //System.out.println(arr[i]);  // 2. 在遍历的过程中，对每一个一维数组继续完成遍历，获取内部存储的每一个元素  //int[] temp = arr[i];  for (int j = 0; j \u0026lt; arr[i].length; j++) {  System.out.println(arr[i][j]);  }  }  } } 3.8 二维数组求和 需求 :\n某公司季度和月份统计的数据如下：单位(万元) 第一季度：22,66,44 第二季度：77,33,88 第三季度：25,45,65 第四季度：11,66,99  步骤 :\n 定义求和变量，准备记录最终累加结果 使用二维数组来存储数据，每个季度是一个一维数组，再将4个一维数组装起来 遍历二维数组，获取所有元素，累加求和 输出最终结果  代码实现 :\npackage com.itheima.test;  public class Test2 {  /* 需求: 某公司季度和月份统计的数据如下：单位(万元) 第一季度：22,66,44 第二季度：77,33,88 第三季度：25,45,65 第四季度：11,66,99 步骤: 1. 定义求和变量，准备记录最终累加结果 2. 使用二维数组来存储数据，每个季度是一个一维数组，再将4个一维数组装起来 3. 遍历二维数组，获取所有元素，累加求和 4. 输出最终结果 */  public static void main(String[] args) {  // 1. 定义求和变量，准备记录最终累加结果  int sum = 0;  // 2. 使用二维数组来存储数据，每个季度是一个一维数组，再将4个一维数组装起来  int[][] arr = { {22,66,44} , {77,33,88} , {25,45,65} , {11,66,99}};  // 3. 遍历二维数组，获取所有元素，累加求和  for (int i = 0; i \u0026lt; arr.length; i++) {  for(int j = 0; j \u0026lt; arr[i].length; j++){  sum += arr[i][j];  }  }  // 4. 输出最终结果  System.out.println(sum);  } } ","permalink":"https://iblog.zone/archives/java-debug%E8%BF%9B%E5%88%B6%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84/","summary":"1.Debug模式 1.1 什么是Debug模式 是供程序员使用的程序调试工具，它可以用于查看程序的执行流程，也可以用于追踪程序执行过程来调试程序。\n1.2 Debug介绍与操作流程   如何加断点\n 选择要设置断点的代码行，在行号的区域后面单击鼠标左键即可    如何运行加了断点的程序\n 在代码区域右键Debug执行    看哪里\n  看Debugger窗口\n  看Console窗口\n    点哪里\n 点Step Into (F7)这个箭头，也可以直接按F7    如何删除断点\n  选择要删除的断点，单击鼠标左键即可\n  如果是多个断点，可以每一个再点击一次。也可以一次性全部删除\n    2. 进制的介绍与书写格式 2.1 进制的介绍与书写格式 代码 :\npublic class Demo1 {  /* 十进制：Java中，数值默认都是10进制，不需要加任何修饰。 二进制：数值前面以0b开头，b大小写都可以。 八进制：数值前面以0开头。 十六进制：数值前面以0x开头，x大小写都可以。 注意: 书写的时候, 虽然加入了进制的标识, 但打印在控制台展示的都是十进制数据. */  public static void main(String[] args) {  System.","title":"Java Debug\u0026进制\u0026二维数组"},{"content":"1. 方法概述 1.1 方法的概念 ​\t方法（method）是将具有独立功能的代码块组织成为一个整体，使其具有特殊功能的代码集\n 注意：  方法必须先创建才可以使用，该过程成为方法定义 方法创建后并不是直接可以运行的，需要手动使用后，才执行，该过程成为方法调用    2. 方法的定义和调用 2.1 无参数方法定义和调用   定义格式：\npublic static void 方法名 ( ) { \t// 方法体; }   范例：\npublic static void method ( ) { \t// 方法体; }   调用格式：\n方法名();   范例：\nmethod();   注意：\n​\t方法必须先定义，后调用，否则程序将报错\n  2.2 方法的调用过程  总结：每个方法在被调用执行的时候，都会进入栈内存，并且拥有自己独立的内存空间，方法内部代码调用完毕之后，会从栈内存中弹栈消失。  2.3 方法练习-奇偶数判断  需求：判断一个数是奇数还是偶数 代码：  public class Demo1Method {  /* 带参数方法的定义格式: public static void 方法名 ( 参数 ) { … … } public static void 方法名 ( 数据类型 变量名 ) { … … } 带参数方法的调用格式: 方法名 ( 参数 ) ; 方法名 ( 变量名/常量值 ) ; tips: 参数可以是一个, 也可以是多个. 需求: 判断一个数是奇数还是偶数 */  public static void main(String[] args) {  isEvenNumber(10);  }   public static void isEvenNumber(int num){  if(num % 2 == 0){  System.out.println(\u0026#34;偶数\u0026#34;);  }else{  System.out.println(\u0026#34;奇数\u0026#34;);  }  } } 3. 带参数方法的定义和调用 3.1 带参数方法定义和调用   定义格式：\n参数：由数据类型和变量名组成 - 数据类型 变量名\n参数范例：int a\npublic static void 方法名 (参数1) { \t方法体; }  public static void 方法名 (参数1, 参数2, 参数3...) { \t方法体; }   范例：\npublic static void isEvenNumber(int number){  ... } public static void getMax(int num1, int num2){  ... }   注意：\n方法定义时，参数中的数据类型与变量名都不能缺少，缺少任意一个程序将报错 方法定义时，多个参数之间使用逗号( ，)分隔      调用格式：\n方法名(参数)；  方法名(参数1,参数2);   范例：\nisEvenNumber(10);  getMax(10,20);  方法调用时，参数的数量与类型必须与方法定义中的设置相匹配，否则程序将报错    3.2 形参和实参  形参：方法定义中的参数  ​ 等同于变量定义格式，例如：int number\n实参：方法调用中的参数  ​ 等同于使用变量或常量，例如： 10 number\n3.3 带参数方法的练习-打印n-m之间所有的奇数   需求：设计一个方法（print） 用于打印 n 到 m 之间所有的奇数\n  思路：\n​\t1：定义方法，名称为print ​ 2：为方法添加两个int类型的形参，准备接受调用者传递过来的实参 ​ 3：方法中设计for循环，循环从n开始，到m结束 ​ 4：循环中加入if判断，是奇数，则打印 ​ 5：main方法中调用print方法，传入两个实际参数\n  代码：\n  package com.itheima.method2;  public class Demo2Method {  public static void main(String[] args) {  // 5：main方法中调用print方法，传入两个实际参数  print(20,10);  }   //1：定义方法，名称为print  // 2：为方法添加两个int类型的形参，准备接受调用者传递过来的实参  public static void print(int n, int m){  System.out.println(n + \u0026#34;到\u0026#34; + m + \u0026#34;之间的奇数为:\u0026#34;);  // 3：方法中设计for循环，循环从n开始，到m结束  for(int i = 20; i \u0026lt;= 10; i++){  // 4：循环中加入if判断，是奇数，则打印  if(i % 2 == 1){  System.out.println(i);  }  }  }  } 4. 带返回值方法的定义和调用 4.1 带返回值方法定义和调用（掌握）   定义格式\npublic static 数据类型 方法名 ( 参数 ) { \treturn 数据 ; }   范例\npublic static boolean isEvenNumber( int number ) { \treturn true ; } public static int getMax( int a, int b ) { \treturn 100 ; }  注意：  方法定义时return后面的返回值与方法定义上的数据类型要匹配，否则程序将报错      调用格式\n方法名 ( 参数 ) ; 数据类型 变量名 = 方法名 ( 参数 ) ;   范例\nisEvenNumber ( 5 ) ; boolean flag = isEvenNumber ( 5 );  注意：  方法的返回值通常会使用变量接收，否则该返回值将无意义      4.2 带返回值方法的练习-求两个数的最大值(应用)   需求：设计一个方法可以获取两个数的较大值，数据来自于参数\n  思路：\n 定义一个方法，声明两个形参接收计算的数值，求出结果并返回 使用 if 语句 得出 a 和 b 之间的最大值，根据情况return具体结果 在main()方法中调用定义好的方法并使用 【 变量保存 】    代码：\n /* 需求：设计一个方法可以获取两个数的较大值，数据来自于参数 1. 定义一个方法，声明两个形参接收计算的数值，求出结果并返回 2. 使用 if 语句 得出 a 和 b 之间的最大值，根据情况return具体结果 3. 在main()方法中调用定义好的方法并使用 【 变量保存 】 */  public static void main(String[] args) {  // 3. 在main()方法中调用定义好的方法并使用 【 变量保存 】  System.out.println(getMax(10,20)); // 输出调用   int result = getMax(10,20);  System.out.println(result);   for(int i = 1; i \u0026lt;= result; i++){  System.out.println(\u0026#34;HelloWorld\u0026#34;);  }   }   // 方法可以获取两个数的较大值  public static int getMax(int a, int b){  if(a \u0026gt; b){  return a;  }else{  return b;  }  }  }   5. 方法的注意事项 5.1 方法的通用格式（掌握）   格式：\npublic static 返回值类型 方法名(参数) {  方法体;  return 数据 ; }   解释：\n  public static 修饰符，目前先记住这个格式\n返回值类型\t方法操作完毕之后返回的数据的数据类型\n​\t如果方法操作完毕，没有数据返回，这里写void，而且方法体中一般不写return\n方法名\t调用方法时候使用的标识\n参数\t由数据类型和变量名组成，多个参数之间用逗号隔开\n方法体\t完成功能的代码块\nreturn\t如果方法操作完毕，有数据返回，用于把数据返回给调用者\n    定义方法时，要做到两个明确\n 明确返回值类型：主要是明确方法操作完毕之后是否有数据返回，如果没有，写void；如果有，写对应的数据类型 明确参数：主要是明确参数的类型和数量    调用方法时的注意：\n void类型的方法，直接调用即可 非void类型的方法，推荐用变量接收调用    5.2 方法的注意事项   方法不能嵌套定义\n  示例代码：\npublic class MethodDemo {  public static void main(String[] args) {   }   public static void methodOne() { \tpublic static void methodTwo() {  // 这里会引发编译错误!!!  }  } }     void表示无返回值，可以省略return，也可以单独的书写return，后面不加数据\n  示例代码：\npublic class MethodDemo {  public static void main(String[] args) {   }  public static void methodTwo() {  //return 100; 编译错误，因为没有具体返回值类型  return;\t //System.out.println(100); return语句后面不能跟数据或代码  } }     6. 方法重载 6.1 方法重载   方法重载概念\n方法重载指同一个类中定义的多个方法之间的关系，满足下列条件的多个方法相互构成重载\n 多个方法在同一个类中 多个方法具有相同的方法名 多个方法的参数不相同，类型不同或者数量不同    注意：\n 重载仅对应方法的定义，与方法的调用无关，调用方式参照标准格式 重载仅针对同一个类中方法的名称与参数进行识别，与返回值无关，换句话说不能通过返回值来判定两个方法是否相互构成重载    正确范例：\npublic class MethodDemo { \tpublic static void fn(int a) {  //方法体  }  public static int fn(double a) {  //方法体  } }  public class MethodDemo { \tpublic static float fn(int a) {  //方法体  }  public static int fn(int a , int b) {  //方法体  } }   错误范例：\npublic class MethodDemo { \tpublic static void fn(int a) {  //方法体  }  public static int fn(int a) { /*错误原因：重载与返回值无关*/  //方法体  } }  public class MethodDemo01 {  public static void fn(int a) {  //方法体  } } public class MethodDemo02 {  public static int fn(double a) { /*错误原因：这是两个类的两个fn方法*/  //方法体  } }   6.2 方法重载练习   需求：使用方法重载的思想，设计比较两个整数是否相同的方法，兼容全整数类型（byte,short,int,long）\n  思路：\n​\t①定义比较两个数字的是否相同的方法compare()方法，参数选择两个int型参数\n​\t②定义对应的重载方法，变更对应的参数类型，参数变更为两个long型参数\n​\t③定义所有的重载方法，两个byte类型与两个short类型参数\n​\t④完成方法的调用，测试运行结果\n  代码：\npublic class MethodTest {  public static void main(String[] args) {  //调用方法  System.out.println(compare(10, 20));  System.out.println(compare((byte) 10, (byte) 20));  System.out.println(compare((short) 10, (short) 20));  System.out.println(compare(10L, 20L));  }   //int  public static boolean compare(int a, int b) {  System.out.println(\u0026#34;int\u0026#34;);  return a == b;  }   //byte  public static boolean compare(byte a, byte b) {  System.out.println(\u0026#34;byte\u0026#34;);  return a == b;  }   //short  public static boolean compare(short a, short b) {  System.out.println(\u0026#34;short\u0026#34;);  return a == b;  }   //long  public static boolean compare(long a, long b) {  System.out.println(\u0026#34;long\u0026#34;);  return a == b;  }  }   7. 方法的参数传递 7.1 方法参数传递基本类型（理解）   测试代码：\npackage com.itheima.param;  public class Test1 {  /* 方法参数传递为基本数据类型 : 传入方法中的, 是具体的数值. */  public static void main(String[] args) {  int number = 100;  System.out.println(\u0026#34;调用change方法前:\u0026#34; + number);  change(number);  System.out.println(\u0026#34;调用change方法后:\u0026#34; + number);  }   public static void change(int number) {  number = 200;  } }   结论：\n 基本数据类型的参数，形式参数的改变，不影响实际参数    结论依据：\n 每个方法在栈内存中，都会有独立的栈空间，方法运行结束后就会弹栈消失    7.2 方法参数传递引用类型   测试代码：\npackage com.itheima.param;  public class Test2 {  /* 方法参数传递为引用数据类型 : 传入方法中的, 是内存地址. */  public static void main(String[] args) {  int[] arr = {10, 20, 30};  System.out.println(\u0026#34;调用change方法前:\u0026#34; + arr[1]);  change(arr);  System.out.println(\u0026#34;调用change方法后:\u0026#34; + arr[1]);  }   public static void change(int[] arr) {  arr[1] = 200;  } }   结论：\n 对于引用类型的参数，形式参数的改变，影响实际参数的值    结论依据：\n 引用数据类型的传参，传入的是地址值，内存中会造成两个引用指向同一个内存的效果，所以即使方法弹栈，堆内存中的数据也已经是改变后的结果    7.3 数组遍历   需求：设计一个方法用于数组遍历，要求遍历的结果是在一行上的。例如：[11, 22, 33, 44, 55]\n  思路：\n  因为要求结果在一行上输出，所以这里需要在学习一个新的输出语句System.out.print(“内容”);\nSystem.out.println(“内容”); 输出内容并换行\nSystem.out.print(“内容”); 输出内容不换行\nSystem.out.println(); 起到换行的作用\n  定义一个数组，用静态初始化完成数组元素初始化\n  定义一个方法，用数组遍历通用格式对数组进行遍历\n  用新的输出语句修改遍历操作\n  调用遍历方法\n    代码：\npackage com.itheima.test;  public class Test1 {  /* 需求：设计一个方法用于数组遍历，要求遍历的结果是在一行上的。例如：[11, 22, 33, 44, 55] 思路： 1.定义一个数组，用静态初始化完成数组元素初始化 2.定义一个方法，对数组进行遍历 3.遍历打印的时候，数据不换行 4.调用遍历方法 */  public static void main(String[] args) {  // 1.定义一个数组，用静态初始化完成数组元素初始化  int[] arr = {11, 22, 33, 44, 55};  // 4.调用遍历方法  printArray(arr);   System.out.println(\u0026#34;另外一段代码逻辑 \u0026#34;);  }   /* 2.定义一个方法，对数组进行遍历 1, 参数 int[] arr 2, 返回值类型 void */  public static void printArray(int[] arr){   System.out.print(\u0026#34;[\u0026#34;);   for (int i = 0; i \u0026lt; arr.length; i++) {   if(i == arr.length -1){  // 如果满足条件, 说明是最后一个元素, 最后一个元素, 特殊处理  System.out.println(arr[i] + \u0026#34;]\u0026#34;);  }else{  // 3.遍历打印的时候，数据不换行  System.out.print(arr[i] + \u0026#34;, \u0026#34;);  }    }  } }   7.4 数组最大值   需求：设计一个方法用于获取数组中元素的最大值\n  思路：\n ①定义一个数组，用静态初始化完成数组元素初始化 ②定义一个方法，用来获取数组中的最大值，最值的认知和讲解我们在数组中已经讲解过了 ③调用获取最大值方法，用变量接收返回结果 ④把结果输出在控制台    代码：\npackage com.itheima.test;  public class Test2 {  /* 需求：设计一个方法用于获取数组中元素的最大值 思路： 1.定义一个数组，用静态初始化完成数组元素初始化 2.定义一个方法，用来获取数组中的最大值 3.调用获取最大值方法，用变量接收返回结果 4.把结果输出在控制台 */  public static void main(String[] args) {  // 1.定义一个数组，用静态初始化完成数组元素初始化  int[] arr = {11, 55, 22, 44, 33};  // 3.调用获取最大值方法，用变量接收返回结果  int max = getMax(arr);  // 4.把结果输出在控制台  System.out.println(max);  }   /* 2.定义一个方法，用来获取数组中的最大值 1, 参数 int[] arr 2, 返回值类型 int */  public static int getMax(int[] arr){  int max = arr[0];  for (int i = 1; i \u0026lt; arr.length; i++) {  if(max \u0026lt; arr[i]){  max = arr[i];  }  }  return max;  } }   7.5 方法同时获取数组最大值和最小值   需求：设计一个方法，该方法能够同时获取数组的最大值，和最小值\n  注意: return语句, 只能带回一个结果.\n  代码：\npublic class Test3 {  /* 需求：设计一个方法，该方法能够同时获取数组的最大值，和最小值 注意: return语句, 只能带回一个结果. */  public static void main(String[] args) {   int[] arr = {11,55,33,22,44};   int[] maxAndMin = getMaxAndMin(arr);   System.out.println(maxAndMin[0]);  System.out.println(maxAndMin[1]);   }   public static int[] getMaxAndMin(int[] arr){  int max = arr[0];  for (int i = 1; i \u0026lt; arr.length; i++) {  if(max \u0026lt; arr[i]){  max = arr[i];  }  }   int min = arr[0];  for (int i = 1; i \u0026lt; arr.length; i++) {  if(min \u0026gt; arr[i]){  min = arr[i];  }  }   int[] maxAndMin = {min, max};   return maxAndMin;  } }   ","permalink":"https://iblog.zone/archives/java%E6%96%B9%E6%B3%95/","summary":"1. 方法概述 1.1 方法的概念 ​\t方法（method）是将具有独立功能的代码块组织成为一个整体，使其具有特殊功能的代码集\n 注意：  方法必须先创建才可以使用，该过程成为方法定义 方法创建后并不是直接可以运行的，需要手动使用后，才执行，该过程成为方法调用    2. 方法的定义和调用 2.1 无参数方法定义和调用   定义格式：\npublic static void 方法名 ( ) { \t// 方法体; }   范例：\npublic static void method ( ) { \t// 方法体; }   调用格式：\n方法名();   范例：\nmethod();   注意：\n​\t方法必须先定义，后调用，否则程序将报错\n  2.2 方法的调用过程  总结：每个方法在被调用执行的时候，都会进入栈内存，并且拥有自己独立的内存空间，方法内部代码调用完毕之后，会从栈内存中弹栈消失。  2.3 方法练习-奇偶数判断  需求：判断一个数是奇数还是偶数 代码：  public class Demo1Method {  /* 带参数方法的定义格式: public static void 方法名 ( 参数 ) { … … } public static void 方法名 ( 数据类型 变量名 ) { … … } 带参数方法的调用格式: 方法名 ( 参数 ) ; 方法名 ( 变量名/常量值 ) ; tips: 参数可以是一个, 也可以是多个.","title":"Java方法"},{"content":"1.数组 1.1 数组介绍 ​\t数组就是存储数据长度固定的容器，存储多个数据的数据类型要一致。\n1.2 数组的定义格式 1.2.1 第一种格式 ​\t数据类型[] 数组名\n​\t示例：\nint[] arr; double[] arr; char[] arr; 1.2.2 第二种格式 ​\t数据类型 数组名[]\n​\t示例：\nint arr[]; double arr[]; char arr[]; 1.3 数组的动态初始化 1.3.1 什么是动态初始化 ​\t数组动态初始化就是只给定数组的长度，由系统给出默认初始化值\n1.3.2 动态初始化格式 数据类型[] 数组名 = new 数据类型[数组长度]; int[] arr = new int[3]; 1.3.3 动态初始化格式详解  等号左边：  int:数组的数据类型 []:代表这是一个数组 arr:代表数组的名称   等号右边：  new:为数组开辟内存空间 int:数组的数据类型 []:代表这是一个数组 5:代表数组的长度    代码 :\npackage com.itheima.array;  public class Demo2Array {  /* 数组的动态初始化: 在初始化的时候, 需要手动指定数组的长度, 系统会为数组容器分配初始值. 动态初始化格式: 数据类型[] 数组名 = new 数据类型[数组的长度]; 注意: 打印数组变量的时候, 会打印出数组的内存地址 [I@10f87f48 : @ : 分隔符 [ : 当前的空间是一个数组类型 I : 当前数组容器中所存储的数据类型 10f87f48 : 十六进制内存地址 0 1 2 3 4 5 6 7 8 9 a b c d e f */  public static void main(String[] args) {  // 数据类型[] 数组名 = new 数据类型[数组的长度];  // 通过new关键字创建了一个int类型的数组容器, 该容器可以存储5个int类型的整数, 该容器被arr数组变量所记录  int[] arr = new int[5];  // [I@10f87f48  System.out.println(arr);   byte[] bArr = new byte[3];  // [B@b4c966a  System.out.println(bArr);   } } 1.4 数组元素访问 1.4.1 什么是索引 ​\t每一个存储到数组的元素，都会自动的拥有一个编号，从0开始。\n​\t这个自动编号称为数组索引(index)，可以通过数组的索引访问到数组中的元素。\n1.4.2访问数组元素格式 数组名[索引]; 1.4.3示例代码 package com.itheima.array;  public class Demo3ArrayIndex {  /* 数组动态初始化: 初始化的时候, 手动指定数组长度, 系统会为数组容器分配初始值. 数组的元素访问格式: 数组名[索引] 索引: 数组中数据的编号方式, 编号从0开始 作用: 访问数组容器中的空间位置 注意: 数组在创建完毕后, 即使没有赋值, 也可以取出, 但取出的元素都是默认初始化值. */  public static void main(String[] args) {  int[] arr = new int[3]; // 0 1 2  System.out.println(arr); // 数组的内存地址 [I@10f87f48   // 数组名[索引] 访问数组容器中的空间位置  System.out.println(arr[0]); // 0 系统自动分配的默认初始化值  System.out.println(arr[1]);  System.out.println(arr[2]);   System.out.println(\u0026#34;--------------\u0026#34;);   // 数组名[索引]  arr[0] = 11;  arr[1] = 22;  arr[2] = 33;   System.out.println(arr[0]);  System.out.println(arr[1]);  System.out.println(arr[2]);  } } 1.5 内存分配 1.5.1 内存概述 ​\t内存是计算机中的重要原件，临时存储区域，作用是运行程序。\n​\t我们编写的程序是存放在硬盘中的，在硬盘中的程序是不会运行的。\n​\t必须放进内存中才能运行，运行完毕后会清空内存。\n​\tJava虚拟机要运行程序，必须要对内存进行空间的分配和管理。\n1.5.2 java中的内存分配  目前我们只需要记住两个内存，分别是：栈内存和堆内存     区域名称 作用     寄存器 给CPU使用，和我们开发无关。   本地方法栈 JVM在使用操作系统功能的时候使用，和我们开发无关。   方法区 存储可以运行的class文件。   堆内存 存储对象或者数组，new来创建的，都存储在堆内存。   方法栈 方法运行时使用的内存，比如main方法运行，进入方法栈中执行。    1.6 Java内存分配-一个数组内存图 1.7 两个数组内存图 1.8 多个数组指向相同内存图 1.9 数组的静态初始化 1.9.1 什么是静态初始化 ​\t在创建数组时，直接将元素确定\n1.9.2 静态初始化格式   完整版格式\n数据类型[] 数组名 = new 数据类型[]{元素1,元素2,...};   简化版格式\n数据类型[] 数组名 = {元素1,元素2,...};   1.9.3示例代码 package com.itheima.array2;  public class Demo1Array {  /* 数组静态初始化 : 初始化时指定每个数组元素的初始值，由系统决定数组长度 完整格式: 数据类型[] 数组名 = new 数据类型[]{数据1,数据2,数据3...}; 简化格式: 数据类型[] 数组名 = {数据1,数据2,数据3...}; */  public static void main(String[] args) {  // 数据类型[] 数组名 = new 数据类型[]{数据1,数据2,数据3...};  int[] arr = new int[]{11,22,33};  System.out.println(arr[0]);  System.out.println(arr[1]);  System.out.println(arr[2]);   // 数据类型[] 数组名 = {数据1,数据2,数据3...};  int[] arr2 = {44,55,66};  System.out.println(arr2);  System.out.println(arr2[0]);  System.out.println(arr2[1]);  System.out.println(arr2[2]);   } } 1.10 数组操作的两个常见问题 1.10.1 索引越界异常   出现原因\npublic class ArrayDemo {  public static void main(String[] args) {  int[] arr = new int[3];  System.out.println(arr[3]);  } } 数组长度为3，索引范围是0~2，但是我们却访问了一个3的索引。\n程序运行后，将会抛出ArrayIndexOutOfBoundsException 数组越界异常。在开发中，数组的越界异常是不能出现的，一旦出现了，就必须要修改我们编写的代码。\n  解决方案\n将错误的索引修改为正确的索引范围即可！\n  1.10.2 空指针异常   出现原因\npublic class ArrayDemo {  public static void main(String[] args) {  int[] arr = new int[3];   //把null赋值给数组  arr = null;  System.out.println(arr[0]);  } } arr = null 这行代码，意味着变量arr将不会在保存数组的内存地址，也就不允许再操作数组了，因此运行的时候会抛出 NullPointerException 空指针异常。在开发中，空指针异常是不能出现的，一旦出现了，就必须要修改我们编写的代码。\n  解决方案\n给数组一个真正的堆内存空间引用即可！\n  1.11 数组遍历   数组遍历：就是将数组中的每个元素分别获取出来，就是遍历。遍历也是数组操作中的基石。\npublic class ArrayTest01 {  public static void main(String[] args) {  int[] arr = { 1, 2, 3, 4, 5 };  System.out.println(arr[0]);  System.out.println(arr[1]);  System.out.println(arr[2]);  System.out.println(arr[3]);  System.out.println(arr[4]);  } } 以上代码是可以将数组中每个元素全部遍历出来，但是如果数组元素非常多，这种写法肯定不行，因此我们需要改造成循环的写法。数组的索引是 0 到 lenght-1 ，可以作为循环的条件出现。\npublic class ArrayTest01 {  public static void main(String[] args) {  //定义数组  int[] arr = {11, 22, 33, 44, 55};   //使用通用的遍历格式  for(int x=0; x\u0026lt;arr.length; x++) {  System.out.println(arr[x]);  }  } }   1.12 数组获取最大值   最大值获取：从数组的所有元素中找出最大值。\n  实现思路：\n 定义变量，保存数组0索引上的元素 遍历数组，获取出数组中的每个元素 将遍历到的元素和保存数组0索引上值的变量进行比较 如果数组元素的值大于了变量的值，变量记录住新的值 数组循环遍历结束，变量保存的就是数组中的最大值    代码实现：\npackage com.itheima.test;  import java.util.Scanner;  public class Test2Array {  /* 需求: 从数组中查找最大值 int[] arr = {12,45,98,73,60}; 实现步骤: 1. 假设数组中的第一个元素为最大值 2. 遍历数组, 获取每一个元素, 准备进行比较 3. 如果比较的过程中, 出现了比max更大的, 让max记录更大的值 4. 循环结束后, 打印最大值. */  public static void main(String[] args) {  int[] arr = {12,45,98,73,60};  // 1. 假设数组中的第一个元素为最大值  int max = arr[0];  // 2. 遍历数组, 获取每一个元素, 准备进行比较  for(int i = 1; i \u0026lt; arr.length; i++){  // 3. 如果比较的过程中, 出现了比max更大的, 让max记录更大的值  if(arr[i] \u0026gt; max){  max = arr[i];  }  }  // 4. 循环结束后, 打印最大值.  System.out.println(\u0026#34;max:\u0026#34; + max);  } }   1.13 数组元素求和   需求：键盘录入5个整数，存储到数组中，并对数组求和\n  思路： 1.创建键盘录入对象，准备键盘录入 2.定义一个求和变量，准备记录累加后的结果 3.动态初始化一个长度为5的int数组，准备存储键盘录入的数值 4.将键盘录入的数值存储到数组中 5.遍历数组，取出每一个元素，并求和 6.输出总和\n  代码实现：\npackage com.itheima.test;  import java.util.Scanner;  public class Test3Array {  /* 需求：键盘录入5个整数，存储到数组中，并对数组求和 思路： 1.创建键盘录入对象，准备键盘录入 2.定义一个求和变量，准备记录累加后的结果 3.动态初始化一个长度为5的int数组，准备存储键盘录入的数值 4.将键盘录入的数值存储到数组中 5.遍历数组，取出每一个元素，并求和 6.输出总和 */  public static void main(String[] args) {  // 1.创建键盘录入对象，准备键盘录入  Scanner sc = new Scanner(System.in);  // 2.定义一个求和变量，准备记录累加后的结果  int sum = 0;  // 3.动态初始化一个长度为5的int数组，准备存储键盘录入的数值  int[] arr = new int[5];  // 4.将键盘录入的数值存储到数组中  for(int i = 0; i \u0026lt; arr.length; i++){  System.out.println(\u0026#34;请输入第\u0026#34; + (i+1) + \u0026#34;个整数:\u0026#34;);  //arr[i] = 10;  arr[i] = sc.nextInt();  }   // 5.遍历数组，取出每一个元素，并求和  for (int i = 0; i \u0026lt; arr.length; i++) {  sum += arr[i];  }   // 6.输出总和  System.out.println(\u0026#34;sum:\u0026#34; + sum);   } }   1.14 数组基本查找【应用】   需求： 已知一个数组 arr = {19, 28, 37, 46, 50}; 键盘录入一个数据，查找该数据在数组中的索引，并在控 制台输出找到的索引值。\n  思路： 1.定义一个数组，用静态初始化完成数组元素的初始化 2.键盘录入要查找的数据，用一个变量接收 3.定义一个索引变量，初始值为-1 4.遍历数组，获取到数组中的每一个元素 5.拿键盘录入的数据和数组中的每一个元素进行比较，如果值相同，就把该值对应的索引赋值给索引变量，并结束循环 6.输出索引变量\n  代码实现：\npublic static void main(String[] args) {  // 1.定义一个数组，用静态初始化完成数组元素的初始化  int[] arr = {19, 28, 37, 46, 50};  // 2.键盘录入要查找的数据，用一个变量接收  Scanner sc = new Scanner(System.in);  System.out.println(\u0026#34;请输入您要查找的元素:\u0026#34;);  int num = sc.nextInt();  // 3.定义一个索引变量，初始值为-1  // 假设要查找的数据, 在数组中就是不存在的  int index = -1;  // 4.遍历数组，获取到数组中的每一个元素  for (int i = 0; i \u0026lt; arr.length; i++) {  // 5.拿键盘录入的数据和数组中的每一个元素进行比较，如果值相同，就把该值对应的索引赋值给索引变量，并结束循环  if(num == arr[i]){  // 如果值相同，就把该值对应的索引赋值给索引变量，并结束循环  index = i;  break;  }  }  // 6.输出索引变量  System.out.println(index);  } }   1.15 评委打分【应用】   需求：在编程竞赛中，有6个评委为参赛的选手打分，分数为0-100的整数分。 选手的最后得分为：去掉一个最高分和一个最低分后 的4个评委平均值 (不考虑小数部分)。\n  思路： 1.定义一个数组，用动态初始化完成数组元素的初始化，长度为6 2.键盘录入评委分数 3.由于是6个评委打分，所以，接收评委分数的操作，用循环 4.求出数组最大值 5.求出数组最小值 6.求出数组总和 7.按照计算规则进行计算得到平均分 8.输出平均分\n  代码实现：\n public static void main(String[] args) {  // 1.定义一个数组，用动态初始化完成数组元素的初始化，长度为6  int[] arr = new int[6];  // 2.键盘录入评委分数  Scanner sc = new Scanner(System.in);  // 3.由于是6个评委打分，所以，接收评委分数的操作，用循环  for (int i = 0; i \u0026lt; arr.length; i++) {  System.out.println(\u0026#34;请输入第\u0026#34; + (i+1) + \u0026#34;个评委的打分:\u0026#34;);  int score = sc.nextInt();  if(score \u0026gt;= 0 \u0026amp;\u0026amp; score \u0026lt;= 100){  // 合法的分值  arr[i] = score;  }else{  // 非法的分值  System.out.println(\u0026#34;您的打分输入有误, 请检查是否是0-100之间的\u0026#34;);  i--;  }  }   // 4.求出数组最大值  int max = arr[0];  for (int i = 1; i \u0026lt; arr.length; i++) {  if(max \u0026lt; arr[i]){  max = arr[i];  }  }   // 5.求出数组最小值  int min = arr[0];  for (int i = 1; i \u0026lt; arr.length; i++) {  if(min \u0026gt; arr[i]){  min = arr[i];  }  }   // 6.求出数组总和  int sum = 0;  for (int i = 0; i \u0026lt; arr.length; i++) {  sum += arr[i];  }   // 7.按照计算规则进行计算得到平均分  int avg = (sum - max - min ) / 4;   // 8.输出平均分  System.out.println(avg);  } }   ","permalink":"https://iblog.zone/archives/java%E6%95%B0%E7%BB%84/","summary":"1.数组 1.1 数组介绍 ​\t数组就是存储数据长度固定的容器，存储多个数据的数据类型要一致。\n1.2 数组的定义格式 1.2.1 第一种格式 ​\t数据类型[] 数组名\n​\t示例：\nint[] arr; double[] arr; char[] arr; 1.2.2 第二种格式 ​\t数据类型 数组名[]\n​\t示例：\nint arr[]; double arr[]; char arr[]; 1.3 数组的动态初始化 1.3.1 什么是动态初始化 ​\t数组动态初始化就是只给定数组的长度，由系统给出默认初始化值\n1.3.2 动态初始化格式 数据类型[] 数组名 = new 数据类型[数组长度]; int[] arr = new int[3]; 1.3.3 动态初始化格式详解  等号左边：  int:数组的数据类型 []:代表这是一个数组 arr:代表数组的名称   等号右边：  new:为数组开辟内存空间 int:数组的数据类型 []:代表这是一个数组 5:代表数组的长度    代码 :","title":"Java数组"},{"content":"截止2022-01-28日有效\nCentOS-Base.repo\n[extras] gpgcheck=0 gpgkey=http://mirrors.tencentyun.com/centos/RPM-GPG-KEY-Centos-6 enabled=1 baseurl=https://mirrors.cloud.tencent.com/centos-vault/6.9/extras/$basearch/ name=Qcloud centos extras - $basearch  [os] gpgcheck=0 gpgkey=http://mirrors.tencentyun.com/centos/RPM-GPG-KEY-Centos-6 enabled=1 baseurl=https://mirrors.cloud.tencent.com/centos-vault/6.9/os/$basearch/ name=Qcloud centos os - $basearch  [updates] gpgcheck=0 gpgkey=http://mirrors.tencentyun.com/centos/RPM-GPG-KEY-Centos-6 enabled=1 baseurl=https://mirrors.cloud.tencent.com/centos-vault/6.9/updates/$basearch/ name=Qcloud centos updates - $basearch epel.repo\n[epel] name=epel for redhat/centos $releasever - $basearch failovermethod=priority enable=1 baseurl=https://mirrors.cloud.tencent.com/epel-archive/6/$basearch/ ","permalink":"https://iblog.zone/archives/centos6-yum%E6%BA%90%E9%85%8D%E7%BD%AE/","summary":"截止2022-01-28日有效\nCentOS-Base.repo\n[extras] gpgcheck=0 gpgkey=http://mirrors.tencentyun.com/centos/RPM-GPG-KEY-Centos-6 enabled=1 baseurl=https://mirrors.cloud.tencent.com/centos-vault/6.9/extras/$basearch/ name=Qcloud centos extras - $basearch  [os] gpgcheck=0 gpgkey=http://mirrors.tencentyun.com/centos/RPM-GPG-KEY-Centos-6 enabled=1 baseurl=https://mirrors.cloud.tencent.com/centos-vault/6.9/os/$basearch/ name=Qcloud centos os - $basearch  [updates] gpgcheck=0 gpgkey=http://mirrors.tencentyun.com/centos/RPM-GPG-KEY-Centos-6 enabled=1 baseurl=https://mirrors.cloud.tencent.com/centos-vault/6.9/updates/$basearch/ name=Qcloud centos updates - $basearch epel.repo\n[epel] name=epel for redhat/centos $releasever - $basearch failovermethod=priority enable=1 baseurl=https://mirrors.cloud.tencent.com/epel-archive/6/$basearch/ ","title":"Centos6 yum源配置"},{"content":"一、简介 rsync 是一个常用的 Linux 应用程序，用于文件同步。\n它可以在本地计算机与远程计算机之间，或者两个本地目录之间同步文件（但不支持两台远程计算机之间的同步）。它也可以当作文件复制工具，替代cp和mv命令。\n它名称里面的r指的是 remote，rsync 其实就是\u0026quot;远程同步\u0026quot;（remote sync）的意思。与其他文件传输工具（如 FTP 或 scp）不同，rsync 的最大特点是会检查发送方和接收方已有的文件，仅传输有变动的部分（默认规则是文件大小或修改时间有变动）。\n二、安装 如果本机或者远程计算机没有安装 rsync，可以用下面的命令安装。\n # Debian $ sudo apt-get install rsync  # Red Hat $ sudo yum install rsync  # Arch Linux $ sudo pacman -S rsync  注意，传输的双方都必须安装 rsync。\n三、基本用法 3.1 -r 参数 本机使用 rsync 命令时，可以作为cp和mv命令的替代方法，将源目录同步到目标目录。\n $ rsync -r source destination  上面命令中，-r表示递归，即包含子目录。注意，-r是必须的，否则 rsync 运行不会成功。source目录表示源目录，destination表示目标目录。\n如果有多个文件或目录需要同步，可以写成下面这样。\n $ rsync -r source1 source2 destination  上面命令中，source1、source2都会被同步到destination目录。\n3.2 -a 参数 -a参数可以替代-r，除了可以递归同步以外，还可以同步元信息（比如修改时间、权限等）。由于 rsync 默认使用文件大小和修改时间决定文件是否需要更新，所以-a比-r更有用。下面的用法才是常见的写法。\n $ rsync -a source destination  目标目录destination如果不存在，rsync 会自动创建。执行上面的命令后，源目录source被完整地复制到了目标目录destination下面，即形成了destination/source的目录结构。\n如果只想同步源目录source里面的内容到目标目录destination，则需要在源目录后面加上斜杠。\n $ rsync -a source/ destination  上面命令执行后，source目录里面的内容，就都被复制到了destination目录里面，并不会在destination下面创建一个source子目录。\n3.3 -n 参数 如果不确定 rsync 执行后会产生什么结果，可以先用-n或--dry-run参数模拟执行的结果。\n $ rsync -anv source/ destination  上面命令中，-n参数模拟命令执行的结果，并不真的执行命令。-v参数则是将结果输出到终端，这样就可以看到哪些内容会被同步。\n3.4 --delete 参数 默认情况下，rsync 只确保源目录的所有内容（明确排除的文件除外）都复制到目标目录。它不会使两个目录保持相同，并且不会删除文件。如果要使得目标目录成为源目录的镜像副本，则必须使用--delete参数，这将删除只存在于目标目录、不存在于源目录的文件。\n $ rsync -av --delete source/ destination  上面命令中，--delete参数会使得destination成为source的一个镜像。\n四、排除文件 4.1 --exclude 参数 有时，我们希望同步时排除某些文件或目录，这时可以用--exclude参数指定排除模式。\n $ rsync -av --exclude=\u0026#39;*.txt\u0026#39; source/ destination # 或者 $ rsync -av --exclude \u0026#39;*.txt\u0026#39; source/ destination  上面命令排除了所有 TXT 文件。\n注意，rsync 会同步以\u0026quot;点\u0026quot;开头的隐藏文件，如果要排除隐藏文件，可以这样写--exclude=\u0026quot;.*\u0026quot;。\n如果要排除某个目录里面的所有文件，但不希望排除目录本身，可以写成下面这样。\n $ rsync -av --exclude \u0026#39;dir1/*\u0026#39; source/ destination  多个排除模式，可以用多个--exclude参数。\n $ rsync -av --exclude \u0026#39;file1.txt\u0026#39; --exclude \u0026#39;dir1/*\u0026#39; source/ destination  多个排除模式也可以利用 Bash 的大扩号的扩展功能，只用一个--exclude参数。\n $ rsync -av --exclude={\u0026#39;file1.txt\u0026#39;,\u0026#39;dir1/*\u0026#39;} source/ destination  如果排除模式很多，可以将它们写入一个文件，每个模式一行，然后用--exclude-from参数指定这个文件。\n $ rsync -av --exclude-from=\u0026#39;exclude-file.txt\u0026#39; source/ destination  4.2 --include 参数 --include参数用来指定必须同步的文件模式，往往与--exclude结合使用。\n $ rsync -av --include=\u0026#34;*.txt\u0026#34; --exclude=\u0026#39;*\u0026#39; source/ destination  上面命令指定同步时，排除所有文件，但是会包括 TXT 文件。\n五、远程同步 5.1 SSH 协议 rsync 除了支持本地两个目录之间的同步，也支持远程同步。它可以将本地内容，同步到远程服务器。\n $ rsync -av source/ username@remote_host:destination  也可以将远程内容同步到本地。\n $ rsync -av username@remote_host:source/ destination  rsync 默认使用 SSH 进行远程登录和数据传输。\n由于早期 rsync 不使用 SSH 协议，需要用-e参数指定协议，后来才改的。所以，下面-e ssh可以省略。\n $ rsync -av -e ssh source/ user@remote_host:/destination  但是，如果 ssh 命令有附加的参数，则必须使用-e参数指定所要执行的 SSH 命令。\n $ rsync -av -e \u0026#39;ssh -p 2234\u0026#39; source/ user@remote_host:/destination  上面命令中，-e参数指定 SSH 使用2234端口。\n5.2 rsync 协议 除了使用 SSH，如果另一台服务器安装并运行了 rsync 守护程序，则也可以用rsync://协议（默认端口873）进行传输。具体写法是服务器与目标目录之间使用双冒号分隔::。\n $ rsync -av source/ 192.168.122.32::module/destination  注意，上面地址中的module并不是实际路径名，而是 rsync 守护程序指定的一个资源名，由管理员分配。\n如果想知道 rsync 守护程序分配的所有 module 列表，可以执行下面命令。\n $ rsync rsync://192.168.122.32  rsync 协议除了使用双冒号，也可以直接用rsync://协议指定地址。\n $ rsync -av source/ rsync://192.168.122.32/module/destination  六、增量备份 rsync 的最大特点就是它可以完成增量备份，也就是默认只复制有变动的文件。\n除了源目录与目标目录直接比较，rsync 还支持使用基准目录，即将源目录与基准目录之间变动的部分，同步到目标目录。\n具体做法是，第一次同步是全量备份，所有文件在基准目录里面同步一份。以后每一次同步都是增量备份，只同步源目录与基准目录之间有变动的部分，将这部分保存在一个新的目标目录。这个新的目标目录之中，也是包含所有文件，但实际上，只有那些变动过的文件是存在于该目录，其他没有变动的文件都是指向基准目录文件的硬链接。\n--link-dest参数用来指定同步时的基准目录。\n $ rsync -a --delete --link-dest /compare/path /source/path /target/path  上面命令中，--link-dest参数指定基准目录/compare/path，然后源目录/source/path跟基准目录进行比较，找出变动的文件，将它们拷贝到目标目录/target/path。那些没变动的文件则会生成硬链接。这个命令的第一次备份时是全量备份，后面就都是增量备份了。\n下面是一个脚本示例，备份用户的主目录。\n #!/bin/bash  # A script to perform incremental backups using rsync  set -o errexit set -o nounset set -o pipefail  readonly SOURCE_DIR=\u0026#34;${HOME}\u0026#34; readonly BACKUP_DIR=\u0026#34;/mnt/data/backups\u0026#34; readonly DATETIME=\u0026#34;$(date \u0026#39;+%Y-%m-%d_%H:%M:%S\u0026#39;)\u0026#34; readonly BACKUP_PATH=\u0026#34;${BACKUP_DIR}/${DATETIME}\u0026#34; readonly LATEST_LINK=\u0026#34;${BACKUP_DIR}/latest\u0026#34;  mkdir -p \u0026#34;${BACKUP_DIR}\u0026#34;  rsync -av --delete \\  \u0026#34;${SOURCE_DIR}/\u0026#34; \\  --link-dest \u0026#34;${LATEST_LINK}\u0026#34; \\  --exclude=\u0026#34;.cache\u0026#34; \\  \u0026#34;${BACKUP_PATH}\u0026#34;  rm -rf \u0026#34;${LATEST_LINK}\u0026#34; ln -s \u0026#34;${BACKUP_PATH}\u0026#34; \u0026#34;${LATEST_LINK}\u0026#34;  上面脚本中，每一次同步都会生成一个新目录${BACKUP_DIR}/${DATETIME}，并将软链接${BACKUP_DIR}/latest指向这个目录。下一次备份时，就将${BACKUP_DIR}/latest作为基准目录，生成新的备份目录。最后，再将软链接${BACKUP_DIR}/latest指向新的备份目录。\n七、配置项 -a、--archive参数表示存档模式，保存所有的元数据，比如修改时间（modification time）、权限、所有者等，并且软链接也会同步过去。\n--append参数指定文件接着上次中断的地方，继续传输。\n--append-verify参数跟--append参数类似，但会对传输完成后的文件进行一次校验。如果校验失败，将重新发送整个文件。\n-b、--backup参数指定在删除或更新目标目录已经存在的文件时，将该文件更名后进行备份，默认行为是删除。更名规则是添加由--suffix参数指定的文件后缀名，默认是~。\n--backup-dir参数指定文件备份时存放的目录，比如--backup-dir=/path/to/backups。\n--bwlimit参数指定带宽限制，默认单位是 KB/s，比如--bwlimit=100。\n-c、--checksum参数改变rsync的校验方式。默认情况下，rsync 只检查文件的大小和最后修改日期是否发生变化，如果发生变化，就重新传输；使用这个参数以后，则通过判断文件内容的校验和，决定是否重新传输。\n--delete参数删除只存在于目标目录、不存在于源目标的文件，即保证目标目录是源目标的镜像。\n-e参数指定使用 SSH 协议传输数据。\n--exclude参数指定排除不进行同步的文件，比如--exclude=\u0026quot;*.iso\u0026quot;。\n--exclude-from参数指定一个本地文件，里面是需要排除的文件模式，每个模式一行。\n--existing、--ignore-non-existing参数表示不同步目标目录中不存在的文件和目录。\n-h参数表示以人类可读的格式输出。\n-h、--help参数返回帮助信息。\n-i参数表示输出源目录与目标目录之间文件差异的详细情况。\n--ignore-existing参数表示只要该文件在目标目录中已经存在，就跳过去，不再同步这些文件。\n--include参数指定同步时要包括的文件，一般与--exclude结合使用。\n--link-dest参数指定增量备份的基准目录。\n-m参数指定不同步空目录。\n--max-size参数设置传输的最大文件的大小限制，比如不超过200KB（--max-size='200k'）。\n--min-size参数设置传输的最小文件的大小限制，比如不小于10KB（--min-size=10k）。\n-n参数或--dry-run参数模拟将要执行的操作，而并不真的执行。配合-v参数使用，可以看到哪些内容会被同步过去。\n-P参数是--progress和--partial这两个参数的结合。\n--partial参数允许恢复中断的传输。不使用该参数时，rsync会删除传输到一半被打断的文件；使用该参数后，传输到一半的文件也会同步到目标目录，下次同步时再恢复中断的传输。一般需要与--append或--append-verify配合使用。\n--partial-dir参数指定将传输到一半的文件保存到一个临时目录，比如--partial-dir=.rsync-partial。一般需要与--append或--append-verify配合使用。\n--progress参数表示显示进展。\n-r参数表示递归，即包含子目录。\n--remove-source-files参数表示传输成功后，删除发送方的文件。\n--size-only参数表示只同步大小有变化的文件，不考虑文件修改时间的差异。\n--suffix参数指定文件名备份时，对文件名添加的后缀，默认是~。\n-u、--update参数表示同步时跳过目标目录中修改时间更新的文件，即不同步这些有更新的时间戳的文件。\n-v参数表示输出细节。-vv表示输出更详细的信息，-vvv表示输出最详细的信息。\n--version参数返回 rsync 的版本。\n-z参数指定同步时压缩数据。\n","permalink":"https://iblog.zone/archives/rsync%E5%B7%A5%E5%85%B7%E4%BD%BF%E7%94%A8/","summary":"一、简介 rsync 是一个常用的 Linux 应用程序，用于文件同步。\n它可以在本地计算机与远程计算机之间，或者两个本地目录之间同步文件（但不支持两台远程计算机之间的同步）。它也可以当作文件复制工具，替代cp和mv命令。\n它名称里面的r指的是 remote，rsync 其实就是\u0026quot;远程同步\u0026quot;（remote sync）的意思。与其他文件传输工具（如 FTP 或 scp）不同，rsync 的最大特点是会检查发送方和接收方已有的文件，仅传输有变动的部分（默认规则是文件大小或修改时间有变动）。\n二、安装 如果本机或者远程计算机没有安装 rsync，可以用下面的命令安装。\n # Debian $ sudo apt-get install rsync  # Red Hat $ sudo yum install rsync  # Arch Linux $ sudo pacman -S rsync  注意，传输的双方都必须安装 rsync。\n三、基本用法 3.1 -r 参数 本机使用 rsync 命令时，可以作为cp和mv命令的替代方法，将源目录同步到目标目录。\n $ rsync -r source destination  上面命令中，-r表示递归，即包含子目录。注意，-r是必须的，否则 rsync 运行不会成功。source目录表示源目录，destination表示目标目录。\n如果有多个文件或目录需要同步，可以写成下面这样。\n $ rsync -r source1 source2 destination  上面命令中，source1、source2都会被同步到destination目录。","title":"rsync工具使用"},{"content":"1. switch语句 1.1 分支语句switch语句   格式\nswitch (表达式) { \tcase 1: \t语句体1; \tbreak; \tcase 2: \t语句体2; \tbreak; \t... \tdefault: \t语句体n+1; \tbreak; }   执行流程：\n 首先计算出表达式的值 其次，和case依次比较，一旦有对应的值，就会执行相应的语句，在执行的过程中，遇到break就会结 束。 最后，如果所有的case都和表达式的值不匹配，就会执行default语句体部分，然后程序结束掉。    1.2 switch案例-减肥计划  需求：键盘录入星期数，显示今天的减肥活动  周一：跑步 周二：游泳 周三：慢走 周四：动感单车 周五：拳击 周六：爬山 周日：好好吃一顿  示例代码：  public static void main(String[] args){ \t// 1. 键盘录入星期数据，使用变量接收 \tScanner sc = new Scanner(System.in); \tSystem.out.println(\u0026#34;请输入\u0026#34;); \tint week = sc.nextInt(); \t// 2. 多情况判断，采用switch语句实现 \tswitch(week){ \t// 3. 在不同的case中，输出对应的减肥计划 \tcase 1: \tSystem.out.println(\u0026#34;跑步\u0026#34;); \tbreak; \tcase 2: \tSystem.out.println(\u0026#34;游泳\u0026#34;); \tbreak; \tcase 3: \tSystem.out.println(\u0026#34;慢走\u0026#34;); \tbreak; \tcase 4: \tSystem.out.println(\u0026#34;动感单车\u0026#34;); \tbreak; \tcase 5: \tSystem.out.println(\u0026#34;拳击\u0026#34;); \tbreak; \tcase 6: \tSystem.out.println(\u0026#34;爬山\u0026#34;); \tbreak; \tcase 7: \tSystem.out.println(\u0026#34;好好吃一顿\u0026#34;); \tbreak; \tdefault: \tSystem.out.println(\u0026#34;您的输入有误\u0026#34;); \tbreak; \t} \t} } 1.3 switch语句case穿透  概述 : 如果switch语句中,case省略了break语句, 就会开始case穿透 需求 : 键盘录入星期数，输出工作日、休息日 (1-5)工作日，(6-7)休息日 示例代码：  /* case穿透是如何产生的? 如果switch语句中,case省略了break语句, 就会开始case穿透. 现象： 当开始case穿透，后续的case就不会具有匹配效果，内部的语句都会执行 直到看见break，或者将整体switch语句执行完毕，才会结束。 */ public static void main(String[] args){ \tScanner sc = new Scanner(System.in); \tSystem.out.println(\u0026#34;请输入星期数:\u0026#34;); \tint week = sc.nextInt(); \t\tswitch(week){ \tcase 1: \tcase 2: \tcase 3: \tcase 4: \tcase 5: \tSystem.out.println(\u0026#34;工作日\u0026#34;); \tbreak; \tcase 6: \tcase 7: \tSystem.out.println(\u0026#34;休息日\u0026#34;); \tbreak; \tdefault: \tSystem.out.println(\u0026#34;您的输入有误\u0026#34;); \tbreak; \t} \t}\t} 2. for循环 2.1 循环语句-for循环   循环：\n循环语句可以在满足循环条件的情况下，反复执行某一段代码，这段被重复执行的代码被称为循环体语句，当反复 执行这个循环体时，需要在合适的时候把循环判断条件修改为false，从而结束循环，否则循环将一直执行下去，形 成死循环。\n  for循环格式：\n  for (初始化语句;条件判断语句;条件控制语句) { \t循环体语句; }   格式解释：\n 初始化语句： 用于表示循环开启时的起始状态，简单说就是循环开始的时候什么样 条件判断语句：用于表示循环反复执行的条件，简单说就是判断循环是否能一直执行下去 循环体语句： 用于表示循环反复执行的内容，简单说就是循环反复执行的事情 条件控制语句：用于表示循环执行中每次变化的内容，简单说就是控制循环是否能执行下去    执行流程：\n①执行初始化语句\n②执行条件判断语句，看其结果是true还是false\n​ 如果是false，循环结束\n​ 如果是true，继续执行\n③执行循环体语句\n④执行条件控制语句\n⑤回到②继续\n  2.2 for循环案例-输出数据1-5和5-1  需求：在控制台输出1-5和5-1的数据 示例代码：  public class ForTest01 {  public static void main(String[] args) { \t//需求：输出数据1-5  for(int i=1; i\u0026lt;=5; i++) { \tSystem.out.println(i); \t} \tSystem.out.println(\u0026#34;--------\u0026#34;); \t//需求：输出数据5-1 \tfor(int i=5; i\u0026gt;=1; i--) { \tSystem.out.println(i); \t}  } } 2.3 for循环案例-求1-5数据和  需求：求1-5之间的数据和，并把求和结果在控制台输出 示例代码：  public class ForTest02 {  public static void main(String[] args) { \t//求和的最终结果必须保存起来，需要定义一个变量，用于保存求和的结果，初始值为0 \tint sum = 0; \t//从1开始到5结束的数据，使用循环结构完成 \tfor(int i=1; i\u0026lt;=5; i++) { \t//将反复进行的事情写入循环结构内部  // 此处反复进行的事情是将数据 i 加到用于保存最终求和的变量 sum 中 \tsum += i; \t/* sum += i;\tsum = sum + i; 第一次：sum = sum + i = 0 + 1 = 1; 第二次：sum = sum + i = 1 + 2 = 3; 第三次：sum = sum + i = 3 + 3 = 6; 第四次：sum = sum + i = 6 + 4 = 10; 第五次：sum = sum + i = 10 + 5 = 15; */ \t} \t//当循环执行完毕时，将最终数据打印出来 \tSystem.out.println(\u0026#34;1-5之间的数据和是：\u0026#34; + sum);  } }  本题要点：  今后遇到的需求中，如果带有求和二字，请立即联想到求和变量 求和变量的定义位置，必须在循环外部，如果在循环内部则计算出的数据将是错误的    2.4 for循环案例-求1-100偶数和  需求：求1-100之间的偶数和，并把求和结果在控制台输出 } 示例代码：  public class ForTest03 {  public static void main(String[] args) { \t//求和的最终结果必须保存起来，需要定义一个变量，用于保存求和的结果，初始值为0 \tint sum = 0; \t//对1-100的数据求和与1-5的数据求和几乎完全一样，仅仅是结束条件不同 \tfor(int i=1; i\u0026lt;=100; i++) { \t//对1-100的偶数求和，需要对求和操作添加限制条件，判断是否是偶数 \tif(i%2 == 0) { \tsum += i; \t} \t} \t//当循环执行完毕时，将最终数据打印出来 \tSystem.out.println(\u0026#34;1-100之间的偶数和是：\u0026#34; + sum);  } } 2.5 for循环案例-水仙花数  需求：在控制台输出所有的“水仙花数” 解释：什么是水仙花数？  水仙花数，指的是一个三位数，个位、十位、百位的数字立方和等于原数  例如153 3*3*3 + 5*5*5 + 1*1*1 = 153     思路：  获取所有的三位数，准备进行筛选，最小的三位数为100，最大的三位数为999，使用for循环获取 获取每一个三位数的个位，十位，百位，做if语句判断是否是水仙花数   示例代码  public class ForTest04 {  public static void main(String[] args) { \t//输出所有的水仙花数必然要使用到循环，遍历所有的三位数，三位数从100开始，到999结束 \tfor(int i=100; i\u0026lt;1000; i++) { \t//在计算之前获取三位数中每个位上的值 \tint ge = i%10; \tint shi = i/10%10; \tint bai = i/10/10%10; \t\t//判定条件是将三位数中的每个数值取出来，计算立方和后与原始数字比较是否相等 \tif(ge*ge*ge + shi*shi*shi + bai*bai*bai == i) { \t//输出满足条件的数字就是水仙花数 \tSystem.out.println(i); \t} \t}  } } 2.6 for循环案例-每行打印2个水仙花数(统计)  需求：在控制台输出所有的“水仙花数”，要求每行打印2个 示例代码：  public class Demo6For { \t/* 需求：在控制台输出所有的“水仙花数”，要求每行打印2个 System.out.print (打印内容);\t打印后不换行 System.out.println(打印内容);\t打印后换行 分析: 1. 定义变量count，用于保存“打印过”的数量，初始值为0 2. 在判定和打印水仙花数的过程中，拼接空格, 但不换行，并在打印后让count变量+1，记录打印过的数量 3. 在每一次count变量+1后，判断是否到达了2的倍数，是的话，换行。 */ \tpublic static void main(String[] args){ \t// 1. 定义变量count，用于保存“打印过”的数量，初始值为0 \tint count = 0; \tfor(int i = 100; i \u0026lt;= 999; i++){ \tint ge = i % 10; \tint shi = i / 10 % 10; \tint bai = i / 10 / 10 % 10; \t\tif(\t(ge*ge*ge + shi*shi*shi + bai*bai*bai) == i){ \t// 2. 在判定和打印水仙花数的过程中，拼接空格, 但不换行，并在打印后让count变量+1，记录打印过的数量 \tSystem.out.print(i + \u0026#34; \u0026#34;); \tcount++; \t// 3. 在每一次count变量+1后，判断是否到达了2的倍数，是的话，换行 \tif(count % 2 == 0){ \tSystem.out.println(); \t} \t} \t} \t} }  本题要点：  今后如果需求带有统计xxx，请先想到计数器变量 计数器变量定义的位置，必须在循环外部    3. while循环 3.1 循环语句-while循环   while循环完整格式：\n初始化语句; while (条件判断语句) { \t循环体语句;  条件控制语句; }   while循环执行流程：\n①执行初始化语句\n②执行条件判断语句，看其结果是true还是false\n​ 如果是false，循环结束\n​ 如果是true，继续执行\n③执行循环体语句\n④执行条件控制语句\n⑤回到②继续\n  示例代码：\n  public class WhileDemo {  public static void main(String[] args) {  //需求：在控制台输出5次\u0026#34;HelloWorld\u0026#34; \t//for循环实现 \tfor(int i=1; i\u0026lt;=5; i++) { \tSystem.out.println(\u0026#34;HelloWorld\u0026#34;); \t} \tSystem.out.println(\u0026#34;--------\u0026#34;); \t//while循环实现 \tint j = 1; \twhile(j\u0026lt;=5) { \tSystem.out.println(\u0026#34;HelloWorld\u0026#34;); \tj++; \t}  } } 3.2 while循环案例-珠穆朗玛峰  需求：世界最高山峰是珠穆朗玛峰(8844.43米=8844430毫米)，假如我有一张足够大的纸，它的厚度是0.1毫米。请问，我折叠多少次，可以折成珠穆朗玛峰的高度? 示例代码：  public class WhileTest {  public static void main(String[] args) { \t//定义一个计数器，初始值为0 \tint count = 0; \t//定义纸张厚度 \tdouble paper = 0.1; \t//定义珠穆朗玛峰的高度 \tint zf = 8844430; \t//因为要反复折叠，所以要使用循环，但是不知道折叠多少次，这种情况下更适合使用while循环 \t//折叠的过程中当纸张厚度大于珠峰就停止了，因此继续执行的要求是纸张厚度小于珠峰高度 \twhile(paper \u0026lt;= zf) { \t//循环的执行过程中每次纸张折叠，纸张的厚度要加倍 \tpaper *= 2; \t//在循环中执行累加，对应折叠了多少次 \tcount++; \t} \t//打印计数器的值 \tSystem.out.println(\u0026#34;需要折叠：\u0026#34; + count + \u0026#34;次\u0026#34;);  } } 4. 循环细节 4.1 循环语句-dowhile循环   完整格式：\n初始化语句; do { \t循环体语句; \t条件控制语句; }while(条件判断语句);   执行流程：\n① 执行初始化语句\n② 执行循环体语句\n③ 执行条件控制语句\n④ 执行条件判断语句，看其结果是true还是false\n如果是false，循环结束\n如果是true，继续执行\n⑤ 回到②继续\n  示例代码：\n  public class DoWhileDemo {  public static void main(String[] args) {  //需求：在控制台输出5次\u0026#34;HelloWorld\u0026#34; \t//for循环实现 \tfor(int i=1; i\u0026lt;=5; i++) { \tSystem.out.println(\u0026#34;HelloWorld\u0026#34;); \t} \tSystem.out.println(\u0026#34;--------\u0026#34;); \t//do...while循环实现 \tint j = 1; \tdo { \tSystem.out.println(\u0026#34;HelloWorld\u0026#34;); \tj++; \t}while(j\u0026lt;=5);  } } 4.2 三种循环的区别  三种循环的区别  for循环和while循环先判断条件是否成立，然后决定是否执行循环体（先判断后执行） do\u0026hellip;while循环先执行一次循环体，然后判断条件是否成立，是否继续执行循环体（先执行后判断）   for循环和while的区别  条件控制语句所控制的自增变量，因为归属for循环的语法结构中，在for循环结束后，就不能再次被访问到了 条件控制语句所控制的自增变量，对于while循环来说不归属其语法结构中，在while循环结束后，该变量还可以继续使用   死循环（无限循环）的三种格式  for(;;){} while(true){} do {} while(true);    4.3 死循环   死循环格式\nfor死循环格式 : for(;;){ } while死循环格式 : while(true){ } do..while死循环格式 : do{ }while(true);   死循环案例\n  /* 问题: 死循环有应用场景吗? 例如: 键盘录入一个1-100之间的整数 顾虑: 键盘录入是用户操作的, 用户就可能会出现一些误操作的现象 */ public static void main(String[] args) {  /* for(;;){ System.out.println(\u0026#34;我停不下来了~\u0026#34;); } */   /* while(true){ System.out.println(\u0026#34;我停不下来了~\u0026#34;); } */   do{  System.out.println(\u0026#34;我停不下来了~\u0026#34;);\t }while(true);   System.out.println(\u0026#34;看看我能被执行吗?~\u0026#34;);\t// 无法访问的语句 } } 4.4 跳转控制语句  跳转控制语句（break）  跳出循环，结束循环   跳转控制语句（continue）  跳过本次循环，继续下次循环   注意： continue只能在循环中进行使用！  public class Demo1Continue { \t/* continue : 跳过某次循环体内容的执行 注意：使用是基于条件控制, 在循环内部使用. 需求: 模拟电梯上行的过程 1-24层, 4层不停. */ \tpublic static void main(String[] args){ \tfor(int i = 1; i \u0026lt;= 24; i++){ \tif(i == 4){ \tcontinue; \t} \tSystem.out.println(i + \u0026#34;层到了~\u0026#34;); \t} \t} \t} public class Demo2Break { \t/* break : 终止循环体内容的执行 注意：使用是基于条件控制的 break语句只能在循环和switch中进行使用. 需求: 模拟20岁工作到80岁, 60岁退休. */ \tpublic static void main(String[] args){ \tfor(int i = 20; i \u0026lt;= 80; i++){ \tif(i == 60){ \tbreak;\t// 结束整个循环 \t} \tSystem.out.println(i + \u0026#34;岁正在上班\u0026#34;); \t} \t} \t} import java.util.Scanner;  public class Test { \t/* 需求：程序运行后，用户可多次查询星期对应的减肥计划，直到输入0，程序结束 步骤: 1. 不明确用户操作几次, 使用死循环包裹业务逻辑 2. 匹配到0的时候，使用break结束循环死循环 */ \tpublic static void main (String[] args){ \t\tlo:while(true){ \tSystem.out.println(\u0026#34;请输入您要查看的星期数:\u0026#34;); \tSystem.out.println(\u0026#34;(如无需继续查看,请输入0退出程序)\u0026#34;); \t\t// 1. 键盘录入星期数据，使用变量接收 \tScanner sc = new Scanner(System.in); \tint week = sc.nextInt(); \t// 2. 多情况判断，采用switch语句实现 \tswitch(week){ \t// 3. 在不同的case中，输出对应的减肥计划 \tcase 0: \tSystem.out.println(\u0026#34;感谢您的使用\u0026#34;); \tbreak lo; \tcase 1: \tSystem.out.println(\u0026#34;跑步\u0026#34;); \tbreak; \tcase 2: \tSystem.out.println(\u0026#34;游泳\u0026#34;); \tbreak; \tcase 3: \tSystem.out.println(\u0026#34;慢走\u0026#34;); \tbreak; \tcase 4: \tSystem.out.println(\u0026#34;动感单车\u0026#34;); \tbreak; \tcase 5: \tSystem.out.println(\u0026#34;拳击\u0026#34;); \tbreak; \tcase 6: \tSystem.out.println(\u0026#34;爬山\u0026#34;); \tbreak; \tcase 7: \tSystem.out.println(\u0026#34;好好吃一顿\u0026#34;); \tbreak; \tdefault: \tSystem.out.println(\u0026#34;您的输入有误\u0026#34;); \tbreak; \t} \t} \t\t\t} } 5. Random 5.1 Random产生随机数（掌握）   概述：\n Random类似Scanner，也是Java提供好的API，内部提供了产生随机数的功能  API后续课程详细讲解，现在可以简单理解为Java已经写好的代码      使用步骤：\n  导入包\nimport java.util.Random;\n  创建对象\nRandom r = new Random();\n  产生随机数\nint num = r.nextInt(10);\n解释： 10代表的是一个范围，如果括号写10，产生的随机数就是0-9，括号写20，参数的随机数则是0-19\n    示例代码：\n  import java.util.Random;  public class Demo1Random { \t/* Random : 产生随机数 1. 导包\t: import java.util.Random; 导包的动作必须出现在类定义的上面 2. 创建对象 : Random r = new Random(); 上面这个格式里面，r 是变量名，可以变，其他的都不允许变 3. 获取随机数 : int number = r.nextInt(10);\t//获取数据的范围：[0,10) 包括0,不包括10 上面这个格式里面，number是变量名，可以变，数字10可以变。其他的都不允许变 需求: 产生随机数1-10之间的 */ \tpublic static void main(String[] args){ \t// 2. 创建对象 \tRandom r = new Random(); \t\tfor(int i = 1; i \u0026lt;= 10; i++){ \t// 3. 获取随机数 \tint num = r.nextInt(10) + 1;\t// 1-10 \tSystem.out.println(num); \t} \t\t\t\t} } 5.3 Random练习-猜数字（应用）   需求：\n程序自动生成一个1-100之间的数字，使用程序实现猜出这个数字是多少？\n当猜错的时候根据不同情况给出相应的提示\nA. 如果猜的数字比真实数字大，提示你猜的数据大了\nB. 如果猜的数字比真实数字小，提示你猜的数据小了\nC. 如果猜的数字与真实数字相等，提示恭喜你猜中了\n  示例代码：\n  import java.util.Scanner; import java.util.Random;  public class Test { \t/* 需求：程序自动生成一个1-100之间的数字，使用程序实现猜出这个数字是多少？ 当猜错的时候根据不同情况给出相应的提示 如果猜的数字比真实数字大，提示你猜的数据大了 如果猜的数字比真实数字小，提示你猜的数据小了 如果猜的数字与真实数字相等，提示恭喜你猜中了 1. 准备Random和Scanner对象, 分别用于产生随机数和键盘录入 2. 使用Random产生一个1-100之间的数, 作为要猜的数 3. 键盘录入用户猜的的数据 4. 使用录入的数据(用户猜的数据)和随机数(要猜的数据)进行比较, 并给出提示 5. 以上内容需要多次进行, 但无法预估用户输入几次可以猜测正确, 使用while(true)死循环包裹 6. 猜对之后, break结束. */ \tpublic static void main(String[] args){ \t// 1. 准备Random和Scanner对象, 分别用于产生随机数和键盘录入 \tRandom r = new Random(); \tScanner sc = new Scanner(System.in); \t// 2. 使用Random产生一个1-100之间的数, 作为要猜的数 \tint randomNum = r.nextInt(100) + 1; \t\t// 5. 以上内容需要多次进行, 但无法预估用户输入几次可以猜测正确, 使用while(true)死循环包裹 \twhile(true){ \t// 3. 键盘录入用户猜的的数据 \tSystem.out.println(\u0026#34;请输入您猜的数据:\u0026#34;); \tint num = sc.nextInt(); \t// 4. 使用录入的数据(用户猜的数据)和随机数(要猜的数据)进行比较, 并给出提示 \tif(num \u0026gt; randomNum){ \tSystem.out.println(\u0026#34;猜大了\u0026#34;); \t}else if(num \u0026lt; randomNum){ \tSystem.out.println(\u0026#34;猜小了\u0026#34;); \t}else{ \t// 6. 猜对之后, break结束. \tSystem.out.println(\u0026#34;恭喜,猜中了\u0026#34;); \tbreak; \t} \t} \t\tSystem.out.println(\u0026#34;感谢您的使用\u0026#34;); \t\t} } ","permalink":"https://iblog.zone/archives/java%E5%BE%AA%E7%8E%AF/","summary":"1. switch语句 1.1 分支语句switch语句   格式\nswitch (表达式) { \tcase 1: \t语句体1; \tbreak; \tcase 2: \t语句体2; \tbreak; \t... \tdefault: \t语句体n+1; \tbreak; }   执行流程：\n 首先计算出表达式的值 其次，和case依次比较，一旦有对应的值，就会执行相应的语句，在执行的过程中，遇到break就会结 束。 最后，如果所有的case都和表达式的值不匹配，就会执行default语句体部分，然后程序结束掉。    1.2 switch案例-减肥计划  需求：键盘录入星期数，显示今天的减肥活动  周一：跑步 周二：游泳 周三：慢走 周四：动感单车 周五：拳击 周六：爬山 周日：好好吃一顿  示例代码：  public static void main(String[] args){ \t// 1. 键盘录入星期数据，使用变量接收 \tScanner sc = new Scanner(System.","title":"Java循环"},{"content":"1 类型转换 在Java中，一些数据类型之间是可以相互转换的。分为两种情况：自动类型转换和强制类型转换。\n1.1 隐式转换(理解) ​\t把一个表示数据范围小的数值或者变量赋值给另一个表示数据范围大的变量。这种转换方式是自动的，直接书写即可。例如：\ndouble num = 10; // 将int类型的10直接赋值给double类型 System.out.println(num); // 输出10.0 ​\t类型从小到大关系图：\n说明：\n 整数默认是int类型，byte、short和char类型数据参与运算均会自动转换为int类型。  byte b1 = 10; byte b2 = 20; byte b3 = b1 + b2; // 第三行代码会报错，b1和b2会自动转换为int类型，计算结果为int，int赋值给byte需要强制类型转换。 // 修改为: int num = b1 + b2; // 或者： byte b3 = (byte) (b1 + b2); boolean类型不能与其他基本数据类型相互转换。  1.2 强制转换(理解) ​\t把一个表示数据范围大的数值或者变量赋值给另一个表示数据范围小的变量。\n​\t强制类型转换格式：目标数据类型 变量名 = (目标数据类型)值或者变量;\n​\t例如：\ndouble num1 = 5.5; int num2 = (int) num1; // 将double类型的num1强制转换为int类型 System.out.println(num2); // 输出5（小数位直接舍弃） 1.3 类型转换案例(理解) 案例代码：\nbyte a = 3; byte b = 4; byte c = a + b; //错误。因为两个byte变量相加，会先提升为int类型 byte d = 3 + 4; //正确。常量优化机制 常量优化机制：\n​\t在编译时，整数常量的计算会直接算出结果，并且会自动判断该结果是否在byte取值范围内，\n​\t在：编译通过\n​\t不在：编译失败\n2. 运算符 2.1 算术运算符 2.1.1 运算符和表达式（了解） 运算符：对常量或者变量进行操作的符号\n表达式：用运算符把常量或者变量连接起来符合java语法的式子就可以称为表达式。\n​ 不同运算符连接的表达式体现的是不同类型的表达式。\n举例说明：\nint a = 10; int b = 20; int c = a + b; +：是运算符，并且是算术运算符。\na + b：是表达式，由于+是算术运算符，所以这个表达式叫算术表达式。\n2.1.2 算术运算符(应用)    符号 作用 说明     + 加 参看小学一年级   - 减 参看小学一年级   * 乘 参看小学二年级，与“×”相同   / 除 参看小学二年级，与“÷”相同   % 取余 获取的是两个数据做除法的余数    注意：\n  /和%的区别：两个数据做除法，/取结果的商，%取结果的余数。\n  整数操作只能得到整数，要想得到小数，必须有浮点数参与运算。\n  int a = 10; int b = 3; System.out.println(a / b); // 输出结果3 System.out.println(a % b); // 输出结果1 2.1.3 字符的“+”操作（理解） char类型参与算术运算，使用的是计算机底层对应的十进制数值。需要我们记住三个字符对应的数值：\n\u0026lsquo;a\u0026rsquo; \u0026ndash; 97\ta-z是连续的，所以\u0026rsquo;b\u0026rsquo;对应的数值是98，\u0026lsquo;c\u0026rsquo;是99，依次递加\n\u0026lsquo;A\u0026rsquo; \u0026ndash; 65\tA-Z是连续的，所以\u0026rsquo;B\u0026rsquo;对应的数值是66，\u0026lsquo;C\u0026rsquo;是67，依次递加\n\u0026lsquo;0\u0026rsquo; \u0026ndash; 48\t0-9是连续的，所以'1\u0026rsquo;对应的数值是49，\u0026lsquo;2\u0026rsquo;是50，依次递加\n// 可以通过使用字符与整数做算术运算，得出字符对应的数值是多少 char ch1 = \u0026#39;a\u0026#39;; System.out.println(ch1 + 1); // 输出98，97 + 1 = 98  char ch2 = \u0026#39;A\u0026#39;; System.out.println(ch2 + 1); // 输出66，65 + 1 = 66  char ch3 = \u0026#39;0\u0026#39;; System.out.println(ch3 + 1); // 输出49，48 + 1 = 49 算术表达式中包含不同的基本数据类型的值的时候，整个算术表达式的类型会自动进行提升。\n提升规则：\nbyte类型，short类型和char类型将被提升到int类型，不管是否有其他类型参与运算。\n整个表达式的类型自动提升到与表达式中最高等级的操作数相同的类型\n​ 等级顺序：byte,short,char \u0026ndash;\u0026gt; int \u0026ndash;\u0026gt; long \u0026ndash;\u0026gt; float \u0026ndash;\u0026gt; double\n例如：\nbyte b1 = 10; byte b2 = 20; // byte b3 = b1 + b2; // 该行报错，因为byte类型参与算术运算会自动提示为int，int赋值给byte可能损失精度 int i3 = b1 + b2; // 应该使用int接收 byte b3 = (byte) (b1 + b2); // 或者将结果强制转换为byte类型 ------------------------------- int num1 = 10; double num2 = 20.0; double num3 = num1 + num2; // 使用double接收，因为num1会自动提升为double类型 2.1.4 字符串的“+”操作（理解） 当“+”操作中出现字符串时，这个”+”是字符串连接符，而不是算术运算。\nSystem.out.println(\u0026#34;itheima\u0026#34;+ 666); // 输出：itheima666 在”+”操作中，如果出现了字符串，就是连接运算符，否则就是算术运算。当连续进行“+”操作时，从左到右逐个执行。\nSystem.out.println(1 + 99 + \u0026#34;年黑马\u0026#34;); // 输出：100年黑马 System.out.println(1 + 2 + \u0026#34;itheima\u0026#34; + 3 + 4); // 输出：3itheima34 // 可以使用小括号改变运算的优先级 System.out.println(1 + 2 + \u0026#34;itheima\u0026#34; + (3 + 4)); // 输出：3itheima7 2.1.5 数值拆分（应用） 需求：\n​\t键盘录入一个三位数，将其拆分为个位，十位，百位，打印在控制台\n示例代码：\nimport java.util.Scanner; public class Test { \tpublic static void main(String[] args) { \t// 1：使用Scanner键盘录入一个三位数 \tScanner sc = new Scanner(System.in); \tSystem.out.println(\u0026#34;请输入一个三位数\u0026#34;); \tint num = sc.nextInt(); \t// 2：个位的计算：数值 % 10 \tint ge = num % 10;\t\t// 3：十位的计算：数值 / 10 % 10 \tint shi = num / 10 % 10;\t\t// 4：百位的计算：数值 / 100 \tint bai = num / 100; \t// 5：将个位, 十位, 百位拼接上正确的字符串, 打印即可 \tSystem.out.println(\u0026#34;整数\u0026#34;+num+\u0026#34;个位为:\u0026#34; + ge); \tSystem.out.println(\u0026#34;整数\u0026#34;+num+\u0026#34;十位为:\u0026#34; + shi); \tSystem.out.println(\u0026#34;整数\u0026#34;+num+\u0026#34;百位为:\u0026#34; + bai); \t\t} } 2.2 自增自减运算符（理解）    符号 作用 说明     ++ 自增 变量的值加1   \u0026ndash; 自减 变量的值减1    注意事项：\n​\t++和\u0026ndash; 既可以放在变量的后边，也可以放在变量的前边。\n​\t单独使用的时候， ++和\u0026ndash; 无论是放在变量的前边还是后边，结果是一样的。\n​\t参与操作的时候，如果放在变量的后边，先拿变量参与操作，后拿变量做++或者\u0026ndash;。\n​\t参与操作的时候，如果放在变量的前边，先拿变量做++或者\u0026ndash;，后拿变量参与操作。\n​\t最常见的用法：单独使用。\nint i = 10; i++; // 单独使用 System.out.println(\u0026#34;i:\u0026#34; + i); // i:11  int j = 10; ++j; // 单独使用 System.out.println(\u0026#34;j:\u0026#34; + j); // j:11  int x = 10; int y = x++; // 赋值运算，++在后边，所以是使用x原来的值赋值给y，x本身自增1 System.out.println(\u0026#34;x:\u0026#34; + x + \u0026#34;, y:\u0026#34; + y); // x:11，y:10  int m = 10; int n = ++m; // 赋值运算，++在前边，所以是使用m自增后的值赋值给n，m本身自增1 System.out.println(\u0026#34;m:\u0026#34; + m + \u0026#34;, m:\u0026#34; + m); // m:11，m:11 练习：\nint x = 10; int y = x++ + x++ + x++; System.out.println(y); // y的值是多少？ /* 解析，三个表达式都是++在后，所以每次使用的都是自增前的值，但程序自左至右执行，所以第一次自增时，使用的是10进行计算，但第二次自增时，x的值已经自增到11了，所以第二次使用的是11，然后再次自增。。。 所以整个式子应该是：int y = 10 + 11 + 12; 输出结果为33。 */ 注意：通过此练习深刻理解自增和自减的规律，但实际开发中强烈建议不要写这样的代码！小心挨打！ 2.3 赋值运算符（应用） 赋值运算符的作用是将一个表达式的值赋给左边，左边必须是可修改的，不能是常量。\n   符号 作用 说明     = 赋值 a=10，将10赋值给变量a   += 加后赋值 a+=b，将a+b的值给a   -= 减后赋值 a-=b，将a-b的值给a   *= 乘后赋值 a*=b，将a×b的值给a   /= 除后赋值 a/=b，将a÷b的商给a   %= 取余后赋值 a%=b，将a÷b的余数给a    注意：\n扩展的赋值运算符隐含了强制类型转换。\nshort s = 10; s = s + 10; // 此行代码报出，因为运算中s提升为int类型，运算结果int赋值给short可能损失精度  s += 10; // 此行代码没有问题，隐含了强制类型转换，相当于 s = (short) (s + 10); 2.4 关系运算符（应用） 关系运算符有6种关系，分别为小于、小于等于、大于、等于、大于等于、不等于。\n   符号 说明     == a==b，判断a和b的值是否相等，成立为true，不成立为false   != a!=b，判断a和b的值是否不相等，成立为true，不成立为false   \u0026gt; a\u0026gt;b，判断a是否大于b，成立为true，不成立为false   \u0026gt;= a\u0026gt;=b，判断a是否大于等于b，成立为true，不成立为false   \u0026lt; a\u0026lt;b，判断a是否小于b，成立为true，不成立为false   \u0026lt;= a\u0026lt;=b，判断a是否小于等于b，成立为true，不成立为false    注意事项：\n​\t关系运算符的结果都是boolean类型，要么是true，要么是false。\n​\t千万不要把“==”误写成“=”，\u0026quot;==\u0026ldquo;是判断是否相等的关系，\u0026quot;=\u0026ldquo;是赋值。\nint a = 10; int b = 20; System.out.println(a == b); // false System.out.println(a != b); // true System.out.println(a \u0026gt; b); // false System.out.println(a \u0026gt;= b); // false System.out.println(a \u0026lt; b); // true System.out.println(a \u0026lt;= b); // true  // 关系运算的结果肯定是boolean类型，所以也可以将运算结果赋值给boolean类型的变量 boolean flag = a \u0026gt; b; System.out.println(flag); // 输出false 2.5 逻辑运算符（应用） 逻辑运算符把各个运算的关系表达式连接起来组成一个复杂的逻辑表达式，以判断程序中的表达式是否成立，判断的结果是 true 或 false。\n   符号 作用 说明     \u0026amp; 逻辑与 a\u0026amp;b，a和b都是true，结果为true，否则为false   | 逻辑或 a|b，a和b都是false，结果为false，否则为true   ^ 逻辑异或 a^b，a和b结果不同为true，相同为false   ! 逻辑非 !a，结果和a的结果正好相反    //定义变量 int i = 10; int j = 20; int k = 30;  //\u0026amp; “与”，并且的关系，只要表达式中有一个值为false，结果即为false System.out.println((i \u0026gt; j) \u0026amp; (i \u0026gt; k)); //false \u0026amp; false,输出false System.out.println((i \u0026lt; j) \u0026amp; (i \u0026gt; k)); //true \u0026amp; false,输出false System.out.println((i \u0026gt; j) \u0026amp; (i \u0026lt; k)); //false \u0026amp; true,输出false System.out.println((i \u0026lt; j) \u0026amp; (i \u0026lt; k)); //true \u0026amp; true,输出true System.out.println(\u0026#34;--------\u0026#34;);  //| “或”，或者的关系，只要表达式中有一个值为true，结果即为true System.out.println((i \u0026gt; j) | (i \u0026gt; k)); //false | false,输出false System.out.println((i \u0026lt; j) | (i \u0026gt; k)); //true | false,输出true System.out.println((i \u0026gt; j) | (i \u0026lt; k)); //false | true,输出true System.out.println((i \u0026lt; j) | (i \u0026lt; k)); //true | true,输出true System.out.println(\u0026#34;--------\u0026#34;);  //^ “异或”，相同为false，不同为true System.out.println((i \u0026gt; j) ^ (i \u0026gt; k)); //false ^ false,输出false System.out.println((i \u0026lt; j) ^ (i \u0026gt; k)); //true ^ false,输出true System.out.println((i \u0026gt; j) ^ (i \u0026lt; k)); //false ^ true,输出true System.out.println((i \u0026lt; j) ^ (i \u0026lt; k)); //true ^ true,输出false System.out.println(\u0026#34;--------\u0026#34;);  //! “非”，取反 System.out.println((i \u0026gt; j)); //false System.out.println(!(i \u0026gt; j)); //!false，,输出true 2.6 短路逻辑运算符（理解）    符号 作用 说明     \u0026amp;\u0026amp; 短路与 作用和\u0026amp;相同，但是有短路效果   || 短路或 作用和|相同，但是有短路效果    在逻辑与运算中，只要有一个表达式的值为false，那么结果就可以判定为false了，没有必要将所有表达式的值都计算出来，短路与操作就有这样的效果，可以提高效率。同理在逻辑或运算中，一旦发现值为true，右边的表达式将不再参与运算。\n  逻辑与\u0026amp;，无论左边真假，右边都要执行。\n  短路与\u0026amp;\u0026amp;，如果左边为真，右边执行；如果左边为假，右边不执行。\n  逻辑或|，无论左边真假，右边都要执行。\n  短路或||，如果左边为假，右边执行；如果左边为真，右边不执行。\n  int x = 3; int y = 4; System.out.println((x++ \u0026gt; 4) \u0026amp; (y++ \u0026gt; 5)); // 两个表达都会运算 System.out.println(x); // 4 System.out.println(y); // 5  System.out.println((x++ \u0026gt; 4) \u0026amp;\u0026amp; (y++ \u0026gt; 5)); // 左边已经可以确定结果为false，右边不参与运算 System.out.println(x); // 4 System.out.println(y); // 4 2.7 三元运算符（理解） 三元运算符语法格式：\n关系表达式 ? 表达式1 : 表达式2; 解释：问号前面的位置是判断的条件，判断结果为boolean型，为true时调用表达式1，为false时调用表达式2。其逻辑为：如果条件表达式成立或者满足则执行表达式1，否则执行第二个。\n举例：\nint a = 10; int b = 20; int c = a \u0026gt; b ? a : b; // 判断 a\u0026gt;b 是否为真，如果为真取a的值，如果为假，取b的值 2.8 三元运算符案例(应用) 需求：\n​\t一座寺庙里住着三个和尚，已知他们的身高分别为150cm、210cm、165cm，请用程序实现获取这三个和尚的最高身高。\npublic class OperatorTest02 { \tpublic static void main(String[] args) { \t//1：定义三个变量用于保存和尚的身高，单位为cm，这里仅仅体现数值即可。 \tint height1 = 150; \tint height2 = 210; \tint height3 = 165;\t\t//2：用三元运算符获取前两个和尚的较高身高值，并用临时身高变量保存起来。 \tint tempHeight = height1 \u0026gt; height2 ? height1 : height2;\t\t//3：用三元运算符获取临时身高值和第三个和尚身高较高值，并用最大身高变量保存。 \tint maxHeight = tempHeight \u0026gt; height3 ? tempHeight : height3;\t\t//4：输出结果 \tSystem.out.println(\u0026#34;maxHeight:\u0026#34; + maxHeight); \t} } 3. 流程控制语句 在一个程序执行的过程中，各条语句的执行顺序对程序的结果是有直接影响的。所以，我们必须清楚每条语句的执行流程。而且，很多时候要通过控制语句的执行顺序来实现我们想要的功能。\n3.1 流程控制语句分类(了解) ​\t顺序结构\n​\t分支结构(if, switch)\n​\t循环结构(for, while, do…while)\n3.2 顺序结构(了解) 顺序结构是程序中最简单最基本的流程控制，没有特定的语法结构，按照代码的先后顺序，依次执行，程序中大多数的代码都是这样执行的。\n顺序结构执行流程图：\n3.3 分支结构之if语句 3.3.1 if语句格式1（理解） 格式： if (关系表达式) {  语句体;\t} 执行流程：\n①首先计算关系表达式的值\n②如果关系表达式的值为true就执行语句体\n③如果关系表达式的值为false就不执行语句体\n④继续执行后面的语句内容\n示例：\npublic class IfDemo { \tpublic static void main(String[] args) { \tSystem.out.println(\u0026#34;开始\u0026#34;);  \t// 如果年龄大于18岁, 就可以上网吧 \tint age = 17; \t\tif(age \u0026gt;= 18){ \t// int a = 10; \tSystem.out.println(\u0026#34;可以上网吧\u0026#34;); \t} \t\tSystem.out.println(\u0026#34;结束\u0026#34;); \t} } 3.3.2 if语句格式2（理解） 格式： if (关系表达式) {  语句体1;\t} else {  语句体2;\t} 执行流程：\n①首先计算关系表达式的值\n②如果关系表达式的值为true就执行语句体1\n③如果关系表达式的值为false就执行语句体2\n④继续执行后面的语句内容\n示例：奇偶数\n​\t任意给出一个整数，请用程序实现判断该整数是奇数还是偶数，并在控制台输出该整数是奇数还是偶数。\npublic class Demo2If { \tpublic static void main(String[] args) { \t// 程序判断一个数, 是奇数还是偶数 \tint num = 9; \t\tif(num % 2 == 0){ \tSystem.out.println(\u0026#34;偶数\u0026#34;); \t}else{ \tSystem.out.println(\u0026#34;奇数\u0026#34;); \t} \t} } 3.3.3 if语句格式3（理解） 格式： if (关系表达式1) {  语句体1;\t} else if (关系表达式2) {  语句体2;\t} … else {  语句体n+1; } 执行流程：\n①首先计算关系表达式1的值\n②如果值为true就执行语句体1；如果值为false就计算关系表达式2的值\n③如果值为true就执行语句体2；如果值为false就计算关系表达式3的值\n④…\n⑤如果没有任何关系表达式为true，就执行语句体n+1。\n示例：\n​\t定义一个在0 ~ 100之间的变量a, 90 ~ 100优秀，80 ~ 89良好，70 ~ 79中等，60 ~ 69及格，0 ~ 59请努力加油！\npublic class Demo3If { \tpublic static void main(String[] args){ \tint score = 65; \tif(score \u0026gt;= 90 \u0026amp;\u0026amp; score \u0026lt;= 100){ \tSystem.out.println(\u0026#34;优秀\u0026#34;); \t}else if (score \u0026gt;= 80 \u0026amp;\u0026amp; score \u0026lt;= 89){ \tSystem.out.println(\u0026#34;良好\u0026#34;); \t}else if (score \u0026gt;= 70 \u0026amp;\u0026amp; score \u0026lt;= 79){ \tSystem.out.println(\u0026#34;中等\u0026#34;); \t}else if (score \u0026gt;= 60 \u0026amp;\u0026amp; score \u0026lt;= 69){ \tSystem.out.println(\u0026#34;及格\u0026#34;); \t}else if (score \u0026gt;= 0 \u0026amp;\u0026amp; score \u0026lt;= 59){ \tSystem.out.println(\u0026#34;请努力加油\u0026#34;); \t}else{ \tSystem.out.println(\u0026#34;成绩有误!\u0026#34;); \t} \t} } 3.3.4 if语句格式3案例（应用） 需求：小明快要期末考试了，小明爸爸对他说，会根据他不同的考试成绩，送他不同的礼物，假如你可以控制小明的得分，请用程序实现小明到底该获得什么样的礼物，并在控制台输出。\n分析：\n​\t①小明的考试成绩未知，可以使用键盘录入的方式获取值\n​\t②由于奖励种类较多，属于多种判断，采用if\u0026hellip;else\u0026hellip;if格式实现\n​\t③为每种判断设置对应的条件\n​\t④为每种判断设置对应的奖励\nimport java.util.Scanner; public class IfTest02 { \tpublic static void main(String[] args){ \t// 1. 使用Scanner录入考试成绩 \tScanner sc = new Scanner(System.in); \tSystem.out.println(\u0026#34;请输入您的成绩:\u0026#34;); \tint score = sc.nextInt(); \t// 2. 判断成绩是否在合法范围内 0~100 \tif(score \u0026gt;=0 \u0026amp;\u0026amp; score \u0026lt;= 100){ \t// 合法成绩 \t// 3. 在合法的语句块中判断成绩范围符合哪一个奖励 \tif(score \u0026gt;= 95 \u0026amp;\u0026amp; score \u0026lt;= 100){ \tSystem.out.println(\u0026#34;自行车一辆\u0026#34;); \t}else if(score \u0026gt;= 90 \u0026amp;\u0026amp; score \u0026lt;= 94){ \tSystem.out.println(\u0026#34;游乐场一次\u0026#34;); \t}else if(score \u0026gt;= 80 \u0026amp;\u0026amp; score \u0026lt;= 89){ \tSystem.out.println(\u0026#34;变形金刚一个\u0026#34;); \t}else { \tSystem.out.println(\u0026#34;挨顿揍, 这座城市又多了一个伤心的人~\u0026#34;); \t} \t}else{ \t// 非法的话, 给出错误提示 \tSystem.out.println(\u0026#34;您的成绩输入有误!\u0026#34;); \t} \t} } ","permalink":"https://iblog.zone/archives/java%E8%BF%90%E7%AE%97%E7%AC%A6/","summary":"1 类型转换 在Java中，一些数据类型之间是可以相互转换的。分为两种情况：自动类型转换和强制类型转换。\n1.1 隐式转换(理解) ​\t把一个表示数据范围小的数值或者变量赋值给另一个表示数据范围大的变量。这种转换方式是自动的，直接书写即可。例如：\ndouble num = 10; // 将int类型的10直接赋值给double类型 System.out.println(num); // 输出10.0 ​\t类型从小到大关系图：\n说明：\n 整数默认是int类型，byte、short和char类型数据参与运算均会自动转换为int类型。  byte b1 = 10; byte b2 = 20; byte b3 = b1 + b2; // 第三行代码会报错，b1和b2会自动转换为int类型，计算结果为int，int赋值给byte需要强制类型转换。 // 修改为: int num = b1 + b2; // 或者： byte b3 = (byte) (b1 + b2); boolean类型不能与其他基本数据类型相互转换。  1.2 强制转换(理解) ​\t把一个表示数据范围大的数值或者变量赋值给另一个表示数据范围小的变量。\n​\t强制类型转换格式：目标数据类型 变量名 = (目标数据类型)值或者变量;\n​\t例如：\ndouble num1 = 5.","title":"Java运算符"},{"content":"1. Java概述 1.1 Java语言背景介绍（了解） 语言：人与人交流沟通的表达方式\n计算机语言：人与计算机之间进行信息交流沟通的一种特殊语言\nJava语言是美国Sun公司（Stanford University Network）在1995年推出的计算机语言\nJava之父：詹姆斯·高斯林（James Gosling）\n2009年，Sun公司被甲骨文公司收购，所以我们现在访问oracle官网即可：https://www.oracle.com\njava语言的三个版本：\n​\tJavaSE: Java 语言的（标准版），用于桌面应用的开发，是其他两个版本的基础\n​\tJavaME: Java 语言的（小型版），用于嵌入式消费类电子设备\n​\tJavaEE: Java 语言的（企业版），用于 Web 方向的网站开发\n1.2 Java语言跨平台原理（理解） Java程序并非是直接运行的，Java编译器将Java源程序编译成与平台无关的字节码文件(class文件)，然后由Java虚拟机（JVM）对字节码文件解释执行。所以在不同的操作系统下，只需安装不同的Java虚拟机即可实现java程序的跨平台。\n1.3 JRE和JDK（记忆） JVM（Java Virtual Machine），Java虚拟机\nJRE（Java Runtime Environment），Java运行环境，包含了JVM和Java的核心类库（Java API）\nJDK（Java Development Kit）称为Java开发工具，包含了JRE和开发工具\n总结：我们只需安装JDK即可，它包含了java的运行环境和虚拟机。\n1.4 JDK的下载和安装（应用） 1.4.1 下载 通过官方网站获取JDK\nhttp://www.oracle.com\n注意：针对不同的操作系统，需要下载对应版本的JDK。\n1.4.2 安装 傻瓜式安装，下一步即可。但默认的安装路径是在C:\\Program Files下，为方便统一管理建议修改安装路径，将与开发相关的软件都安装到一个目录下，例如：E:\\develop。\n注意：安装路径不要包含中文或者空格等特殊字符（使用纯英文目录）。\n1.4.3 JDK的安装目录介绍    目录名称 说明     bin 该路径下存放了JDK的各种工具命令。javac和java就放在这个目录。   conf 该路径下存放了JDK的相关配置文件。   include 该路径下存放了一些平台特定的头文件。   jmods 该路径下存放了JDK的各种模块。   legal 该路径下存放了JDK各模块的授权文档。   lib 该路径下存放了JDK工具的一些补充JAR包。    2. 第一个演示程序 2.1 常用DOS命令（应用） 在接触集成开发环境之前，我们需要使用命令行窗口对java程序进行编译和运行，所以需要知道一些常用DOS命令。\n1、打开命令行窗口的方式：win + r打开运行窗口，输入cmd，回车。\n2、常用命令及其作用\n   操作 说明     盘符名称: 盘符切换。E:回车，表示切换到E盘。   dir 查看当前路径下的内容。   cd 目录 进入单级目录。cd itheima   cd .. 回退到上一级目录。   cd 目录1\\目录2... 进入多级目录。cd itheima\\JavaSE   cd \\ 回退到盘符目录。   cls 清屏。   exit 退出命令提示符窗口。    2.2 Path环境变量的配置（应用） 2.2.1 为什么配置环境变量 开发Java程序，需要使用JDK提供的开发工具（比如javac.exe、java.exe等命令），而这些工具在JDK的安装目录的bin目录下，如果不配置环境变量，那么这些命令只可以在该目录下执行。我们不可能把所有的java文件都放到JDK的bin目录下，所以配置环境变量的作用就是可以使bin目录下的java相关命令可以在任意目录下使用。\n2.3 HelloWorld案例（应用） HelloWorld案例是指在计算机屏幕上输出“HelloWorld”这行文字。\n各种计算机语言都习惯使用该案例作为第一个演示案例。\n2.3.1 Java程序开发运行流程 开发Java程序，需要三个步骤：编写程序，编译程序，运行程序。\n2.3.2 HelloWorld案例的编写 1、新建文本文档文件，修改名称为HelloWorld.java。\n2、用记事本打开HelloWorld.java文件，输写程序内容。\npublic class HelloWorld { \tpublic static void main(String[] args) { \tSystem.out.println(\u0026#34;HelloWorld\u0026#34;); \t} } 2.3.3 HelloWorld案例的编译和运行 存文件，打开命令行窗口，将目录切换至java文件所在目录，编译java文件生成class文件，运行class文件。\n 编译：javac 文件名.java\n范例：javac HelloWorld.java\n执行：java 类名\n范例：java HelloWorld\n 2.4 HelloWorld案例详解（理解） 2.5 HelloWorld案例常见问题（理解） 2.5.1 BUG 在电脑系统或程序中，隐藏着的一些未被发现的缺陷或问题统称为bug（漏洞）。\n2.5.2 BUG的解决 1、具备识别BUG的能力：多看\n2、具备分析BUG的能力：多思考，多查资料\n3、具备解决BUG的能力：多尝试，多总结\n2.5.3 HelloWorld案例常见问题 1、非法字符问题。Java中的符号都是英文格式的。\n2、大小写问题。Java语言对大小写敏感（区分大小写）。\n3、在系统中显示文件的扩展名，避免出现HelloWorld.java.txt文件。\n4、编译命令后的java文件名需要带文件后缀.java\n5、运行命令后的class文件名（类名）不带文件后缀\n2.6 Notepad++软件的安装和使用（应用） 2.6.1 什么要使用Notepad++软件 Notepad++功能比windows中的自带记事本功能强大，除了可以用来制作一般的纯文字说明文件，也十分适合编写计算机程序代码。Notepad++有行号，能够快速定位问题位置，还有语法高亮度显示、代码折叠等功能。而且它是免费的。\n2.6.2 Notepad++软件安装 安装：傻瓜式安装，一直下一步即可。建议也安装到统一的开发软件目录下，比如E:\\develop。\n2.6.3Notepad++软件配置 安装完毕之后，为了使用方便，做一个简单的配置：修改默认语言和编码。\n3. java基础语法 3.1 注释（理解） 注释是对代码的解释和说明文字，可以提高程序的可读性，因此在程序中添加必要的注释文字十分重要。Java中的注释分为三种：\n单行注释。单行注释的格式是使用//，从//开始至本行结尾的文字将作为注释文字。\n// 这是单行注释文字 多行注释。多行注释的格式是使用/* 和 */将一段较长的注释括起来。\n/* 这是多行注释文字 这是多行注释文字 这是多行注释文字 */ 注意：多行注释不能嵌套使用。 文档注释。文档注释以/**开始，以*/结束。（以后讲）\n3.2 关键字（理解） 关键字是指被java语言赋予了特殊含义的单词。\n关键字的特点：\n​\t关键字的字母全部小写。\n​\t常用的代码编辑器对关键字都有高亮显示，比如现在我们能看到的public、class、static等。\n3.3 常量（应用） 常量：在程序运行过程中，其值不可以发生改变的量。\nJava中的常量分类：\n​\t字符串常量 用双引号括起来的多个字符（可以包含0个、一个或多个），例如\u0026quot;a\u0026quot;、\u0026ldquo;abc\u0026rdquo;、\u0026ldquo;中国\u0026quot;等\n​\t整数常量 整数，例如：-10、0、88等\n​\t小数常量 小数，例如：-5.5、1.0、88.88等\n​\t字符常量 用单引号括起来的一个字符，例如：\u0026lsquo;a\u0026rsquo;、\u0026lsquo;5\u0026rsquo;、\u0026lsquo;B\u0026rsquo;、\u0026lsquo;中\u0026rsquo;等\n​\t布尔常量 布尔值，表示真假，只有两个值true和false\n​\t空常量 一个特殊的值，空值，值为null\n除空常量外，其他常量均可使用输出语句直接输出。\npublic class Demo {  public static void main(String[] args) {  System.out.println(10); // 输出一个整数  System.out.println(5.5); // 输出一个小数  System.out.println(\u0026#39;a\u0026#39;); // 输出一个字符  System.out.println(true); // 输出boolean值true  System.out.println(\u0026#34;欢迎来到黑马程序员\u0026#34;); // 输出字符串  } } 3.4 变量的介绍(理解) 变量的定义格式：\n​\t数据类型 变量名 = 数据值；\n​\t数据类型：为空间中存储的数据加入类型限制。整数？小数？\n​\t变量名：自己要为空间起的名字，没有难度\n​\t数据值： 空间中要存储的数值，没有难度\n3.5 数据类型（应用） 3.5.1 计算机存储单元 我们知道计算机是可以用来存储数据的，但是无论是内存还是硬盘，计算机存储设备的最小信息单元叫“位（bit）”，我们又称之为“比特位”，通常用小写的字母”b”表示。而计算机中最基本的存储单元叫“字节（byte）”，\n通常用大写字母”B”表示，字节是由连续的8个位组成。\n除了字节外还有一些常用的存储单位，其换算单位如下：\n1B（字节） = 8bit\n1KB = 1024B\n1MB = 1024KB\n1GB = 1024MB\n1TB = 1024GB\n3.5.2 Java中的数据类型 Java是一个强类型语言，Java中的数据必须明确数据类型。在Java中的数据类型包括基本数据类型和引用数据类型两种。\nJava中的基本数据类型：\n   数据类型 关键字 内存占用 取值范围     整数类型 byte 1 -128~127    short 2 -32768~32767    int(默认) 4 -2的31次方到2的31次方-1    long 8 -2的63次方到2的63次方-1   浮点类型 float 4 负数：-3.402823E+38到-1.401298E-45 正数： 1.401298E-45到3.402823E+38    double(默认) 8 负数：-1.797693E+308到-4.9000000E-324 正数：4.9000000E-324 到1.797693E+308   字符类型 char 2 0-65535   布尔类型 boolean 1 true，false    说明：\n​\te+38表示是乘以10的38次方，同样，e-45表示乘以10的负45次方。\n​\t在java中整数默认是int类型，浮点数默认是double类型。\n3.6 变量（应用） 3.6.1 变量的定义 变量：在程序运行过程中，其值可以发生改变的量。\n从本质上讲，变量是内存中的一小块区域，其值可以在一定范围内变化。\n变量的定义格式：\n数据类型 变量名 = 初始化值; // 声明变量并赋值 int age = 18; System.out.println(age); 或者(扩展)\n// 先声明，后赋值（使用前赋值即可） 数据类型 变量名; 变量名 = 初始化值; double money; money = 55.5; System.out.println(money); 还可以(扩展)\n在同一行定义多个同一种数据类型的变量，中间使用逗号隔开。但不建议使用这种方式，降低程序的可读性。\nint a = 10, b = 20; // 定义int类型的变量a和b，中间使用逗号隔开 System.out.println(a); System.out.println(b);  int c,d; // 声明int类型的变量c和d，中间使用逗号隔开 c = 30; d = 40; System.out.println(c); System.out.println(d); 3.6.2 变量的修改 int a = 10; a = 30; //修改变量的值 System.out.println(a); 变量前面不加数据类型时，表示修改已存在的变量的值。\n3.7 变量的注意事项(理解)  在同一对花括号中，变量名不能重复。 变量在使用之前，必须初始化（赋值）。 定义long类型的变量时，需要在整数的后面加L（大小写均可，建议大写）。因为整数默认是int类型，整数太大可能超出int范围。 定义float类型的变量时，需要在小数的后面加F（大小写均可，建议大写）。因为浮点数的默认类型是double， double的取值范围是大于float的，类型不兼容。  3.8 键盘录入（理解） 我们可以通过 Scanner 类来获取用户的输入。使用步骤如下：\n1、导包。Scanner 类在java.util包下，所以需要将该类导入。导包的语句需要定义在类的上面。\nimport java.util.Scanner; 2、创建Scanner对象。\nScanner sc = new Scanner(System.in);// 创建Scanner对象，sc表示变量名，其他均不可变 3、接收数据\nint i = sc.nextInt(); // 表示将键盘录入的值作为int数返回。 示例：\nimport java.util.Scanner; public class ScannerDemo { \tpublic static void main(String[] args) { \t//创建对象 \tScanner sc = new Scanner(System.in); \t//接收数据 \tint a = sc.nextInt(); \t//输出数据 \tSystem.out.println(a); \t} } 3.9 标识符（理解） 标识符是用户编程时使用的名字，用于给类、方法、变量、常量等命名。\nJava中标识符的组成规则：\n​\t由字母、数字、下划线“_”、美元符号“$”组成，第一个字符不能是数字。\n​\t不能使用java中的关键字作为标识符。\n​\t标识符对大小写敏感（区分大小写）。\nJava中标识符的命名约定：\n​\t小驼峰式命名：变量名、方法名\n​\t首字母小写，从第二个单词开始每个单词的首字母大写。\n​\t大驼峰式命名：类名\n​\t每个单词的首字母都大写。\n​\t另外，标识符的命名最好可以做到见名知意\n​\t例如：username、studentNumber等。\n","permalink":"https://iblog.zone/archives/java%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA%E5%8F%8A%E5%85%A5%E9%97%A8/","summary":"1. Java概述 1.1 Java语言背景介绍（了解） 语言：人与人交流沟通的表达方式\n计算机语言：人与计算机之间进行信息交流沟通的一种特殊语言\nJava语言是美国Sun公司（Stanford University Network）在1995年推出的计算机语言\nJava之父：詹姆斯·高斯林（James Gosling）\n2009年，Sun公司被甲骨文公司收购，所以我们现在访问oracle官网即可：https://www.oracle.com\njava语言的三个版本：\n​\tJavaSE: Java 语言的（标准版），用于桌面应用的开发，是其他两个版本的基础\n​\tJavaME: Java 语言的（小型版），用于嵌入式消费类电子设备\n​\tJavaEE: Java 语言的（企业版），用于 Web 方向的网站开发\n1.2 Java语言跨平台原理（理解） Java程序并非是直接运行的，Java编译器将Java源程序编译成与平台无关的字节码文件(class文件)，然后由Java虚拟机（JVM）对字节码文件解释执行。所以在不同的操作系统下，只需安装不同的Java虚拟机即可实现java程序的跨平台。\n1.3 JRE和JDK（记忆） JVM（Java Virtual Machine），Java虚拟机\nJRE（Java Runtime Environment），Java运行环境，包含了JVM和Java的核心类库（Java API）\nJDK（Java Development Kit）称为Java开发工具，包含了JRE和开发工具\n总结：我们只需安装JDK即可，它包含了java的运行环境和虚拟机。\n1.4 JDK的下载和安装（应用） 1.4.1 下载 通过官方网站获取JDK\nhttp://www.oracle.com\n注意：针对不同的操作系统，需要下载对应版本的JDK。\n1.4.2 安装 傻瓜式安装，下一步即可。但默认的安装路径是在C:\\Program Files下，为方便统一管理建议修改安装路径，将与开发相关的软件都安装到一个目录下，例如：E:\\develop。\n注意：安装路径不要包含中文或者空格等特殊字符（使用纯英文目录）。\n1.4.3 JDK的安装目录介绍    目录名称 说明     bin 该路径下存放了JDK的各种工具命令。javac和java就放在这个目录。   conf 该路径下存放了JDK的相关配置文件。   include 该路径下存放了一些平台特定的头文件。   jmods 该路径下存放了JDK的各种模块。   legal 该路径下存放了JDK各模块的授权文档。   lib 该路径下存放了JDK工具的一些补充JAR包。    2.","title":"Java环境搭建及入门"},{"content":"由于oracle jdk1.8_301扫描出漏洞，故升级jdk到openjdk 1.8.0_312，升级后出现兼容性问题，访问数据库出现以下错误\nThe server selected protocol version TLS10 is not accepted by client preferences [TLS12] 查询后为新版的 JDK 不推荐使用旧的 TLSV1.0 的协议，所以默认删除 TLS10 的支持导致，可按照下面方法修复\ncd /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64/jre  cd lib/security/  vim java.security  # 698行，将 TLSv1, TLSv1.1, 3DES_EDE_CBC 删除，删除后为以下内容 jdk.tls.disabledAlgorithms=SSLv3, RC4, DES, MD5withRSA, \\  DH keySize \u0026lt; 1024, EC keySize \u0026lt; 224, anon, NULL, \\  include jdk.disabled.namedCurves 再次访问，问题解决\n","permalink":"https://iblog.zone/archives/the-server-selected-protocol-version-tls10-is-not-accepted-by-client-preferences-tls12-%E8%BF%9E%E6%8E%A5%E6%95%B0%E6%8D%AE%E5%BA%93%E6%8A%A5%E9%94%99/","summary":"由于oracle jdk1.8_301扫描出漏洞，故升级jdk到openjdk 1.8.0_312，升级后出现兼容性问题，访问数据库出现以下错误\nThe server selected protocol version TLS10 is not accepted by client preferences [TLS12] 查询后为新版的 JDK 不推荐使用旧的 TLSV1.0 的协议，所以默认删除 TLS10 的支持导致，可按照下面方法修复\ncd /usr/lib/jvm/java-1.8.0-openjdk-1.8.0.312.b07-1.el7_9.x86_64/jre  cd lib/security/  vim java.security  # 698行，将 TLSv1, TLSv1.1, 3DES_EDE_CBC 删除，删除后为以下内容 jdk.tls.disabledAlgorithms=SSLv3, RC4, DES, MD5withRSA, \\  DH keySize \u0026lt; 1024, EC keySize \u0026lt; 224, anon, NULL, \\  include jdk.disabled.namedCurves 再次访问，问题解决","title":"The server selected protocol version TLS10 is not accepted by client preferences [TLS12] 连接数据库报错"},{"content":"1、安装xrdp\napt-get install xrdp 2、接下来安装xfce4\napt-get instlal xfce4 3、最重要的一步\n用vim打开 /etc/xrdp/startwm.sh\nvim /etc/xrdp/startwm.sh 4、在里面添加\n echo “xfce4-session” \u0026gt;~/.xsession 5、启动xrdp\nservice xrdp start 6、创建vnc用户\nuseradd -m vnc passwd vnc 7、连接\n","permalink":"https://iblog.zone/archives/%E4%BD%BF%E7%94%A8xrdp%E8%BF%9E%E6%8E%A5kali%E8%BF%9C%E7%A8%8B%E6%A1%8C%E9%9D%A2/","summary":"1、安装xrdp\napt-get install xrdp 2、接下来安装xfce4\napt-get instlal xfce4 3、最重要的一步\n用vim打开 /etc/xrdp/startwm.sh\nvim /etc/xrdp/startwm.sh 4、在里面添加\n echo “xfce4-session” \u0026gt;~/.xsession 5、启动xrdp\nservice xrdp start 6、创建vnc用户\nuseradd -m vnc passwd vnc 7、连接","title":"使用xrdp连接kali远程桌面"},{"content":"kali-linux-2021.2安装openvas 1.安装 sudo apt-get update // 软件库更新  sudo apt-get upgrade // 软件升级  sudo apt-get dist-upgrade // 升级系统  # 由于在2021.1版本中，openvas已经改名为gvm，所以使用以下命令安装openvas sudo apt-get install gvm 使用gvm-setup安装openvas\n安装完成，注意这里的密码，你可以通过gvmd \u0026ndash;user=admin \u0026ndash;new-password=admin修改密码为admin（如果修改无效，请参考最下面的操作命令）\n启动服务gvm-start，并通过netstat -antp查看状态\n访问https://127.0.0.1:9392即可,注意这里是https\n如果需要在其他地址访问，则需要修改服务监听地址\nvim /lib/systemd/system/greenbone-security-assistant.service 2.操作命令 这里附上完整流程所需的命令\n//安装过程命令 sudo apt-get update // 软件库更新 sudo apt-get upgrade // 软件升级 sudo apt-get dist-upgrade // 升级系统 apt-get install gvm //下载安装包 gvm-setup //安装 安装完成 //这里得注意记住初始密码 gvm-check-setup //检查安装是否成功 gvm-start //启动服务 netstat -antp //查看状态，特别注意这里有个空格 浏览器访问https://127.0.0.1:9392 //这里注意是https vim /lib/systemd/system/greenbone-security-assistant.service //修改监听地址 gvm-stop gvm-start //重启生效 //其他命令 gvmd --user=admin --new-password=admin //修改密码为admin,如果这条命令无效请参考下一条 runuser -u _gvm -- gvmd --user=admin --new-password=admin //修改密码为admin gvm-stop //关闭服务 sudo gvm-feed-update //第一次安装后，不用升级,但后期使用记得升级特征库 ","permalink":"https://iblog.zone/archives/kali-linux-2021.2%E5%AE%89%E8%A3%85openvas/","summary":"kali-linux-2021.2安装openvas 1.安装 sudo apt-get update // 软件库更新  sudo apt-get upgrade // 软件升级  sudo apt-get dist-upgrade // 升级系统  # 由于在2021.1版本中，openvas已经改名为gvm，所以使用以下命令安装openvas sudo apt-get install gvm 使用gvm-setup安装openvas\n安装完成，注意这里的密码，你可以通过gvmd \u0026ndash;user=admin \u0026ndash;new-password=admin修改密码为admin（如果修改无效，请参考最下面的操作命令）\n启动服务gvm-start，并通过netstat -antp查看状态\n访问https://127.0.0.1:9392即可,注意这里是https\n如果需要在其他地址访问，则需要修改服务监听地址\nvim /lib/systemd/system/greenbone-security-assistant.service 2.操作命令 这里附上完整流程所需的命令\n//安装过程命令 sudo apt-get update // 软件库更新 sudo apt-get upgrade // 软件升级 sudo apt-get dist-upgrade // 升级系统 apt-get install gvm //下载安装包 gvm-setup //安装 安装完成 //这里得注意记住初始密码 gvm-check-setup //检查安装是否成功 gvm-start //启动服务 netstat -antp //查看状态，特别注意这里有个空格 浏览器访问https://127.0.0.1:9392 //这里注意是https vim /lib/systemd/system/greenbone-security-assistant.","title":"kali-linux-2021.2安装openvas"},{"content":"基于Istio实现微服务治理 微服务架构可谓是当前软件开发领域的技术热点，它在各种博客、社交媒体和会议演讲上的出镜率非常之高，无论是做基础架构还是做业务系统的工程师，对微服务都相当关注，而这个现象与热度到目前为止，已经持续了近 5 年之久。\n尤其是近些年来，微服务架构逐渐发展成熟，从最初的星星之火到现在的大规模的落地与实践，几乎已经成为分布式环境下的首选架构。微服务成为时下技术热点，大量互联网公司都在做微服务架构的落地和推广。同时，也有很多传统企业基于微服务和容器，在做互联网技术转型。\n而在这个技术转型中，国内有一个趋势，以 Spring Cloud 与 Dubbo 为代表的微服务开发框架非常普及和受欢迎。然而软件开发没有银弹，基于这些传统微服务框架构建的应用系统在享受其优势的同时，痛点也越加明显。这些痛点包括但不限于以下几点：\n 侵入性强。想要集成 SDK 的能力，除了需要添加相关依赖，往往还需要在业务代码中增加一部分的代码、或注解、或配置；业务代码与治理层代码界限不清晰。 升级成本高。每次升级都需要业务应用修改 SDK 版本，重新进行功能回归测试，并且对每一台机器进行部署上线，而这对于业务方来说，与业务的快速迭代开发是有冲突的，大多不愿意停下来做这些与业务目标不太相关的事情。 版本碎片化严重。由于升级成本高，而中间件却不会停止向前发展的步伐，久而久之，就会导致线上不同服务引用的 SDK 版本不统一、能力参差不齐，造成很难统一治理。 中间件演变困难。由于版本碎片化严重，导致中间件向前演进的过程中就需要在代码中兼容各种各样的老版本逻辑，带着 “枷锁” 前行，无法实现快速迭代。 内容多、门槛高。Spring Cloud 被称为微服务治理的全家桶，包含大大小小几十个组件，内容相当之多，往往需要几年时间去熟悉其中的关键组件。而要想使用 Spring Cloud 作为完整的治理框架，则需要深入了解其中原理与实现，否则遇到问题还是很难定位。 治理功能不全。不同于 RPC 框架，Spring Cloud 作为治理全家桶的典型，也不是万能的，诸如协议转换支持、多重授权机制、动态请求路由、故障注入、灰度发布等高级功能并没有覆盖到。而这些功能往往是企业大规模落地不可获缺的功能，因此公司往往还需要投入其它人力进行相关功能的自研或者调研其它组件作为补充。  Service Mesh 服务网格 架构和概念 目的是解决系统架构微服务化后的服务间通信和治理问题。设计初衷是提供一种通用的服务治理方案。\nSidecar 在软件系统架构中特指边车模式。这个模式的灵感来源于我们生活中的边三轮：即在两轮摩托车的旁边添加一个边车的方式扩展现有的服务和功能。\n这个模式的精髓在于实现了数据面（业务逻辑）和控制面的解耦：原来两轮摩托车的驾驶者集中注意力跑赛道，边车上的领航员专注周围信息和地图，专注导航。\nService Mesh 这个服务网络专注于处理服务和服务间的通讯。其主要负责构造一个稳定可靠的服务通讯的基础设施，并让整个架构更为的先进和 Cloud Native。在工程中，Service Mesh 基本来说是一组轻量级的与应用逻辑服务部署在一起的服务代理，并且对于应用服务是透明的。\n开源实现 第一代服务网格 Linkerd和Envoy Linkerd 使用Scala编写，是业界第一个开源的service mesh方案。作者 William Morgan 是 service mesh 的布道师和践行者。Envoy 基于C++ 11编写，无论是理论上还是实际上，后者性能都比 Linkderd 更好。这两个开源实现都是以 sidecar 为核心，绝大部分关注点都是如何做好proxy，并完成一些通用控制面的功能。 但是，当你在容器中大量部署 sidecar 以后，如何管理和控制这些 sidecar 本身就是一个不小的挑战。于是，第二代 Service Mesh 应运而生。\n第二代服务网格 Istio Istio 是 Google 和 IBM 两位巨人联合 Lyft 的合作开源项目。是当前最主流的service mesh方案，也是事实上的第二代 service mesh 标准。\n安装Istio https://istio.io/latest/docs/setup/getting-started/\n下载 Istio 下载内容将包含：安装文件、示例和 istioctl 命令行工具。\n  访问 Istio release 页面下载与您操作系统对应的安装文件。在 macOS 或 Linux 系统中，也可以通过以下命令下载最新版本的 Istio：\n$ wget https://github.com/istio/istio/releases/download/1.7.3/istio-1.7.3-linux-amd64.tar.gz   解压并切换到 Istio 包所在目录下。例如：Istio 包名为 istio-1.7.3，则：\n$ tar zxf istio-1.7.3-linux-amd64.tar.gz $ ll istio-1.7.3 drwxr-x--- 2 root root 22 Sep 27 08:33 bin -rw-r--r-- 1 root root 11348 Sep 27 08:33 LICENSE drwxr-xr-x 6 root root 66 Sep 27 08:33 manifests -rw-r----- 1 root root 756 Sep 27 08:33 manifest.yaml -rw-r--r-- 1 root root 5756 Sep 27 08:33 README.md drwxr-xr-x 20 root root 330 Sep 27 08:33 samples drwxr-x--- 3 root root 133 Sep 27 08:33 tools   将 istioctl 客户端拷贝到 path 环境变量中\n$ cp bin/istioctl /bin/   配置命令自动补全\nistioctl 自动补全的文件位于 tools 目录。通过复制 istioctl.bash 文件到您的 home 目录，然后添加下行内容到您的 .bashrc 文件执行 istioctl tab 补全文件：\n$ cp tools/istioctl.bash ~ $ source ~/istioctl.bash   安装istio组件 https://istio.io/latest/zh/docs/setup/install/istioctl/#display-the-configuration-of-a-profile\n使用istioctl直接安装：\n$ istioctl install --set profile=demo ✔ Istio core installed ✔ Istiod installed ✔ Egress gateways installed ✔ Ingress gateways installed ✔ Installation complete  $ kubectl -n istio-system get po NAME READY STATUS RESTARTS AGE istio-egressgateway-7bf76dd59-n9t5l 1/1 Running 0 77s istio-ingressgateway-586dbbc45d-xphjb 1/1 Running 0 77s istiod-6cc5758d8c-pz28m 1/1 Running 0 84s istio针对不同的环境，提供了几种不同的初始化部署的profile\n# 查看提供的profile类型 $ istioctl profile list  # 获取kubernetes的yaml： $ istioctl manifest generate --set profile=demo \u0026gt; istio-kubernetes-manifest.yaml 卸载 $ istioctl manifest generate --set profile=demo | kubectl delete -f - 快速入门 场景一 模型图 资源清单 front-tomcat-dpl-v1.yaml\napiVersion: apps/v1 kind: Deployment metadata:  labels:  app: front-tomcat  version: v1  name: front-tomcat-v1  namespace: istio-demo spec:  replicas: 1  selector:  matchLabels:  app: front-tomcat  version: v1  template:  metadata:  labels:  app: front-tomcat  version: v1  spec:  containers:  - image: consol/tomcat-7.0:latest  name: front-tomcat bill-service-dpl-v1.yaml\napiVersion: apps/v1 kind: Deployment metadata:  labels:  service: bill-service  version: v1  name: bill-service-v1  namespace: istio-demo spec:  replicas: 1  selector:  matchLabels:  service: bill-service  version: v1  template:  metadata:  labels:  service: bill-service  version: v1  spec:  containers:  - image: nginx:alpine  name: bill-service  command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;this is bill-service-v1\u0026#39;\u0026gt;/usr/share/nginx/html/index.html;nginx -g \u0026#39;daemon off;\u0026#39;\u0026#34;] bill-service-svc.yaml\napiVersion: v1 kind: Service metadata:  labels:  service: bill-service  name: bill-service  namespace: istio-demo spec:  ports:  - name: http  port: 9999  protocol: TCP  targetPort: 80  selector:  service: bill-service  type: ClusterIP 操作 $ kubectl create namespace istio-demo $ kubectl apply -f front-tomcat-dpl-v1.yaml $ kubectl apply -f bill-service-dpl-v1.yaml $ kubectl apply -f bill-service-svc.yaml  $ kubectl -n istio-demo exec front-tomcat-v1-548b46d488-r7wv8 -- curl -s bill-service this is bill-service-v1 场景二 后台账单服务更新v2版本，前期规划90%的流量访问v1版本，导入10%的流量到v2版本\n模型图 资源清单 新增bill-service-dpl-v2.yaml\napiVersion: apps/v1 kind: Deployment metadata:  labels:  service: bill-service  version: v2  name: bill-service-v2  namespace: istio-demo spec:  replicas: 1  selector:  matchLabels:  service: bill-service  version: v2  template:  metadata:  labels:  service: bill-service  version: v2  spec:  containers:  - image: nginx:alpine  name: bill-service  command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;hello, this is bill-service-v2\u0026#39;\u0026gt;/usr/share/nginx/html/index.html;nginx -g \u0026#39;daemon off;\u0026#39;\u0026#34;] 此时，访问规则会按照v1和v2的pod各50%的流量分配。\n$ kubectl apply -f bill-service-dpl-v2.yaml $ kubectl -n istio-demo exec front-tomcat-v1-548b46d488-r7wv8 -- curl -s bill-service:9999 使用Istio 注入：\n$ istioctl kube-inject -f bill-service-dpl-v1.yaml|kubectl apply -f - $ istioctl kube-inject -f bill-service-dpl-v2.yaml|kubectl apply -f - $ istioctl kube-inject -f front-tomcat-dpl-v1.yaml|kubectl apply -f - 若想实现上述需求，需要解决如下两个问题：\n 让访问账单服务的流量按照我们期望的比例，其实是一条路由规则，如何定义这个规则 如何区分两个版本的服务  两个新的资源类型：VirtualService和DestinationRule\nbill-service-destnation-rule.yaml\napiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: dest-bill-service  namespace: istio-demo spec:  host: bill-service  subsets:  - name: v1  labels:  version: v1  - name: v2  labels:  version: v2 bill-service-virtualservice.yaml\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: vs-bill-service  namespace: istio-demo spec:  hosts:  - bill-service  http:  - name: bill-service-route  route:  - destination:  host: bill-service  subset: v1  weight: 90  - destination:  host: bill-service  subset: v2  weight: 10 使用client验证流量分配是否生效。\n$ kubectl apply -f bill-service-virtualservice.yaml $ kubectl apply -f bill-service-destnation-rule.yaml $ kubectl -n istio-demo exec front-tomcat-v1-78cf497978-ltxpf -c front-tomcat -- curl -s bill-service 服务网格细节剖析 执行的操作：\n 使用istioctl为pod注入了sidecar 创建了virtualservice和destinationrule  如何最终影响到了pod的访问行为？\n宏观角度 nginx的配置中，可以提供类似如下的配置片段实现按照权重的转发：\n因为nginx是代理层，可以转发请求，istio也实现了流量转发的效果，肯定也有代理层，并且识别了前面创建的虚拟服务中定义的规则。\n$ istioctl kube-inject -f front-tomcat-dpl-v1.yaml 可以看到注入后yaml中增加了很多内容：\npod被istio注入后，被纳入到服务网格中，每个pod都会添加一个名为istio-proxy的容器（常说的sidecar容器），istio-proxy容器中有两个进程，一个是piolot-agent，一个是envoy\n$ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c istio-proxy bash # ps aux 目前已知：\n 在istio网格内，每个Pod都会被注入一个envoy代理 envoy充当nginx的角色，做为proxy代理，负责接管pod的入口和出口流量  目前，还需要搞清楚几个问题：\n istio-init初始化容器作用是什么？ istio-proxy如何接管业务服务的出入口流量？  认识envoy Envoy 是为云原生应用设计的代理。\n可以和nginx做类比： https://fuckcloudnative.io/posts/migrating-from-nginx-to-envoy/\n$ docker run -d --name envoy -v `pwd`/envoy.yaml:/etc/envoy/envoy.yaml -p 10000:10000 envoyproxy/envoy-alpine:v1.15.2  $ curl localhost:10000 envoy.yaml\nadmin:  access_log_path: /tmp/admin_access.log  address:  socket_address: { address: 127.0.0.1, port_value: 9901 }  static_resources:  listeners:  - name: listener_0  address:  socket_address: { address: 0.0.0.0, port_value: 10000 }  filter_chains:  - filters:  - name: envoy.http_connection_manager  config:  stat_prefix: ingress_http  codec_type: AUTO  route_config:  name: local_route  virtual_hosts:  - name: local_service  domains: [\u0026#34;*\u0026#34;]  routes:  - match: { prefix: \u0026#34;/\u0026#34; }  route: { cluster: some_service }  http_filters:  - name: envoy.router  clusters:  - name: some_service  connect_timeout: 2s  type: STATIC  lb_policy: ROUND_ROBIN  hosts: [{ socket_address: { address: 10.111.219.247, port_value: 9999 }}] 脑补一下网络代理程序的流程，比如作为一个代理，首先要能获取请求流量，通常是采用监听端口的方式实现；其次拿到请求数据后需要对其做微处理，例如附加 Header 或校验某个 Header 字段的内容等，这里针对来源数据的层次不同，可以分为 L3/L4/L7，然后将请求转发出去；转发这里又可以衍生出如果后端是一个集群，需要从中挑选一台机器，如何挑选又涉及到负载均衡等。\n listener : Envoy 的监听地址。Envoy 会暴露一个或多个 Listener 来监听客户端的请求。 filter : 过滤器。在 Envoy 中指的是一些“可插拔”和可组合的逻辑处理层，是 Envoy 核心逻辑处理单元。 route_config : 路由规则配置。即将请求路由到后端的哪个集群。 cluster : 服务提供方集群。Envoy 通过服务发现定位集群成员并获取服务，具体路由到哪个集群成员由负载均衡策略决定。  envoy的xDS Envoy的启动配置文件分为两种方式：静态配置和动态配置。\n 静态配置是将所有信息都放在配置文件中，启动的时候直接加载。 动态配置需要提供一个Envoy的服务端，用于动态生成Envoy需要的服务发现接口，这里叫XDS，通过发现服务来动态的调整配置信息，Istio就是实现了v2的API。  Envoy 接收到请求后，会先走 FilterChain，通过各种 L3/L4/L7 Filter 对请求进行微处理，然后再路由到指定的集群，并通过负载均衡获取一个目标地址，最后再转发出去。\n其中每一个环节可以静态配置，也可以动态服务发现，也就是所谓的 xDS。这里的 x 是一个代词，类似云计算里的 XaaS 可以指代 IaaS、PaaS、SaaS 等。\n所以，envoy的架构大致的样子如下：\nDownstream\n下游（downstream）主机连接到 Envoy，发送请求并或获得响应。\nUpstream\n上游（upstream）主机获取来自 Envoy 的链接请求和响应。\n监听器\n 除了过滤器链之外，还有一种过滤器叫监听器过滤器（Listener filters），它会在过滤器链之前执行，用于操纵连接的元数据。这样做的目的是，无需更改 Envoy 的核心代码就可以方便地集成更多功能。 每个监听器都可以配置多个过滤器链（Filter Chains），监听器会根据 filter_chain_match 中的匹配条件将流量转交到对应的过滤器链，其中每一个过滤器链都由一个或多个网络过滤器（Network filters）组成。这些过滤器用于执行不同的代理任务，如速率限制，TLS 客户端认证，HTTP 连接管理，MongoDB 嗅探，原始 TCP 代理等。  envoy在微服务治理中的工作环境 可以在服务旁运行，以平台无关的方式提供必要的特性，所有到服务的流量都通过 Envoy 代理，这里 Envoy 扮演的就是 Sidecar 的角色。\n针对于k8s的pod来讲：\n在istio中，envoy的位置：\n很明显，istio中，envoy进行流量治理，更多的使用的是XDS进行配置更新，而我们知道，XDS需要有服务端来提供接口，istiod中的pilot组件则提供了xDS服务端接口的实现 。\n工作原理 目前为止，我们可以知道大致的工作流程：\n 用户端，通过创建服务治理的规则（VirtualService、DestinationRule等资源类型），存储到ETCD中 istio控制平面中的Pilot服务监听上述规则，转换成envoy可读的规则配置，通过xDS接口同步给各envoy envoy通过xDS获取最新的配置后，动态reload，进而改变流量转发的策略  思考两个问题：\n istio中envoy的动态配置到底长什么样子？ 在istio的网格内，front-tomcat访问到bill-service，流量的流向是怎么样的？  针对问题1：\n每个envoy进程启动的时候，会在127.0.0.1启动监听15000端口\n$ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c istio-proxy bash # netstat -nltp # curl localhost:15000/help # curl localhost:15000/config_dump 针对问题2：\n$ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c front-tomcat bash # curl bill-service:9999 按照之前的认知，\n现在为什么流量分配由5：5 变成了9：1？流量经过envoy了的处理\nenvoy如何接管由front-tomcat容器发出的请求流量？（istio-init\n回顾iptables：\nIstio 给应用 Pod 注入的配置主要包括：\n  Init 容器 istio-init\nIstio 在 pod 中注入的 Init 容器名为 istio-init，作用是为 pod 设置 iptables 端口转发。\n我们在上面 Istio 注入完成后的 YAML 文件中看到了该容器的启动命令是：\nistio-iptables -p 15001 -z 15006 -u 1337 -m REDIRECT -i \u0026#39;*\u0026#39; -x \u0026#34;\u0026#34; -b \u0026#39;*\u0026#39; -d 15090,15021,15020 Init 容器的启动入口是 istio-iptables 命令行，该命令行工具的用法如下：\n$ istio-iptables [flags]  -p: 指定重定向所有 TCP 出站流量的 sidecar 端口（默认为 $ENVOY_PORT = 15001）  -m: 指定入站连接重定向到 sidecar 的模式，“REDIRECT” 或 “TPROXY”（默认为 $ISTIO_INBOUND_INTERCEPTION_MODE)  -b: 逗号分隔的入站端口列表，其流量将重定向到 Envoy（可选）。使用通配符 “*” 表示重定向所有端口。为空时表示禁用所有入站重定向（默认为 $ISTIO_INBOUND_PORTS）  -d: 指定要从重定向到 sidecar 中排除的入站端口列表（可选），以逗号格式分隔。使用通配符“*” 表示重定向所有入站流量（默认为 $ISTIO_LOCAL_EXCLUDE_PORTS）  -o：逗号分隔的出站端口列表，不包括重定向到 Envoy 的端口。  -i: 指定重定向到 sidecar 的 IP 地址范围（可选），以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量。空列表将禁用所有出站重定向（默认为 $ISTIO_SERVICE_CIDR）  -x: 指定将从重定向中排除的 IP 地址范围，以逗号分隔的 CIDR 格式列表。使用通配符 “*” 表示重定向所有出站流量（默认为 $ISTIO_SERVICE_EXCLUDE_CIDR）。  -k：逗号分隔的虚拟接口列表，其入站流量（来自虚拟机的）将被视为出站流量。  -g：指定不应用重定向的用户的 GID。(默认值与 -u param 相同)  -u：指定不应用重定向的用户的 UID。通常情况下，这是代理容器的 UID（默认值是 1337，即 istio-proxy 的 UID）。  -z: 所有进入 pod/VM 的 TCP 流量应被重定向到的端口（默认 $INBOUND_CAPTURE_PORT = 15006）。 以上传入的参数都会重新组装成 iptables 规则，关于 Istio 中端口用途请参考 Istio 官方文档。\n这条启动命令的作用是：\n 将应用容器的所有入站流量都转发到 envoy的 15006 端口（15090 端口（Envoy Prometheus telemetry）和 15020 端口（Ingress Gateway）除外，15021（sidecar健康检查）端口） 将所有出站流量都重定向到 sidecar 代理（通过 15001 端口） 上述规则对id为1337用户除外，因为1337是istio-proxy自身的流量  该容器存在的意义就是让 sidecar 代理可以拦截pod所有的入站（inbound）流量以及出站（outbound）流量，这样就可以实现由sidecar容器来接管流量，进尔实现流量管控。\n因为 Init 容器初始化完毕后就会自动终止，因为我们无法登陆到容器中查看 iptables 信息，但是 Init 容器初始化结果会保留到应用容器和 sidecar 容器中。\n# 查看front-tomcat服务的istio-proxy容器的id $ docker ps |grep front-tomcat d02fa8217f2f consol/tomcat-7.0 \u0026#34;/bin/sh -c /opt/tom…\u0026#34; 2 days ago Up 2 days  k8s_front-tomcat_front-tomcat-v1-78cf497978-ppwwk_istio-demo_f03358b1-ed17-4811-ac7e-9f70e6bd797b_0  # 根据容器id获取front-tomcat容器在宿主机中的进程 $ docker inspect d02fa8217f2f|grep -i pid  \u0026#34;Pid\u0026#34;: 28834,  \u0026#34;PidMode\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;PidsLimit\u0026#34;: null, # 进入该进程的网络命名空间 $ nsenter -n --target 28834 # 查看命名空间的iptables规则 $ iptables -t nat -vnL # PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上。 Chain PREROUTING (policy ACCEPT 148 packets, 8880 bytes)  pkts bytes target prot opt in out source destination  148 8880 ISTIO_INBOUND tcp -- * * 0.0.0.0/0 0.0.0.0/0  # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链。 Chain INPUT (policy ACCEPT 148 packets, 8880 bytes)  pkts bytes target prot opt in out source destination  # OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上。 Chain OUTPUT (policy ACCEPT 46 packets, 3926 bytes)  pkts bytes target prot opt in out source destination  8 480 ISTIO_OUTPUT tcp -- * * 0.0.0.0/0 0.0.0.0/0 # POSTROUTING 链：所有数据包流出网卡时都要先进入POSTROUTING 链，内核根据数据包目的地判断是否需要转发出去，我们看到此处未做任何处理。 Chain POSTROUTING (policy ACCEPT 46 packets, 3926 bytes)  pkts bytes target prot opt in out source destination  # ISTIO_INBOUND 链：将所有入站流量重定向到 ISTIO_IN_REDIRECT 链上，目的地为 15090，15020，15021端口的流量除外，发送到以上两个端口的流量将返回 iptables 规则链的调用点，即 PREROUTING 链的后继 POSTROUTING。 Chain ISTIO_INBOUND (1 references)  pkts bytes target prot opt in out source destination  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15008  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15090  143 8580 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15021  5 300 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15020  0 0 ISTIO_IN_REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0  # ISTIO_IN_REDIRECT 链：将所有入站流量跳转到本地的 15006 端口，至此成功的拦截了流量到sidecar中。 Chain ISTIO_IN_REDIRECT (3 references)  pkts bytes target prot opt in out source destination  0 0 REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15006  # ISTIO_OUTPUT 链：选择需要重定向到 Envoy（即本地） 的出站流量，所有非 localhost 的流量全部转发到 ISTIO_REDIRECT。为了避免流量在该 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。如果目的地非 localhost 就跳转到 ISTIO_REDIRECT；如果流量是来自 istio-proxy 用户空间的，那么就跳出该链，返回它的调用链继续执行下一条规则（OUTPUT 的下一条规则，无需对流量进行处理）；所有的非 istio-proxy 用户空间的目的地是 localhost 的流量就跳转到 ISTIO_REDIRECT。 Chain ISTIO_OUTPUT (1 references)  pkts bytes target prot opt in out source destination  0 0 RETURN all -- * lo 127.0.0.6 0.0.0.0/0  0 0 ISTIO_IN_REDIRECT all -- * lo 0.0.0.0/0 !127.0.0.1 owner UID match 1337  0 0 RETURN all -- * lo 0.0.0.0/0 0.0.0.0/0 ! owner UID match 1337  8 480 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner UID match 1337  0 0 ISTIO_IN_REDIRECT all -- * lo 0.0.0.0/0 !127.0.0.1 owner GID match 1337  0 0 RETURN all -- * lo 0.0.0.0/0 0.0.0.0/0 ! owner GID match 1337  0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner GID match 1337  0 0 RETURN all -- * * 0.0.0.0/0 127.0.0.1  0 0 ISTIO_REDIRECT all -- * * 0.0.0.0/0 0.0.0.0/0  # ISTIO_REDIRECT 链：将所有流量重定向到 Sidecar（即本地） 的 15001 端口。 Chain ISTIO_REDIRECT (1 references)  pkts bytes target prot opt in out source destination  0 0 REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15001   $ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c istio-proxy bash istio-proxy@front-tomcat-v1-78cf497978-ppwwk:/$ netstat -nltp Active Internet connections (only servers) Proto Recv-Q Send-Q Local Address Foreign Address State PID/Program name tcp 0 0 127.0.0.1:15000 0.0.0.0:* LISTEN 16/envoy tcp 0 0 0.0.0.0:15001 0.0.0.0:* LISTEN 16/envoy tcp 0 0 0.0.0.0:15006 0.0.0.0:* LISTEN 16/envoy tcp 0 0 127.0.0.1:8005 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:8009 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:8778 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:15021 0.0.0.0:* LISTEN 16/envoy tcp 0 0 0.0.0.0:8080 0.0.0.0:* LISTEN - tcp 0 0 0.0.0.0:15090 0.0.0.0:* LISTEN 16/envoy tcp6 0 0 :::15020 :::* LISTEN 1/pilot-agent 说明pod内的出站流量请求被监听在15001端口的envoy的进程接收到，进而就走到了envoy的Listener -\u0026gt; route -\u0026gt; cluster -\u0026gt; endpoint 转发流程。\n问题就转变为：如何查看envoy的配置，跟踪转发的过程？\n调试envoy 我们知道，envoy的配置非常复杂，直接在config_dump里去跟踪xDS的过程非常繁琐。因此istio提供了调试命令，方便查看envoy的流量处理流程。\n$ istioctl proxy-config -h 比如，通过如下命令可以查看envoy的监听器：\n# 查看15001的监听 $ istioctl proxy-config listener front-tomcat-v1-78cf497978-vv9wj.istio-demo --port 15001 -ojson # virtualOutbound的监听不做请求处理，hiddenEnvoyDeprecatedUseOriginalDst: true, 直接转到原始的请求对应的监听器中  # 查看访问端口是9999的监听器 $ istioctl proxy-config listener front-tomcat-v1-78cf497978-ppwwk.istio-demo --port 9999 -ojson ...  {  \u0026#34;name\u0026#34;: \u0026#34;0.0.0.0_9999\u0026#34;,  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;,  \u0026#34;portValue\u0026#34;: 9999  }  },  \u0026#34;filterChains\u0026#34;: [  {  \u0026#34;filterChainMatch\u0026#34;: {  \u0026#34;applicationProtocols\u0026#34;: [  \u0026#34;http/1.0\u0026#34;,  \u0026#34;http/1.1\u0026#34;,  \u0026#34;h2c\u0026#34;  ]  },  \u0026#34;filters\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;,  \u0026#34;typedConfig\u0026#34;: {  \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/envoy.extensions.filters.network.http_connection_manager.v3.HttpConnectionManager\u0026#34;,  \u0026#34;statPrefix\u0026#34;: \u0026#34;outbound_0.0.0.0_9999\u0026#34;,  \u0026#34;rds\u0026#34;: {  \u0026#34;configSource\u0026#34;: {  \u0026#34;ads\u0026#34;: {},  \u0026#34;resourceApiVersion\u0026#34;: \u0026#34;V3\u0026#34;  },  \u0026#34;routeConfigName\u0026#34;: \u0026#34;9999\u0026#34;  }, ...  envoy收到请求后，会转给监听器进行处理请求，监听器先匹配address和port和socket都一致的Listener，如果没找到再找port一致，address==0.0.0.0的Listener\n 发现istio会为网格内的Service Port创建名为0.0.0.0_\u0026lt;Port\u0026gt;的虚拟监听器，本例中为0.0.0.0_9999。\nenvoy的15001端口收到请求后，直接转到了0.0.0.0_9999，进而转到了\u0026quot;routeConfigName\u0026quot;: \u0026quot;9999\u0026quot;，即9999这个route中。\n下面，看下route的内容：\n$ istioctl pc route front-tomcat-v1-78cf497978-ppwwk.istio-demo --name 9999 NOTE: This output only contains routes loaded via RDS. NAME DOMAINS MATCH VIRTUAL SERVICE 9999 bill-service /* vs-bill-service.istio-demo  # 发现了前面创建的virtual service $ istioctl pc route front-tomcat-v1-78cf497978-ppwwk.istio-demo --name 9999 -ojson [  {  \u0026#34;name\u0026#34;: \u0026#34;9999\u0026#34;,  \u0026#34;virtualHosts\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;allow_any\u0026#34;,  \u0026#34;domains\u0026#34;: [  \u0026#34;*\u0026#34;  ],  \u0026#34;routes\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;allow_any\u0026#34;,  \u0026#34;match\u0026#34;: {  \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;  },  \u0026#34;route\u0026#34;: {  \u0026#34;cluster\u0026#34;: \u0026#34;PassthroughCluster\u0026#34;,  \u0026#34;timeout\u0026#34;: \u0026#34;0s\u0026#34;,  \u0026#34;maxGrpcTimeout\u0026#34;: \u0026#34;0s\u0026#34;  }  }  ],  \u0026#34;includeRequestAttemptCount\u0026#34;: true  },  {  \u0026#34;name\u0026#34;: \u0026#34;bill-service.istio-demo.svc.cluster.local:9999\u0026#34;,  \u0026#34;domains\u0026#34;: [  \u0026#34;bill-service.istio-demo.svc.cluster.local\u0026#34;,  \u0026#34;bill-service.istio-demo.svc.cluster.local:9999\u0026#34;,  \u0026#34;bill-service\u0026#34;,  \u0026#34;bill-service:9999\u0026#34;,  \u0026#34;bill-service.istio-demo.svc.cluster\u0026#34;,  \u0026#34;bill-service.istio-demo.svc.cluster:9999\u0026#34;,  \u0026#34;bill-service.istio-demo.svc\u0026#34;,  \u0026#34;bill-service.istio-demo.svc:9999\u0026#34;,  \u0026#34;bill-service.istio-demo\u0026#34;,  \u0026#34;bill-service.istio-demo:9999\u0026#34;,  \u0026#34;10.111.219.247\u0026#34;,  \u0026#34;10.111.219.247:9999\u0026#34;  ],  \u0026#34;routes\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;bill-service-route\u0026#34;,  \u0026#34;match\u0026#34;: {  \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;  },  \u0026#34;route\u0026#34;: {  \u0026#34;weightedClusters\u0026#34;: {  \u0026#34;clusters\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;outbound|9999|v1|bill-service.istio-demo.svc.cluster.local\u0026#34;,  \u0026#34;weight\u0026#34;: 90  },  {  \u0026#34;name\u0026#34;: \u0026#34;outbound|9999|v2|bill-service.istio-demo.svc.cluster.local\u0026#34;,  \u0026#34;weight\u0026#34;: 10  }  ]  }, ... 满足访问domains列表的会优先匹配到，我们访问的是10.111.219.247:9999，因此匹配bill-service.istio-demo.svc.cluster.local:9999这组虚拟hosts，进而使用到基于weight的集群配置。\n我们看到，流量按照预期的配置进行了转发：\n90% -\u0026gt; outbound|9999|v1|bill-service.istio-demo.svc.cluster.local 10% -\u0026gt; outbound|9999|v2|bill-service.istio-demo.svc.cluster.local 下面，看一下cluster的具体内容：\n$ istioctl pc cluster front-tomcat-v1-78cf497978-ppwwk.istio-demo --fqdn bill-service.istio-demo.svc.cluster.local -ojson ...  \u0026#34;name\u0026#34;: \u0026#34;outbound|9999|v1|bill-service.istio-demo.svc.cluster.local\u0026#34;,  \u0026#34;type\u0026#34;: \u0026#34;EDS\u0026#34;,  \u0026#34;edsClusterConfig\u0026#34;: {  \u0026#34;edsConfig\u0026#34;: {  \u0026#34;ads\u0026#34;: {},  \u0026#34;resourceApiVersion\u0026#34;: \u0026#34;V3\u0026#34;  },  \u0026#34;serviceName\u0026#34;: \u0026#34;outbound|9999|v1|bill-service.istio-demo.svc.cluster.local\u0026#34;  }, ... 我们发现，endpoint列表是通过eds获取的，因此，查看endpoint信息：\n$ istioctl pc endpoint front-tomcat-v1-78cf497978-ppwwk.istio-demo --cluster \u0026#39;outbound|9999|v1|bill-service.istio-demo.svc.cluster.local\u0026#39; -ojson [  {  \u0026#34;name\u0026#34;: \u0026#34;outbound|9999|v1|bill-service.istio-demo.svc.cluster.local\u0026#34;,  \u0026#34;addedViaApi\u0026#34;: true,  \u0026#34;hostStatuses\u0026#34;: [  {  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;10.244.0.17\u0026#34;,  \u0026#34;portValue\u0026#34;: 80  }  }, ... 目前为止，经过envoy的规则，流量从front-tomcat的pod中知道要发往10.244.0.7:80 这个pod地址。前面提到过，envoy不止接管出站流量，入站流量同样会接管。\n下面看下流量到达bill-service-v1的pod后的处理：\n先回顾前面的iptables规则，除特殊情况以外，所有的出站流量被监听在15001端口的envoy进程拦截处理，同样的，分析bill-service-v1的iptables规则可以发现，监听在15006端口的envoy进程通过在PREROUTING链上添加规则，同样将进入pod的入站流量做了拦截。\n# PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上。 Chain PREROUTING (policy ACCEPT 148 packets, 8880 bytes)  pkts bytes target prot opt in out source destination  148 8880 ISTIO_INBOUND tcp -- * * 0.0.0.0/0 0.0.0.0/0  # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链。 Chain INPUT (policy ACCEPT 148 packets, 8880 bytes)  pkts bytes target prot opt in out source destination  # ISTIO_INBOUND 链：将所有入站流量重定向到 ISTIO_IN_REDIRECT 链上，目的地为 15090，15020，15021端口的流量除外，发送到以上两个端口的流量将返回 iptables 规则链的调用点，即 PREROUTING 链的后继 POSTROUTING。 Chain ISTIO_INBOUND (1 references)  pkts bytes target prot opt in out source destination  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15008  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15090  143 8580 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15021  5 300 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15020  0 0 ISTIO_IN_REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0  # ISTIO_IN_REDIRECT 链：将所有入站流量跳转到本地的 15006 端口，至此成功的拦截了流量到sidecar中。 Chain ISTIO_IN_REDIRECT (3 references)  pkts bytes target prot opt in out source destination  0 0 REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15006 15006端口是一个名为 virtualInbound虚拟入站监听器，\n$ istioctl pc l bill-service-v1-6c95ccb747-vwt2d.istio-demo --port 15006 -ojson\u0026gt;/tmp/15006.inbound.json 相比于VirtualOutbound， virtualInbound 不会再次转给别的虚拟监听器，而是直接由本监听器的filterChains处理，本例中我们可以发现本机目标地址为80的http请求，转发到了inbound|9999|http|bill-service.istio-demo.svc.cluster.local这个集群中。\n查看该集群的信息：\n$ istioctl pc cluster bill-service-v1-6c95ccb747-vwt2d.istio-demo -h $ istioctl pc cluster bill-service-v1-6c95ccb747-vwt2d.istio-demo --direction inbound -ojson [  {  \u0026#34;name\u0026#34;: \u0026#34;inbound|9999|http|bill-service.istio-demo.svc.cluster.local\u0026#34;,  \u0026#34;type\u0026#34;: \u0026#34;STATIC\u0026#34;,  \u0026#34;connectTimeout\u0026#34;: \u0026#34;10s\u0026#34;,  \u0026#34;loadAssignment\u0026#34;: {  \u0026#34;clusterName\u0026#34;: \u0026#34;inbound|9999|http|bill-service.istio-demo.svc.cluster.local\u0026#34;,  \u0026#34;endpoints\u0026#34;: [  {  \u0026#34;lbEndpoints\u0026#34;: [  {  \u0026#34;endpoint\u0026#34;: {  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;127.0.0.1\u0026#34;,  \u0026#34;portValue\u0026#34;: 80  }  }  }  }  ]  }  ]  },  \u0026#34;circuitBreakers\u0026#34;: {  \u0026#34;thresholds\u0026#34;: [  {  \u0026#34;maxConnections\u0026#34;: 4294967295,  \u0026#34;maxPendingRequests\u0026#34;: 4294967295,  \u0026#34;maxRequests\u0026#34;: 4294967295,  \u0026#34;maxRetries\u0026#34;: 4294967295  }  ]  }  } ] 一点小知识 同一个Pod，不同的表现\n$ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c front-tomcat bash # curl bill-service:9999  $ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c istio-proxy bash # curl bill-service:9999 可以发现，在front-tomcat中的访问请求，是受到我们设置的 9：1的流量分配规则限制的，但是istio-proxy中的访问是不受限制的。\n istio-proxy自身，发起的往10.244.0.17的请求，使用的用户是 uid=1337(istio-proxy)，因此不会被istio-init初始化的防火墙规则拦截，可以直接走pod的网络进行通信。\n istio服务网格内，流量请求完全绕过了kube-proxy组件\n通过上述流程调试，我们可以得知，front-tomcat中访问bill-service:9999，流量是没有用到kube-proxy维护的宿主机中的iptables规则的。\n验证一下：\n# 停掉kube-proxy $ kubectl -n kube-system edit daemonset kube-proxy ...  dnsPolicy: ClusterFirst  hostNetwork: true  nodeSelector:  beta.kubernetes.io/os: linux1 #把此处修改一个不存在的label值   priorityClassName: system-node-critical ...  #清理iptables规则 $ iptables -F -t nat  # 访问测试 $ kubectl -n istio-demo get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE bill-service ClusterIP 10.111.219.247 \u0026lt;none\u0026gt; 9999/TCP 2d18h $ curl 10.111.219.247:9999  # 进入front-tomcat容器进行访问 $ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c front-tomcat bash # curl 10.111.219.247:9999  # curl bill-service:9999 会因为dns解析失败而访问失败，手动配置namespaceserver即可  $ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c istio-proxy bash # curl curl 10.111.219.247:9999  集群内的Service都相应的创建了虚拟出站监听器\n$ kubectl -n istio-demo exec -ti front-tomcat-v1-78cf497978-ppwwk -c front-tomcat bash # curl sonarqube.jenkins:9000  $ istioctl pc l front-tomcat-v1-78cf497978-ppwwk.istio-demo --port 9000 ADDRESS PORT MATCH DESTINATION 10.97.243.33 9000 App: HTTP Route: sonarqube.jenkins.svc.cluster.local:9000 10.97.243.33 9000 ALL Cluster: outbound|9000||sonarqube.jenkins.svc.cluster.local  $ istioctl pc r front-tomcat-v1-78cf497978-ppwwk.istio-demo --name \u0026#39;sonarqube.jenkins.svc.cluster.local:9000\u0026#39;  $ istioctl pc ep front-tomcat-v1-78cf497978-ppwwk.istio-demo --cluster \u0026#39;outbound|9000||sonarqube.jenkins.svc.cluster.local\u0026#39; virtualOutBound 15001 \u0026ndash;\u0026gt; virtial listener 10.97.243.33_9000 \u0026ndash;\u0026gt; route sonarqube.jenkins.svc.cluster.local:9000 \u0026ndash;\u0026gt; cluster outbound|9000||sonarqube.jenkins.svc.cluster.local \u0026ndash;\u0026gt; 10.244.1.13:9000\n场景三 模型图 资源清单 front-tomcat-service.yaml\napiVersion: v1 kind: Service metadata:  labels:  app: front-tomcat  name: front-tomcat  namespace: istio-demo spec:  ports:  - name: http  port: 8080  protocol: TCP  targetPort: 8080  selector:  app: front-tomcat  type: ClusterIP front-tomcat-v2-dpl.yaml\napiVersion: apps/v1 kind: Deployment metadata:  labels:  app: front-tomcat  version: v2  name: front-tomcat-v2  namespace: istio-demo spec:  replicas: 1  selector:  matchLabels:  app: front-tomcat  version: v2  template:  metadata:  labels:  app: front-tomcat  version: v2  spec:  containers:  - image: consol/tomcat-7.0:latest  name: front-tomcat  command: [\u0026#34;/bin/sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;echo \u0026#39;hello tomcat version2\u0026#39;\u0026gt;/opt/tomcat/webapps/ROOT/index.html;/opt/tomcat/bin/deploy-and-run.sh;\u0026#34;] front-tomcat-virtualservice.yaml\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: front-tomcat  namespace: istio-demo spec:  hosts:  - front-tomcat  http:  - name: front-tomcat-route  route:  - destination:  host: front-tomcat  subset: v1  weight: 90  - destination:  host: front-tomcat  subset: v2  weight: 10 --- apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: front-tomcat  namespace: istio-demo spec:  host: front-tomcat  subsets:  - name: v1  labels:  version: v1  - name: v2  labels:  version: v2 $ kubectl apply -f front-tomcat-service.yaml $ kubectl apply -f \u0026lt;(istioctl kube-inject -f front-tomcat-v2-dpl.yaml) $ kubectl apply -f front-tomcat-virtualservice.yaml 使用ingress来访问网格服务 front-tomcat-ingress.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata:  name: front-tomcat  namespace: istio-demo spec:  rules:  - host: tomcat.istio-demo.com  http:  paths:  - backend:  serviceName: front-tomcat  servicePort: 8080  path: / status:  loadBalancer: {} 使用浏览器访问查看效果。\n只有网格内部访问会遵从virtualservice的规则，在宿主机中直接访问Service的ClusterIP还是按照默认的规则转发。\nIngress：对接ingress controller，实现外部流量进入集群内部，只适用于 HTTP 流量，使用方式也很简单，只能对 service、port、HTTP 路径等有限字段匹配来路由流量，这导致它无法路由如 MySQL、Redis 和各种私有 RPC 等 TCP 流量。要想直接路由南北向的流量，只能使用 Service 的 LoadBalancer 或 NodePort，前者需要云厂商支持，后者需要进行额外的端口管理。有些 Ingress controller 支持暴露 TCP 和 UDP 服务，但是只能使用 Service 来暴露，Ingress 本身是不支持的，例如 nginx ingress controller，服务暴露的端口是通过创建 ConfigMap 的方式来配置的。\ningressgateway访问网格服务 对于入口流量管理，您可能会问： 为什么不直接使用 Kubernetes Ingress API ？ 原因是 Ingress API 无法表达 Istio 的路由需求。 Ingress 试图在不同的 HTTP 代理之间取一个公共的交集，因此只能支持最基本的 HTTP 路由，最终导致需要将代理的其他高级功能放入到注解（annotation）中，而注解的方式在多个代理之间是不兼容的，无法移植。\nIstio Gateway 通过将 L4-L6 配置与 L7 配置分离的方式克服了 Ingress 的这些缺点。 Gateway 只用于配置 L4-L6 功能（例如，对外公开的端口，TLS 配置），所有主流的L7代理均以统一的方式实现了这些功能。 然后，通过在 Gateway 上绑定 VirtualService 的方式，可以使用标准的 Istio 规则来控制进入 Gateway 的 HTTP 和 TCP 流量。\nfront-tomcat-gateway.yaml\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata:  name: front-tomcat-gateway  namespace: istio-demo spec:  selector:  istio: ingressgateway # use istio default controller  servers:  - port:  number: 80  name: http  protocol: HTTP  hosts:  - tomcat.istio-demo.com 效果是在Istio的ingress网关上加了一条规则，允许``tomcat.istio-demo.com` 的外部http流量进入到网格中，但是只是接受访问和流量输入，当流量到达这个网关时，它还不知道发送到哪里去。\n网关已准备好接收流量，我们必须告知它将收到的流量发往何处，这就用到了前面使用过的VirtualService。\n要为进入上面的 Gateway 的流量配置相应的路由，必须为同一个 host 定义一个 VirtualService，并使用配置中的 gateways 字段绑定到前面定义的 Gateway 上\nfront-tomcat-gateway-virtualservice.yaml\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: gateway-front-tomcat  namespace: istio-demo spec:  gateways:  - front-tomcat-gateway  hosts:  - tomcat.istio-demo.com  http:  - name: front-tomcat-route  route:  - destination:  host: front-tomcat  subset: v1  weight: 90  - destination:  host: front-tomcat  subset: v2  weight: 10 该网关列表指定，只有通过我们指定的网关 front-tomcat-gateway 的流量是允许的。所有其他外部请求将被拒绝，并返回 404 响应。\n 请注意，在此配置中，来自网格中其他服务的内部请求不受这些规则约束\n $ kubectl apply -f front-tomcat-gateway-virtualservice.yaml $ kubectl apply -f front-tomcat-gateway.yaml 模拟访问：\n$ kubectl -n istio-system get service istio-ingressgateway -o jsonpath=\u0026#39;{.spec.ports[?(@.name==\u0026#34;http2\u0026#34;)].nodePort}\u0026#39; 31995 $ curl -HHost:tomcat.istio-demo.com 172.21.51.67:31995/ 172.21.51.67:31995地址从何而来？\n浏览器访问: http://tomcat.istio-demo.com:31995/\n如何实现不加端口访问网格内服务？\n# 在一台80端口未被占用的机器中，如ip为172.21.51.69 $ docker run -d --restart=always -p 80:80 --name istio-nginx nginx:alpine  # 在容器的/etc/nginx/conf.d/目录中，新增配置文件 $ cat front-tomcat.conf upstream front-tomcat {  server 172.21.51.67:31995; } server {  listen 80;  listen [::]:80;  server_name tomcat.istio-demo.com;   location / {  proxy_set_header Host $host;  proxy_set_header X-Real-IP $remote_addr;  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  proxy_http_version 1.1;  proxy_pass http://front-tomcat;  } }  $ nginx -s reload 本地配置hosts\n172.21.51.69 tomcat.istio-demo.com 直接访问http://tomcat.istio-demo.com 即可实现外部域名访问到网格内部服务\n实例演示 实例介绍 创建bookinfo实例：\n$ kubectl create namespace bookinfo $ kubectl -n bookinfo create -f samples/bookinfo/platform/kube/bookinfo.yaml $ kubectl -n bookinfo get po NAME READY STATUS RESTARTS AGE details-v1-5974b67c8-wclnd 1/1 Running 0 34s productpage-v1-64794f5db4-jsdbg 1/1 Running 0 33s ratings-v1-c6cdf8d98-jrfrn 1/1 Running 0 33s reviews-v1-7f6558b974-kq6kj 1/1 Running 0 33s reviews-v2-6cb6ccd848-qdg2k 1/1 Running 0 34s reviews-v3-cc56b578-kppcx 1/1 Running 0 34s 该应用由四个单独的微服务构成。 这个应用模仿在线书店的一个分类，显示一本书的信息。 页面上会显示一本书的描述，书籍的细节（ISBN、页数等），以及关于这本书的一些评论。\nBookinfo 应用分为四个单独的微服务：\n productpage. 这个微服务会调用 details 和 reviews 两个微服务，用来生成页面。 details. 这个微服务中包含了书籍的信息。 reviews. 这个微服务中包含了书籍相关的评论。它还会调用 ratings 微服务。 ratings. 这个微服务中包含了由书籍评价组成的评级信息。  reviews 微服务有 3 个版本：\n  v1 版本不会调用 ratings 服务。\n  v2 版本会调用 ratings 服务，并使用 1 到 5 个黑色星形图标来显示评分信息。\n  v3 版本会调用 ratings 服务，并使用 1 到 5 个红色星形图标来显示评分信息。\n  Bookinfo 是一个异构应用，几个微服务是由不同的语言编写的。这些服务对 Istio 并无依赖，但是构成了一个有代表性的服务网格的例子：它由多个服务、多个语言构成，并且 reviews 服务具有多个版本。\n使用ingress访问productpage服务：\ningress-productpage.yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata:  name: productpage  namespace: bookinfo spec:  rules:  - host: productpage.bookinfo.com  http:  paths:  - backend:  serviceName: productpage  servicePort: 9080  path: / status:  loadBalancer: {} 如何实现更细粒度的流量管控？\n注入sidecar容器 如何注入sidecar容器   使用istioctl kube-inject\n$ kubectl -n bookinfo apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml)   为命名空间打label\n# 给命名空间打标签，这样部署在该命名空间的服务会自动注入sidecar容器 $ kubectl label namespace dafault istio-injection=enabled   注入bookinfo $ kubectl -n bookinfo apply -f \u0026lt;(istioctl kube-inject -f samples/bookinfo/platform/kube/bookinfo.yaml) 流量路由 实现ingress解决不了的按照比例分配流量\ningress-gateway访问productpage productpage-gateway.yaml\napiVersion: networking.istio.io/v1alpha3 kind: Gateway metadata:  name: productpage-gateway  namespace: bookinfo spec:  selector:  istio: ingressgateway # use istio default controller  servers:  - port:  number: 80  name: http  protocol: HTTP  hosts:  - productpage.bookinfo.com productpage-virtualservice.yaml\napiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: gateway-front-tomcat  namespace: bookinfo spec:  gateways:  - productpage-gateway  hosts:  - productpage.bookinfo.com  http:  - route:  - destination:  host: productpage  port:  number: 9080 配置nginx，使用域名80端口访问。\nupstream bookinfo-productpage {  server 172.21.51.67:31995; } server {  listen 80;  listen [::]:80;  server_name productpage.bookinfo.com;   location / {  proxy_set_header Host $host;  proxy_set_header X-Real-IP $remote_addr;  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  proxy_http_version 1.1;  proxy_pass http://bookinfo-productpage;  } } 权重路由 只想访问reviews-v3\n$ cat virtual-service-reviews-v3.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: reviews  namespace: bookinfo spec:  hosts:  - reviews  http:  - route:  - destination:  host: reviews  subset: v3  $ cat destination-rule-reviews.yaml apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: reviews  namespace: bookinfo spec:  host: reviews  trafficPolicy:  loadBalancer:  simple: RANDOM  subsets:  - name: v1  labels:  version: v1  - name: v2  labels:  version: v2  - name: v3  labels:  version: v3  $ kubectl apply -f virtual-service-reviews-v3.yaml  # 访问productpage测试 实现如下流量分配：\n90% -\u0026gt; reivews-v1 10% -\u0026gt; reviews-v2 0% -\u0026gt; reviews-v3 $ cat virtual-service-reviews-90-10.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: reviews  namespace: bookinfo spec:  hosts:  - reviews  http:  - route:  - destination:  host: reviews  subset: v1  weight: 90  - destination:  host: reviews  subset: v2  weight: 10  $ kubectl apply -f virtual-service-reviews-90-10.yaml  # 假如v2版本的副本数扩容为3，v2版本的流量会如何分配？ 会不会变成30%？ $ kubectl -n bookinfo scale deploy reviews-v2 --replicas=3 访问路径路由 实现效果如下：\n# 定义允许外部流量进入网格,直接编辑已有gateway $ kubectl -n bookinfo edit gw productpage-gateway  $ cat bookinfo-routing-with-uri-path.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: bookinfo  namespace: bookinfo spec:  gateways:  - productpage-gateway  hosts:  - bookinfo.com  http:  - name: productpage-route  match:  - uri:  prefix: /productpage  route:  - destination:  host: productpage  - name: reviews-route  match:  - uri:  prefix: /reviews  route:  - destination:  host: reviews  - name: ratings-route  match:  - uri:  prefix: /ratings  route:  - destination:  host: ratings nginx中新增配置：\nupstream bookinfo {  server 172.21.51.67:31995; } server {  listen 80;  listen [::]:80;  server_name bookinfo.com;   location / {  proxy_set_header Host $host;  proxy_set_header X-Real-IP $remote_addr;  proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;  proxy_http_version 1.1;  proxy_pass http://bookinfo;  } } 访问：\nhttp://bookinfo.com/productpage\nhttp://bookinfo.com/ratings/1\n实际的访问对应为：\nbookinfo.com/productpage -\u0026gt; productpage:8090/productpage bookinfo.com/ratings -\u0026gt; ratings:9080/ratings bookinfo.com/reviews -\u0026gt; reviews:9080/reviews 实际访问productpage页面，由于需要引用css、js等静态资源，因此需要补充对/static路径的转发：\n...  http:  - name: productpage-route  match:  - uri:  prefix: /productpage  - uri:  prefix: /static  route:  - destination:  host: productpage ... virtualservice的配置中并未指定service的port端口，转发同样可以生效？\n 注意，若service中只有一个端口，则不用显式指定端口号，会自动转发到该端口中\n 路径重写 如果想实现rewrite的功能，\nbookinfo.com/rate -\u0026gt; ratings:8090/ratings ...  - name: ratings-route  match:  - uri:  prefix: /rate  rewrite:  uri: \u0026#34;/ratings\u0026#34;  route:  - destination:  host: ratings ... 匹配优先级 登录发现/login的匹配也没有添加，后续有可能有别的，因此可以在规则列表最后添加一个规则，作为默认的转发规则。\n$ kubectl -n bookinfo edit vs bookinfo ...  - name: default-route  route:  - destination:  host: productpage DestinationRule 转发策略 默认会使用轮询策略，此外也支持如下负载均衡模型，可以在 DestinationRule 中使用这些模型，将请求分发到特定的服务或服务子集。\n Random：将请求转发到一个随机的实例上 Weighted：按照指定的百分比将请求转发到实例上 Least requests：将请求转发到具有最少请求数目的实例上  apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: my-destination-rule spec:  host: my-svc  trafficPolicy: #默认的负载均衡策略模型为随机  loadBalancer:  simple: RANDOM  subsets:  - name: v1  #subset1，将流量转发到具有标签 version:v1 的 deployment 对应的服务上  labels:  version: v1  - name: v2  #subset2，将流量转发到具有标签 version:v2 的 deployment 对应的服务上,指定负载均衡为轮询  labels:  version: v2  trafficPolicy:  loadBalancer:  simple: ROUND_ROBIN  - name: v3  #subset3，将流量转发到具有标签 version:v3 的 deployment 对应的服务上  labels:  version: v3 使用https 方式一：把证书绑定在外部的nginx中， nginx 443端口监听外网域名并转发请求到Istio Ingress网关IP+http端口 ，如果使用公有云lb的话（如slb，clb），可以在lb层绑定证书\n方式二：在istio侧使用证书\nhttps://istio.io/latest/docs/tasks/traffic-management/ingress/secure-ingress/\nheader头路由 $ cat virtual-service-reviews-header.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: reviews  namespace: bookinfo spec:  hosts:  - reviews  http:  - match:  - headers:  end-user:  exact: luffy  route:  - destination:  host: reviews  subset: v2  - route:  - destination:  host: reviews  subset: v3 $ kubectl apply -f virtual-service-reviews-header.yaml  # 刷新观察http://bookinfo.com/productpage 更多支持的匹配类型可以在此处查看。\nhttps://istio.io/latest/docs/reference/config/networking/virtual-service/#HTTPMatchRequest\n流量镜像 介绍 很多情况下，当我们对服务做了重构，或者我们对项目做了重大优化时，怎么样保证服务是健壮的呢？在传统的服务里，我们只能通过大量的测试，模拟在各种情况下服务的响应情况。虽然也有手工测试、自动化测试、压力测试等一系列手段去检测它，但是测试本身就是一个样本化的行为，即使测试人员再完善它的测试样例，无法全面的表现出线上服务的一个真实流量形态 。\n流量镜像的设计，让这类问题得到了最大限度的解决。流量镜像讲究的不再是使用少量样本去评估一个服务的健壮性，而是在不影响线上坏境的前提下将线上流量持续的镜像到我们的预发布坏境中去，让重构后的服务在上线之前就结结实实地接受一波真实流量的冲击与考验，让所有的风险全部暴露在上线前夕，通过不断的暴露问题，解决问题让服务在上线前夕就拥有跟线上服务一样的健壮性。由于测试坏境使用的是真实流量，所以不管从流量的多样性，真实性，还是复杂性上都将能够得以展现，同时预发布服务也将表现出其最真实的处理能力和对异常的处理能力。\n实践 # 准备httpbin v1 $ cat httpbin-v1.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: httpbin-v1  namespace: bookinfo spec:  replicas: 1  selector:  matchLabels:  app: httpbin  version: v1  template:  metadata:  labels:  app: httpbin  version: v1  spec:  containers:  - image: docker.io/kennethreitz/httpbin  imagePullPolicy: IfNotPresent  name: httpbin  command: [\u0026#34;gunicorn\u0026#34;, \u0026#34;--access-logfile\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:80\u0026#34;, \u0026#34;httpbin:app\u0026#34;]  $ istioctl kube-inject -f httpbin-v1.yaml | kubectl create -f - $ curl $(kubectl -n bookinfo get po -l version=v1,app=httpbin -ojsonpath=\u0026#39;{.items[0].status.podIP}\u0026#39;)/headers {  \u0026#34;headers\u0026#34;: {  \u0026#34;Accept\u0026#34;: \u0026#34;*/*\u0026#34;,  \u0026#34;Content-Length\u0026#34;: \u0026#34;0\u0026#34;,  \u0026#34;Host\u0026#34;: \u0026#34;10.244.0.88\u0026#34;,  \u0026#34;User-Agent\u0026#34;: \u0026#34;curl/7.29.0\u0026#34;,  \u0026#34;X-B3-Sampled\u0026#34;: \u0026#34;1\u0026#34;,  \u0026#34;X-B3-Spanid\u0026#34;: \u0026#34;777c7af4458c5b81\u0026#34;,  \u0026#34;X-B3-Traceid\u0026#34;: \u0026#34;6b98ea81618deb4f777c7af4458c5b81\u0026#34;  } }  # 准备httpbin v2 $ cat httpbin-v2.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: httpbin-v2  namespace: bookinfo spec:  replicas: 1  selector:  matchLabels:  app: httpbin  version: v2  template:  metadata:  labels:  app: httpbin  version: v2  spec:  containers:  - image: docker.io/kennethreitz/httpbin  imagePullPolicy: IfNotPresent  name: httpbin  command: [\u0026#34;gunicorn\u0026#34;, \u0026#34;--access-logfile\u0026#34;, \u0026#34;-\u0026#34;, \u0026#34;-b\u0026#34;, \u0026#34;0.0.0.0:80\u0026#34;, \u0026#34;httpbin:app\u0026#34;]  $ istioctl kube-inject -f httpbin-v2.yaml | kubectl create -f -  # Service文件 $ cat httpbin-svc.yaml apiVersion: v1 kind: Service metadata:  name: httpbin  namespace: bookinfo  labels:  app: httpbin spec:  ports:  - name: http  port: 8000  targetPort: 80  selector:  app: httpbin  $ kubectl apply -f httpbin-svc.yaml  # 使用bookinfo.com/httpbin访问,因此直接修改bookinfo这个virtualservice即可 $ kubectl -n bookinfo get vs NAME GATEWAYS HOSTS bookinfo [bookinfo-gateway] [bookinfo.com] gateway-front-tomcat [productpage-gateway] [productpage.bookinfo.com] reviews [reviews] $ kubectl -n bookinfo edit vs bookinfo #添加httpbin的规则 ...  - match:  - uri:  prefix: /httpbin  name: httpbin-route  rewrite:  uri: /  route:  - destination:  host: httpbin  subset: v1 ... # 创建gateway和virtualservice,由于都是使用http请求，因此，直接 $ cat httpbin-destinationRule.yaml apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: httpbin  namespace: bookinfo spec:  host: httpbin  subsets:  - name: v1  labels:  version: v1  - name: v2  labels:  version: v2 $ kubectl apply -f httpbin-destinationRule.yaml  # 访问http://bookinfo.com/httpbin/headers，查看日志  # 为httpbin-v1添加mirror设置，mirror点为httpbin-v2 $ kubectl -n bookinfo edit vs bookinfo ...  - match:  - uri:  prefix: /httpbin  name: httpbin-route  rewrite:  uri: /  route:  - destination:  host: httpbin  subset: v1  mirror:  host: httpbin  subset: v2  mirror_percent: 100 ... 重试 在网络环境不稳定的情况下，会出现暂时的网络不可达现象，这时需要重试机制，通过多次尝试来获取正确的返回信息。 istio 可以通过简单的配置来实现重试功能，让开发人员无需关注重试部分的代码实现，专心实现业务代码。\n实践 浏览器访问http://bookinfo.com/httpbin/status/502\n# 此时查看httpbin-v1的日志，显示一条状态码为502的日志 $ kubectl -n bookinfo logs -f httpbin-v1-5967569c54-sp874 -c istio-proxy [2020-11-09T10:26:48.907Z] \u0026#34;GET /httpbin/status/502 HTTP/1.1\u0026#34; 502 - \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 0 5 4 \u0026#34;172.21.50.140,10.244.0.1\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36\u0026#34; \u0026#34;767cf33b-b8cf-9804-8c48-df131393c8a5\u0026#34; \u0026#34;bookinfo.com\u0026#34; \u0026#34;127.0.0.1:80\u0026#34; inbound|8000|http|httpbin.bookinfo.svc.cluster.local 127.0.0.1:48376 10.244.0.90:80 10.244.0.1:0 outbound_.8000_.v1_.httpbin.bookinfo.svc.cluster.local default 我们为httpbin服务设置重试机制，这里设置如果服务在 2 秒内没有返回正确的返回值，就进行重试，重试的条件为返回码为5xx，重试 3 次。\n$ kubectl -n bookinfo edit vs bookinfo ...  - match:  - uri:  prefix: /httpbin  mirror:  host: httpbin  subset: v2  mirror_percent: 100  name: httpbin-route  retries:  attempts: 3  perTryTimeout: 2s  retryOn: 5xx  rewrite:  uri: /  route:  - destination:  host: httpbin  subset: v1 ...  # 再次查看httpbin-v1的日志，显示四条状态码为502的日志 熔断 介绍 熔断（Circuit Breaker），原是指当电流超过规定值时断开电路，进行短路保护或严重过载保护的机制 。对于微服务系统而言，熔断尤为重要，它可以使系统在遭遇某些模块故障时，通过服务降级等方式来提高系统核心功能的可用性，得以应对来自故障、潜在峰值或其他未知网络因素的影响。\n准备环境 Istio 是通过 Envoy Proxy 来实现熔断机制的，Envoy 强制在网络层面配置熔断策略，这样就不必为每个应用程序单独配置或重新编程。下面就通过一个示例来演示如何为 Istio 网格中的服务配置熔断的连接数、请求数和异常检测。\n  创建httpbin服务\n  创建测试客户端\n我们已经为 httpbin 服务设置了熔断策略，接下来创建一个 Java 客户端，用来向后端服务发送请求，观察是否会触发熔断策略。这个客户端可以控制连接数量、并发数、待处理请求队列，使用这一客户端，能够有效的触发前面在目标规则中设置的熔断策略。该客户端的 deployment yaml 内容如下：\n# httpbin-client-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: httpbin-client-v1  namespace: bookinfo spec:  replicas: 1  selector:  matchLabels:  app: httpbin-client-v1  version: v1  template:  metadata:  labels:  app: httpbin-client-v1  version: v1  spec:  containers:  - image: ceposta/http-envoy-client-standalone:latest  imagePullPolicy: IfNotPresent  name: httpbin-client  command: [\u0026#34;/bin/sleep\u0026#34;,\u0026#34;infinity\u0026#34;] 这里我们会把给客户端也进行 Sidecar 的注入，以此保证 Istio 对网络交互的控制：\n$ kubectl apply -f \u0026lt;(istioctl kube-inject -f httpbin-client-deploy.yaml)   验证 先尝试通过单线程（NUM_THREADS=1）创建一个连接，并进行 5 次调用（默认值：NUM_CALLS_PER_CLIENT=5）：\n$ CLIENT_POD=$(kubectl get pod -n bookinfo | grep httpbin-client | awk \u0026#39;{ print $1 }\u0026#39;) $ kubectl -n bookinfo exec -it $CLIENT_POD -c httpbin-client -- sh -c \u0026#39;export URL_UNDER_TEST=http://httpbin:8000/get export NUM_THREADS=1 \u0026amp;\u0026amp; java -jar http-client.jar\u0026#39; 下面尝试把线程数提高到 2：\n$ kubectl -n bookinfo exec -it $CLIENT_POD -c httpbin-client -- sh -c \u0026#39;export URL_UNDER_TEST=http://httpbin:8000/get export NUM_THREADS=2 \u0026amp;\u0026amp; java -jar http-client.jar\u0026#39; 创建DestinationRule， 针对 httpbin 服务设置熔断策略：\n$ kubectl apply -f - \u0026lt;\u0026lt;EOF apiVersion: networking.istio.io/v1alpha3 kind: DestinationRule metadata:  name: httpbin  namespace: bookinfo spec:  host: httpbin  trafficPolicy:  connectionPool:  tcp:  maxConnections: 1  http:  http1MaxPendingRequests: 1  maxRequestsPerConnection: 1 EOF  maxConnections : 限制对后端服务发起的 HTTP/1.1 连接数，如果超过了这个限制，就会开启熔断。 maxPendingRequests : 限制待处理请求列表的长度， 如果超过了这个限制，就会开启熔断。 maxRequestsPerConnection : 在任何给定时间内限制对后端服务发起的 HTTP/2 请求数，如果超过了这个限制，就会开启熔断。  可以查看设置的熔断策略在envoy的配置片段：\n$ istioctl pc cluster httpbin-client-v1-56b86fb85c-vg5pp.bookinfo --fqdn httpbin.bookinfo.svc.cluster.local -ojson ...  \u0026#34;connectTimeout\u0026#34;: \u0026#34;10s\u0026#34;,  \u0026#34;maxRequestsPerConnection\u0026#34;: 1,  \u0026#34;circuitBreakers\u0026#34;: {  \u0026#34;thresholds\u0026#34;: [  {  \u0026#34;maxConnections\u0026#34;: 1,  \u0026#34;maxPendingRequests\u0026#34;: 1,  \u0026#34;maxRequests\u0026#34;: 4294967295,  \u0026#34;maxRetries\u0026#34;: 4294967295  }  ]  }, ... 再次验证熔断。\n故障注入与超时机制 在一个微服务架构的系统中，为了让系统达到较高的健壮性要求，通常需要对系统做定向错误测试。比如电商中的订单系统、支付系统等若出现故障那将是非常严重的生产事故，因此必须在系统设计前期就需要考虑多样性的异常故障并对每一种异常设计完善的恢复策略或优雅的回退策略，尽全力规避类似事故的发生，使得当系统发生故障时依然可以正常运作。而在这个过程中，服务故障模拟一直以来是一个非常繁杂的工作。\nistio提供了无侵入式的故障注入机制，让开发测试人员在不用调整服务程序的前提下，通过配置即可完成对服务的异常模拟。目前，包含两类：\n abort：非必配项，配置一个 Abort 类型的对象。用来注入请求异常类故障。简单的说，就是用来模拟上游服务对请求返回指定异常码时，当前的服务是否具备处理能力。 delay：非必配项，配置一个 Delay 类型的对象。用来注入延时类故障。通俗一点讲，就是人为模拟上游服务的响应时间，测试在高延迟的情况下，当前的服务是否具备容错容灾的能力。  延迟与超时 目前针对luffy登录用户，访问服务的示意为：\nproductpage --\u0026gt; reviews v2 --\u0026gt; ratings  \\  -\u0026gt; details 可以通过如下方式，为ratings服务注入2秒的延迟：\n$ cat virtualservice-ratings-2s-delay.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: ratings  namespace: bookinfo spec:  hosts:  - ratings  http:  - fault:  delay:  percentage:  value: 100  fixedDelay: 2s  route:  - destination:  host: ratings  $ kubectl apply -f virtualservice-ratings-2s-delay.yaml # 再次访问http://bookinfo.com/productpage，可以明显感觉2s的延迟,network可以看到 可以查看对应的envoy的配置：\n$ istioctl pc r ratings-v1-556cfbd589-89ml4.bookinfo --name 9080 -ojson 此时的调用为:\nproductpage --\u0026gt; reviews v2 -（延迟2秒）-\u0026gt; ratings  \\  -\u0026gt; details 此时，为reviews服务添加请求超时时间：\n$ kubectl -n bookinfo edit vs reviews ...  http:  - match:  - headers:  end-user:  exact: luffy  route:  - destination:  host: reviews  subset: v2  timeout: 1s  - route:  - destination:  host: reviews  subset: v3 ... 此使的调用关系为：\nproductpage -（0.5秒超时）-\u0026gt; reviews v2 -（延迟2秒）-\u0026gt; ratings  \\  -\u0026gt; details 此时，如果使用非luffy用户，则会出现只延迟，不会失败的情况。\n删除延迟：\n$ kubectl -n bookinfo delete vs ratings 状态码 $ cat virtualservice-details-aborted.yaml apiVersion: networking.istio.io/v1alpha3 kind: VirtualService metadata:  name: details  namespace: bookinfo spec:  hosts:  - details  http:  - fault:  abort:  percentage:  value: 50  httpStatus: 500  route:  - destination:  host: details  $ kubectl apply -f virtualservice-details-aborted.yaml  # 再次刷新查看details的状态，查看productpage的日志 $ kubectl -n bookinfo logs -f $(kubectl -n bookinfo get po -l app=productpage -ojsonpath=\u0026#39;{.items[0].metadata.name}\u0026#39;) -c istio-proxy [2020-11-09T09:00:16.020Z] \u0026#34;GET /details/0 HTTP/1.1\u0026#34; 500 FI \u0026#34;-\u0026#34; \u0026#34;-\u0026#34; 0 18 0 - \u0026#34;-\u0026#34; \u0026#34;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/86.0.4240.183 Safari/537.36\u0026#34; \u0026#34;f0387bb6-a445-922c-89ab-689dfbf548f8\u0026#34; \u0026#34;details:9080\u0026#34; \u0026#34;-\u0026#34; - - 10.111.67.169:9080 10.244.0.52:56552 - - 可观察性 安装集成组件 https://istio.io/latest/docs/ops/integrations\n  Grafana\n$ kubectl apply -f samples/addons/grafana.yaml   Jaeger\n$ kubectl apply -f samples/addons/jaeger.yaml   Kiali\n# 完善扩展组件地址： grafana url: \u0026#34;http://grafana.istio.com\u0026#34; tracing url: \u0026#34;http://jaeger.istio.com\u0026#34; $ kubectl apply -f samples/addons/kiali.yaml   Prometheus\n$ kubectl apply -f samples/addons/prometheus.yaml   prometheus Prometheus：\n$ cat prometheus-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: prometheus  namespace: istio-system spec:  rules:  - host: prometheus.istio.com  http:  paths:  - backend:  serviceName: prometheus  servicePort: 9090  path: / status:  loadBalancer: {}  $ kubectl apply -f prometheus-ingress.yaml 查看默认添加的targets列表：\n其中最核心的是kubernetes-pods 的监控，服务网格内的每个服务都作为一个target被监控，而且服务流量指标直接由sidecar容器来提供指标。\n$ kubectl -n bookinfo get po -owide $ curl 10.244.0.53:15020/stats/prometheus 对于这些监控指标采集的数据，可以在grafana中查看到。\n$ cat grafana-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: grafana  namespace: istio-system spec:  rules:  - host: grafana.istio.com  http:  paths:  - backend:  serviceName: grafana  servicePort: 3000  path: / status:  loadBalancer: {}   $ for i in $(seq 1 10000); do curl -s -o /dev/null \u0026#34;http://bookinfo.com/productpage\u0026#34;; done 访问界面后，可以查看到Istio Mesh Dashboard等相关的dashboard，因为在grafana的资源文件中， 中以 ConfigMap 的形式挂载了 Istio各个组件的仪表盘 JSON 配置文件：\n$ kubectl -n istio-system get cm istio-services-grafana-dashboards NAME DATA AGE istio-services-grafana-dashboards 3 7d1h jaeger $ cat jaeger-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: jaeger  namespace: istio-system spec:  rules:  - host: jaeger.istio.com  http:  paths:  - backend:  serviceName: tracing  servicePort: 80  path: / status:  loadBalancer: {}  $ kubectl apply -f jaeger-ingress.yaml kiali kiali 是一个 可观测性分析服务\n$ cat kiali-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: kiali  namespace: istio-system spec:  rules:  - host: kiali.istio.com  http:  paths:  - backend:  serviceName: kiali  servicePort: 20001  path: / status:  loadBalancer: {}  $ kubectl apply -f kiali-ingress.yaml 集成了Prometheus、grafana、tracing、log、\n展望未来 Kubernets已经成为了容器调度编排的事实标准，而容器正好可以作为微服务的最小工作单元，从而发挥微服务架构的最大优势。所以我认为未来微服务架构会围绕Kubernetes展开。而Istio和Conduit这类Service Mesh天生就是为了Kubernetes设计，它们的出现补足了Kubernetes在微服务间服务通讯上的短板。虽然Dubbo、Spring Cloud等都是成熟的微服务框架，但是它们或多或少都会和具体语言或应用场景绑定，并只解决了微服务Dev层面的问题。若想解决Ops问题，它们还需和诸如Cloud Foundry、Mesos、Docker Swarm或Kubernetes这类资源调度框架做结合：\n但是这种结合又由于初始设计和生态，有很多适用性问题需要解决。\nKubernetes则不同，它本身就是一个和开发语言无关的、通用的容器管理平台，它可以支持运行云原生和传统的容器化应用。并且它覆盖了微服务的Dev和Ops阶段，结合Service Mesh，它可以为用户提供完整端到端的微服务体验。\n所以我认为，未来的微服务架构和技术栈可能是如下形式：\n多云平台为微服务提供了资源能力（计算、存储和网络等），容器作为最小工作单元被Kubernetes调度和编排，Service Mesh管理微服务的服务通信，最后通过API Gateway向外暴露微服务的业务接口。\n未来随着以Kubernetes和Service Mesh为标准的微服务框架的盛行，将大大降低微服务实施的成本，最终为微服务落地以及大规模使用提供坚实的基础和保障。\n小结   第一代为Spring Cloud为代表的服务治理能力，是和业务代码紧耦合的，没法跨编程语言去使用\n  为了可以实现通用的服务治理能力，istio会为每个业务pod注入一个sidecar代理容器\n  为了能够做到服务治理，需要接管pod内的出入流量，因此通过注入的时候引入初始化容器istio-init实现pod内防火墙规则的初始化，分别将出入站流量拦截到pod内的15001和15006端口\n  同时，注入了istio-proxy容器，利用envoy代理，监听了15001和15006端口，对流量进行处理\n  istio在istio-system命名空间启动了istiod服务，用于监听用户写入etcd中的流量规则，转换成envoy可度的配置片段，通过envoy支持的xDS协议，同步到网格内的各envoy中\n  envoy获取规则后，做reload，直接应用到了用户期望的转发行为\n  envoy提供了强大的流量处理规则，包含了流量路由、镜像、重试、熔断、故障注入等，同时，也内置了分布式追踪、Prometheus监控的实现，业务应用对于这一切都是感知不到的\n  最后，通过分析bookinfo中，从 Productpage服务调用Reviews服务的 请求流程 来回顾istio重点：\n  Productpage发起对Reviews服务的调用：http://reviews:9080/reviews/0\n  请求被Productpage Pod的iptable规则拦截，重定向到本地的15001端口\n# OUTPUT 链：将所有出站数据包跳转到 ISTIO_OUTPUT 链上。 Chain OUTPUT (policy ACCEPT 46 packets, 3926 bytes)  pkts bytes target prot opt in out source destination  8 480 ISTIO_OUTPUT tcp -- * * 0.0.0.0/0 0.0.0.0/0  # ISTIO_OUTPUT 链：选择需要重定向到 Envoy（即本地） 的出站流量，所有非 localhost 的流量全部转发到 ISTIO_REDIRECT。为了避免流量在该 Pod 中无限循环，所有到 istio-proxy 用户空间的流量都返回到它的调用点中的下一条规则，本例中即 OUTPUT 链，因为跳出 ISTIO_OUTPUT 规则之后就进入下一条链 POSTROUTING。如果目的地非 localhost 就跳转到 ISTIO_REDIRECT；如果流量是来自 istio-proxy 用户空间的，那么就跳出该链，返回它的调用链继续执行下一条规则（OUTPUT 的下一条规则，无需对流量进行处理）；所有的非 istio-proxy 用户空间的目的地是 localhost 的流量就跳转到 ISTIO_REDIRECT。 Chain ISTIO_OUTPUT (1 references)  pkts bytes target prot opt in out source destination  0 0 RETURN all -- * lo 127.0.0.6 0.0.0.0/0  0 0 ISTIO_IN_REDIRECT all -- * lo 0.0.0.0/0 !127.0.0.1 owner UID match 1337  0 0 RETURN all -- * lo 0.0.0.0/0 0.0.0.0/0 ! owner UID match 1337  8 480 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner UID match 1337  0 0 ISTIO_IN_REDIRECT all -- * lo 0.0.0.0/0 !127.0.0.1 owner GID match 1337  0 0 RETURN all -- * lo 0.0.0.0/0 0.0.0.0/0 ! owner GID match 1337  0 0 RETURN all -- * * 0.0.0.0/0 0.0.0.0/0 owner GID match 1337  0 0 RETURN all -- * * 0.0.0.0/0 127.0.0.1  0 0 ISTIO_REDIRECT all -- * * 0.0.0.0/0 0.0.0.0/0  # ISTIO_REDIRECT 链：将所有流量重定向到 Sidecar（即本地） 的 15001 端口。 Chain ISTIO_REDIRECT (1 references)  pkts bytes target prot opt in out source destination  0 0 REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15001   15001端口上监听的Envoy Virtual Outbound Listener收到了该请求\n$ istioctl pc listener productpage-v1-785b4dbc96-2cw5v.bookinfo --port 15001 -ojson|more [  {  \u0026#34;name\u0026#34;: \u0026#34;virtualOutbound\u0026#34;,  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;,  \u0026#34;portValue\u0026#34;: 15001  }  }, ...   请求被Virtual Outbound Listener根据原目标IP（通配）和端口（9080）转发到0.0.0.0_9080这个 outbound listener\n$ istioctl pc listener productpage-v1-785b4dbc96-2cw5v.bookinfo --port 9080 -ojson [  {  \u0026#34;name\u0026#34;: \u0026#34;0.0.0.0_9080\u0026#34;,  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;,  \u0026#34;portValue\u0026#34;: 9080  }  },  \u0026#34;filterChains\u0026#34;: [  {  \u0026#34;filterChainMatch\u0026#34;: {  \u0026#34;applicationProtocols\u0026#34;: [  \u0026#34;http/1.0\u0026#34;,  \u0026#34;http/1.1\u0026#34;,  \u0026#34;h2c\u0026#34;  ]  },  \u0026#34;filters\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;,  \u0026#34;typedConfig\u0026#34;: {  \u0026#34;statPrefix\u0026#34;: \u0026#34;outbound_0.0.0.0_9080\u0026#34;,  \u0026#34;rds\u0026#34;: {  \u0026#34;configSource\u0026#34;: {  \u0026#34;ads\u0026#34;: {},  \u0026#34;resourceApiVersion\u0026#34;: \u0026#34;V3\u0026#34;  },  \u0026#34;routeConfigName\u0026#34;: \u0026#34;9080\u0026#34;  },   根据0.0.0.0_9080 listener的http_connection_manager filter配置,该请求采用“9080” route进行分发\n  9080的route中，根据domains进行匹配，将请求交给 outbound|9080|v2|reviews.bookinfo.svc.cluster.local这个cluster处理\n$ istioctl pc route productpage-v1-785b4dbc96-2cw5v.bookinfo --name 9080 -ojson ...  {  \u0026#34;name\u0026#34;: \u0026#34;reviews.bookinfo.svc.cluster.local:9080\u0026#34;,  \u0026#34;domains\u0026#34;: [  \u0026#34;reviews.bookinfo.svc.cluster.local\u0026#34;,  \u0026#34;reviews.bookinfo.svc.cluster.local:9080\u0026#34;,  \u0026#34;reviews\u0026#34;,  \u0026#34;reviews:9080\u0026#34;,  \u0026#34;reviews.bookinfo.svc.cluster\u0026#34;,  \u0026#34;reviews.bookinfo.svc.cluster:9080\u0026#34;,  \u0026#34;reviews.bookinfo.svc\u0026#34;,  \u0026#34;reviews.bookinfo.svc:9080\u0026#34;,  \u0026#34;reviews.bookinfo\u0026#34;,  \u0026#34;reviews.bookinfo:9080\u0026#34;,  \u0026#34;10.109.133.236\u0026#34;,  \u0026#34;10.109.133.236:9080\u0026#34;  ],  \u0026#34;routes\u0026#34;: [  {  \u0026#34;match\u0026#34;: {  \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;,  \u0026#34;caseSensitive\u0026#34;: true,  \u0026#34;headers\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;end-user\u0026#34;,  \u0026#34;exactMatch\u0026#34;: \u0026#34;luffy\u0026#34;  }  ]  },  \u0026#34;route\u0026#34;: {  \u0026#34;cluster\u0026#34;: \u0026#34;outbound|9080|v2|reviews.bookinfo.svc.cluster.local\u0026#34;,  \u0026#34;timeout\u0026#34;: \u0026#34;1s\u0026#34;,   该cluster为EDS\n$ istioctl pc cluster productpage-v1-785b4dbc96-2cw5v.bookinfo --fqdn reviews.bookinfo.svc.cluster.local --direction outbound -ojson ...  \u0026#34;name\u0026#34;: \u0026#34;outbound|9080|v3|reviews.bookinfo.svc.cluster.local\u0026#34;,  \u0026#34;type\u0026#34;: \u0026#34;EDS\u0026#34;,  \u0026#34;edsClusterConfig\u0026#34;: {  \u0026#34;edsConfig\u0026#34;: {  \u0026#34;ads\u0026#34;: {},  \u0026#34;resourceApiVersion\u0026#34;: \u0026#34;V3\u0026#34;  },  \u0026#34;serviceName\u0026#34;: \u0026#34;outbound|9080|v3|reviews.bookinfo.svc.cluster.local\u0026#34;  },  \u0026#34;connectTimeout\u0026#34;: \u0026#34;10s\u0026#34;,  \u0026#34;lbPolicy\u0026#34;: \u0026#34;RANDOM\u0026#34;,  \u0026#34;circuitBreakers\u0026#34;: {  \u0026#34;thresholds\u0026#34;: [  {  \u0026#34;maxConnections\u0026#34;: 4294967295,  \u0026#34;maxPendingRequests\u0026#34;: 4294967295,  \u0026#34;maxRequests\u0026#34;: 4294967295,  \u0026#34;maxRetries\u0026#34;: 4294967295  }  ]  },   查寻EDS对应的endpoint列表\n$ istioctl pc endpoint productpage-v1-785b4dbc96-2cw5v.bookinfo --cluster \u0026#34;outbound|9080|v3|reviews.bookinfo.svc.cluster.local\u0026#34; -ojson [  {  \u0026#34;name\u0026#34;: \u0026#34;outbound|9080|v3|reviews.bookinfo.svc.cluster.local\u0026#34;,  \u0026#34;addedViaApi\u0026#34;: true,  \u0026#34;hostStatuses\u0026#34;: [  {  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;10.244.0.37\u0026#34;,  \u0026#34;portValue\u0026#34;: 9080  }  }, ...   envoy进程得到了最终需要访问的地址（reviews-v3的podip：port），由envoy做proxy转发出去\n  此时，虽然还是会走一遍envoy的防火墙规则，但是由于是1337用户发起的请求，因此不会被再次拦截，直接走kubernetes的集群网络发出去\n  请求到达reviews-v3， 被iptable规则拦截，重定向到本地的15006端口\n# PREROUTING 链：用于目标地址转换（DNAT），将所有入站 TCP 流量跳转到 ISTIO_INBOUND 链上。 Chain PREROUTING (policy ACCEPT 148 packets, 8880 bytes)  pkts bytes target prot opt in out source destination  148 8880 ISTIO_INBOUND tcp -- * * 0.0.0.0/0 0.0.0.0/0  # INPUT 链：处理输入数据包，非 TCP 流量将继续 OUTPUT 链。 Chain INPUT (policy ACCEPT 148 packets, 8880 bytes)  pkts bytes target prot opt in out source destination  # ISTIO_INBOUND 链：将所有入站流量重定向到 ISTIO_IN_REDIRECT 链上，目的地为 15090，15020，15021端口的流量除外，发送到以上两个端口的流量将返回 iptables 规则链的调用点，即 PREROUTING 链的后继 POSTROUTING。 Chain ISTIO_INBOUND (1 references)  pkts bytes target prot opt in out source destination  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15008  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:22  0 0 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15090  143 8580 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15021  5 300 RETURN tcp -- * * 0.0.0.0/0 0.0.0.0/0 tcp dpt:15020  0 0 ISTIO_IN_REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0  # ISTIO_IN_REDIRECT 链：将所有入站流量跳转到本地的 15006 端口，至此成功的拦截了流量到sidecar中。 Chain ISTIO_IN_REDIRECT (3 references)  pkts bytes target prot opt in out source destination  0 0 REDIRECT tcp -- * * 0.0.0.0/0 0.0.0.0/0 redir ports 15006   被监听在15006端口的envoy进程处理\n$ istioctl pc listener reviews-v3-6c7d64cd96-75f4x.bookinfo --port 15006 -ojson|more [  {  \u0026#34;name\u0026#34;: \u0026#34;virtualInbound\u0026#34;,  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;0.0.0.0\u0026#34;,  \u0026#34;portValue\u0026#34;: 15006  }  }, ...   VirtualInbound不再转给别的监听器，根据自身过滤器链的匹配条件，请求被Virtual Inbound Listener内部配置的Http connection manager filter处理 ， 该filter设置的路由配置为将其发送给 inbound|9080|http|reviews.bookinfo.svc.cluster.local这个inbound的cluster\n {  \u0026#34;filterChainMatch\u0026#34;: {  \u0026#34;destinationPort\u0026#34;: 9080  },  \u0026#34;filters\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;istio.metadata_exchange\u0026#34;,  \u0026#34;typedConfig\u0026#34;: {  \u0026#34;@type\u0026#34;: \u0026#34;type.googleapis.com/udpa.type.v1.TypedStruct\u0026#34;,  \u0026#34;value\u0026#34;: {  \u0026#34;protocol\u0026#34;: \u0026#34;istio-peer-exchange\u0026#34;  }  }  },  {  \u0026#34;name\u0026#34;: \u0026#34;envoy.filters.network.http_connection_manager\u0026#34;,  \u0026#34;typedConfig\u0026#34;: {  \u0026#34;statPrefix\u0026#34;: \u0026#34;inbound_0.0.0.0_9080\u0026#34;,  \u0026#34;routeConfig\u0026#34;: {  \u0026#34;name\u0026#34;: \u0026#34;inbound|9080|http|reviews.bookinfo.svc.cluster.local\u0026#34;,  \u0026#34;virtualHosts\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;inbound|http|9080\u0026#34;,  \u0026#34;domains\u0026#34;: [  \u0026#34;*\u0026#34;  ],  \u0026#34;routes\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;default\u0026#34;,  \u0026#34;match\u0026#34;: {  \u0026#34;prefix\u0026#34;: \u0026#34;/\u0026#34;  },  \u0026#34;route\u0026#34;: {  \u0026#34;cluster\u0026#34;: \u0026#34;inbound|9080|http|reviews.bookinfo.svc.cluster.local\u0026#34;,  \u0026#34;timeout\u0026#34;: \u0026#34;0s\u0026#34;,  \u0026#34;maxGrpcTimeout\u0026#34;: \u0026#34;0s\u0026#34;  }   inbound|9080|http|reviews.bookinfo.svc.cluster.local配置的endpoint为本机的127.0.0.1:9080,因此转发到Pod内部的Reviews服务的9080端口进行处理。\n$ istioctl pc endpoint reviews-v3-6c7d64cd96-75f4x.bookinfo --cluster \u0026#34;inbound|9080|http|reviews.bookinfo.svc.cluster.local\u0026#34; -ojson [  {  \u0026#34;name\u0026#34;: \u0026#34;inbound|9080|http|reviews.bookinfo.svc.cluster.local\u0026#34;,  \u0026#34;addedViaApi\u0026#34;: true,  \u0026#34;hostStatuses\u0026#34;: [  {  \u0026#34;address\u0026#34;: {  \u0026#34;socketAddress\u0026#34;: {  \u0026#34;address\u0026#34;: \u0026#34;127.0.0.1\u0026#34;,  \u0026#34;portValue\u0026#34;: 9080  }  },   ","permalink":"https://iblog.zone/archives/%E5%9F%BA%E4%BA%8Eistio%E5%AE%9E%E7%8E%B0%E5%BE%AE%E6%9C%8D%E5%8A%A1%E6%B2%BB%E7%90%86/","summary":"基于Istio实现微服务治理 微服务架构可谓是当前软件开发领域的技术热点，它在各种博客、社交媒体和会议演讲上的出镜率非常之高，无论是做基础架构还是做业务系统的工程师，对微服务都相当关注，而这个现象与热度到目前为止，已经持续了近 5 年之久。\n尤其是近些年来，微服务架构逐渐发展成熟，从最初的星星之火到现在的大规模的落地与实践，几乎已经成为分布式环境下的首选架构。微服务成为时下技术热点，大量互联网公司都在做微服务架构的落地和推广。同时，也有很多传统企业基于微服务和容器，在做互联网技术转型。\n而在这个技术转型中，国内有一个趋势，以 Spring Cloud 与 Dubbo 为代表的微服务开发框架非常普及和受欢迎。然而软件开发没有银弹，基于这些传统微服务框架构建的应用系统在享受其优势的同时，痛点也越加明显。这些痛点包括但不限于以下几点：\n 侵入性强。想要集成 SDK 的能力，除了需要添加相关依赖，往往还需要在业务代码中增加一部分的代码、或注解、或配置；业务代码与治理层代码界限不清晰。 升级成本高。每次升级都需要业务应用修改 SDK 版本，重新进行功能回归测试，并且对每一台机器进行部署上线，而这对于业务方来说，与业务的快速迭代开发是有冲突的，大多不愿意停下来做这些与业务目标不太相关的事情。 版本碎片化严重。由于升级成本高，而中间件却不会停止向前发展的步伐，久而久之，就会导致线上不同服务引用的 SDK 版本不统一、能力参差不齐，造成很难统一治理。 中间件演变困难。由于版本碎片化严重，导致中间件向前演进的过程中就需要在代码中兼容各种各样的老版本逻辑，带着 “枷锁” 前行，无法实现快速迭代。 内容多、门槛高。Spring Cloud 被称为微服务治理的全家桶，包含大大小小几十个组件，内容相当之多，往往需要几年时间去熟悉其中的关键组件。而要想使用 Spring Cloud 作为完整的治理框架，则需要深入了解其中原理与实现，否则遇到问题还是很难定位。 治理功能不全。不同于 RPC 框架，Spring Cloud 作为治理全家桶的典型，也不是万能的，诸如协议转换支持、多重授权机制、动态请求路由、故障注入、灰度发布等高级功能并没有覆盖到。而这些功能往往是企业大规模落地不可获缺的功能，因此公司往往还需要投入其它人力进行相关功能的自研或者调研其它组件作为补充。  Service Mesh 服务网格 架构和概念 目的是解决系统架构微服务化后的服务间通信和治理问题。设计初衷是提供一种通用的服务治理方案。\nSidecar 在软件系统架构中特指边车模式。这个模式的灵感来源于我们生活中的边三轮：即在两轮摩托车的旁边添加一个边车的方式扩展现有的服务和功能。\n这个模式的精髓在于实现了数据面（业务逻辑）和控制面的解耦：原来两轮摩托车的驾驶者集中注意力跑赛道，边车上的领航员专注周围信息和地图，专注导航。\nService Mesh 这个服务网络专注于处理服务和服务间的通讯。其主要负责构造一个稳定可靠的服务通讯的基础设施，并让整个架构更为的先进和 Cloud Native。在工程中，Service Mesh 基本来说是一组轻量级的与应用逻辑服务部署在一起的服务代理，并且对于应用服务是透明的。\n开源实现 第一代服务网格 Linkerd和Envoy Linkerd 使用Scala编写，是业界第一个开源的service mesh方案。作者 William Morgan 是 service mesh 的布道师和践行者。Envoy 基于C++ 11编写，无论是理论上还是实际上，后者性能都比 Linkderd 更好。这两个开源实现都是以 sidecar 为核心，绝大部分关注点都是如何做好proxy，并完成一些通用控制面的功能。 但是，当你在容器中大量部署 sidecar 以后，如何管理和控制这些 sidecar 本身就是一个不小的挑战。于是，第二代 Service Mesh 应运而生。","title":"基于Istio实现微服务治理"},{"content":"Spring Cloud微服务项目交付 微服务扫盲篇 微服务并没有一个官方的定义，想要直接描述微服务比较困难，我们可以通过对比传统WEB应用，来理解什么是微服务。\n单体应用架构 如下是传统打车软件架构图：\n这种单体应用比较适合于小项目，优点是：\n 开发简单直接，集中式管理 基本不会重复开发 功能都在本地，没有分布式的管理开销和调用开销  当然它的缺点也十分明显，特别对于互联网公司来说：\n 开发效率低：所有的开发在一个项目改代码，递交代码相互等待，代码冲突不断 代码维护难：代码功能耦合在一起，新人不知道何从下手 部署不灵活：构建时间长，任何小修改必须重新构建整个项目，这个过程往往很长 稳定性不高：一个微不足道的小问题，可以导致整个应用挂掉 扩展性不够：无法满足高并发情况下的业务需求  微服务应用架构 微服务架构的设计思路不是开发一个巨大的单体式应用，而是将应用分解为小的、互相连接的微服务。一个微服务完成某个特定功能，比如乘客管理和下单管理等。每个微服务都有自己的业务逻辑和适配器。一些微服务还会提供API接口给其他微服务和应用客户端使用。\n比如，前面描述的系统可被分解为：\n每个业务逻辑都被分解为一个微服务，微服务之间通过REST API通信。一些微服务也会向终端用户或客户端开发API接口。但通常情况下，这些客户端并不能直接访问后台微服务，而是通过API Gateway来传递请求。API Gateway一般负责服务路由、负载均衡、缓存、访问控制和鉴权等任务。\n微服务架构优点：\n 解决了复杂性问题。它将单体应用分解为一组服务。虽然功能总量不变，但应用程序已被分解为可管理的模块或服务 体系结构使得每个服务都可以由专注于此服务的团队独立开发。只要符合服务API契约，开发人员可以自由选择开发技术。这就意味着开发人员可以采用新技术编写或重构服务，由于服务相对较小，所以这并不会对整体应用造成太大影响 微服务架构可以使每个微服务独立部署。这些更改可以在测试通过后立即部署。所以微服务架构也使得CI／CD成为可能  微服务架构问题及挑战 微服务的一个主要缺点是微服务的分布式特点带来的复杂性。开发人员需要基于RPC或者消息实现微服务之间的调用和通信，而这就使得服务之间的发现、服务调用链的跟踪和质量问题变得的相当棘手。\n 微服务的一大挑战是跨多个服务的更改  比如在传统单体应用中，若有A、B、C三个服务需要更改，A依赖B，B依赖C。我们只需更改相应的模块，然后一次性部署即可。 在微服务架构中，我们需要仔细规划和协调每个服务的变更部署。我们需要先更新C，然后更新B，最后更新A。   部署基于微服务的应用也要复杂得多  单体应用可以简单的部署在一组相同的服务器上，然后前端使用负载均衡即可。 微服务由不同的大量服务构成。每种服务可能拥有自己的配置、应用实例数量以及基础服务地址。这里就需要不同的配置、部署、扩展和监控组件。此外，我们还需要服务发现机制，以便服务可以发现与其通信的其他服务的地址    以上问题和挑战可大体概括为：\n API Gateway 服务间调用 服务发现 服务容错 服务部署 数据调用  https://www.kancloud.cn/owenwangwen/open-capacity-platform/1480155，自助餐吃吃喝喝，竟然秒懂微服务\n微服务框架 如何应对上述挑战，出现了如下微服务领域的框架：\n  Spring Cloud（各个微服务基于Spring Boot实现）\n  Dubbo\n  Service Mesh\n  Linkerd\n  Envoy\n  Conduit\n  Istio\n    了解Spring Cloud https://spring.io\n核心项目及组件 https://spring.io/projects\n与Dubbo对比 做一个简单的功能对比：\n   核心要素 Dubbo Spring Cloud     服务注册中心 Zookeeper Spring Cloud Netflix Eureka   服务调用方式 RPC REST API   服务监控 Dubbo-monitor Spring Boot Admin   断路器 不完善 Spring Cloud Netflix Hystrix   服务网关 无 Spring Cloud Netflix Zuul   分布式配置 无 Spring Cloud Config   服务跟踪 无 Spring Cloud Sleuth   消息总线 无 Spring Cloud Bus   数据流 无 Spring Cloud Stream   批量任务 无 Spring Cloud Task   …… …… ……    从上图可以看出其实Dubbo的功能只是Spring Cloud体系的一部分。\n这样对比是不够公平的，首先Dubbo是SOA时代的产物，它的关注点主要在于服务的调用，流量分发、流量监控和熔断。而Spring Cloud诞生于微服务架构时代，考虑的是微服务治理的方方面面，另外由于依托了Spirng、Spirng Boot的优势之上，两个框架在开始目标就不一致，Dubbo定位服务治理、Spirng Cloud是一个生态。\nSpring Boot交付实践 从零开始创建Spring Boot项目 通过File \u0026gt; New \u0026gt; Project，新建工程，选择Spring Initializr\n配置Project Metadata：\n配置Dependencies依赖包：\n选择：Web分类中的Spring web和Template Engines中的Thymeleaf\n配置maven settings.xml：\n默认使用IDE自带的maven，换成自己下载的，下载地址：\n链接: https://pan.baidu.com/s/1z9dRGv_4bS1uxBtk5jsZ2Q 提取码: 3gva\n解压后放到D:\\software\\apache-maven-3.6.3,修改D:\\software\\apache-maven-3.6.3\\conf\\settings.xml 文件：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;settings xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0\u0026#34;  xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xsi:schemaLocation=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\u0026#34;\u0026gt;  \u0026lt;localRepository\u0026gt;D:\\opt\\maven-repo\u0026lt;/localRepository\u0026gt;   \u0026lt;pluginGroups\u0026gt;  \u0026lt;/pluginGroups\u0026gt;   \u0026lt;proxies\u0026gt;  \u0026lt;/proxies\u0026gt;   \u0026lt;servers\u0026gt;  \u0026lt;/servers\u0026gt;   \u0026lt;mirrors\u0026gt; \t\u0026lt;mirror\u0026gt;  \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt;  \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt;  \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt;  \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/repositories/central/\u0026lt;/url\u0026gt;  \u0026lt;/mirror\u0026gt;  \u0026lt;mirror\u0026gt;  \u0026lt;id\u0026gt;nexus-aliyun\u0026lt;/id\u0026gt;  \u0026lt;mirrorOf\u0026gt;*\u0026lt;/mirrorOf\u0026gt;  \u0026lt;name\u0026gt;Nexus aliyun\u0026lt;/name\u0026gt;  \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public\u0026lt;/url\u0026gt;  \u0026lt;/mirror\u0026gt;  \u0026lt;/mirrors\u0026gt;  \u0026lt;/settings\u0026gt;  替换springboot版本为2.3.5.RELEASE\n 直接启动项目并访问本地服务：localhost:8080\n编写功能代码 创建controller包及HelloController.java文件\npackage com.luffy.demo.controller;  import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RestController;  @RestController public class HelloController {   @RequestMapping(value = \u0026#34;/hello\u0026#34;, method = RequestMethod.GET)  public String hello(String name) {  return \u0026#34;Hello, \u0026#34; + name;  } 保存并在浏览器中访问localhost:8080/hello?name=luffy\n如果页面复杂，如何实现？\n在resources/templates/目录下新建index.html\n\u0026lt;!DOCTYPE html\u0026gt; \u0026lt;html\u0026gt; \u0026lt;head\u0026gt;  \u0026lt;title\u0026gt;Devops\u0026lt;/title\u0026gt;  \u0026lt;meta http-equiv=\u0026#34;Content-Type\u0026#34; content=\u0026#34;text/html; charset=UTF-8\u0026#34; /\u0026gt; \u0026lt;/head\u0026gt; \u0026lt;body\u0026gt; \u0026lt;div class=\u0026#34;container\u0026#34;\u0026gt;  \u0026lt;h3 th:text=\u0026#34;${requestname}\u0026#34;\u0026gt;\u0026lt;/h3\u0026gt;  \u0026lt;a id=\u0026#34;rightaway\u0026#34; href=\u0026#34;#\u0026#34; th:href=\u0026#34;@{/rightaway}\u0026#34; \u0026gt;立即返回\u0026lt;/a\u0026gt;  \u0026lt;a id=\u0026#34;sleep\u0026#34; href=\u0026#34;#\u0026#34; th:href=\u0026#34;@{/sleep}\u0026#34;\u0026gt;延时返回\u0026lt;/a\u0026gt; \u0026lt;/div\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 完善HelloController.java的内容：\npackage com.luffy.demo.controller;  import org.springframework.web.bind.annotation.RequestMapping; import org.springframework.web.bind.annotation.RequestMethod; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.servlet.ModelAndView;  @RestController public class HelloController {   @RequestMapping(value = \u0026#34;/hello\u0026#34;, method = RequestMethod.GET)  public String hello(String name) {  return \u0026#34;Hello, \u0026#34; + name;  }   @RequestMapping(\u0026#34;/\u0026#34;)  public ModelAndView index(ModelAndView mv) {  mv.setViewName(\u0026#34;index\u0026#34;);  mv.addObject(\u0026#34;requestname\u0026#34;, \u0026#34;This is index\u0026#34;);  return mv;  }   @RequestMapping(\u0026#34;/rightaway\u0026#34;)  public ModelAndView returnRightAway(ModelAndView mv) {  mv.setViewName(\u0026#34;index\u0026#34;);  mv.addObject(\u0026#34;requestname\u0026#34;,\u0026#34;This request is RightawayApi\u0026#34;);  return mv;  }   @RequestMapping(\u0026#34;/sleep\u0026#34;)  public ModelAndView returnSleep(ModelAndView mv) throws InterruptedException {  Thread.sleep(2*1000);  mv.setViewName(\u0026#34;index\u0026#34;);  mv.addObject(\u0026#34;requestname\u0026#34;,\u0026#34;This request is SleepApi\u0026#34;+\u0026#34;,it will sleep 2s !\u0026#34;);  return mv;  } } 如何在java项目中使用maven 为什么需要maven 考虑一个常见的场景：以项目A为例，开发过程中，需要依赖B-2.0.jar的包，如果没有maven，那么正常做法是把B-2.0.jar拷贝到项目A中，但是如果B-2.0.jar还依赖C.jar，我们还需要去找到C.jar的包，因此，在开发阶段需要花费在项目依赖方面的精力会很大。\n因此，开发人员需要找到一种方式，可以管理java包的依赖关系，并可以方便的引入到项目中。\nmaven如何工作 查看pom.xml\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-thymeleaf\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; 可以直接在项目中添加上dependency ，这样来指定项目的依赖包。\n思考：如果spring-boot-starter-thymeleaf包依赖别的包，怎么办？\nspring-boot-starter-thymeleaf同时也是一个maven项目，也有自己的pom.xml\n查看一下：\n \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.3.3.RELEASE\u0026lt;/version\u0026gt;  \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.thymeleaf\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;thymeleaf-spring5\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;3.0.11.RELEASE\u0026lt;/version\u0026gt;  \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.thymeleaf.extras\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;thymeleaf-extras-java8time\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;3.0.4.RELEASE\u0026lt;/version\u0026gt;  \u0026lt;scope\u0026gt;compile\u0026lt;/scope\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt; 这样的话，使用maven的项目，只需要在自己的pom.xml中把所需的最直接的依赖包定义上，而不用关心这些被依赖的jar包自身是否还有别的依赖。剩下的都交给maven去搞定。\n如何搞定？maven可以根据pom.xml中定义的依赖实现包的查找\n去哪查找？maven仓库，存储jar包的地方。\n当我们执行 Maven 构建命令时，Maven 开始按照以下顺序查找依赖的库：\n本地仓库：\n  Maven 的本地仓库，在安装 Maven 后并不会创建，它是在第一次执行 maven 命令的时候才被创建。\n  运行 Maven 的时候，Maven 所需要的任何包都是直接从本地仓库获取的。如果本地仓库没有，它会首先尝试从远程仓库下载构件至本地仓库，然后再使用本地仓库的包。\n  默认情况下，不管Linux还是 Windows，每个用户在自己的用户目录下都有一个路径名为 .m2/respository/ 的仓库目录。\n  Maven 本地仓库默认被创建在 %USER_HOME% 目录下。要修改默认位置，在 %M2_HOME%\\conf 目录中的 Maven 的 settings.xml 文件中定义另一个路径。\n\u0026lt;settings xmlns=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0\u0026#34;  xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xsi:schemaLocation=\u0026#34;http://maven.apache.org/SETTINGS/1.0.0 http://maven.apache.org/xsd/settings-1.0.0.xsd\u0026#34;\u0026gt;  \u0026lt;localRepository\u0026gt;D:\\opt\\maven-repo\u0026lt;/localRepository\u0026gt; \u0026lt;/settings\u0026gt;   中央仓库：\nMaven 中央仓库是由 Maven 社区提供的仓库，中央仓库包含了绝大多数流行的开源Java构件，以及源码、作者信息、SCM、信息、许可证信息等。一般来说，简单的Java项目依赖的构件都可以在这里下载到。\n中央仓库的关键概念：\n 这个仓库由 Maven 社区管理。 不需要配置，maven中集成了地址 http://repo1.maven.org/maven2 需要通过网络才能访问。  私服仓库：\n通常使用 sonatype Nexus来搭建私服仓库。搭建完成后，需要在 setting.xml中进行配置，比如：\n\u0026lt;profile\u0026gt;  \u0026lt;id\u0026gt;localRepository\u0026lt;/id\u0026gt;  \u0026lt;repositories\u0026gt;  \u0026lt;repository\u0026gt;  \u0026lt;id\u0026gt;myRepository\u0026lt;/id\u0026gt;  \u0026lt;name\u0026gt;myRepository\u0026lt;/name\u0026gt;  \u0026lt;url\u0026gt;http://127.0.0.1:8081/nexus/content/repositories/myRepository/\u0026lt;/url\u0026gt;  \u0026lt;releases\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/releases\u0026gt;  \u0026lt;snapshots\u0026gt;  \u0026lt;enabled\u0026gt;true\u0026lt;/enabled\u0026gt;  \u0026lt;/snapshots\u0026gt;  \u0026lt;/repository\u0026gt;  \u0026lt;/repositories\u0026gt; \u0026lt;/profile\u0026gt; 方便起见，我们直接使用国内ali提供的仓库，修改 maven 根目录下的 conf 文件夹中的 setting.xml 文件，在 mirrors 节点上，添加内容如下：\n\u0026lt;mirrors\u0026gt;  \u0026lt;mirror\u0026gt;  \u0026lt;id\u0026gt;alimaven\u0026lt;/id\u0026gt;  \u0026lt;name\u0026gt;aliyun maven\u0026lt;/name\u0026gt;  \u0026lt;url\u0026gt;http://maven.aliyun.com/nexus/content/groups/public/\u0026lt;/url\u0026gt;  \u0026lt;mirrorOf\u0026gt;central\u0026lt;/mirrorOf\u0026gt;  \u0026lt;/mirror\u0026gt; \u0026lt;/mirrors\u0026gt; 在执行构建的时候，maven会自动将所需的包下载到本地仓库中，所以第一次构建速度通常会慢一些，后面速度则很快。\n那么maven是如何找到对应的jar包的？\n我们可以访问 https://mvnrepository.com/ 查看在仓库中的jar包的样子。\n\u0026lt;!-- https://mvnrepository.com/artifact/commons-collections/commons-collections --\u0026gt; \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;commons-collections\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;commons-collections\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;3.2.2\u0026lt;/version\u0026gt; \u0026lt;/dependency\u0026gt; 刚才看到spring-boot-starter-thymeleaf的依赖同样有上述属性，因此maven就可以根据这三项属性，到对应的仓库中去查找到所需要的依赖包，并下载到本地。\n其中groupId、artifactId、version共同保证了包在仓库中的唯一性，这也就是为什么maven项目的pom.xml中都先配置这几项的原因，因为项目最终发布到远程仓库中，供别人调用。\n思考：我们项目的dependency中为什么没有写version ?\n是因为sprintboot项目的上面有人，来看一下项目parent的写法：\n \u0026lt;parent\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.3.3.RELEASE\u0026lt;/version\u0026gt;  \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt;  \u0026lt;/parent\u0026gt; parent模块中定义过的dependencies，在子项目中引用的话，不需要指定版本，这样可以保证所有的子项目都使用相同版本的依赖包。\n生命周期及mvn命令实践 Maven有三套相互独立的生命周期，分别是clean、default和site。每个生命周期包含一些阶段（phase），阶段是有顺序的，后面的阶段依赖于前面的阶段。\n clean生命周期，清理项目  清理：mvn clean　\u0026ndash;删除target目录，也就是将class文件等删除   default生命周期，项目的构建等核心阶段  编译：mvn compile　\u0026ndash;src/main/java目录java源码编译生成class （target目录下） 测试：mvn test　\u0026ndash;src/test/java 执行目录下的测试用例 打包：mvn package　\u0026ndash;生成压缩文件：java项目#jar包；web项目#war包，也是放在target目录下 安装：mvn install　\u0026ndash;将压缩文件(jar或者war)上传到本地仓库 部署|发布：mvn deploy　\u0026ndash;将压缩文件上传私服   site生命周期，建立和发布项目站点  站点 : mvn site \u0026ndash;生成项目站点文档    各个生命周期相互独立，一个生命周期的阶段前后依赖。 生命周期阶段需要绑定到某个插件的目标才能完成真正的工作，比如test阶段正是与maven-surefire-plugin的test目标相绑定了 。\n举例如下：\n  mvn clean\n调用clean生命周期的clean阶段\n  mvn test\n调用default生命周期的test阶段，实际执行test以及之前所有阶段\n  mvn clean install\n调用clean生命周期的clean阶段和default的install阶段，实际执行clean，install以及之前所有阶段\n  在linux环境中演示：\n创建gitlab组，luffy-spring-cloud,在该组下创建项目springboot-demo\n  提交代码到git仓库\n$ git init $ git remote add origin http://gitlab.luffy.com/luffy-spring-cloud/springboot-demo.git $ git add . $ git commit -m \u0026#34;Initial commit\u0026#34; $ git push -u origin master   使用tools容器来运行\n$ docker run --rm -ti 172.21.51.67:5000/devops/tools:v3 bash bash-5.0# mvn -v bash: mvn: command not found # 由于idea工具自带了maven，所以可以直接在ide中执行mvn命令。在tools容器中，需要安装mvn命令 为tools镜像集成mvn：\n将本地的apache-maven-3.6.3放到tools项目中，修改settings.xml配置\n... \u0026lt;localRepository\u0026gt;/opt/maven-repo\u0026lt;/localRepository\u0026gt; ... 然后修改Dockerfile，添加如下部分:\n#-----------------安装 maven--------------------#COPY apache-maven-3.6.3 /usr/lib/apache-maven-3.6.3RUN ln -s /usr/lib/apache-maven-3.6.3/bin/mvn /usr/local/bin/mvn \u0026amp;\u0026amp; chmod +x /usr/local/bin/mvnENV MAVEN_HOME=/usr/lib/apache-maven-3.6.3#------------------------------------------------#  去master节点拉取最新代码，构建最新的tools镜像：\n # k8s-master节点  $ git pull  $ docker build . -t 172.21.51.67:5000/devops/tools:v4 -f Dockerfile  $ docker push 172.21.51.67:5000/devops/tools:v4 再次尝试mvn命令：\n$ docker run --rm -ti 172.21.51.67:5000/devops/tools:v4 bash  bash-5.0# mvn -v  bash-5.0# git clone http://gitlab.luffy.com/luffy-spring-cloud/springboot-demo.git  bash-5.0# cd springboot-demo  bash-5.0# mvn clean  # 观察/opt/maven目录  bash-5.0# mvn package  # 多阶段组合  bash-5.0# mvn clean package 想系统学习maven，可以参考： https://www.runoob.com/maven/maven-pom.html\nSpringboot服务镜像制作 通过mvn package命令拿到服务的jar包后，我们可以使用如下命令启动服务：\n$ java -jar demo-0.0.1-SNAPSHOT.jar 因此，需要准备Dockerfile来构建镜像：\nFROMopenjdk:8-jdk-alpineCOPY target/springboot-demo-0.0.1-SNAPSHOT.jar app.jarCMD [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java -jar /app.jar\u0026#34; ]我们可以为构建出的镜像指定名称：\n \u0026lt;build\u0026gt;  \u0026lt;finalName\u0026gt;${project.artifactId}\u0026lt;/finalName\u0026gt;\u0026lt;!--打jar包去掉版本号--\u0026gt;  ... Dockerfile对应修改：\nFROMopenjdk:8-jdk-alpineCOPY target/springboot-demo.jar app.jarCMD [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java -jar /app.jar\u0026#34; ]执行镜像构建，验证服务启动是否正常：\n$ docker build . -t springboot-demo:v1 -f Dockerfile  $ docker run -d --name springboot-demo -p 8080:8080 springboot-demo:v1  $ curl localhost:8080 接入CICD流程 之前已经实现了shared-library，并且把python项目接入到了CICD 流程中。因此，可以直接使用已有的流程，把spring boot项目接入进去。\n Jenkinsfile sonar-project.properties deploy/deployment.yaml deploy/service.yaml deploy/ingress.yaml configmap/devops-config  Jenkinsfile @Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}  options { \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/demo/springboot-demo\u0026#34;  IMAGE_CREDENTIAL = \u0026#34;credential-registry\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  PROJECT = \u0026#34;springboot-demo\u0026#34;  }  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;mvn-package\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  sh \u0026#39;mvn clean package\u0026#39;  }  }  }  }  stage(\u0026#39;CI\u0026#39;){  failFast true  parallel {  stage(\u0026#39;Unit Test\u0026#39;) {  steps {  echo \u0026#34;Unit Test Stage Skip...\u0026#34;  }  }  stage(\u0026#39;Code Scan\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script {  devops.scan().start()  }  }  }  }  }  }   stage(\u0026#39;docker-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.docker(  \u0026#34;${IMAGE_REPO}\u0026#34;,  \u0026#34;${GIT_COMMIT}\u0026#34;,  IMAGE_CREDENTIAL  ).build().push()  }  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.deploy(\u0026#34;deploy\u0026#34;,true,\u0026#34;deploy/deployment.yaml\u0026#34;).start()  }  }  }  }  }  post {  success {  script{  devops.notificationSuccess(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  failure {  script{  devops.notificationFailure(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  } } sonar-project.properties sonar.projectKey=springboot-demo sonar.projectName=springboot-demo # if you want disabled the DTD verification for a proxy problem for example, true by default # JUnit like test report, default value is test.xml sonar.sources=src/main/java sonar.language=java sonar.tests=src/test/java sonar.java.binaries=target/classes deploy/deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: springboot-demo  namespace: {{NAMESPACE}} spec:  replicas: 1  selector:  matchLabels:  app: springboot-demo  template:  metadata:  labels:  app: springboot-demo  spec:  containers:  - name: springboot-demo  image: {{IMAGE_URL}}  imagePullPolicy: IfNotPresent  ports:  - containerPort: 8080  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  livenessProbe:  httpGet:  path: /  port: 8080  scheme: HTTP  initialDelaySeconds: 120  periodSeconds: 15  timeoutSeconds: 3  readinessProbe:  httpGet:  path: /  port: 8080  scheme: HTTP  initialDelaySeconds: 120  timeoutSeconds: 2  periodSeconds: 15 deploy/service.yaml apiVersion: v1 kind: Service metadata:  name: springboot-demo  namespace: {{NAMESPACE}} spec:  ports:  - port: 8080  protocol: TCP  targetPort: 8080  selector:  app: springboot-demo  sessionAffinity: None  type: ClusterIP status:  loadBalancer: {} deploy/ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: springboot-demo  namespace: {{NAMESPACE}} spec:  rules:  - host: {{INGRESS_SPRINGBOOTDEMO}}  http:  paths:  - backend:  serviceName: springboot-demo  servicePort: 8080  path: / status:  loadBalancer: {} 维护devops-config的configmap，添加INGRESS_SPRINGBOOTDEMO配置项：\n$ kubectl -n dev edit cm devops-config ... data:  INGRESS_MYBLOG: blog-dev.luffy.com  INGRESS_SPRINGBOOTDEMO: springboot-dev.luffy.com  NAMESPACE: dev ... 更新Jenkins中的jnlp-slave-pod模板镜像：\n172.21.51.67:5000/devops/tools:v4 由于镜像中maven的目录是/opt/maven-repo，而slave-pod是执行完任务后会销毁，因此需要将maven的数据目录挂载出来，不然每次构建都会重新拉取所有依赖的jar包：\n配置Jenkins流水线：\n添加单元测试覆盖率 单元测试这块内容一直没有把覆盖率统计到sonarqube端，本节看下怎么样将单元测试的结果及覆盖率展示到Jenkins及sonarqube平台中。\n为了展示效果，我们先添加一个单元测试文件HelloControllerTest：\npackage com.luffy.demo;  import org.junit.jupiter.api.BeforeEach; import org.junit.jupiter.api.Test; import org.slf4j.Logger; import org.slf4j.LoggerFactory; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.boot.test.context.SpringBootTest; import org.springframework.http.MediaType; import org.springframework.test.context.web.WebAppConfiguration; import org.springframework.test.web.servlet.MockMvc; import org.springframework.test.web.servlet.request.MockMvcRequestBuilders; import org.springframework.test.web.servlet.result.MockMvcResultHandlers; import org.springframework.test.web.servlet.result.MockMvcResultMatchers; import org.springframework.test.web.servlet.setup.MockMvcBuilders; import org.springframework.web.context.WebApplicationContext;  @SpringBootTest @WebAppConfiguration public class HelloControllerTests {   private static final Logger logger = LoggerFactory.getLogger(HelloControllerTests.class);  @Autowired  private WebApplicationContext webApplicationContext;   private MockMvc mockMvc;   @BeforeEach  public void setMockMvc() {  mockMvc = MockMvcBuilders.webAppContextSetup(webApplicationContext).build();  }   @Test  public void index(){  try {  mockMvc.perform(MockMvcRequestBuilders.post(\u0026#34;/\u0026#34;)  .contentType(MediaType.APPLICATION_JSON)  ).andExpect(MockMvcResultMatchers.status().isOk())  .andDo(MockMvcResultHandlers.print());  }catch (Exception e) {  e.printStackTrace();  }   }   @Test  public void rightaway(){  try {  mockMvc.perform(MockMvcRequestBuilders.post(\u0026#34;/rightaway\u0026#34;)  .contentType(MediaType.APPLICATION_JSON)  ).andExpect(MockMvcResultMatchers.status().isOk())  .andDo(MockMvcResultHandlers.print());  }catch (Exception e) {  e.printStackTrace();  }   }   @Test  public void sleep(){  try {  mockMvc.perform(MockMvcRequestBuilders.post(\u0026#34;/sleep\u0026#34;)  .contentType(MediaType.APPLICATION_JSON)  ).andExpect(MockMvcResultMatchers.status().isOk())  .andDo(MockMvcResultHandlers.print());  }catch (Exception e) {  e.printStackTrace();  }   } } jacoco：监控JVM中的调用，生成监控结果（默认保存在jacoco.exec文件中），然后分析此结果，配合源代码生成覆盖率报告。\n如何引入jacoco测试：\n \u0026lt;plugin\u0026gt;  \u0026lt;groupId\u0026gt;org.jacoco\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;jacoco-maven-plugin\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;0.7.8\u0026lt;/version\u0026gt;  \u0026lt;executions\u0026gt;  \u0026lt;execution\u0026gt;  \u0026lt;goals\u0026gt;  \u0026lt;goal\u0026gt;prepare-agent\u0026lt;/goal\u0026gt;  \u0026lt;/goals\u0026gt;  \u0026lt;configuration\u0026gt;  \u0026lt;destFile\u0026gt;${project.build.directory}/coverage-reports/jacoco.exec\u0026lt;/destFile\u0026gt;  \u0026lt;/configuration\u0026gt;  \u0026lt;/execution\u0026gt;  \u0026lt;execution\u0026gt;  \u0026lt;id\u0026gt;default-report\u0026lt;/id\u0026gt;  \u0026lt;phase\u0026gt;test\u0026lt;/phase\u0026gt;  \u0026lt;goals\u0026gt;  \u0026lt;goal\u0026gt;report\u0026lt;/goal\u0026gt;  \u0026lt;/goals\u0026gt;  \u0026lt;configuration\u0026gt;  \u0026lt;dataFile\u0026gt;${project.build.directory}/coverage-reports/jacoco.exec\u0026lt;/dataFile\u0026gt;  \u0026lt;outputDirectory\u0026gt;${project.reporting.outputDirectory}/jacoco\u0026lt;/outputDirectory\u0026gt;  \u0026lt;/configuration\u0026gt;  \u0026lt;/execution\u0026gt;  \u0026lt;/executions\u0026gt;  \u0026lt;/plugin\u0026gt; 其中：\n prepare-agent，会把agent准备好，这样在执行用例的时候，就会使用agent检测到代码执行的过程，通常将结果保存在jacoco.exec中 report，分析保存的jacoco.exec文件，生成报告  在IDE中添加，观察插件的goal，执行mvn test，观察执行过程。\n有了上述内容后，如何将结果发布到sonarqube中？\n提交最新代码，查看sonarqube的分析结果。\nSpring Cloud开发、交付实践 https://spring.io/projects/spring-cloud#overview\n1、Netflix是一家做视频的网站，可以这么说该网站上的美剧应该是最火的。\n2、Netflix是一家没有CTO的公司，正是这样的组织架构能使产品与技术无缝的沟通，从而能快速迭代出更优秀的产品。在当时软件敏捷开发中，Netflix的更新速度不亚于当年的微信后台变更，虽然微信比Netflix迟发展，但是当年微信的灰度发布和敏捷开发应该算是业界最猛的。\n3、Netflix由于做视频的原因，访问量非常的大，从而促使其技术快速的发展在背后支撑着，也正是如此，Netflix开始把整体的系统往微服务上迁移。\n4、Netflix的微服务做的不是最早的，但是确是最大规模的在生产级别微服务的尝试。也正是这种大规模的生产级别尝试，在服务器运维上依托AWS云。当然AWS云同样受益于Netflix的大规模业务不断的壮大。\n5、Netflix的微服务大规模的应用，在技术上毫无保留的把一整套微服务架构核心技术栈开源了出来，叫做Netflix OSS，也正是如此，在技术上依靠开源社区的力量不断的壮大。\n6、Spring Cloud是构建微服务的核心，而Spring Cloud是基于Spring Boot来开发的。\n7、Pivotal在Netflix开源的一整套核心技术产品线的同时，做了一系列的封装，就变成了Spring Cloud；虽然Spring Cloud到现在为止不只有Netflix提供的方案可以集成，还有很多方案，但Netflix是最成熟的。\n 本课程基于SpringBoot 2.3.6.RELEASE 和Spring Cloud Hoxton.SR9版本\n 微服务场景 开发APP，提供个人的花呗账单管理。\n 注册、登录、账单查询 用户服务，账单管理服务   Eureka服务注册中心 在SpringCloud体系中，我们知道服务之间的调用是通过http协议进行调用的。而注册中心的主要目的就是维护这些服务的服务列表。\nhttps://docs.spring.io/spring-cloud-netflix/docs/2.2.5.RELEASE/reference/html/\n新建项目 pom中引入spring-cloud的依赖：\nhttps://spring.io/projects/spring-cloud#overview\n\u0026lt;properties\u0026gt;  \u0026lt;spring.cloud-version\u0026gt;Hoxton.SR9\u0026lt;/spring.cloud-version\u0026gt; \u0026lt;/properties\u0026gt; \u0026lt;dependencyManagement\u0026gt;  \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${spring.cloud-version}\u0026lt;/version\u0026gt;  \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;  \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt; \u0026lt;/dependencyManagement\u0026gt; 引入eureka-server的依赖：\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-server\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; 启动eureka服务 https://docs.spring.io/spring-cloud-netflix/docs/2.2.5.RELEASE/reference/html/#spring-cloud-eureka-server-standalone-mode\napplication.yml\nserver: port: 8761 eureka: client: service-url: defaultZone: http://${eureka.instance.hostname}:${server.port}/eureka/ register-with-eureka: false fetch-registry: false instance: hostname: localhost 启动类：\npackage com.luffy.eureka;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.netflix.eureka.server.EnableEurekaServer;  @SpringBootApplication @EnableEurekaServer public class EurekaServerApplication {  public static void main(String[] args) {  SpringApplication.run(EurekaServerApplication.class, args);  } } 启动访问localhost:8761测试\n创建spring cloud项目三部曲：\n 引入依赖包 修改application.yml配置文件 启动类添加注解  eureka认证 没有认证，不安全，添加认证：\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-security\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; application.yml\nserver:  port: 8761 eureka:  client:  service-url:  defaultZone: http://${spring.security.user.name}:${spring.security.user.password}@${eureka.instance.hostname}:${server.port}/eureka/  register-with-eureka: false  fetch-registry: false  instance:  hostname: localhost spring:  security:  user:  name: ${EUREKA_USER:admin}  password: ${EUREKA_PASS:admin} 注册服务到eureka 新建项目，user-service（选择Spring Cloud依赖和SpringBoot Web依赖），用来提供用户查询功能。\n三部曲：\n pom.xml，并添加依赖 创建application.yml配置文件 创建Springboot启动类，并配置注解  pom.xml添加：\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; application.yml server:  port: 7000 eureka:  client:  serviceUrl:  defaultZone: http://${EUREKA_USER:admin}:${EUREKA_PASS:admin}@localhost:8761/eureka/ 启动类：\npackage com.luffy.user;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient;  //注意这里也可使用@EnableEurekaClient //但由于springcloud是灵活的，注册中心支持eureka、consul、zookeeper等 //若写了具体的注册中心注解，则当替换成其他注册中心时，又需要替换成对应的注解了。 //所以 直接使用@EnableDiscoveryClient 启动发现。 //这样在替换注册中心时，只需要替换相关依赖即可。 @EnableDiscoveryClient @SpringBootApplication public class UserServiceApplication {  public static void main(String[] args) {  SpringApplication.run(UserServiceApplication.class, args);  } } 报错：\nc.n.d.s.t.d.RetryableEurekaHttpClient : Request execution failed with message: com.fasterxml.jackson.databind.exc.MismatchedInputException: Root name \u0026#39;timestamp\u0026#39; does not match expected (\u0026#39;instance\u0026#39;) for type [simple type, class com.netflix.appinfo.InstanceInfo] 新版本的security默认开启csrf了，关掉，在注册中心新建一个类，继承WebSecurityConfigurerAdapter来关闭 ,\u0026gt; 注意，是在eureka server端关闭。\npackage com.luffy.eureka;  import org.springframework.context.annotation.Configuration; import org.springframework.security.config.annotation.web.builders.HttpSecurity; import org.springframework.security.config.annotation.web.configuration.EnableWebSecurity; import org.springframework.security.config.annotation.web.configuration.WebSecurityConfigurerAdapter;  @EnableWebSecurity @Configuration public class WebSecurityConfig extends WebSecurityConfigurerAdapter {   @Override  protected void configure(HttpSecurity http) throws Exception {  http.csrf().disable(); //关闭csrf  http.authorizeRequests().anyRequest().authenticated().and().httpBasic(); //开启认证  } } 再次启动发现可以注册，但是地址是\napplication.yaml\nserver:  port: 7000 eureka:  client:  serviceUrl:  defaultZone: http://${EUREKA_USER:admin}:${EUREKA_PASS:admin}@localhost:8761/eureka/  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: user-service spring:  application:  name: user-service Eurake有一个配置参数eureka.server.renewalPercentThreshold，定义了renews 和renews threshold的比值，默认值为0.85。当server在15分钟内，比值低于percent，即少了15%的微服务心跳，server会进入自我保护状态\n默认情况下，如果Eureka Server在一定时间内没有接收到某个微服务实例的心跳，Eureka Server将会注销该实例（默认90秒）。但是当网络分区故障发生时，微服务与Eureka Server之间无法正常通信，这就可能变得非常危险了，因为微服务本身是健康的，此时本不应该注销这个微服务。\nEureka Server通过“自我保护模式”来解决这个问题，当Eureka Server节点在短时间内丢失过多客户端时（可能发生了网络分区故障），那么这个节点就会进入自我保护模式。一旦进入该模式，Eureka Server就会保护服务注册表中的信息，不再删除服务注册表中的数据（也就是不会注销任何微服务）。当网络故障恢复后，该Eureka Server节点会自动退出自我保护模式。\n自我保护模式是一种对网络异常的安全保护措施。使用自我保护模式，而让Eureka集群更加的健壮、稳定。\n开发阶段可以通过配置：eureka.server.enable-self-preservation=false关闭自我保护模式。\n生产阶段，理应以默认值进行配置。\n至于具体具体的配置参数，可至官网查看：http://cloud.spring.io/spring-cloud-static/Finchley.RELEASE/single/spring-cloud.html#_appendix_compendium_of_configuration_properties\n高可用 高可用：\n 优先保证可用性 各个节点都是平等的，1个节点挂掉不会影响正常节点的工作，剩余的节点依然可以提供注册和查询服务 在向某个Eureka注册时如果发现连接失败，则会自动切换至其它节点，只要有一台Eureka还在，就能保证注册服务可用(保证可用性)  注意点：\n  多实例的话eureka.instance.instance-id需要保持不一样，否则会当成同一个\n  eureka.instance.hostname要与defaultZone里的地址保持一致\n  各个eureka的spring.application.name相同\n  拷贝eureka服务，分别命名eureka-ha-peer1和eureka-ha-peer2\n修改模块的pom.xml\n\u0026lt;artifactId\u0026gt;eureka-ha-peer1\u0026lt;/artifactId\u0026gt; 修改配置文件application.yml，注意集群服务，需要各个eureka的spring.application.name相同\nserver:  port: ${EUREKA_PORT:8762} eureka:  client:  service-url:  defaultZone: ${EUREKA_SERVER:http://${spring.security.user.name}:${spring.security.user.password}@peer1:8762/eureka/,http://${spring.security.user.name}:${spring.security.user.password}@peer2:8763/eureka/}  fetch-registry: true  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  hostname: peer1 spring:  security:  user:  name: ${EUREKA_USER:admin}  password: ${EUREKA_PASS:admin}  application:  name: eureka-cluster 设置hosts文件\n127.0.0.1 peer1 peer2 服务提供者若想连接高可用的eureka，需要修改：\n defaultZone: http://${EUREKA_USER:admin}:${EUREKA_PASS:admin}@peer1:8762/eureka/,http://${EUREKA_USER:admin}:${EUREKA_PASS:admin}@peer2:8763/eureka/ k8s交付 分析：\n高可用互相注册，但是需要知道对方节点的地址。k8s中pod ip是不固定的，如何将高可用的eureka服务使用k8s交付？\n  方案一：创建三个Deployment+三个Service\n  方案二：使用statefulset管理\n  eureka-statefulset.yaml # eureka-statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata:  name: eureka-cluster  namespace: dev spec:  serviceName: \u0026#34;eureka\u0026#34;  replicas: 3  selector:  matchLabels:  app: eureka-cluster  template:  metadata:  labels:  app: eureka-cluster  spec:  containers:  - name: eureka  image: 172.21.51.67:5000/spring-cloud/eureka-cluster:v1  ports:  - containerPort: 8761  resources:  requests:  memory: 400Mi  cpu: 50m  limits:  memory: 2Gi  cpu: 2000m  env:  - name: MY_POD_NAME  valueFrom:  fieldRef:  fieldPath: metadata.name  - name: JAVA_OPTS  value: -XX:+UnlockExperimentalVMOptions  -XX:+UseCGroupMemoryLimitForHeap  -XX:MaxRAMFraction=2  -XX:CICompilerCount=8  -XX:ActiveProcessorCount=8  -XX:+UseG1GC  -XX:+AggressiveOpts  -XX:+UseFastAccessorMethods  -XX:+UseStringDeduplication  -XX:+UseCompressedOops  -XX:+OptimizeStringConcat  - name: EUREKA_SERVER  value: \u0026#34;http://admin:admin@eureka-cluster-0.eureka:8761/eureka/,http://admin:admin@eureka-cluster-1.eureka:8761/eureka/,http://admin:admin@eureka-cluster-2.eureka:8761/eureka/\u0026#34;  - name: EUREKA_INSTANCE_HOSTNAME  value: ${MY_POD_NAME}.eureka  - name: EUREKA_PORT  value: \u0026#34;8761\u0026#34; eureka-headless-service.yaml apiVersion: v1 kind: Service metadata:  name: eureka  namespace: dev  labels:  app: eureka spec:  ports:  - port: 8761  name: eureka  clusterIP: None  selector:  app: eureka-cluster 想通过ingress访问eureka，需要使用有头服务\napiVersion: v1 kind: Service metadata:  name: eureka-ingress  namespace: dev  labels:  app: eureka-cluster spec:  ports:  - port: 8761  name: eureka-cluster  selector:  app: eureka-cluster --- apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: eureka-cluster  namespace: dev spec:  rules:  - host: eureka-cluster.luffy.com  http:  paths:  - backend:  serviceName: eureka-ingress  servicePort: 8761  path: / status:  loadBalancer: {} 使用StatefulSet管理有状态服务 使用StatefulSet创建多副本pod的情况：\napiVersion: apps/v1 kind: StatefulSet metadata:  name: nginx-statefulset  labels:  app: nginx-sts spec:  replicas: 3  serviceName: \u0026#34;nginx\u0026#34;  selector:  matchLabels:  app: nginx-sts  template:  metadata:  labels:  app: nginx-sts  spec:  containers:  - name: nginx  image: nginx:alpine  ports:  - containerPort: 80 无头服务Headless Service\nkind: Service apiVersion: v1 metadata:  name: nginx spec:  selector:  app: nginx-sts  ports:  - protocol: TCP  port: 80  targetPort: 80  clusterIP: None $ kubectl -n spring exec -ti nginx-statefulset-0 sh / # curl nginx-statefulset-2.nginx 接入CICD流程 所需的文件:\n   在pom.xml中重写jar包名称：\n\u0026lt;finalName\u0026gt;${project.artifactId}\u0026lt;/finalName\u0026gt; Dockerfile FROM openjdk:8-jdk-alpine ADD target/eureka.jar app.jar ENV JAVA_OPTS=\u0026#34;\u0026#34; CMD [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java $JAVA_OPTS -jar /app.jar\u0026#34; ] Jenkinsfile @Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}  options { \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/spring-cloud/eureka-cluster\u0026#34;  IMAGE_CREDENTIAL = \u0026#34;credential-registry\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  PROJECT = \u0026#34;eureka-cluster\u0026#34;  }  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;mvn-package\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  sh \u0026#39;mvn clean package\u0026#39;  }  }  }  }  stage(\u0026#39;CI\u0026#39;){  failFast true  parallel {  stage(\u0026#39;Unit Test\u0026#39;) {  steps {  echo \u0026#34;Unit Test Stage Skip...\u0026#34;  }  }  stage(\u0026#39;Code Scan\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script {  devops.scan().start()  }  }  }  }  }  }   stage(\u0026#39;docker-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.docker(  \u0026#34;${IMAGE_REPO}\u0026#34;,  \u0026#34;${GIT_COMMIT}\u0026#34;,  IMAGE_CREDENTIAL  ).build().push()  }  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.deploy(\u0026#34;deploy\u0026#34;,false,\u0026#34;deploy/statefulset.yaml\u0026#34;).start()  }  }  }  }  }  post {  success {  script{  devops.notificationSuccess(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  failure {  script{  devops.notificationFailure(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  } } sonar-project.properties sonar.projectKey=eureka-cluster sonar.projectName=eureka-cluster # if you want disabled the DTD verification for a proxy problem for example, true by default # JUnit like test report, default value is test.xml sonar.sources=src/main/java sonar.language=java sonar.tests=src/test/java sonar.java.binaries=target/classes 模板化k8s资源清单：\n# eureka-statefulset.yaml apiVersion: apps/v1 kind: StatefulSet metadata:  name: eureka-cluster  namespace: {{NAMESPACE}} spec:  serviceName: \u0026#34;eureka\u0026#34;  replicas: 3  selector:  matchLabels:  app: eureka-cluster  template:  metadata:  labels:  app: eureka-cluster  spec:  containers:  - name: eureka  image: {{IMAGE_URL}} ... 维护新组件的ingress:\n$ kubectl -n dev edit configmap devops-config ...  INGRESS_EUREKA: eureka.luffy.com ... 部署k8s集群时，将eureka的集群地址通过参数的形式传递到pod内部，因此本地开发时，直接按照单点模式进行：\nserver:  port: ${EUREKA_PORT:8761} eureka:  client:  service-url:  defaultZone: ${EUREKA_SERVER:http://${spring.security.user.name}:${spring.security.user.password}@localhost:8761/eureka/}  fetch-registry: true  register-with-eureka: true  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  hostname: ${EUREKA_INSTANCE_HOSTNAME:localhost}  prefer-ip-address: true spring:  security:  user:  name: ${EUREKA_USER:admin}  password: ${EUREKA_PASS:admin}  application:  name: eureka-cluster 提交项目：\n创建develop分支，CICD部署开发环境\n 停掉eureka-ha\n 微服务间调用 服务提供者 前面已经将用户服务注册到了eureka注册中心，但是还没有暴漏任何API给服务消费者调用。\n新建controller类：\npackage com.luffy.userservice.controller;   import com.luffy.userservice.entity.User; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RestController;  import java.util.Random;  @RestController public class UserController {   @GetMapping(\u0026#34;/user\u0026#34;)  public String getUserService(){  return \u0026#34;this is user-service\u0026#34;;  }   @GetMapping(\u0026#34;/user-nums\u0026#34;)  public Integer getUserNums(){  return new Random().nextInt(100);  }   //{\u0026#34;id\u0026#34;: 123, \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;age\u0026#34;: 20, \u0026#34;sex\u0026#34;: \u0026#34;male\u0026#34;}  @GetMapping(\u0026#34;/user/{id}\u0026#34;)  public User getUserInfo(@PathVariable(\u0026#34;id\u0026#34;) int id){  User user = new User();  user.setId(id);  user.setAge(20);  user.setName(\u0026#34;zhangsan\u0026#34;);  user.setSex(\u0026#34;male\u0026#34;);  return user;  } } 实体类User.java\npackage com.luffy.userservice.entity;  public class User {  private int id;  private String name;  private int age;  private String sex;   public int getAge() {  return age;  }   public int getId() {  return id;  }   public String getName() {  return name;  }   public String getSex() {  return sex;  }   public void setAge(int age) {  this.age = age;  }   public void setId(int id) {  this.id = id;  }   public void setName(String name) {  this.name = name;  }   public void setSex(String sex) {  this.sex = sex;  } } application.yml 增加从环境变量中读取EUREKA_SERVER和EUREKA_INSTANCE_HOSTNAME配置\nserver:  port: 7000 eureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@localhost:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:user-service} spring:  application:  name: user-service CICD持续交付服务提供者 deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: user-service  namespace: {{NAMESPACE}} spec:  replicas: 1  selector:  matchLabels:  app: user-service  template:  metadata:  labels:  app: user-service  spec:  containers:  - name: user-service  image: {{IMAGE_URL}}  imagePullPolicy: IfNotPresent  ports:  - containerPort: 7000  resources:  requests:  memory: 400Mi  cpu: 50m  limits:  memory: 2Gi  cpu: 2000m  env:  - name: JAVA_OPTS  value: -XX:+UnlockExperimentalVMOptions  -XX:+UseCGroupMemoryLimitForHeap  -XX:MaxRAMFraction=2  -XX:CICompilerCount=8  -XX:ActiveProcessorCount=8  -XX:+UseG1GC  -XX:+AggressiveOpts  -XX:+UseFastAccessorMethods  -XX:+UseStringDeduplication  -XX:+UseCompressedOops  -XX:+OptimizeStringConcat  - name: EUREKA_SERVER  value: \u0026#34;http://admin:admin@eureka-cluster-0.eureka:8761/eureka/,http://admin:admin@eureka-cluster-1.eureka:8761/eureka/,http://admin:admin@eureka-cluster-2.eureka:8761/eureka/\u0026#34;  - name: INSTANCE_HOSTNAME  valueFrom:  fieldRef:  fieldPath: metadata.name service.yaml apiVersion: v1 kind: Service metadata:  name: user-service  namespace: {{NAMESPACE}} spec:  ports:  - port: 7000  protocol: TCP  targetPort: 7000  selector:  app: user-service  sessionAffinity: None  type: ClusterIP status:  loadBalancer: {} ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: user-service  namespace: {{NAMESPACE}} spec:  rules:  - host: {{INGRESS_USER_SERVICE}}  http:  paths:  - backend:  serviceName: user-service  servicePort: 7000  path: / status:  loadBalancer: {} ingress配置：\n$ kubectl -n dev edit configmap devops-config ... data:  INGRESS_MYBLOG: blog-dev.luffy.com  INGRESS_SPRINGBOOTDEMO: springboot-dev.luffy.com  INGRESS_USER_SERVICE: user-service-dev.luffy.com  NAMESPACE: dev ... Jenkinsfile @Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}  options { \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/spring-cloud/user-service\u0026#34;  IMAGE_CREDENTIAL = \u0026#34;credential-registry\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  PROJECT = \u0026#34;user-service\u0026#34;  }  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;mvn-package\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  sh \u0026#39;mvn clean package\u0026#39;  }  }  }  }  stage(\u0026#39;CI\u0026#39;){  failFast true  parallel {  stage(\u0026#39;Unit Test\u0026#39;) {  steps {  echo \u0026#34;Unit Test Stage Skip...\u0026#34;  }  }  stage(\u0026#39;Code Scan\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script {  devops.scan().start()  }  }  }  }  }  }   stage(\u0026#39;docker-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.docker(  \u0026#34;${IMAGE_REPO}\u0026#34;,  \u0026#34;${GIT_COMMIT}\u0026#34;,  IMAGE_CREDENTIAL  ).build().push()  }  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.deploy(\u0026#34;deploy\u0026#34;,true,\u0026#34;deploy/deployment.yaml\u0026#34;).start()  }  }  }  }  }  post {  success {  script{  devops.notificationSuccess(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  failure {  script{  devops.notificationFailure(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  } } pom.xml \u0026lt;finalName\u0026gt;${project.artifactId}\u0026lt;/finalName\u0026gt; Dockerfile FROM openjdk:8-jdk-alpine COPY target/user-service.jar app.jar ENV JAVA_OPTS=\u0026#34;\u0026#34; CMD [ \u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;java $JAVA_OPTS-jar /app.jar\u0026#34; ] sonar-project.properties sonar.projectKey=user-service sonar.projectName=user-service # if you want disabled the DTD verification for a proxy problem for example, true by default # JUnit like test report, default value is test.xml sonar.sources=src/main/java sonar.language=java sonar.tests=src/test/java sonar.java.binaries=target/classes 创建user-service项目，提交代码：\ngit init git remote add origin http://gitlab.luffy.com/luffy-spring-cloud/user-service.git git add . git commit -m \u0026#34;Initial commit\u0026#34; git push -u origin master  # 提交到develop分支 git checkout -b develop git push -u origin develop 创建Jenkins任务，测试自动部署\n访问http://user-service-dev.luffy.com/ 验证\n服务消费者 RestTemplate 在Spring中，提供了RestTemplate。RestTemplate是Spring提供的用于访问Rest服务的客户端。而在SpringCloud中也是使用此服务进行服务调用的。\n创建bill-service模块 新的模块初始化三部曲：\n pom.xml 启动类 配置文件  pom.xml 添加如下内容:\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; 全量内容如下:\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt;  \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;  \u0026lt;parent\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.3.6.RELEASE\u0026lt;/version\u0026gt;  \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt;  \u0026lt;/parent\u0026gt;  \u0026lt;groupId\u0026gt;com.luffy\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;bill-service\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt;  \u0026lt;name\u0026gt;bill-service\u0026lt;/name\u0026gt;  \u0026lt;description\u0026gt;bill-service\u0026lt;/description\u0026gt;   \u0026lt;properties\u0026gt;  \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt;  \u0026lt;spring-cloud.version\u0026gt;Hoxton.SR9\u0026lt;/spring-cloud.version\u0026gt;  \u0026lt;/properties\u0026gt;   \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-web\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;  \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;  \u0026lt;exclusions\u0026gt;  \u0026lt;exclusion\u0026gt;  \u0026lt;groupId\u0026gt;org.junit.vintage\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;junit-vintage-engine\u0026lt;/artifactId\u0026gt;  \u0026lt;/exclusion\u0026gt;  \u0026lt;/exclusions\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;   \u0026lt;dependencyManagement\u0026gt;  \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${spring-cloud.version}\u0026lt;/version\u0026gt;  \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;  \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;  \u0026lt;/dependencyManagement\u0026gt;   \u0026lt;build\u0026gt;  \u0026lt;finalName\u0026gt;${project.artifactId}\u0026lt;/finalName\u0026gt;\u0026lt;!--打jar包去掉版本号--\u0026gt;  \u0026lt;plugins\u0026gt;  \u0026lt;plugin\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt;  \u0026lt;/plugin\u0026gt;  \u0026lt;/plugins\u0026gt;  \u0026lt;/build\u0026gt;  \u0026lt;/project\u0026gt; BillServiceApplication package com.luffy.billservice;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient;  @SpringBootApplication @EnableDiscoveryClient public class BillServiceApplication {  public static void main(String[] args) {  SpringApplication.run(BillService.class, args);  } } application.yml\nserver:  port: 7001 eureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@localhost:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:bill-service} spring:  application:  name: bill-service BillController\npackage com.luffy.billservice.controller;  import org.springframework.beans.factory.annotation.Autowired; import org.springframework.context.annotation.Bean; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.client.RestTemplate;  @RestController public class BillController {   @Bean  public RestTemplate restTemplate() {  return new RestTemplate();  }   @Autowired  private RestTemplate restTemplate;   @GetMapping(\u0026#34;/bill/user\u0026#34;)  public String getUserInfo(){  return restTemplate.getForObject(\u0026#34;http://localhost:7000/user\u0026#34;, String.class);  } } 问题：\n 服务调用采用指定IP+Port方式，注册中心未使用 多个服务负载均衡  使用注册中心实现服务调用 修改BillController\npackage com.luffy.billservice.controller;  import org.springframework.beans.factory.annotation.Autowired; import org.springframework.cloud.client.loadbalancer.LoadBalanced; import org.springframework.context.annotation.Bean; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.client.RestTemplate;  @RestController public class BillController {   @Bean  @LoadBalanced  public RestTemplate restTemplate() {  return new RestTemplate();  }   @Autowired  private RestTemplate restTemplate;   @GetMapping(\u0026#34;/bill/user\u0026#34;)  public String getUserInfo(){  return restTemplate.getForObject(\u0026#34;http://user-service/user\u0026#34;, String.class);  } } 访问测试\n总体来说，就是通过为加入@LoadBalanced注解的RestTemplate添加一个请求拦截器，在请求前通过拦截器获取真正的请求地址，最后进行服务调用。\n友情提醒：若被@LoadBalanced注解的RestTemplate访问正常的服务地址，如http://127.0.0.1:8080/hello时，是会提示无法找到此服务的。\n具体原因：serverid必须是我们访问的服务名称 ，当我们直接输入ip的时候获取的server是null，就会抛出异常。\n如果想继续调用，可以通过如下方式：\npackage com.luffy.billservice.controller;  import org.springframework.beans.factory.annotation.Autowired; import org.springframework.beans.factory.annotation.Qualifier; import org.springframework.cloud.client.loadbalancer.LoadBalanced; import org.springframework.context.annotation.Bean; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController; import org.springframework.web.client.RestTemplate;  @RestController public class BillController {   @Bean  @LoadBalanced  public RestTemplate restTemplate() {  return new RestTemplate();  }   @Autowired  private RestTemplate restTemplate;   @Bean(\u0026#34;normalRestTemplate\u0026#34;)  public RestTemplate normalRestTemplate() {  return new RestTemplate();  }   @Autowired  @Qualifier(\u0026#34;normalRestTemplate\u0026#34;)  RestTemplate normalRestTemplate;    @GetMapping(\u0026#34;/service/user\u0026#34;)  public String getUserInfo(){  return restTemplate.getForObject(\u0026#34;http://user-service/user\u0026#34;, String.class);  }   @GetMapping(\u0026#34;/normal\u0026#34;)  public String normal() {  return normalRestTemplate.getForObject(\u0026#34;http://localhost:7000/user\u0026#34;, String.class);  } } Ribbon 负载均衡 再启动一个user-service-instance2，复制user-service项目\n修改user-service-instance2的application.yml的server.port\nserver:  port: 7002 eureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@peer1:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:user-service} spring:  application:  name: user-service 修改user-service-instance2的UserController.java，为了可以区分是哪个服务提供者的实例提供的服务\npackage com.luffy.userservice.controller;   import com.luffy.userservice.entity.User; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RestController;  import java.util.Random;  @RestController public class UserController {   @GetMapping(\u0026#34;/user\u0026#34;)  public String getUserService(){  return \u0026#34;this is user-service-instance2\u0026#34;;  }   @GetMapping(\u0026#34;/user-nums\u0026#34;)  public Integer getUserNums(){  return new Random().nextInt(100);  }   //{\u0026#34;id\u0026#34;: 123, \u0026#34;name\u0026#34;: \u0026#34;张三\u0026#34;, \u0026#34;age\u0026#34;: 20, \u0026#34;sex\u0026#34;: \u0026#34;male\u0026#34;}  @GetMapping(\u0026#34;/user/{id}\u0026#34;)  public User getUserInfo(@PathVariable(\u0026#34;id\u0026#34;) int id){  User user = new User();  user.setId(id);  user.setAge(20);  user.setName(\u0026#34;zhangsan\u0026#34;);  user.setSex(\u0026#34;male\u0026#34;);  return user;  } } 访问bill-service，查看调用结果（默认是轮询策略）\nSpring Cloud Ribbon是一个基于Http和TCP的客服端负载均衡工具，它是基于Netflix Ribbon实现的。与Eureka配合使用时，Ribbon可自动从Eureka Server (注册中心)获取服务提供者地址列表，并基于负载均衡算法，通过在客户端中配置ribbonServerList来设置服务端列表去轮询访问以达到均衡负载的作用。\n eureka-client中包含了ribbon的包，所以不需要单独引入\n 如何修改调用策略？\n 代码中指定rule的规则 配置文件配置  在bill-service中新建package，com.luffy.rule，注意不能被springboot扫描到，不然规则就成了全局规则，所有的ribbonclient都会应用到该规则。\npackage com.luffy.rule;  import com.netflix.loadbalancer.*; import org.springframework.context.annotation.Bean; import org.springframework.context.annotation.Configuration;  @Configuration public class RandomConfiguration {   @Bean  public IRule ribbonRule() {  // new BestAvailableRule();  // new WeightedResponseTimeRule();  return new RandomRule();  } } 修改BillController\nimport com.luffy.rule.RandomConfiguration; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient; import org.springframework.cloud.netflix.ribbon.RibbonClient;  @SpringBootApplication @EnableDiscoveryClient @RibbonClient(name = \u0026#34;user-service\u0026#34;, configuration = RandomConfiguration.class) public class BillServiceApplication {  public static void main(String[] args) {  SpringApplication.run(BillServiceApplication.class, args);  } } 配置文件方式： https://docs.spring.io/spring-cloud-netflix/docs/2.2.5.RELEASE/reference/html/#customizing-the-ribbon-client-by-setting-properties\n注释掉代码：\npackage com.luffy.ticket;  import com.luffy.rule.RandomConfiguration; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient; import org.springframework.cloud.netflix.ribbon.RibbonClient;  @SpringBootApplication @EnableDiscoveryClient //@RibbonClient(name = \u0026#34;USER-SERVICE\u0026#34;, configuration = RandomConfiguration.class) public class TicketApplication {  public static void main(String[] args) {  SpringApplication.run(TicketApplication.class, args);  } } 修改配置文件：\nserver:  port: ${SERVER_PORT:9000}  spring:  application:  name: bill-service  eureka:  client:  service-url:  defaultZone: ${EUREKA_SERVER:http://admin:admin@peer1:8762/eureka/,http://admin:admin@peer2:8763/eureka/}  instance:  prefer-ip-address: true  instance-id: ${spring.cloud.client.ip-address}:${server.port} user-service:  ribbon:  NFLoadBalancerRuleClassName: com.netflix.loadbalancer.RandomRule 声明式服务Feign 从上一章节，我们知道，当我们要调用一个服务时，需要知道服务名和api地址，这样才能进行服务调用，服务少时，这样写觉得没有什么问题，但当服务一多，接口参数很多时，上面的写法就显得不够优雅了。所以，接下来，来说说一种更好更优雅的调用服务的方式：Feign。\n Feign是Netflix开发的声明式、模块化的HTTP客户端。Feign可帮助我们更好更快的便捷、优雅地调用HTTP API。\n 在Spring Cloud中，使用Feign非常简单——创建一个接口，并在接口上添加一些注解。Feign支持多种注释，例如Feign自带的注解或者JAX-RS注解等 Spring Cloud对Feign进行了增强，使Feign支持了Spring MVC注解，并整合了Ribbon和 Eureka,从而让Feign 的使用更加方便。只需要通过创建接口并用注解来配置它既可完成对Web服务接口的绑定。\nhttps://github.com/OpenFeign/feign\n对bill-service项目添加openfeign的依赖引入：\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-openfeign\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; 启动类中引入Feign注解：\npackage com.luffy.billservice;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient; import org.springframework.cloud.openfeign.EnableFeignClients;  @SpringBootApplication @EnableDiscoveryClient @EnableFeignClients public class BillServiceApplication {   public static void main(String[] args) {  SpringApplication.run(BillServiceApplication.class, args);  }  } 建立interface\npackage com.luffy.billservice.interfaces;  import com.luffy.billservice.entity.User; import org.springframework.cloud.openfeign.FeignClient; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PathVariable;  @FeignClient(name=\u0026#34;user-service\u0026#34;) public interface UserServiceCli {   @GetMapping(\u0026#34;/user\u0026#34;)  public String getUserService();   @GetMapping(\u0026#34;/user/{id}\u0026#34;)  public User getUserInfo(@PathVariable(\u0026#34;id\u0026#34;) int id); } 拷贝User类到当前项目：\npackage com.luffy.billservice.entity;  public class User {  private int id;  private String name;  private int age;  private String sex;   public int getAge() {  return age;  }   public int getId() {  return id;  }   public String getName() {  return name;  }   public String getSex() {  return sex;  }   public void setAge(int age) {  this.age = age;  }   public void setId(int id) {  this.id = id;  }   public void setName(String name) {  this.name = name;  }   public void setSex(String sex) {  this.sex = sex;  } } 修改BillController\npackage com.luffy.billservice.controller;  import com.luffy.billservice.entity.User; import com.luffy.billservice.interfaces.UserServiceCli; import org.springframework.beans.factory.annotation.Autowired; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PathVariable; import org.springframework.web.bind.annotation.RestController;  @RestController public class BillController {   @Autowired  private UserServiceCli userServiceCli;    @GetMapping(\u0026#34;/bill/user\u0026#34;)  public String getUserInfo(){  return userServiceCli.getUserService();  }   @GetMapping(\u0026#34;/bill/user/{id}\u0026#34;)  public User getUserInfo(@PathVariable(\u0026#34;id\u0026#34;) int id){  return userServiceCli.getUserInfo(id);  //return restTemplate.getForObject(\u0026#34;http://USER-SERVICE/user/\u0026#34; + id, String.class);  } } CICD持续交付服务消费者 拷贝user-service的交付文件，替换如下：\n user-service -\u0026gt; bill-service 7000 -\u0026gt; 7001 INGRESS_USER_SERVICE -\u0026gt; INGRESS_BILL_SERVICE  $ kubectl -n dev edit configmap devops-config ... data:  INGRESS_MYBLOG: blog-dev.luffy.com  INGRESS_SPRINGBOOTDEMO: springboot-dev.luffy.com  INGRESS_USER_SERVICE: user-service-dev.luffy.com  INGRESS_BILL_SERVICE: user-service-dev.luffy.com  NAMESPACE: dev ... 创建develop分支，提交代码到gitlab仓库，验证持续交付\n前面主要讲解了下服务消费者如何利用原生、ribbon、fegin三种方式进行服务调用的，其实每种调用方式都是使用restTemplate来进行调用的，只是有些进行了增强，目的是使用起来更简单高效。\nHystrix 断路器 为什么需要断路器？\nA作为服务提供者，B为A的服务消费者，C和D是B的服务消费者。A不可用引起了B的不可用，并将不可用像滚雪球一样放大到C和D时，雪崩效应就形成了。\n因此，需要实现一种机制，可以做到自动监控服务状态并根据调用情况进行自动处理。\n  记录时间周期内服务调用失败次数\n  维护断路器的打开、关闭、半开三种状态\n  提供fallback机制\n  修改bill-service项目：\npom.xml\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-hystrix\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; application.xml\nfeign:  hystrix:  enabled: true 启动类添加注解@EnableCircuitBreaker\npackage com.luffy.billservice;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.circuitbreaker.EnableCircuitBreaker; import org.springframework.cloud.client.discovery.EnableDiscoveryClient; import org.springframework.cloud.openfeign.EnableFeignClients;  @SpringBootApplication @EnableDiscoveryClient @EnableFeignClients @EnableCircuitBreaker public class BillServiceApplication {   public static void main(String[] args) {  SpringApplication.run(BillServiceApplication.class, args);  }  } UserServiceCli.java package com.luffy.bill.interfaces;  import com.luffy.bill.entity.User; import org.springframework.cloud.openfeign.FeignClient; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.PathVariable;  @FeignClient(name=\u0026#34;user-service\u0026#34;, fallback = UserServiceFallbackImpl.class) public interface UserServiceCli {   @GetMapping(\u0026#34;/user\u0026#34;)  public String getUserService();   @GetMapping(\u0026#34;/user/{id}\u0026#34;)  public User getUserInfo(@PathVariable(\u0026#34;id\u0026#34;) int id); } UserServiceFallbackImpl.java package com.luffy.billservice.interfaces;  import com.luffy.billservice.entity.User; import org.springframework.stereotype.Component;  @Component(\u0026#34;fallback\u0026#34;) public class UserServiceFallbackImpl implements UserServiceCli{   @Override  public String getUserService() {  return \u0026#34;fallback user service\u0026#34;;  }   @Override  public User getUserInfo(int id) {  User user = new User();  user.setId(1);  user.setName(\u0026#34;feign-fallback\u0026#34;);  return user;  } } 停止user-service测试熔断及fallback。\nHystrix Dashboard 前面一章，我们讲解了如何整合Hystrix。而在实际情况下，使用了Hystrix的同时,还会对其进行实时的数据监控，反馈各类指标数据。今天我们就将讲解下Hystrix Dashboard和Turbine.其中Hystrix Dashboard是一款针对Hystrix进行实时监控的工具，通过Hystrix Dashboard我们可以在直观地看到各Hystrix Command的请求响应时间, 请求成功率等数据,监控单个实例内的指标情况。后者Turbine，能够将多个实例指标数据进行聚合的工具。\n在eureka注册中心处访问bill-service的服务actuator地址: http://192.168.136.1:7001/actuator/info\n若访问不了,需要添加如下内容:\n  为服务消费者bill-service的pom.xml添加依赖:\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   修改application.yml配置:\nmanagement:  endpoints:  web:  exposure:  include: \u0026#34;*\u0026#34;   访问http://localhost:9000/actuator/hystrix.stream 即可访问到断路器的执行状态，但是显示不太友好，因此需要dashboard。\n新建项目，hystrix-dashboard\n Hystrix-dashboard(仪表盘)是一款针对Hystrix进行实时监控的工具，通过Hystrix Dashboard我们可以在直观地看到各Hystrix Command的请求响应时间, 请求成功率等数据。\n pom.xml引入依赖包：\n\u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt;  \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;  \u0026lt;parent\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.3.6.RELEASE\u0026lt;/version\u0026gt;  \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt;  \u0026lt;/parent\u0026gt;  \u0026lt;groupId\u0026gt;com.luffy\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;hystrix-dashboard\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt;  \u0026lt;name\u0026gt;hystrix-dashboard\u0026lt;/name\u0026gt;  \u0026lt;description\u0026gt;hystrxi dashboard\u0026lt;/description\u0026gt;   \u0026lt;properties\u0026gt;  \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt;  \u0026lt;spring.cloud-version\u0026gt;Hoxton.SR9\u0026lt;/spring.cloud-version\u0026gt;  \u0026lt;/properties\u0026gt;   \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-hystrix-dashboard\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;  \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;  \u0026lt;exclusions\u0026gt;  \u0026lt;exclusion\u0026gt;  \u0026lt;groupId\u0026gt;org.junit.vintage\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;junit-vintage-engine\u0026lt;/artifactId\u0026gt;  \u0026lt;/exclusion\u0026gt;  \u0026lt;/exclusions\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;   \u0026lt;dependencyManagement\u0026gt;  \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${spring.cloud-version}\u0026lt;/version\u0026gt;  \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;  \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;  \u0026lt;/dependencyManagement\u0026gt;   \u0026lt;build\u0026gt;  \u0026lt;plugins\u0026gt;  \u0026lt;plugin\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt;  \u0026lt;/plugin\u0026gt;  \u0026lt;/plugins\u0026gt;  \u0026lt;/build\u0026gt;  \u0026lt;/project\u0026gt; 启动类加上@EnableHystrixDashboard注解：\npackage com.luffy.hystrixdashboard;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.netflix.hystrix.dashboard.EnableHystrixDashboard;  @SpringBootApplication @EnableHystrixDashboard public class HystrixDashboardApplication {   public static void main(String[] args) {  SpringApplication.run(HystrixDashboardApplication.class, args);  }  } application.yml\n#应用名称 server:  port: 9696  spring:  application:  name: hystrix-dashboard hystrix:  dashboard:  proxy-stream-allow-list: \u0026#34;*\u0026#34; 访问localhost:9696/hystrix\n  实心圆：它有颜色和大小之分，分别代表实例的监控程度和流量大小。如上图所示，它的健康度从绿色、黄色、橙色、红色递减。通过该实心圆的展示，我们就可以在大量的实例中快速的发现故障实例和高压力实例。\n  曲线：用来记录 2 分钟内流量的相对变化，我们可以通过它来观察到流量的上升和下降趋势。\n  其他一些数量指标如下图所示\n   提交代码到gitlab仓库\n Turbine hystrix只能实现单个微服务的监控，可是一般项目中是微服务是以集群的形式搭建，一个一个的监控不现实。而Turbine的原理是，建立一个turbine服务，并注册到eureka中，并发现eureka上的hystrix服务。通过配置turbine会自动收集所需hystrix的监控信息，最后通过dashboard展现，以达到集群监控的效果。\n简单来说，就是通过注册到注册中心，发现其他服务的hystrix服务，然后进行聚合数据，最后通过自身的端点输出到仪表盘上进行个性化展示。这我们就监控一个turbine应用即可，当有新增的应用加入时，我们只需要配置下turbine参数即可。\n微服务网关 为什么需要网关 在微服务框架中，每个对外服务都是独立部署的，对外的api或者服务地址都不是不尽相同的。对于内部而言，很简单，通过注册中心自动感知即可。但我们大部分情况下，服务都是提供给外部系统进行调用的，不可能同享一个注册中心。同时一般上内部的微服务都是在内网的，和外界是不连通的。而且，就算我们每个微服务对外开放，对于调用者而言，调用不同的服务的地址或者参数也是不尽相同的，这样就会造成消费者客户端的复杂性，同时想想，可能微服务可能是不同的技术栈实现的，有的是http、rpc或者websocket等等，也会进一步加大客户端的调用难度。所以，一般上都有会有个api网关，根据请求的url不同，路由到不同的服务上去，同时入口统一了，还能进行统一的身份鉴权、日志记录、分流等操作。\n网关的功能   减少api请求次数\n  限流\n  缓存\n  统一认证\n  降低微服务的复杂度\n  支持混合通信协议(前端只和api通信，其他的由网关调用)\n  \u0026hellip;\n  Zuul实践 新建模块，gateway-zuul,(spring cloud)\npom.xml中需要引入zuul和eureka服务发现的依赖\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-zuul\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; 启动类添加注解\npackage com.luffy.gateway;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient; import org.springframework.cloud.netflix.zuul.EnableZuulProxy;  @SpringBootApplication @EnableZuulProxy @EnableDiscoveryClient public class ZuulGatewayApplication {  public static void main(String[] args) {  SpringApplication.run(ZuulGatewayApplication.class, args);  } } 配置文件：\nserver:  port: 10000  spring:  application:  name: gateway-zuul  eureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@localhost:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:gateway-zuul} 启动后，访问：\nhttp://localhost:10000/bill-service/bill/user/1\nhttp://localhost:10000/user-service/user\n通过如下方式，配置短路径：\nzuul:  routes:  user-service: /users/**  bill-service:  path: /bill/**  service-id: bill-service http://localhost:10000/users/user/1  ---\u0026gt; http://localhost:7000/user/2 http://localhost:10000/user-service/user/1    http://localhost:10000/bill/service/user/2   ---\u0026gt;http://localhost:7001/service/user/2 http://localhost:10000/bill-service/service/user/2 zuul如何指定对外暴漏api的path，如：\n所有的api都是这样：http://zuul-host:zuul-port/apis/，可以添加zuul.prefix：/apis\n配置一下配置文件\nmanagement:  endpoints:  web:  exposure:  include: \u0026#34;*\u0026#34; 可以访问到zuul的route列表， http://localhost:10000/actuator/routes/ ，添加details可以访问到详细信息\n{ \u0026#34;/apis/users/**\u0026#34;: \u0026#34;user-service\u0026#34;, \u0026#34;/apis/bill/**\u0026#34;: \u0026#34;bill-service\u0026#34;, \u0026#34;/apis/bill-service/**\u0026#34;: \u0026#34;bill-service\u0026#34;, \u0026#34;/apis/user-service/**\u0026#34;: \u0026#34;user-service\u0026#34; }  提交代码到代码仓库\n 集中配置中心 Spring Cloud Config 配置中心提供了一个中心化的外部配置，默认使用git存储配置信息，这样就可以对配置信息进行版本管理。\n实践  创建代码仓库configure-repo，用于集中存储配置文件 创建项目config-server，用于接受各项目的连接，提供配置文件读取服务 修改user-service服务，验证通过config-server读取集中配置库中的配置文件  代码仓库：\n  新建gitlab项目，http://gitlab.luffy.com/luffy-spring-cloud/configure-repo.git\n  准备配置文件\nconfigs/common-dev.yml\ndatasource:  url: jdbc:mysql://mysql-dev:3306/  driverClassName: com.mysql.jdbc.Driver  username: xxx  password: xxxxxx luffy: city configs/user-service-dev.yml\nlogging:  level:  org.springframework.cloud: debug env: dev configs/common-test.yml\nenv: test   提交代码到master分支\n  新建项目，config-server\n修改pom.xml（springboot和springcloud的版本）\n  springboot 版本\n\u0026lt;version\u0026gt;2.3.6.RELEASE\u0026lt;/version\u0026gt;   spring-cloud 版本\n \u0026lt;properties\u0026gt;  \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt;  \u0026lt;spring-cloud.version\u0026gt;Hoxton.SR9\u0026lt;/spring-cloud.version\u0026gt;  \u0026lt;/properties\u0026gt;   添加config-server 的依赖\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-config-server\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   修改application.yml\nserver:  port: 8088  spring:  application:  name: config-server  profiles:  active: git  cloud:  config:  server:  git:  uri: http://gitlab.luffy.com/luffy-spring-cloud/configure-repo.git  username: ${GIT_USER:root}  password: ${GIT_PSW:1qaz2wsx}  default-label: master  search-paths: configs  #native:  # searchLocations: classpath:/configs/{profile} 修改启动类，添加注解\npackage com.luffy.configserver;  import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.config.server.EnableConfigServer;  @SpringBootApplication @EnableConfigServer public class ConfigServerApplication {   public static void main(String[] args) {  SpringApplication.run(ConfigServerApplication.class, args);  }  } 启动config-server,访问:\nhttp://localhost:8088/common/dev http://localhost:8088/user-service/dev 修改user-service服务，从config-server读取配置\n  添加使用统一配置中心的依赖：\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-config\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   新建bootstrap.yml，不能放在application.yml中，因为bootstrap的加载早于应用程序bean启动的加载，因此，删掉application.yml，直接使用bootstrap.yml\nserver:  port: 7000   eureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@localhost:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:user-service} spring:  application:  name: user-service  cloud:  config:  uri: http://localhost:8088  profile: dev  #当前读取dev环境的配置  name: user-service, common  # 从user-service-dev.yml,common-dev.yml中读取   新建ValueController.java\npackage com.luffy.userservice.controller;  import org.springframework.beans.factory.annotation.Value; import org.springframework.cloud.context.config.annotation.RefreshScope; import org.springframework.web.bind.annotation.GetMapping; import org.springframework.web.bind.annotation.RestController;  @RestController public class ValueController {   @Value(\u0026#34;${env}\u0026#34;)  private String env;   @Value(\u0026#34;${datasource.url}\u0026#34;)  private String datasource;   @Value(\u0026#34;${spring.application.name}\u0026#34;)  private String applicationName;   @GetMapping(\u0026#34;/value/env\u0026#34;)  public String getValueEnv(){  return \u0026#34;current env is \u0026#34; + env;  }   @GetMapping(\u0026#34;/value/application\u0026#34;)  public String getValueApplication(){  return \u0026#34;current env is \u0026#34; + applicationName;  }   @GetMapping(\u0026#34;/value/datasource\u0026#34;)  public Object getDatasource(){  return datasource;  }  }   访问如下页面进行验证\n$ localhost:7000/value/env $ localhost:7000/value/application $ localhost:7000/value/datasource   高可用 config-server多个实例，如何配置客户端？\n config-server 作为服务提供者，注册到eureka服务注册中心 user-service配置从注册中心获取config-server的服务  config-server注册到服务注册中心\n  pom.xml添加eureka依赖包\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   application.yml中连接服务注册中心\neureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@localhost:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:config-server}   启动类添加注解\n@EnableDiscoveryClient   修改user-service，从注册中心发现服务\n  修改bootstrap.yml\nserver:  port: 7000  spring:  cloud:  config:  profile: dev  discovery:  enabled: true  service-id: config-server  name: user-service, common  eureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@peer1:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:user-service}   客户端配置刷新 配置中心的配置变动后，客户端如何获取最新的配置。\n  修改ValueController.java，添加注解\n@RestController @RefreshScope public class ValueController   添加actuator包\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-actuator\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   开放显示所有管理接口\nmanagement:  endpoints:  web:  exposure:  include: \u0026#34;*\u0026#34;   重启user-service\n  修改configure-repo中的配置并提交，访问http://localhost:7000/value/env\n  执行刷新\n$ curl -XPOST http://localhost:7000/actuator/refresh   再次访问http://localhost:7000/value/env\n  调用链路追踪 介绍 服务追踪的追踪单元是从客户发起请求（request）抵达被追踪系统的边界开始，到被追踪系统向客户返回响应（response）为止的过程，称为一个 trace。每个 trace 中会调用若干个服务，为了记录调用了哪些服务，以及每次调用的消耗时间等信息，在每次调用服务时，埋入一个调用记录，称为一个 span。这样，若干个有序的 span 就组成了一个 trace。在系统向外界提供服务的过程中，会不断地有请求和响应发生，也就会不断生成 trace，把这些带有 span 的 trace 记录下来，就可以描绘出一幅系统的服务拓扑图。附带上 span 中的响应时间，以及请求成功与否等信息，就可以在发生问题的时候，找到异常的服务；根据历史数据，还可以从系统整体层面分析出哪里性能差，定位性能优化的目标。\nSpring Cloud Sleuth 为服务之间调用提供链路追踪。通过 Sleuth 可以很清楚的了解到一个服务请求经过了哪些服务，每个服务处理花费了多长。从而让我们可以很方便的理清各微服务间的调用关系。此外 Sleuth 可以帮助我们：\n 耗时分析: 通过 Sleuth 可以很方便的了解到每个采样请求的耗时，从而分析出哪些服务调用比较耗时; 链路优化: 对于调用比较频繁的服务，可以针对这些服务实施一些优化措施。 可视化错误: 对于程序未捕捉的异常，可以通过集成 Zipkin 服务界面上看到; Spring Cloud Sleuth 可以结合 Zipkin，将信息发送到 Zipkin，利用 Zipkin 的存储来存储信息，利用 Zipkin UI 来展示数据。  https://zipkin.io/pages/quickstart\n启动zipkin apiVersion: apps/v1 kind: Deployment metadata:  name: zipkin  namespace: dev spec:  replicas: 1  selector:  matchLabels:  app: zipkin  template:  metadata:  labels:  app: zipkin  spec:  containers:  - name: zipkin  image: openzipkin/zipkin:2.22  imagePullPolicy: IfNotPresent  ports:  - containerPort: 9411  resources:  requests:  memory: 400Mi  cpu: 50m  limits:  memory: 2Gi  cpu: 2000m --- apiVersion: v1 kind: Service metadata:  name: zipkin  namespace: dev spec:  ports:  - port: 9411  protocol: TCP  targetPort: 9411  selector:  app: zipkin  sessionAffinity: None  type: ClusterIP status:  loadBalancer: {} --- apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: zipkin  namespace: dev spec:  rules:  - host: zipkin.luffy.com  http:  paths:  - backend:  serviceName: zipkin  servicePort: 9411  path: / status:  loadBalancer: {} 实践 分别对bill-service和user-service进行改造：\npom.xml中添加：\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-zipkin\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt; application.yml\nspring:  zipkin:  base-url: http://zipkin.luffy.com  # zipkin服务器的地址  sender:  type: web  # 设置使用http的方式传输数据  sleuth:  sampler:  probability: 1 # 设置抽样采集为100%，默认为0.1，即10% logging:  level:  org.springframework.cloud: debug 访问zuul网关的接口http://localhost:10000/apis/bill-service/bill/user/2\n2020-11-14 19:28:49.274 DEBUG [bill-service,949aa3570daa1031,43ea952f1e5e36eb,true] 36852 \u0026mdash; [-user-service-6] c.s.i.w.c.f.TraceLoadBalancerFeignClient : Before send\nbill-service,949aa3570daa1031,43ea952f1e5e36eb,true 说明：\n bill-service： 服务名称 949aa3570daa1031： 是TranceId，一条链路中，只有一个TranceId 43ea952f1e5e36eb：则是spanId，链路中的基本工作单元id true：表示是否将数据输出到其他服务，true则会把信息输出到其他可视化的服务上观察  SpringBoot Admin监控 新建项目，springboot-admin\npom.xml \u0026lt;?xml version=\u0026#34;1.0\u0026#34; encoding=\u0026#34;UTF-8\u0026#34;?\u0026gt; \u0026lt;project xmlns=\u0026#34;http://maven.apache.org/POM/4.0.0\u0026#34; xmlns:xsi=\u0026#34;http://www.w3.org/2001/XMLSchema-instance\u0026#34;  xsi:schemaLocation=\u0026#34;http://maven.apache.org/POM/4.0.0 https://maven.apache.org/xsd/maven-4.0.0.xsd\u0026#34;\u0026gt;  \u0026lt;modelVersion\u0026gt;4.0.0\u0026lt;/modelVersion\u0026gt;  \u0026lt;parent\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-parent\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.3.6.RELEASE\u0026lt;/version\u0026gt;  \u0026lt;relativePath/\u0026gt; \u0026lt;!-- lookup parent from repository --\u0026gt;  \u0026lt;/parent\u0026gt;  \u0026lt;groupId\u0026gt;com.luffy\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;springboot-admin\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;0.0.1-SNAPSHOT\u0026lt;/version\u0026gt;  \u0026lt;name\u0026gt;springboot-admin\u0026lt;/name\u0026gt;  \u0026lt;description\u0026gt;Demo project for Spring Boot\u0026lt;/description\u0026gt;   \u0026lt;properties\u0026gt;  \u0026lt;java.version\u0026gt;1.8\u0026lt;/java.version\u0026gt;  \u0026lt;spring-cloud.version\u0026gt;Hoxton.SR9\u0026lt;/spring-cloud.version\u0026gt;  \u0026lt;/properties\u0026gt;   \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;de.codecentric\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-admin-starter-server\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.2.1\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter-netflix-eureka-client\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-starter\u0026lt;/artifactId\u0026gt;  \u0026lt;/dependency\u0026gt;   \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-starter-test\u0026lt;/artifactId\u0026gt;  \u0026lt;scope\u0026gt;test\u0026lt;/scope\u0026gt;  \u0026lt;exclusions\u0026gt;  \u0026lt;exclusion\u0026gt;  \u0026lt;groupId\u0026gt;org.junit.vintage\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;junit-vintage-engine\u0026lt;/artifactId\u0026gt;  \u0026lt;/exclusion\u0026gt;  \u0026lt;/exclusions\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;   \u0026lt;dependencyManagement\u0026gt;  \u0026lt;dependencies\u0026gt;  \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.cloud\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-cloud-dependencies\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;${spring-cloud.version}\u0026lt;/version\u0026gt;  \u0026lt;type\u0026gt;pom\u0026lt;/type\u0026gt;  \u0026lt;scope\u0026gt;import\u0026lt;/scope\u0026gt;  \u0026lt;/dependency\u0026gt;  \u0026lt;/dependencies\u0026gt;  \u0026lt;/dependencyManagement\u0026gt;   \u0026lt;build\u0026gt;  \u0026lt;plugins\u0026gt;  \u0026lt;plugin\u0026gt;  \u0026lt;groupId\u0026gt;org.springframework.boot\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-maven-plugin\u0026lt;/artifactId\u0026gt;  \u0026lt;/plugin\u0026gt;  \u0026lt;/plugins\u0026gt;  \u0026lt;/build\u0026gt;  \u0026lt;/project\u0026gt; 配置文件\nserver:  port: 8769  spring:  application:  name: springboot-admin  eureka:  client:  serviceUrl:  defaultZone: ${EUREKA_SERVER:http://admin:admin@localhost:8761/eureka/}  instance:  instance-id: ${eureka.instance.hostname}:${server.port}  prefer-ip-address: true  hostname: ${INSTANCE_HOSTNAME:springboot-admin} 启动类\npackage com.luffy.springbootadmin;  import de.codecentric.boot.admin.server.config.EnableAdminServer; import org.springframework.boot.SpringApplication; import org.springframework.boot.autoconfigure.SpringBootApplication; import org.springframework.cloud.client.discovery.EnableDiscoveryClient;  @SpringBootApplication @EnableDiscoveryClient @EnableAdminServer public class SpringbootAdminApplication {   public static void main(String[] args) {  SpringApplication.run(SpringbootAdminApplication.class, args);  }  } 客户端，所有注册到eureka的服务，添加依赖即可\n \u0026lt;dependency\u0026gt;  \u0026lt;groupId\u0026gt;de.codecentric\u0026lt;/groupId\u0026gt;  \u0026lt;artifactId\u0026gt;spring-boot-admin-starter-client\u0026lt;/artifactId\u0026gt;  \u0026lt;version\u0026gt;2.2.1\u0026lt;/version\u0026gt;  \u0026lt;/dependency\u0026gt; 小结   伴随着业务场景复杂度的提高，单体架构应用弊端显现，微服务的思想逐步盛行，微服务架构带来诸多便捷的同时，也带来了很多问题，最主要的是多个微服务的服务治理（服务发现、调用、负载均衡、跟踪）\n  为了解决服务治理问题，出现了微服务框架（Dubbo、Spring Cloud等）\n  Spring Cloud是一个大的生态，基于Java语言封装了一系列的工具，方便业务直接使用来解决上述服务治理相关的问题\n  Spring Cloud Netflix 体系下提供了eureka、ribbon、feign、hystrix、zuul等工具结合spring cloud sleuth合zipkin实现服务跟踪\n  SpringBoot是微服务的开发框架，通过maven与Spring Cloud生态中的组件集成，极大方便了java应用程序的交付\n  https://blog.csdn.net/smallsunl/article/details/78778790\n问题：\n 无论是Dubbo还是SpringCloud，均属于Java语言体系下的产物，跨语言没法共用，同时，通过走了一遍内部集成的过程，可以清楚的发现，服务治理过程中，各模块的集成，均需要对原始业务逻辑形成侵入。 在kubernetes的生态下，已经与生俱来带了很多好用的功能（自动服务发现与负载均衡） 服务治理的根本其实是网络节点通信的治理，因此，以istio为代表的第二代服务治理平台开始逐步兴起  ","permalink":"https://iblog.zone/archives/springboot%E4%B8%8Espringcloud%E5%BE%AE%E6%9C%8D%E5%8A%A1%E9%A1%B9%E7%9B%AE%E4%BA%A4%E4%BB%98/","summary":"Spring Cloud微服务项目交付 微服务扫盲篇 微服务并没有一个官方的定义，想要直接描述微服务比较困难，我们可以通过对比传统WEB应用，来理解什么是微服务。\n单体应用架构 如下是传统打车软件架构图：\n这种单体应用比较适合于小项目，优点是：\n 开发简单直接，集中式管理 基本不会重复开发 功能都在本地，没有分布式的管理开销和调用开销  当然它的缺点也十分明显，特别对于互联网公司来说：\n 开发效率低：所有的开发在一个项目改代码，递交代码相互等待，代码冲突不断 代码维护难：代码功能耦合在一起，新人不知道何从下手 部署不灵活：构建时间长，任何小修改必须重新构建整个项目，这个过程往往很长 稳定性不高：一个微不足道的小问题，可以导致整个应用挂掉 扩展性不够：无法满足高并发情况下的业务需求  微服务应用架构 微服务架构的设计思路不是开发一个巨大的单体式应用，而是将应用分解为小的、互相连接的微服务。一个微服务完成某个特定功能，比如乘客管理和下单管理等。每个微服务都有自己的业务逻辑和适配器。一些微服务还会提供API接口给其他微服务和应用客户端使用。\n比如，前面描述的系统可被分解为：\n每个业务逻辑都被分解为一个微服务，微服务之间通过REST API通信。一些微服务也会向终端用户或客户端开发API接口。但通常情况下，这些客户端并不能直接访问后台微服务，而是通过API Gateway来传递请求。API Gateway一般负责服务路由、负载均衡、缓存、访问控制和鉴权等任务。\n微服务架构优点：\n 解决了复杂性问题。它将单体应用分解为一组服务。虽然功能总量不变，但应用程序已被分解为可管理的模块或服务 体系结构使得每个服务都可以由专注于此服务的团队独立开发。只要符合服务API契约，开发人员可以自由选择开发技术。这就意味着开发人员可以采用新技术编写或重构服务，由于服务相对较小，所以这并不会对整体应用造成太大影响 微服务架构可以使每个微服务独立部署。这些更改可以在测试通过后立即部署。所以微服务架构也使得CI／CD成为可能  微服务架构问题及挑战 微服务的一个主要缺点是微服务的分布式特点带来的复杂性。开发人员需要基于RPC或者消息实现微服务之间的调用和通信，而这就使得服务之间的发现、服务调用链的跟踪和质量问题变得的相当棘手。\n 微服务的一大挑战是跨多个服务的更改  比如在传统单体应用中，若有A、B、C三个服务需要更改，A依赖B，B依赖C。我们只需更改相应的模块，然后一次性部署即可。 在微服务架构中，我们需要仔细规划和协调每个服务的变更部署。我们需要先更新C，然后更新B，最后更新A。   部署基于微服务的应用也要复杂得多  单体应用可以简单的部署在一组相同的服务器上，然后前端使用负载均衡即可。 微服务由不同的大量服务构成。每种服务可能拥有自己的配置、应用实例数量以及基础服务地址。这里就需要不同的配置、部署、扩展和监控组件。此外，我们还需要服务发现机制，以便服务可以发现与其通信的其他服务的地址    以上问题和挑战可大体概括为：\n API Gateway 服务间调用 服务发现 服务容错 服务部署 数据调用  https://www.kancloud.cn/owenwangwen/open-capacity-platform/1480155，自助餐吃吃喝喝，竟然秒懂微服务\n微服务框架 如何应对上述挑战，出现了如下微服务领域的框架：\n  Spring Cloud（各个微服务基于Spring Boot实现）\n  Dubbo","title":"SpringBoot与SpringCloud微服务项目交付"},{"content":"基于sharedLibrary进行CI/CD流程的优化 由于公司内部项目众多，大量的项目使用同一套流程做CICD\n 那么势必会存在大量的重复代码 一旦某个公共的地方需要做调整，每个项目都需要修改  因此本章主要通过使用groovy实现Jenkins的sharedLibrary的开发，以提取项目在CICD实践过程中的公共逻辑，提供一系列的流程的接口供公司内各项目调用。\n开发完成后，对项目进行Jenkinsfile的改造，最后仅需通过简单的Jenkinsfile的配置，即可优雅的完成CICD流程的整个过程，此方式已在大型企业内部落地应用。\nLibrary工作模式 由于流水线被组织中越来越多的项目所采用，常见的模式很可能会出现。 在多个项目之间共享流水线有助于减少冗余并保持代码 \u0026ldquo;DRY\u0026rdquo;。\n流水线支持引用 \u0026ldquo;共享库\u0026rdquo; ，可以在外部源代码控制仓库中定义并加载到现有的流水线中。\n@Library(\u0026#39;my-shared-library\u0026#39;) _ 在实际运行过程中，会把library中定义的groovy功能添加到构建目录中：\n/var/jenkins_home/jobs/test-maven-build/branches/feature-CDN-2904.cm507o/builds/2/libs/my-shared-library/vars/devops.groovy 使用library后，Jenkinsfile大致的样子如下：\n@Library(\u0026#39;my-shared-library\u0026#39;) _  ...  stages {  stage(\u0026#39;build image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  devops.buildImage(\u0026#34;Dockerfile\u0026#34;,\u0026#34;172.21.51.67:5000/demo:latest\u0026#34;)  }  }  }  }   post {  success {  script {  container(\u0026#39;tools\u0026#39;) {  devops.notificationSuccess(\u0026#34;dingTalk\u0026#34;)  }  }  }  } ... 开发环境搭建 补录章节：Groovy及SpringBoot、SpringCloud都会使用\n java groovy intelliJ idea  下载安装包 链接：https://pan.baidu.com/s/1B-bg2_IsB8dU7_62IEtnTg 提取码：wx6j\n安装java 安装路径：D:\\software\\jdk\n环境变量：\n JAVA_HOME D:\\software\\jdk CLASSPATH .;%JAVA_HOME%\\lib\\dt.jar;%JAVA_HOME%\\lib\\tools.jar; PATH %JAVA_HOME%\\bin  安装groovy 解压路径：D:\\software\\groovy-3.0.2\n环境变量：\n GROOVY_PATH D:\\software\\groovy-3.0.2 PATH D:\\software\\groovy-3.0.2\\bin  安装idea 安装路径：D:\\software\\IntelliJ IDEA 2019.2.3\n新建项目测试\nLibrary代码结构介绍 共享库的目录结构如下:\n(root) +- src # Groovy source files | +- org | +- foo | +- Bar.groovy # for org.foo.Bar class +- vars | +- foo.groovy # for global \u0026#39;foo\u0026#39; variable | +- foo.txt # help for \u0026#39;foo\u0026#39; variable src 目录应该看起来像标准的 Java 源目录结构。当执行流水线时，该目录被添加到类路径下。\nvars 目录定义可从流水线访问的全局变量的脚本。 每个 *.groovy 文件的基名应该是一个 Groovy (~ Java) 标识符, 通常是 camelCased。\nGroovy基本语法介绍 新建Groovy项目\n  变量\n使用数据类型的本地语法，或者使用def关键字\n// Defining a variable in lowercase int x = 5;  // Defining a variable in uppercase int X = 6;  // Defining a variable with the underscore in it\u0026#39;s name def _Name = \u0026#34;Joe\u0026#34;;  println(x); println(X); println(_Name);   方法\n  调用本地方法\ndef sum(int a, int b){  return a + b }  println(sum(1,2))   调用类中的方法\n# Hello.groovy package demo  def sayHi(String content) {  return (\u0026#34;hi, \u0026#34; + content) }    # Demo.groovy import demo.Hello  def demo() {  return new Hello().sayHi(\u0026#34;devops\u0026#34;) } println(demo())    # 级联调用 # Hello.groovy package demo  def init(String content) {  this.content = content  return this }  def sayHi() {  println(\u0026#34;hi, \u0026#34; + this.content)  return this }  def sayBye() {  println(\u0026#34;bye \u0026#34; + this.content) }   # Demo.groovy import demo.Hello  def demo() {  new Hello().init(\u0026#34;devops\u0026#34;).sayHi().sayBye() }  demo()     异常捕获\ndef exceptionDemo(){  try {  def val = 10 / 0  println(val)  }catch(Exception e) {  println(e.toString())  throw e  } } exceptionDemo()   计时器与循环\nimport groovy.time.TimeCategory   use( TimeCategory ) {  def endTime = TimeCategory.plus(new Date(), TimeCategory.getSeconds(15))  def counter = 0  while(true) {  println(counter++)  sleep(1000)  if (new Date() \u0026gt;= endTime) {  println(\u0026#34;done\u0026#34;)  break  }  } }   解析yaml文件\nimport org.yaml.snakeyaml.Yaml  def readYaml(){  def content = new File(\u0026#39;myblog.yaml\u0026#39;).text  Yaml parser = new Yaml()  def data = parser.load(content)  def kind = data[\u0026#34;kind\u0026#34;]  def name = data[\u0026#34;metadata\u0026#34;][\u0026#34;name\u0026#34;]  println(kind)  println(name) } readYaml()   library与Jenkins集成 先来看一下如何使用shared library实现最简单的helloworld输出功能，来理清楚使用shared library的流程。\nHello.groovy package com.luffy.devops  /** * @author Yongxin * @version v0.1 */  /** * say hello * @param content */ def hello(String content) {  this.content = content  return this }   def sayHi() {  echo \u0026#34;Hi, ${this.content},how are you?\u0026#34;  return this }  def answer() {  echo \u0026#34;${this.content}: fine, thank you, and you?\u0026#34;  return this }  def sayBye() {  echo \u0026#34;i am fine too , ${this.content}, Bye!\u0026#34;  return this } 在gitlab创建项目，把library代码推送到镜像仓库。\n配置Jenkins [系统管理] -\u0026gt; [系统设置] -\u0026gt; [ Global Pipeline Libraries ]\n Library Name：luffy-devops Default Version：master Source Code Management：Git  Jenkinsfile中引用 jenkins/pipelines/p11.yaml\n@Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}   stages {  stage(\u0026#39;hello-devops\u0026#39;) {  steps {  script {  devops.hello(\u0026#34;树哥\u0026#34;).sayHi().answer().sayBye()  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 创建vars/devops.groovy\nimport com.luffy.devops.Hello  def hello(String content) {  return new Hello().hello(content) } library集成镜像构建及推送 需要实现的逻辑点：\n docker build，docker push，docker login 账户密码，jenkins凭据，（library中获取凭据内容）， docker login 172.21.51.67:5000 try catch  镜像构建逻辑实现 devops.groovy\n/** * * @param repo, 172.21.51.67:5000/demo/myblog/xxx/ * @param tag, v1.0 * @param dockerfile * @param credentialsId * @param context */ def docker(String repo, String tag, String credentialsId, String dockerfile=\u0026#34;Dockerfile\u0026#34;, String context=\u0026#34;.\u0026#34;) {  return new Docker().docker(repo, tag, credentialsId, dockerfile, context) } Docker.groovy\n逻辑中需要注意的点：\n 构建和推送镜像，需要登录仓库（需要认证） 构建成功或者失败，需要将结果推给gitlab端 为了将构建过程推送到钉钉消息中，需要将构建信息统一收集  package com.luffy.devops  /**  *  * @param repo  * @param tag  * @param credentialsId  * @param dockerfile  * @param context  * @return  */ def docker(String repo, String tag, String credentialsId, String dockerfile=\u0026#34;Dockerfile\u0026#34;, String context=\u0026#34;.\u0026#34;){  this.repo = repo  this.tag = tag  this.dockerfile = dockerfile  this.credentialsId = credentialsId  this.context = context  this.fullAddress = \u0026#34;${this.repo}:${this.tag}\u0026#34;  this.isLoggedIn = false  return this }   /**  * build image  * @return  */ def build() {  this.login()  retry(3) {  try {  sh \u0026#34;docker build ${this.context} -t ${this.fullAddress} -f ${this.dockerfile} \u0026#34;  }catch (Exception exc) {  throw exc  }  return this  } }   /**  * push image  * @return  */ def push() {  this.login()  retry(3) {  try {  sh \u0026#34;docker push ${this.fullAddress}\u0026#34;  }catch (Exception exc) {  throw exc  }  }  return this }  /**  * docker registry login  * @return  */ def login() {  if(this.isLoggedIn || credentialsId == \u0026#34;\u0026#34;){  return this  }  // docker login  withCredentials([usernamePassword(credentialsId: this.credentialsId, usernameVariable: \u0026#39;USERNAME\u0026#39;, passwordVariable: \u0026#39;PASSWORD\u0026#39;)]) {  def regs = this.getRegistry()  retry(3) {  try {  sh \u0026#34;docker login ${regs} -u $USERNAME -p $PASSWORD\u0026#34;  } catch (Exception exc) {  echo \u0026#34;docker login err, \u0026#34; + exc.toString()  }  }  }  this.isLoggedIn = true;  return this; }  /**  * get registry server  * @return  */ def getRegistry(){  def sp = this.repo.split(\u0026#34;/\u0026#34;)  if (sp.size() \u0026gt; 1) {  return sp[0]  }  return this.repo } Jenkinsfile\n需要先在Jenkins端创建仓库登录凭据credential-registry\n@Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}  options { \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/demo/myblog\u0026#34;  IMAGE_CREDENTIAL = \u0026#34;credential-registry\u0026#34;  }  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;docker-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.docker(  \u0026#34;${IMAGE_REPO}\u0026#34;,  \u0026#34;${GIT_COMMIT}\u0026#34;,  IMAGE_CREDENTIAL  ).build().push()  }  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  }  } } 丰富构建通知逻辑 目前的构建镜像逻辑中缺少如下内容：\n try逻辑中，若发生异常，是否该把异常抛出  若直接抛出异常可能会导致多次重复的异常信息 若不抛出，则如果未构建成功镜像，流水线感知不到错误   通知gitlab端构建任务及状态 构建通知格式  需要针对上述问题，做出优化\n  优化try逻辑\ndef build() {  this.login()  def isSuccess = false  def errMsg  retry(3) {  try {  sh \u0026#34;docker build ${this.context} -t ${this.fullAddress} -f ${this.dockerfile}\u0026#34;  isSuccess = true  }catch (Exception err) {  //ignore  errMsg = err.toString()  }  // check if build success  if(isSuccess){  //todo  }else {  // throw exception，aborted pipeline  error errMsg  }  return this  } }   通知gitlab端构建任务及状态\ndef build() {  this.login()  def isSuccess = false  def errMsg = \u0026#34;\u0026#34;  retry(3) {  try {  sh \u0026#34;docker build ${this.context} -t ${this.fullAddress} -f ${this.dockerfile} \u0026#34;  isSuccess = true  }catch (Exception err) {  //ignore  errMsg = err.toString()  }  // check if build success  def stage = env.STAGE_NAME + \u0026#39;-build\u0026#39;  if(isSuccess){  updateGitlabCommitStatus(name: \u0026#39;${stage}\u0026#39;, state: \u0026#39;success\u0026#39;)  }else {  updateGitlabCommitStatus(name: \u0026#39;${stage}\u0026#39;, state: \u0026#39;failed\u0026#39;)  // throw exception，aborted pipeline  error errMsg  }   return this  } }   钉钉消息通知格式\n由于每个stage都需要构建通知任务，因此抽成公共的逻辑，为各stage调用\nBuildMessage.groovy\npackage com.luffy.devops  def updateBuildMessage(String source, String add) {  if(!source){  source = \u0026#34;\u0026#34;  }  env.BUILD_TASKS = source + add + \u0026#34;\\n \\n\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026#34;  return env.BUILD_TASKS } Docker.groovy 中调用\ndef getObject(String repo, String tag, String credentialsId, String dockerfile=\u0026#34;Dockerfile\u0026#34;, String context=\u0026#34;.\u0026#34;){ \t...  this.msg = new BuildMessage()  return this }   ...  def build() { ...  // check if build success  def stage = env.STAGE_NAME + \u0026#39;-build\u0026#39;  if(isSuccess){  updateGitlabCommitStatus(name: \u0026#39;${stage}\u0026#39;, state: \u0026#39;success\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${stage} OK... √\u0026#34;)  }else {  updateGitlabCommitStatus(name: \u0026#39;${stage}\u0026#39;, state: \u0026#39;failed\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${stage} Failed... x\u0026#34;)  // throw exception，aborted pipeline  error errMsg  }   return this  } }   使用Jenkinsfile来验证上述修改是否正确：\n@Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}  options { \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/demo/myblog\u0026#34;  IMAGE_CREDENTIAL = \u0026#34;credential-registry\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  }  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;git-log\u0026#39;) {  steps {  script{  sh \u0026#34;git log --oneline -n 1 \u0026gt; gitlog.file\u0026#34;  env.GIT_LOG = readFile(\u0026#34;gitlog.file\u0026#34;).trim()  }  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.docker(  \u0026#34;${IMAGE_REPO}\u0026#34;,  \u0026#34;${GIT_COMMIT}\u0026#34;,  IMAGE_CREDENTIAL  ).build().push()  }  }  }  }  }  post {  success {  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😄👍 构建成功 👍😄 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${env.BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  }  } } 接下来需要将push和login方法做同样的改造\n最终的Docker.groovy文件为：\npackage com.luffy.devops  /**  *  * @param repo  * @param tag  * @param credentialsId  * @param dockerfile  * @param context  * @return  */ def docker(String repo, String tag, String credentialsId, String dockerfile=\u0026#34;Dockerfile\u0026#34;, String context=\u0026#34;.\u0026#34;){  this.repo = repo  this.tag = tag  this.dockerfile = dockerfile  this.credentialsId = credentialsId  this.context = context  this.fullAddress = \u0026#34;${this.repo}:${this.tag}\u0026#34;  this.isLoggedIn = false  this.msg = new BuildMessage()  return this }   /**  * build image  * @return  */ def build() {  this.login()  def isSuccess = false  def errMsg = \u0026#34;\u0026#34;  retry(3) {  try {  sh \u0026#34;docker build ${this.context} -t ${this.fullAddress} -f ${this.dockerfile} \u0026#34;  isSuccess = true  }catch (Exception err) {  //ignore  errMsg = err.toString()  }  // check if build success  def stage = env.STAGE_NAME + \u0026#39;-build\u0026#39;  if(isSuccess){  updateGitlabCommitStatus(name: \u0026#34;${stage}\u0026#34;, state: \u0026#39;success\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${stage} OK... √\u0026#34;)  }else {  updateGitlabCommitStatus(name: \u0026#34;${stage}\u0026#34;, state: \u0026#39;failed\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${stage} Failed... x\u0026#34;)  // throw exception，aborted pipeline  error errMsg  }   return this  } }   /**  * push image  * @return  */ def push() {  this.login()  def isSuccess = false  def errMsg = \u0026#34;\u0026#34;  retry(3) {  try {  sh \u0026#34;docker push ${this.fullAddress}\u0026#34;  isSuccess = true  }catch (Exception err) {  //ignore  errMsg = err.toString()  }  }  // check if build success  def stage = env.STAGE_NAME + \u0026#39;-push\u0026#39;  if(isSuccess){  updateGitlabCommitStatus(name: \u0026#34;${stage}\u0026#34;, state: \u0026#39;success\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${stage} OK... √\u0026#34;)  }else {  updateGitlabCommitStatus(name: \u0026#34;${stage}\u0026#34;, state: \u0026#39;failed\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${stage} Failed... x\u0026#34;)  // throw exception，aborted pipeline  error errMsg  }  return this }  /**  * docker registry login  * @return  */ def login() {  if(this.isLoggedIn || credentialsId == \u0026#34;\u0026#34;){  return this  }  // docker login  withCredentials([usernamePassword(credentialsId: this.credentialsId, usernameVariable: \u0026#39;USERNAME\u0026#39;, passwordVariable: \u0026#39;PASSWORD\u0026#39;)]) {  def regs = this.getRegistry()  retry(3) {  try {  sh \u0026#34;docker login ${regs} -u $USERNAME -p $PASSWORD\u0026#34;  } catch (Exception ignored) {  echo \u0026#34;docker login err, ${ignored.toString()}\u0026#34;  }  }  }  this.isLoggedIn = true;  return this; }  /**  * get registry server  * @return  */ def getRegistry(){  def sp = this.repo.split(\u0026#34;/\u0026#34;)  if (sp.size() \u0026gt; 1) {  return sp[0]  }  return this.repo } 再次测试构建\nlibrary集成k8s服务部署 library实现部署简单版 devops.groovy\n/**  * kubernetes deployer  * @param resourcePath  */ def deploy(String resourcePath){  return new Deploy().init(resourcePath) } 新增Deploy.groovy\npackage com.luffy.devops  def init(String resourcePath){  this.resourcePath = resourcePath  this.msg = new BuildMessage()  return this }   def start(){  try{  //env.CURRENT_IMAGE用来存储当前构建的镜像地址，需要在Docker.groovy中设置值  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${env.CURRENT_IMAGE}#g\u0026#39; ${this.resourcePath}/*\u0026#34;  sh \u0026#34;kubectl apply -f ${this.resourcePath}\u0026#34;  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${env.stage_name} OK... √\u0026#34;)  } catch (Exception exc){  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;failed\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${env.stage_name} fail... √\u0026#34;)  throw exc  } } 修改Docker.groovy\ndef push() {  this.login()  def isSuccess = false  def errMsg = \u0026#34;\u0026#34;  retry(3) {  try {  sh \u0026#34;docker push ${this.fullAddress}\u0026#34;  //把当前推送的镜像地址记录在环境变量中  env.CURRENT_IMAGE = this.fullAddress  isSuccess = true  }catch (Exception err) {  //ignore  errMsg = err.toString()  } Jenkinsfile 中添加如下部分：\n stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.deploy(\u0026#34;deploy\u0026#34;).start()  }  }  }  } library实现自动部署优化版 简单版本最明显的问题就是无法检测部署后的Pod状态，如果想做集成测试，通常要等到最新版本的Pod启动后再开始。因此有必要在部署的时候检测Pod是否正常运行。\n比如要去检查myblog应用的pod是否部署正常，人工检查的大致步骤：\n  kubectl -n luffy get pod，查看pod列表\n  找到列表中带有myblog关键字的running的pod\n  查看上述running pod数，是否和myblog的deployment中定义的replicas副本数一致\n  若一致，则检查结束，若不一致，可能稍等几秒钟，再次执行相同的检查操作\n  如果5分钟了还没有检查通过，则大概率是pod有问题，通过查看日志进一步排查\n  如何通过library代码实现上述过程：\n  library如何获取myblog的pod列表？\n  首先要知道本次部署的是哪个workload，因此需要调用者传递workload的yaml文件路径\n  library解析workload.yaml文件，找到如下值：\n pod所在的namespace pod中使用的labels标签    使用如下命令查找该workload关联的pod\n$ kubectl -n \u0026lt;namespace\u0026gt; get po -l \u0026lt;key1=value1\u0026gt; -l \u0026lt;key2=value2\u0026gt;  # 如查找myblog的pod $ kubectl -n luffy get po -l app=myblog     如何确定步骤1中的pod的状态？\n# 或者可以直接进行提取状态 $ kubectl -n luffy get po -l app=myblog -ojsonpath=\u0026#39;{.items[0].status.phase}\u0026#39;  # 以json数组的形式存储 $ kubectl -n luffy get po -l app=myblog -o json   如何检测所有的副本数都是正常的？\n# 以json数组的形式存储 $ kubectl -n luffy get po -l app=myblog -o json  # 遍历数组，检测每一个pod查看是否均正常（terminating和evicted除外）   如何实现在5分钟的时间内，若pod状态符合预期，则退出检测循环，若不符合预期则继续检测\nuse( TimeCategory ) {  def endTime = TimeCategory.plus(new Date(), TimeCategory.getMinutes(timeoutMinutes,5))  while (true) {  if (new Date() \u0026gt;= endTime) {  //超时了，则宣告pod状态不对  updateGitlabCommitStatus(name: \u0026#39;deploy\u0026#39;, state: \u0026#39;failed\u0026#39;)  throw new Exception(\u0026#34;deployment timed out...\u0026#34;)  }  //循环检测当前deployment下的pod的状态  try {  if (this.isDeploymentReady()) {  readyCount++  if(readyCount \u0026gt; 5){  updateGitlabCommitStatus(name: \u0026#39;deploy\u0026#39;, state: \u0026#39;success\u0026#39;)  break;  }  }else {  readyCount = 0  }catch (Exception exc){  echo exc.toString()  }  //每次检测若不满足所有pod均正常，则sleep 5秒钟后继续检测  sleep(5)  }  }   devops.groovy\n通过添加参数 watch来控制是否在pipeline中观察pod的运行状态\n/** * * @param resourcePath * @param watch * @param workloadFilePath * @return */ def deploy(String resourcePath, Boolean watch = true, String workloadFilePath){  return new Deploy().init(resourcePath, watch, workloadFilePath) } 完整版的Deploy.groovy\npackage com.luffy.devops  import org.yaml.snakeyaml.Yaml import groovy.json.JsonSlurperClassic import groovy.time.TimeCategory     def init(String resourcePath, Boolean watch, String workloadFilePath) {  this.resourcePath = resourcePath  this.msg = new BuildMessage()  this.watch = watch  this.workloadFilePath = workloadFilePath  if(!resourcePath \u0026amp;\u0026amp; !workloadFilePath){  throw Exception(\u0026#34;illegal resource path\u0026#34;)  }  return this }   def start(){  try{  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${env.CURRENT_IMAGE}#g\u0026#39; ${this.resourcePath}/*\u0026#34;  sh \u0026#34;kubectl apply -f ${this.resourcePath}\u0026#34;  } catch (Exception exc){  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;failed\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${env.stage_name} fail... √\u0026#34;)  throw exc  }   if (this.watch) {   // 初始化workload文件  initWorkload()  String namespace = this.workloadNamespace  String name = env.workloadName  if(env.workloadType.toLowerCase() == \u0026#34;deployment\u0026#34;){  echo \u0026#34;begin watch pod status from deployment ${env.workloadName}...\u0026#34;  monitorDeployment(namespace, name)  }else {  //todo  echo \u0026#34;workload type ${env.workloadType} does not support for now...\u0026#34;  }   }else {  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${env.STAGE_NAME} OK... √\u0026#34;)  }  }  def initWorkload() {  try {  def content = readFile this.workloadFilePath  Yaml parser = new Yaml()  def data = parser.load(content)  def kind = data[\u0026#34;kind\u0026#34;]  if (!kind) {  throw Exception(\u0026#34;workload file ${kind} illegal, will exit pipeline!\u0026#34;)  }  env.workloadType = kind  echo \u0026#34;${data}\u0026#34;  this.workloadNamespace = data[\u0026#34;metadata\u0026#34;][\u0026#34;namespace\u0026#34;]  if (!this.workloadNamespace){  this.workloadNamespace = \u0026#34;default\u0026#34;  }  env.workloadName = data[\u0026#34;metadata\u0026#34;][\u0026#34;name\u0026#34;]   } catch (Exception exc) {  echo \u0026#34;failed to readFile ${this.workloadFilePath},exception: ${exc}.\u0026#34;  throw exc  } }  /** * * @param namespace * @param name * @param timeoutMinutes * @param sleepTime * @return */ def monitorDeployment(String namespace, String name, int timeoutMinutes = 5, sleepTime = 3) {  def readyCount = 0  def readyTarget = 3  use( TimeCategory ) {  def endTime = TimeCategory.plus(new Date(), TimeCategory.getMinutes(timeoutMinutes))  def lastRolling  while (true) {  // checking timeout  if (new Date() \u0026gt;= endTime) {  echo \u0026#34;timeout, printing logs...\u0026#34;  this.printContainerLogs(lastRolling)  updateGitlabCommitStatus(name: \u0026#39;deploy\u0026#39;, state: \u0026#39;failed\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${env.STAGE_NAME} Failed... x\u0026#34;)  throw new Exception(\u0026#34;deployment timed out...\u0026#34;)  }  // checking deployment status  try {  def rolling = this.getResource(namespace, name, \u0026#34;deployment\u0026#34;)  lastRolling = rolling  if (this.isDeploymentReady(rolling)) {  readyCount++  echo \u0026#34;ready total count: ${readyCount}\u0026#34;  if (readyCount \u0026gt;= readyTarget) {  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${env.STAGE_NAME} OK... √\u0026#34;)  break  }   } else {  readyCount = 0  echo \u0026#34;reseting ready total count: ${readyCount}，print pods event logs\u0026#34;  this.printContainerLogs(lastRolling)  sh \u0026#34;kubectl get pod -n ${namespace} -o wide\u0026#34;  }  } catch (Exception exc) {  updateGitlabCommitStatus(name: \u0026#39;deploy\u0026#39;, state: \u0026#39;failed\u0026#39;)  this.msg.updateBuildMessage(env.BUILD_RESULT, \u0026#34;${env.STAGE_NAME} Failed... ×\u0026#34;)  echo \u0026#34;error: ${exc}\u0026#34;  }  sleep(sleepTime)  }  }  return this }  def getResource(String namespace = \u0026#34;default\u0026#34;, String name, String kind=\u0026#34;deployment\u0026#34;) {  sh \u0026#34;kubectl get ${kind} -n ${namespace} ${name} -o json \u0026gt; ${namespace}-${name}-yaml.yml\u0026#34;  def jsonStr = readFile \u0026#34;${namespace}-${name}-yaml.yml\u0026#34;  def jsonSlurper = new JsonSlurperClassic()  def jsonObj = jsonSlurper.parseText(jsonStr)  return jsonObj }   def printContainerLogs(deployJson) {  if (deployJson == null) {  return;  }  def namespace = deployJson.metadata.namespace  def name = deployJson.metadata.name  def labels=\u0026#34;\u0026#34;  deployJson.spec.template.metadata.labels.each { k, v -\u0026gt;  labels = \u0026#34;${labels} -l=${k}=${v}\u0026#34;  }  sh \u0026#34;kubectl describe pods -n ${namespace} ${labels}\u0026#34; }  def isDeploymentReady(deployJson) {  def status = deployJson.status  def replicas = status.replicas  def unavailable = status[\u0026#39;unavailableReplicas\u0026#39;]  def ready = status[\u0026#39;readyReplicas\u0026#39;]  if (unavailable != null) {  return false  }  def deployReady = (ready != null \u0026amp;\u0026amp; ready == replicas)  // get pod information  if (deployJson.spec.template.metadata != null \u0026amp;\u0026amp; deployReady) {  if (deployJson.spec.template.metadata.labels != null) {  def labels=\u0026#34;\u0026#34;  def namespace = deployJson.metadata.namespace  def name = deployJson.metadata.name  deployJson.spec.template.metadata.labels.each { k, v -\u0026gt;  labels = \u0026#34;${labels} -l=${k}=${v}\u0026#34;  }  if (labels != \u0026#34;\u0026#34;) {  sh \u0026#34;kubectl get pods -n ${namespace} ${labels} -o json \u0026gt; ${namespace}-${name}-json.json\u0026#34;  def jsonStr = readFile \u0026#34;${namespace}-${name}-json.json\u0026#34;  def jsonSlurper = new JsonSlurperClassic()  def jsonObj = jsonSlurper.parseText(jsonStr)  def totalCount = 0  def readyCount = 0  jsonObj.items.each { k, v -\u0026gt;  echo \u0026#34;pod phase ${k.status.phase}\u0026#34;  if (k.status.phase != \u0026#34;Terminating\u0026#34; \u0026amp;\u0026amp; k.status.phase != \u0026#34;Evicted\u0026#34;) {  totalCount++;  if (k.status.phase == \u0026#34;Running\u0026#34;) {  readyCount++;  }  }  }  echo \u0026#34;Pod running count ${totalCount} == ${readyCount}\u0026#34;  return totalCount \u0026gt; 0 \u0026amp;\u0026amp; totalCount == readyCount  }  }  }  return deployReady } 修改Jenkinsfile 调用部分：\n stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.deploy(\u0026#34;deploy\u0026#34;, true, \u0026#34;deploy/deployment.yaml\u0026#34;).start()  }  }  }  } library实现即时消息推送 实现消息通知 由于发送消息通知属于通用的功能，因此有必要把消息通知抽象成为通用的功能。\ndevops.groovy\n/**  * notificationSuccess  * @param project  * @param receiver  * @param credentialsId  * @param title  * @return  */ def notificationSuccess(String project, String receiver=\u0026#34;dingTalk\u0026#34;, String credentialsId=\u0026#34;dingTalk\u0026#34;, String title=\u0026#34;\u0026#34;){  new Notification().getObject(project, receiver, credentialsId, title).notification(\u0026#34;success\u0026#34;) }  /**  * notificationFailed  * @param project  * @param receiver  * @param credentialsId  * @param title  * @return  */ def notificationFailed(String project, String receiver=\u0026#34;dingTalk\u0026#34;, String credentialsId=\u0026#34;dingTalk\u0026#34;, String title=\u0026#34;\u0026#34;){  new Notification().getObject(project, receiver, credentialsId, title).notification(\u0026#34;failure\u0026#34;) } 新建Notification.groovy文件：\npackage com.luffy.devops  /** * * @param type * @param credentialsId * @param title * @return */ def getObject(String project, String receiver, String credentialsId, String title) {  this.project = project  this.receiver = receiver  this.credentialsId = credentialsId  this.title = title  return this }   def notification(String type){  String msg =\u0026#34;😄👍 ${this.title} 👍😄\u0026#34;   if (this.title == \u0026#34;\u0026#34;) {  msg = \u0026#34;😄👍 流水线成功啦 👍😄\u0026#34;  }  // failed  if (type == \u0026#34;failure\u0026#34;) {  msg =\u0026#34;😖❌ ${this.title} ❌😖\u0026#34;  if (this.title == \u0026#34;\u0026#34;) {  msg = \u0026#34;😖❌ 流水线失败了 ❌😖\u0026#34;  }  } \tString title = msg  // rich notify msg  msg = genNotificationMessage(msg)  if( this.receiver == \u0026#34;dingTalk\u0026#34;) {  try {  //new DingTalk().markDown(title, msg, this.credentialsId)  } catch (Exception ignored) {}  }else if(this.receiver == \u0026#34;wechat\u0026#34;) {  //todo  }else if (this.receiver == \u0026#34;email\u0026#34;){  //todo  }else{  error \u0026#34;no support notify type!\u0026#34;  } }   /** * get notification msg * @param msg * @return */ def genNotificationMessage(msg) {  // project  msg = \u0026#34;${msg} \\n **项目名称**: ${this.project}\u0026#34;  // get git log  def gitlog = \u0026#34;\u0026#34;  try {  sh \u0026#34;git log --oneline -n 1 \u0026gt; gitlog.file\u0026#34;  gitlog = readFile \u0026#34;gitlog.file\u0026#34;  } catch (Exception ignored) {}   if (gitlog != null \u0026amp;\u0026amp; gitlog != \u0026#34;\u0026#34;) {  msg = \u0026#34;${msg} \\n **Git log**: ${gitlog}\u0026#34;  }  // get git branch  def gitbranch = env.BRANCH_NAME  if (gitbranch != null \u0026amp;\u0026amp; gitbranch != \u0026#34;\u0026#34;) {  msg = \u0026#34;${msg} \\n **Git branch**: ${gitbranch}\u0026#34;  }  // build tasks  msg = \u0026#34;${msg} \\n **Build Tasks**: ${env.BUILD_TASKS}\u0026#34;   // get buttons  msg = msg + getButtonMsg()  return msg } def getButtonMsg(){  String res = \u0026#34;\u0026#34;  def buttons = [  [  \u0026#34;title\u0026#34;: \u0026#34;查看流水线\u0026#34;,  \u0026#34;actionURL\u0026#34;: \u0026#34;${env.RUN_DISPLAY_URL}\u0026#34;  ],  [  \u0026#34;title\u0026#34;: \u0026#34;代码扫描结果\u0026#34;,  \u0026#34;actionURL\u0026#34;: \u0026#34;http://sonar.luffy.com/dashboard?id=${this.project}\u0026#34;  ]  ]  buttons.each() {  if(res == \u0026#34;\u0026#34;){  res = \u0026#34; \\n \u0026gt;\u0026#34;  }  res = \u0026#34;${res} --- [\u0026#34;+it[\u0026#34;title\u0026#34;]+\u0026#34;](\u0026#34;+it[\u0026#34;actionURL\u0026#34;]+\u0026#34;) \u0026#34;  }  return res } 新建DingTalk.groovy文件：\npackage com.luffy.devops  import groovy.json.JsonOutput   def sendRequest(method, data, credentialsId, Boolean verbose=false, codes=\u0026#34;100:399\u0026#34;) {  def reqBody = new JsonOutput().toJson(data)  withCredentials([usernamePassword(credentialsId: credentialsId, usernameVariable: \u0026#39;USERNAME\u0026#39;, passwordVariable: \u0026#39;PASSWORD\u0026#39;)]) {  def response = httpRequest(  httpMode:method,  url: \u0026#34;https://oapi.dingtalk.com/robot/send?access_token=${PASSWORD}\u0026#34;,  requestBody:reqBody,  validResponseCodes: codes,  contentType: \u0026#34;APPLICATION_JSON\u0026#34;,  quiet: !verbose  )  } }  def markDown(String title, String text, String credentialsId, Boolean verbose=false) {  def data = [  \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;,  \u0026#34;markdown\u0026#34;: [  \u0026#34;title\u0026#34;: title,  \u0026#34;text\u0026#34;: text  ]  ]  this.sendRequest(\u0026#34;POST\u0026#34;, data, credentialsId, verbose) } 需要用到Http Request来发送消息，安装一下插件：http_request\njenkinsfile调用 @Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}  options { \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/demo/myblog\u0026#34;  IMAGE_CREDENTIAL = \u0026#34;credential-registry\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  }  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;docker-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.docker(  \u0026#34;${IMAGE_REPO}\u0026#34;,  \u0026#34;${GIT_COMMIT}\u0026#34;,  IMAGE_CREDENTIAL  ).build().push()  }  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.deploy(\u0026#34;deploy\u0026#34;,true,\u0026#34;deploy/deployment.yaml\u0026#34;).start()  }  }  }  }  }  post {  success {  script{  devops.notificationSuccess(\u0026#34;myblog\u0026#34;,\u0026#34;dingTalk\u0026#34;)  }  }  failure {  script{  devops.notificationFailure(\u0026#34;myblog\u0026#34;,\u0026#34;dingTalk\u0026#34;)  }  }  } } library集成代码扫描 sonarqube代码扫描作为通用功能，同样可以使用library实现。\ndevops.groovy\n/** * sonarqube scanner * @param projectVersion * @param waitScan * @return */ def scan(String projectVersion=\u0026#34;\u0026#34;, Boolean waitScan = true) {  return new Sonar().init(projectVersion, waitScan) } 新建Sonar.groovy\n 可以传递projectVersion作为sonarqube的扫描版本 参数waitScan来设置是否等待本次扫描是否通过  package com.luffy.devops   def init(String projectVersion=\u0026#34;\u0026#34;, Boolean waitScan = true) {  this.waitScan = waitScan  this.msg = new BuildMessage()  if (projectVersion == \u0026#34;\u0026#34;){  projectVersion = sh(returnStdout: true, script: \u0026#39;git log --oneline -n 1|cut -d \u0026#34; \u0026#34; -f 1\u0026#39;)  }  sh \u0026#34;echo \u0026#39;\\nsonar.projectVersion=${projectVersion}\u0026#39; \u0026gt;\u0026gt; sonar-project.properties\u0026#34;  sh \u0026#34;cat sonar-project.properties\u0026#34;  return this }  def start() {  try {  this.startToSonar()  }  catch (Exception exc) {  throw exc  }  return this }  def startToSonar() {  withSonarQubeEnv(\u0026#39;sonarqube\u0026#39;) {  sh \u0026#34;sonar-scanner -X;\u0026#34;  sleep 5  }  if(this.waitScan){  //wait 3min  timeout(time: 3, unit: \u0026#39;MINUTES\u0026#39;) {  def qg = waitForQualityGate()  String stage = \u0026#34;${env.stage_name}\u0026#34;  if (qg.status != \u0026#39;OK\u0026#39;) {  this.msg.updateBuildMessage(env.BUILD_TASKS, \u0026#34;${stage} Failed... ×\u0026#34;)  updateGitlabCommitStatus(name: \u0026#34;${stage}\u0026#34;, state: \u0026#39;failed\u0026#39;)  error \u0026#34;Pipeline aborted due to quality gate failure: ${qg.status}\u0026#34;  }else{  this.msg.updateBuildMessage(env.BUILD_RESULT, \u0026#34;${stage} OK... √\u0026#34;)  updateGitlabCommitStatus(name: \u0026#34;${stage}\u0026#34;, state: \u0026#39;success\u0026#39;)  }  }  }else{  echo \u0026#34;skip waitScan\u0026#34;  }  return this } Jenkinsfile新增如下部分：\n stage(\u0026#39;CI\u0026#39;){  failFast true  parallel {  stage(\u0026#39;Unit Test\u0026#39;) {  steps {  echo \u0026#34;Unit Test Stage Skip...\u0026#34;  }  }  stage(\u0026#39;Code Scan\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script {  devops.scan().start()  }  }  }  }  }  } 集成robot自动化测试 关于集成测试，我们需要知道的几点:\n 测试人员进行编写 侧重于不同模块的接口调用，对新加的功能进行验证 注重新版本对以前的集成用例进行回归  因此，更多的应该是跨模块去测试，而且测试用例是测试人员去维护，因此不适合把代码放在开发的git仓库中。\n本节要实现的工作：\n 创建新的git仓库robot-cases，用于存放robot测试用例 为robot-cases项目创建Jenkinsfile 配置Jenkins任务，实现该项目的自动化执行 在myblog模块的流水线中，对该流水线项目进行调用  初始化robot-cases项目   新建gitlab项目，名称为robot-cases\n  clone到本地\n  本地拷贝myblog项目的robot.txt\nrobot-cases/ └── myblog  └── robot.txt   配置Jenkinsfile及自动化任务 robot-cases/ ├── Jenkinsfile └── myblog  └── robot.txt Jenkinsfile\n多个业务项目的测试用例都在一个仓库中，因此需要根据参数设置来决定执行哪个项目的用例\npipeline {  agent {  label \u0026#39;jnlp-slave\u0026#39;  }  \toptions {  timeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;Test\u0026#39;) {  steps {  script {  container(\u0026#39;tools\u0026#39;){  switch(env.comp){  case \u0026#34;myblog\u0026#34;:  env.testDir = \u0026#34;myblog\u0026#34;  break  case \u0026#34;business1\u0026#34;:  env.testDir = \u0026#34;business1\u0026#34;  break  default:  env.testDir = \u0026#34;all\u0026#34;  break  }  sh \u0026#39;robot -d artifacts/ ${testDir}/*\u0026#39;  step([  $class : \u0026#39;RobotPublisher\u0026#39;,  outputPath: \u0026#39;artifacts/\u0026#39;,  outputFileName : \u0026#34;output.xml\u0026#34;,  disableArchiveOutput : false,  passThreshold : 100,  unstableThreshold: 80.0,  onlyCritical : true,  otherFiles : \u0026#34;*.png\u0026#34;  ])  archiveArtifacts artifacts: \u0026#39;artifacts/*\u0026#39;, fingerprint: true  }  }  }  }  } } 如何实现将env.comp 传递进去？\n配置流水线的参数化构建任务并验证参数化构建\nlibrary集成触发任务 由于多个项目均需要触发自动构建，因此可以在library中抽象方法，实现接收comp参数，并在library中实现对robot-cases项目的触发。\ndevops.groovy\n/** * * @param comp * @return */ def robotTest(String comp=\u0026#34;\u0026#34;){  new Robot().acceptanceTest(comp) } 新建Robot.groovy文件\npackage com.luffy.devops  def acceptanceTest(comp) {  try{  echo \u0026#34;Trigger to execute Acceptance Testing\u0026#34;  def rf = build job: \u0026#39;robot-cases\u0026#39;,  parameters: [  string(name: \u0026#39;comp\u0026#39;, value: comp)  ],  wait: true,  propagate: false  def result = rf.getResult()  def msg = \u0026#34;${env.STAGE_NAME}... \u0026#34;  if (result == \u0026#34;SUCCESS\u0026#34;){  msg += \u0026#34;√ success\u0026#34;  }else if(result == \u0026#34;UNSTABLE\u0026#34;){  msg += \u0026#34;⚠ unstable\u0026#34;  }else{  msg += \u0026#34;× failure\u0026#34;  }  echo rf.getAbsoluteUrl()  env.ROBOT_TEST_URL = rf.getAbsoluteUrl()  new BuildMessage().updateBuildMessage(env.BUILD_TASKS, msg)  } catch (Exception exc) {  echo \u0026#34;trigger execute Acceptance Testing exception: ${exc}\u0026#34;  new BuildMessage().updateBuildMessage(env.BUILD_RESULT, msg)  } } 修改Jenkinsfile测试调用\n stage(\u0026#39;integration test\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.robotTest(\u0026#34;myblog\u0026#34;)  }  }  }  } 多环境的CICD自动化实现 实现目标及效果 目前项目存在develop和master两个分支，Jenkinsfile中配置的都是构建部署到相同的环境，实际的场景中，代码仓库的项目往往不同的分支有不同的作用，我们可以抽象出一个工作流程：\n  开发人员提交代码到develop分支\n  Jenkins自动使用develop分支做单测、代码扫描、镜像构建（以commit id为镜像tag）、服务部署到开发环境\n  开发人员使用开发环境自测\n  测试完成后，在gitlab提交merge request请求，将代码合并至master分支\n  需要发版时，在gitlab端基于master分支创建tag（v2.3.0）\n  Jenkins自动检测到tag，拉取tag关联的代码做单测、代码扫描、镜像构建（以代码的tag为镜像的tag）、服务部署到测试环境、执行集成测试用例，输出测试报告\n  测试人员进行手动测试\n  上线\n  实现思路 以myblog项目为例，目前已经具备的是develop分支代码提交后，可以自动实现：\n 单元测试、代码扫描 镜像构建 k8s服务部署 robot集成用例测试  和上述目标相比，差异点：\n myblog应用目前只有一套环境，在luffy命名空间中。我们新建两个命名空间：  dev，用作部署开发环境 test，用作部署集成测试环境   需要根据不同的分支来执行不同的任务，有两种方案实现：  develop和master分支使用不同的Jenkinsfile  可行性很差，因为代码合并工作很繁琐 维护成本高，多个分支需要维护多个Jenkinsfile   使用同一套Jenkinsfile，配合library和模板来实现一套Jenkinsfile适配多套环境  改造Jenkinsfile，实现根据分支来选择任务 需要将deploy目录中所有和特定环境绑定的内容模板化 在library中实现根据不同的分支，来替换模板中的内容      Jenkinsfile根据分支选择任务 使用when关键字，配合正则表达式，实现分支的过滤选择：\npipeline {  agent any  stages {  stage(\u0026#39;Example Build\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  }  }  stage(\u0026#39;Example Deploy\u0026#39;) {  when {  expression { BRANCH_NAME ==~ \u0026#34;develop\u0026#34; }  }  steps {  echo \u0026#39;Deploying to develop env\u0026#39;  }  }  } } 分别在develop和master分支进行验证。\n针对本例，可以对Jenkinsfile做如下调整：\n...  stage(\u0026#39;integration test\u0026#39;) {  when {  expression { BRANCH_NAME ==~ /v.*/ }  }  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.robotTest(PROJECT)  }  }  }  } ... 模板化k8s的资源清单 因为需要使用同一套模板和Jenkinsfile来部署到不同的环境，因此势必要对资源清单进行模板化，前面的内容中只将deployment.yaml放到了项目的deploy清单目录，此处将部署myblog用到的资源清单均补充进去，包含：\n deployment.yaml service.yaml ingress.yaml configmap.yaml secret.yaml  涉及到需要进行模板化的内容包括：\n  镜像地址\n  命名空间\n  ingress的域名信息\n  模板化后的文件：\n$ cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: myblog  namespace: {{NAMESPACE}} spec:  replicas: 1 #指定Pod副本数  selector: #指定Pod的选择器  matchLabels:  app: myblog  template:  metadata:  labels: #给Pod打label  app: myblog  spec:  containers:  - name: myblog  image: {{IMAGE_URL}}  imagePullPolicy: IfNotPresent  env:  - name: MYSQL_HOST  valueFrom:  configMapKeyRef:  name: myblog  key: MYSQL_HOST  - name: MYSQL_PORT  valueFrom:  configMapKeyRef:  name: myblog  key: MYSQL_PORT  - name: MYSQL_USER  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_USER  - name: MYSQL_PASSWD  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_PASSWD  ports:  - containerPort: 8002  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  livenessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒  periodSeconds: 15 # 执行探测的频率  timeoutSeconds: 2 # 探测超时时间  readinessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10  timeoutSeconds: 2  periodSeconds: 15  $ cat configmap.yaml apiVersion: v1 data:  MYSQL_HOST: mysql  MYSQL_PORT: \u0026#34;3306\u0026#34; kind: ConfigMap metadata:  name: myblog  namespace: {{NAMESPACE}}  $ cat secret.yaml apiVersion: v1 data:  MYSQL_PASSWD: MTIzNDU2  MYSQL_USER: cm9vdA== kind: Secret metadata:  name: myblog  namespace: {{NAMESPACE}} type: Opaque  $ cat service.yaml apiVersion: v1 kind: Service metadata:  name: myblog  namespace: {{NAMESPACE}} spec:  ports:  - port: 80  protocol: TCP  targetPort: 8002  selector:  app: myblog  sessionAffinity: None  type: ClusterIP status:  loadBalancer: {}  $ cat ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: myblog  namespace: {{NAMESPACE}} spec:  rules:  - host: {{INGRESS_MYBLOG}}  http:  paths:  - backend:  serviceName: myblog  servicePort: 80  path: / status:  loadBalancer: {} 实现library配置替换逻辑 我们需要实现使用相同的模板，做到如下事情：\n 根据代码分支来部署到不同的命名空间  develop分支部署到开发环境，使用命名空间 dev v.*部署到测试环境，使用命名空间 test   不同环境使用不同的ingress地址来访问  开发环境，blog-dev.luffy.com 测试环境，blog-test.luffy.com    如何实现？sharedlibrary\n所有的逻辑都会经过library这一层，我们具有完全可控权。\n前面已经替换过镜像地址了，我们只需要实现如下逻辑：\n 检测当前代码分支，替换命名空间 检测当前代码分支，替换Ingress地址  问题来了，如何检测构建的触发是develop分支还是tag分支？\n答案是：env.TAG_NAME，由tag分支触发的构建，环境变量中会带有TAG_NAME，且值为gitlab中的tag名称。\n做个演示：\n使用如下的Jenkinsfile，查看由master分支触发和由tag分支触发，printenv的值有什么不同\npipeline {  agent any  stages {  stage(\u0026#39;Example Build\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;Example Deploy\u0026#39;) {  when {  expression { BRANCH_NAME ==~ \u0026#34;develop\u0026#34; }  }  steps {  echo \u0026#39;Deploying to develop env\u0026#39;  }  }  } } 我们可以选择和替换image镜像地址一样，来执行替换：\ndef tplHandler(){  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${env.CURRENT_IMAGE}#g\u0026#39; ${this.resourcePath}/*\u0026#34;  String namespace = \u0026#34;dev\u0026#34;  String ingress = \u0026#34;blog-dev.luffy.com\u0026#34;  if(env.TAG_NAME){  namespace = \u0026#34;test\u0026#34;  ingress = \u0026#34;blog-test.luffy.com\u0026#34;  }  sh \u0026#34;sed -i \u0026#39;s#{{NAMESPACE}}#${namespace}#g\u0026#39; ${this.resourcePath}/*\u0026#34;  sh \u0026#34;sed -i \u0026#39;s#{{INGRESS_MYBLOG}}#${ingress}#g\u0026#39; ${this.resourcePath}/*\u0026#34; } 但是我们的library是要为多个项目提供服务的，如果采用上述方式，则每加入一个项目，都需要对library做改动，形成了强依赖。因此需要想一种更优雅的方式来进行替换。\n思路：\n  开发环境和集成测试环境里准备一个configmap，取名为 devops-config\n  configmap的内容大致如下：\n  开发环境\nNAMESPACE=dev INGRESS_MYBLOG=blog-dev.luffy.com   测试环境\nNAMESPACE=test INGRESS_MYBLOG=blog-test.luffy.com     约定：configmap的key值，拼接{{KEY}}则为代码中需要替换的模板部分，configmap的该key对应的value，则为该模板要被替换的值的内容。比如：\nNAMESPACE=dev INGRESS_MYBLOG=blog-dev.luffy.com {{NAMESPACE}} =\u0026gt; dev {{INGRESS_MYBLOG}} -\u0026gt; blog-dev.luffy.com 意思是约定项目的deploy的资源清单中：\n 所有的{{NAMESPACE}}被替换为dev 所有的{{INGRESS_MYBLOG}}被替换为blog-dev.luffy.com    在library的逻辑中，实现读取触发当前构建的代码分支所关联的namespace下的devops-config这个configmap，然后遍历里面的值进行模板替换即可。\n  这样，则以后再有新增的项目，则只需要维护devops-config配置文件即可，shared-library则不需要随着项目的增加而进行修改，通过这种方式实现library和具体的项目解耦。\ndef tplHandler(){  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${env.CURRENT_IMAGE}#g\u0026#39; ${this.resourcePath}/*\u0026#34;  String namespace = \u0026#34;dev\u0026#34;  if(env.TAG_NAME){  namespace = \u0026#34;test\u0026#34;  }  try {  def configMapData = this.getResource(namespace, \u0026#34;devops-config\u0026#34;, \u0026#34;configmap\u0026#34;)[\u0026#34;data\u0026#34;]  configMapData.each { k, v -\u0026gt;  echo \u0026#34;key is ${k}, val is ${v}\u0026#34;  sh \u0026#34;sed -i \u0026#39;s#{{${k}}}#${v}#g\u0026#39; ${this.resourcePath}/*\u0026#34;  }  }catch (Exception exc) {  echo \u0026#34;failed to get devops-config data,exception: ${exc}.\u0026#34;  throw exc  } } 准备多环境   创建开发和测试环境的命名空间\n#  $ kubectl create namespace dev $ kubectl create namespace test   分别在dev和test命名空间准备mysql数据库。演示功能，因此mysql未作持久化\n$ cat mysql-all.yaml apiVersion: v1 kind: Service metadata:  name: mysql  namespace: dev spec:  ports:  - port: 3306  protocol: TCP  targetPort: 3306  selector:  app: mysql  type: ClusterIP --- apiVersion: v1 kind: Secret metadata:  name: myblog  namespace: dev type: Opaque data:  MYSQL_USER: cm9vdA==  MYSQL_PASSWD: MTIzNDU2 --- apiVersion: apps/v1 kind: Deployment metadata:  name: mysql  namespace: dev spec:  replicas: 1 #指定Pod副本数  selector: #指定Pod的选择器  matchLabels:  app: mysql  template:  metadata:  labels: #给Pod打label  app: mysql  spec:  containers:  - name: mysql  image: 172.21.51.67:5000/mysql:5.7-utf8  ports:  - containerPort: 3306  env:  - name: MYSQL_USER  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_USER  - name: MYSQL_ROOT_PASSWORD  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_PASSWD  - name: MYSQL_DATABASE  value: \u0026#34;myblog\u0026#34;  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  readinessProbe:  tcpSocket:  port: 3306  initialDelaySeconds: 5  periodSeconds: 10  # 创建开发环境的数据库 $ kubectl create -f mysql-all.yaml  # 替换dev命名空间，创建测试环境的数据库 $ sed -i \u0026#39;s/namespace: dev/namespace: test/g\u0026#39; mysql-all.yaml $ kubectl create -f mysql-all.yaml   对myblog项目的k8s资源清单模板化改造\n {{NAMESPACE}} {{INGRESS_MYBLOG}} {{IMAGE_URL}}    初始化开发环境和测试环境的devops-config\n# 开发环境 $ cat devops-config-dev.txt NAMESPACE=dev INGRESS_MYBLOG=blog-dev.luffy.com  $ kubectl -n dev create configmap devops-config --from-env-file=devops-config-dev.txt  # 测试环境 $ cat devops-config-test.txt NAMESPACE=test INGRESS_MYBLOG=blog-test.luffy.com  $ kubectl -n test create configmap devops-config --from-env-file=devops-config-test.txt   提交最新的library代码\n  提交最新的python-demo项目代码\n@Library(\u0026#39;luffy-devops\u0026#39;) _  pipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}  options { \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}  environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/myblog\u0026#34;  IMAGE_CREDENTIAL = \u0026#34;credential-registry\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  PROJECT = \u0026#34;myblog\u0026#34;  }  stages {  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  }  }  stage(\u0026#39;CI\u0026#39;){  failFast true  parallel {  stage(\u0026#39;Unit Test\u0026#39;) {  steps {  echo \u0026#34;Unit Test Stage Skip...\u0026#34;  }  }  stage(\u0026#39;Code Scan\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script {  devops.scan().start()  }  }  }  }  }  }  stage(\u0026#39;docker-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.docker(  \u0026#34;${IMAGE_REPO}\u0026#34;,  \u0026#34;${GIT_COMMIT}\u0026#34;,  IMAGE_CREDENTIAL  ).build().push()  }  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.deploy(\u0026#34;deploy\u0026#34;,true,\u0026#34;deploy/deployment.yaml\u0026#34;).start()  }  }  }  }  stage(\u0026#39;integration test\u0026#39;) {  when {  expression { BRANCH_NAME ==~ /v.*/ }  }  steps {  container(\u0026#39;tools\u0026#39;) {  script{  devops.robotTest(PROJECT)  }  }  }  }  }  post {  success {  script{  devops.notificationSuccess(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  failure {  script{  devops.notificationFailure(PROJECT,\u0026#34;dingTalk\u0026#34;)  }  }  } }   验证多环境自动部署 模拟如下流程：\n  提交代码到develop分支，观察是否部署到dev的命名空间中，注意，第一次部署，需要执行migrate操作：\n $ kubectl -n dev exec myblog-9f9f7c8cd-k6tbj python3 manage.py migrate   配置hosts解析，测试使用http://blog-dev.luffy.com/blog/index/进行访问到develop分支最新版本\n  合并代码至master分支\n  在gitlab中创建tag，观察是否自动部署至test的命名空间中，且使用myblog-test.luffy.com/blog/index/可以访问到最新版本\n  实现打tag后自动部署 我们发现，打了tag以后，多分支流水线中可以识别到该tag，但是并不会自动部署该tag的代码。因此，我们来使用一个新的插件：Basic Branch Build Strategies Plugin\n安装并配置多分支流水线，注意Build strategies 设置：\n Regular branches Tags  Ignore tags newer than 可以不用设置，不然会默认不自动构建新打的tag Ignore tags older than    优化镜像部署逻辑 针对部署到测试环境的代码，由于已经打了tag了，因此，我们期望构建出来的镜像地址可以直接使用代码的tag作为镜像的tag。\n思路一：直接在Jenkinsfile调用devops.docker时传递tag名称\n思路二：在shared-library中，根据env.TAG_NAME来判断当前是否是tag分支的构建，若TAG_NAME不为空，则可以在构建镜像时使用TAG_NAME作为镜像的tag\n很明显我们更期望使用思路二的方式来实现，因此，需要调整如下逻辑：\ndef docker(String repo, String tag, String credentialsId, String dockerfile=\u0026#34;Dockerfile\u0026#34;, String context=\u0026#34;.\u0026#34;){  this.repo = repo  this.tag = tag  if(env.TAG_NAME){  this.tag = env.TAG_NAME  }  this.dockerfile = dockerfile  this.credentialsId = credentialsId  this.context = context  this.fullAddress = \u0026#34;${this.repo}:${this.tag}\u0026#34;  this.isLoggedIn = false  this.msg = new BuildMessage()  return this } 提交代码，并进行测试，观察是否使用tag作为镜像标签进行部署。\n小结 Jenkins-shared-library的代码地址： https://gitee.com/agagin/jenkins-shared-library\n目标：让devops流程更好用\n 项目更简便的接入 devops流程更方便维护  思路：把各项目中公用的逻辑，抽象成方法，放到独立的library项目中，在各项目中引入shared-library项目，调用library提供的方法。\n 镜像构建、推送 k8s服务部署、监控 钉钉消息推送 代码扫描 robot集成测试  为了兼容多环境的CICD，因此采用模板与数据分离的方式，项目中的定义模板，shared-library中实现模板替换。为了实现shared-library与各项目解耦，使用configmap来维护模板与真实数据的值，思路是约定大于配置。\n","permalink":"https://iblog.zone/archives/%E5%9F%BA%E4%BA%8Esharedlibrary%E8%BF%9B%E8%A1%8Ccicd%E6%B5%81%E7%A8%8B%E7%9A%84%E4%BC%98%E5%8C%96/","summary":"基于sharedLibrary进行CI/CD流程的优化 由于公司内部项目众多，大量的项目使用同一套流程做CICD\n 那么势必会存在大量的重复代码 一旦某个公共的地方需要做调整，每个项目都需要修改  因此本章主要通过使用groovy实现Jenkins的sharedLibrary的开发，以提取项目在CICD实践过程中的公共逻辑，提供一系列的流程的接口供公司内各项目调用。\n开发完成后，对项目进行Jenkinsfile的改造，最后仅需通过简单的Jenkinsfile的配置，即可优雅的完成CICD流程的整个过程，此方式已在大型企业内部落地应用。\nLibrary工作模式 由于流水线被组织中越来越多的项目所采用，常见的模式很可能会出现。 在多个项目之间共享流水线有助于减少冗余并保持代码 \u0026ldquo;DRY\u0026rdquo;。\n流水线支持引用 \u0026ldquo;共享库\u0026rdquo; ，可以在外部源代码控制仓库中定义并加载到现有的流水线中。\n@Library(\u0026#39;my-shared-library\u0026#39;) _ 在实际运行过程中，会把library中定义的groovy功能添加到构建目录中：\n/var/jenkins_home/jobs/test-maven-build/branches/feature-CDN-2904.cm507o/builds/2/libs/my-shared-library/vars/devops.groovy 使用library后，Jenkinsfile大致的样子如下：\n@Library(\u0026#39;my-shared-library\u0026#39;) _  ...  stages {  stage(\u0026#39;build image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  devops.buildImage(\u0026#34;Dockerfile\u0026#34;,\u0026#34;172.21.51.67:5000/demo:latest\u0026#34;)  }  }  }  }   post {  success {  script {  container(\u0026#39;tools\u0026#39;) {  devops.notificationSuccess(\u0026#34;dingTalk\u0026#34;)  }  }  }  } .","title":"基于sharedLibrary进行CICD流程的优化"},{"content":"基于Kubernetes的DevOps平台实践 持续集成工具：\n Jenkins gitlabci Tekton  本章基于k8s集群部署gitlab、sonarQube、Jenkins等工具，并把上述工具集成到Jenkins中，以Django项目和SpringBoot项目为例，通过多分支流水线及Jenkinsfile实现项目代码提交到不同的仓库分支，实现自动代码扫描、单元测试、docker容器构建、k8s服务的自动部署。\n DevOps、CI、CD介绍 Jenkins、sonarQube、gitlab的快速部署 Jenkins初体验 流水线入门及Jenkinsfile使用 Jenkins与Kubernetes的集成 sonarQube代码扫描与Jenkins的集成 实践Django项目的基于Jenkinsfile实现开发、测试环境的CI/CD  DevOps、CI、CD介绍 Continuous Integration (CI) / Continuous Delivery (CD)\n软件交付流程\n一个软件从零开始到最终交付，大概包括以下几个阶段：规划、编码、构建、测试、发布、部署和维护，基于这些阶段，我们的软件交付模型大致经历了几个阶段：\n瀑布式流程 前期需求确立之后，软件开发人员花费数周和数月编写代码，把所有需求一次性开发完，然后将代码交给QA（质量保障）团队进行测试，然后将最终的发布版交给运维团队去部署。瀑布模型，简单来说，就是等一个阶段所有工作完成之后，再进入下一个阶段。这种模式的问题也很明显，产品迭代周期长，灵活性差。一个周期动辄几周几个月，适应不了当下产品需要快速迭代的场景。\n敏捷开发 任务由大拆小，开发、测试协同工作，注重开发敏捷，不重视交付敏捷\nDevOps 开发、测试、运维协同工作, 持续开发+持续交付。\n我们是否可以认为DevOps = 提倡开发、测试、运维协同工作来实现持续开发、持续交付的一种软件交付模式？\n大家想一下为什么最初的开发模式没有直接进入DevOps的时代？\n原因是：沟通成本。\n各角色人员去沟通协作的时候都是手动去做，交流靠嘴，靠人去指挥，很显然会出大问题。所以说不能认为DevOps就是一种交付模式，因为解决不了沟通协作成本，这种模式就不具备可落地性。\n那DevOps时代如何解决角色之间的成本问题？DevOps的核心就是自动化。自动化的能力靠什么来支撑，工具和技术。\nDevOps工具链\n靠这些工具和技术，才实现了自动化流程，进而解决了协作成本，使得devops具备了可落地性。因此我们可以大致给devops一个定义：\ndevops = 提倡开发、测试、运维协同工作来实现持续开发、持续交付的一种软件交付模式 + 基于工具和技术支撑的自动化流程的落地实践。\n因此devops不是某一个具体的技术，而是一种思想+自动化能力，来使得构建、测试、发布软件能够更加地便捷、频繁和可靠的落地实践。本次课程核心内容就是要教会大家如何利用工具和技术来实现完整的DevOps平台的建设。我们主要使用的工具有：\n gitlab，代码仓库，企业内部使用最多的代码版本管理工具。 Jenkins， 一个可扩展的持续集成引擎，用于自动化各种任务，包括构建、测试和部署软件。 robotFramework， 基于Python的自动化测试框架 sonarqube，代码质量管理平台 maven，java包构建管理工具 Kubernetes Docker  Jenkins初体验 Kubernetes环境中部署jenkins 其他部署方式\n注意点：\n 第一次启动很慢 因为后面Jenkins会与kubernetes集群进行集成，会需要调用kubernetes集群的api，因此安装的时候创建了ServiceAccount并赋予了cluster-admin的权限 默认部署到jenkins=true的节点 初始化容器来设置权限 ingress来外部访问 数据存储通过hostpath挂载到宿主机中  jenkins/jenkins-all.yaml\napiVersion: v1 kind: Namespace metadata:  name: jenkins --- apiVersion: v1 kind: ServiceAccount metadata:  name: jenkins  namespace: jenkins --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata:  name: jenkins-crb roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-admin subjects: - kind: ServiceAccount  name: jenkins  namespace: jenkins --- apiVersion: apps/v1 kind: Deployment metadata:  name: jenkins-master  namespace: jenkins spec:  replicas: 1  selector:  matchLabels:  devops: jenkins-master  template:  metadata:  labels:  devops: jenkins-master  spec:  nodeSelector:  jenkins: \u0026#34;true\u0026#34;  serviceAccount: jenkins #Pod 需要使用的服务账号  initContainers:  - name: fix-permissions  image: busybox  command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown -R 1000:1000 /var/jenkins_home\u0026#34;]  securityContext:  privileged: true  volumeMounts:  - name: jenkinshome  mountPath: /var/jenkins_home  containers:  - name: jenkins  image: jenkinsci/blueocean:1.23.2  imagePullPolicy: IfNotPresent  ports:  - name: http #Jenkins Master Web 服务端口  containerPort: 8080  - name: slavelistener #Jenkins Master 供未来 Slave 连接的端口  containerPort: 50000  volumeMounts:  - name: jenkinshome  mountPath: /var/jenkins_home  env:  - name: JAVA_OPTS  value: \u0026#34;-Xms4096m -Xmx5120m -Duser.timezone=Asia/Shanghai -Dhudson.model.DirectoryBrowserSupport.CSP=\u0026#34;  volumes:  - name: jenkinshome  hostPath:  path: /var/jenkins_home/ --- apiVersion: v1 kind: Service metadata:  name: jenkins  namespace: jenkins spec:  ports:  - name: http  port: 8080  targetPort: 8080  - name: slavelistener  port: 50000  targetPort: 50000  type: ClusterIP  selector:  devops: jenkins-master --- apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: jenkins-web  namespace: jenkins spec:  rules:  - host: jenkins.luffy.com  http:  paths:  - backend:  serviceName: jenkins  servicePort: 8080  path: / 创建服务：\n## 为k8s-slave1打标签，将jenkins-master部署在k8s-slave1节点 $ kubectl label node k8s-slave1 jenkins=true ## 部署服务 $ kubectl create -f jenkins-all.yaml ## 查看服务 $ kubectl -n jenkins get po NAME READY STATUS RESTARTS AGE jenkins-master-767df9b574-lgdr5 1/1 Running 0 20s  # 查看日志，第一次启动提示需要完成初始化设置 $ kubectl -n jenkins logs -f jenkins-master-767df9b574-lgdr5 ...... *************************************************************  Jenkins initial setup is required. An admin user has been created and a password generated. Please use the following password to proceed to installation:  5396b4e1c395450f8360efd8ee641b18  This may also be found at: /var/jenkins_home/secrets/initialAdminPassword  ************************************************************* 访问服务：\n配置hosts解析，172.21.51.67 jenkins.luffy.com，然后使用浏览器域名访问服务。第一次访问需要大概几分钟的初始化时间。\n使用jenkins启动日志中的密码，或者执行下面的命令获取解锁的管理员密码：\n$ kubectl -n jenkins exec jenkins-master-767df9b574-lgdr5 bash / # cat /var/jenkins_home/secrets/initialAdminPassword 35b083de1d25409eaef57255e0da481a 点击叉号，跳过选择安装推荐的插件环节，直接进入Jenkins。由于默认的插件地址安装非常慢，我们可以替换成国内清华的源，进入 jenkins 工作目录，目录下面有一个 updates 的目录，下面有一个 default.json 文件，我们执行下面的命令替换插件地址：\n$ cd /var/jenkins_home/updates $ sed -i \u0026#39;s/http:\\/\\/updates.jenkins-ci.org\\/download/https:\\/\\/mirrors.tuna.tsinghua.edu.cn\\/jenkins/g\u0026#39; default.json $ sed -i \u0026#39;s/http:\\/\\/www.google.com/https:\\/\\/www.baidu.com/g\u0026#39; default.json  暂时先不用重新启动pod，汉化后一起重启。\n 选择右上角admin-\u0026gt;configure-\u0026gt;password重新设置管理员密码，设置完后，会退出要求重新登录，使用admin/xxxxxx(新密码)，登录即可。\n安装汉化插件 Jenkins -\u0026gt; manage Jenkins -\u0026gt; Plugin Manager -\u0026gt; Avaliable，搜索 chinese关键字\n选中后，选择[Install without restart]，等待下载完成，然后点击[ Restart Jenkins when installation is complete and no jobs are running ]，让Jenkins自动重启\n启动后，界面默认变成中文。\nJenkins基本使用演示 演示目标  代码提交gitlab，自动触发Jenkins任务 Jenkins任务完成后发送钉钉消息通知  演示准备 gitlab代码仓库搭建\nhttps://github.com/sameersbn/docker-gitlab\n## 全量部署的组件 $ gitlab-ctl status run: alertmanager: (pid 1987) 27s; run: log: (pid 1986) 27s run: gitaly: (pid 1950) 28s; run: log: (pid 1949) 28s run: gitlab-exporter: (pid 1985) 27s; run: log: (pid 1984) 27s run: gitlab-workhorse: (pid 1956) 28s; run: log: (pid 1955) 28s run: logrotate: (pid 1960) 28s; run: log: (pid 1959) 28s run: nginx: (pid 2439) 1s; run: log: (pid 1990) 27s run: node-exporter: (pid 1963) 28s; run: log: (pid 1962) 28s run: postgres-exporter: (pid 1989) 27s; run: log: (pid 1988) 27s run: postgresql: (pid 1945) 28s; run: log: (pid 1944) 28s run: prometheus: (pid 1973) 28s; run: log: (pid 1972) 28s run: puma: (pid 1968) 28s; run: log: (pid 1966) 28s run: redis: (pid 1952) 28s; run: log: (pid 1951) 28s run: redis-exporter: (pid 1971) 28s; run: log: (pid 1964) 28s run: sidekiq: (pid 1969) 28s; run: log: (pid 1967) 28s 部署分析：\n 依赖postgres 依赖redis  使用k8s部署：\n  准备secret文件\n$ cat gitlab-secret.txt postgres.user.root=root postgres.pwd.root=1qaz2wsx  $ kubectl -n jenkins create secret generic gitlab-secret --from-env-file=gitlab-secret.txt   部署postgres\n注意点：\n 使用secret来引用账户密码 使用postgres=true来指定节点  $ cat postgres.yaml apiVersion: v1 kind: Service metadata:  name: postgres  labels:  app: postgres  namespace: jenkins spec:  ports:  - name: server  port: 5432  targetPort: 5432  protocol: TCP  selector:  app: postgres --- apiVersion: apps/v1 kind: Deployment metadata:  namespace: jenkins  name: postgres  labels:  app: postgres spec:  replicas: 1  selector:  matchLabels:  app: postgres  template:  metadata:  labels:  app: postgres  spec:  nodeSelector:  postgres: \u0026#34;true\u0026#34;  tolerations:  - operator: \u0026#34;Exists\u0026#34;  containers:  - name: postgres  image: 172.21.51.67:5000/postgres:11.4 #若本地没有启动该仓库，换成postgres:11.4  imagePullPolicy: \u0026#34;IfNotPresent\u0026#34;  ports:  - containerPort: 5432  env:  - name: POSTGRES_USER #PostgreSQL 用户名  valueFrom:  secretKeyRef:  name: gitlab-secret  key: postgres.user.root  - name: POSTGRES_PASSWORD #PostgreSQL 密码  valueFrom:  secretKeyRef:  name: gitlab-secret  key: postgres.pwd.root  resources:  limits:  cpu: 1000m  memory: 2048Mi  requests:  cpu: 50m  memory: 100Mi  volumeMounts:  - mountPath: /var/lib/postgresql/data  name: postgredb  volumes:  - name: postgredb  hostPath:  path: /var/lib/postgres/   #部署到k8s-slave2节点 $ kubectl label node k8s-slave2 postgres=true  #创建postgres $ kubectl create -f postgres.yaml  # 创建数据库gitlab,为后面部署gitlab组件使用 $ kubectl -n jenkins exec -ti postgres-7ff9b49f4c-nt8zh bash root@postgres-7ff9b49f4c-nt8zh:/# psql root=# create database gitlab; CREATE DATABASE   部署redis\n$ cat redis.yaml apiVersion: v1 kind: Service metadata:  name: redis  labels:  app: redis  namespace: jenkins spec:  ports:  - name: server  port: 6379  targetPort: 6379  protocol: TCP  selector:  app: redis --- apiVersion: apps/v1 kind: Deployment metadata:  namespace: jenkins  name: redis  labels:  app: redis spec:  replicas: 1  selector:  matchLabels:  app: redis  template:  metadata:  labels:  app: redis  spec:  tolerations:  - operator: \u0026#34;Exists\u0026#34;  containers:  - name: redis  image: sameersbn/redis:4.0.9-2  imagePullPolicy: \u0026#34;IfNotPresent\u0026#34;  ports:  - containerPort: 6379  resources:  limits:  cpu: 1000m  memory: 2048Mi  requests:  cpu: 50m  memory: 100Mi  # 创建 $ kubectl create -f redis.yaml   部署gitlab\n注意点：\n 使用ingress暴漏服务 添加annotation，指定nginx端上传大小限制，否则推送代码时会默认被限制1m大小，相当于给nginx设置client_max_body_size的限制大小 使用gitlab=true来选择节点 使用服务发现地址来访问postgres和redis 在secret中引用数据库账户和密码 数据库名称为gitlab  $ cat gitlab.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: gitlab  namespace: jenkins  annotations:  nginx.ingress.kubernetes.io/proxy-body-size: \u0026#34;50m\u0026#34; spec:  rules:  - host: gitlab.luffy.com  http:  paths:  - backend:  serviceName: gitlab  servicePort: 80  path: / --- apiVersion: v1 kind: Service metadata:  name: gitlab  labels:  app: gitlab  namespace: jenkins spec:  ports:  - name: server  port: 80  targetPort: 80  protocol: TCP  selector:  app: gitlab --- apiVersion: apps/v1 kind: Deployment metadata:  namespace: jenkins  name: gitlab  labels:  app: gitlab spec:  replicas: 1  selector:  matchLabels:  app: gitlab  template:  metadata:  labels:  app: gitlab  spec:  nodeSelector:  gitlab: \u0026#34;true\u0026#34;  tolerations:  - operator: \u0026#34;Exists\u0026#34;  containers:  - name: gitlab  image: sameersbn/gitlab:13.2.2  imagePullPolicy: \u0026#34;IfNotPresent\u0026#34;  env:  - name: GITLAB_HOST  value: \u0026#34;gitlab.luffy.com\u0026#34;  - name: GITLAB_PORT  value: \u0026#34;80\u0026#34;  - name: GITLAB_SECRETS_DB_KEY_BASE  value: \u0026#34;long-and-random-alpha-numeric-string\u0026#34;  - name: GITLAB_SECRETS_DB_KEY_BASE  value: \u0026#34;long-and-random-alpha-numeric-string\u0026#34;  - name: GITLAB_SECRETS_SECRET_KEY_BASE  value: \u0026#34;long-and-random-alpha-numeric-string\u0026#34;  - name: GITLAB_SECRETS_OTP_KEY_BASE  value: \u0026#34;long-and-random-alpha-numeric-string\u0026#34;  - name: DB_HOST  value: \u0026#34;postgres\u0026#34;  - name: DB_NAME  value: \u0026#34;gitlab\u0026#34;  - name: DB_USER  valueFrom:  secretKeyRef:  name: gitlab-secret  key: postgres.user.root  - name: DB_PASS  valueFrom:  secretKeyRef:  name: gitlab-secret  key: postgres.pwd.root  - name: REDIS_HOST  value: \u0026#34;redis\u0026#34;  - name: REDIS_PORT  value: \u0026#34;6379\u0026#34;  ports:  - containerPort: 80  resources:  limits:  cpu: 2000m  memory: 5048Mi  requests:  cpu: 100m  memory: 500Mi  volumeMounts:  - mountPath: /home/git/data  name: data  volumes:  - name: data  hostPath:  path: /var/lib/gitlab/  #部署到k8s-slave2节点 $ kubectl label node k8s-slave2 gitlab=true  # 创建 $ kubectl create -f gitlab.yaml   配置hosts解析：\n172.21.51.67 gitlab.luffy.com 设置root密码\n访问http://gitlab.luffy.com，设置管理员密码\n配置k8s-master节点的hosts\n$ echo \u0026#34;172.21.51.67 gitlab.luffy.com\u0026#34; \u0026gt;\u0026gt;/etc/hosts myblog项目推送到gitlab\nmkdir demo cp -r myblog demo/ cd demo/myblog git remote rename origin old-origin git remote add origin http://gitlab.luffy.com/root/myblog.git git push -u origin --all git push -u origin --tags 钉钉推送\n官方文档\n  配置机器人\n  试验发送消息\n$ curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=67e81175c6ebacb1307e83f62680f36fbcf4524e8f43971cf2fb2049bc58723d\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;我就是我, 是不一样的烟火\u0026#34; } }\u0026#39;   演示过程 流程示意图：\n  安装gitlab plugin\n插件中心搜索并安装gitlab，直接安装即可\n  配置Gitlab\n系统管理-\u0026gt;系统配置-\u0026gt;Gitlab，其中的API Token，需要从下个步骤中获取\n  获取AccessToken\n登录gitlab，选择user-\u0026gt;Settings-\u0026gt;access tokens新建一个访问token\n  配置host解析\n由于我们的Jenkins和gitlab域名是本地解析，因此需要让gitlab和Jenkins服务可以解析到对方的域名。两种方式：\n  在容器内配置hosts\n  配置coredns的静态解析\n hosts {  172.21.51.67 jenkins.luffy.com gitlab.luffy.com  fallthrough  }     创建自由风格项目\n gitlab connection 选择为刚创建的gitlab 源码管理选择Git，填项项目地址 新建一个 Credentials 认证，使用用户名密码方式，配置gitlab的用户和密码 构建触发器选择 Build when a change is pushed to GitLab 生成一个Secret token 保存    到gitlab配置webhook\n 进入项目下settings-\u0026gt;Integrations URL： http://jenkins.luffy.com/project/free Secret Token 填入在Jenkins端生成的token Add webhook test push events，报错：Requests to the local network are not allowed    设置gitlab允许向本地网络发送webhook请求\n访问 Admin Aera -\u0026gt; Settings -\u0026gt; Network ，展开Outbound requests\nCollapse，勾选第一项即可。再次test push events，成功。\n  配置free项目，增加构建步骤，执行shell，将发送钉钉消息的shell保存\n  提交代码到gitlab仓库，查看构建是否自动执行\n    Master-Slaves（agent）模式 上面演示的任务，默认都是在master节点执行的，多个任务都在master节点执行，对master节点的性能会造成一定影响，如何将任务分散到不同的节点，做成多slave的方式？\n  添加slave节点\n  系统管理 -\u0026gt; 节点管理 -\u0026gt; 新建节点\n  比如添加172.21.51.68，选择固定节点，保存\n  远程工作目录/opt/jenkins_jobs\n  标签为任务选择节点的依据，如172.21.51.68\n  启动方式选择通过java web启动代理，代理是运行jar包，通过JNLP（是一种允许客户端启动托管在远程Web服务器上的应用程序的协议 ）启动连接到master节点服务中\n    执行java命令启动agent服务\n## 登录172.21.51.68，下载agent.jar $ wget http://jenkins.luffy.com/jnlpJars/agent.jar ## 会提示找不到agent错误，因为没有配置地址解析，由于连接jenkins master会通过50000端口，直接使用cluster-ip $ kubectl -n jenkins get svc #在master节点执行查询cluster-ip地址 NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE jenkins ClusterIP 10.99.204.208 \u0026lt;none\u0026gt; 8080/TCP,50000/TCP 4h8m  ## 再次回到68节点 $ wget 10.99.204.208:8080/jnlpJars/agent.jar $ java -jar agent.jar -jnlpUrl http://10.99.204.208:8080/computer/172.21.51.68/slave-agent.jnlp -secret 4be4d164f861d2830835653567867a1e695b30c320d35eca2be9f5624f8712c8 -workDir \u0026#34;/opt/jenkins_jobs\u0026#34; ... INFO: Remoting server accepts the following protocols: [JNLP4-connect, Ping] Apr 01, 2020 7:03:51 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Agent discovery successful  Agent address: 10.99.204.208  Agent port: 50000  Identity: e4:46:3a🇩🇪86:24:8e:15:09:13:3d:a7:4e:07:04:37 Apr 01, 2020 7:03:51 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Handshaking Apr 01, 2020 7:03:51 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Connecting to 10.99.204.208:50000 Apr 01, 2020 7:03:51 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Trying protocol: JNLP4-connect Apr 01, 2020 7:04:02 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Remote identity confirmed: e4:46:3a🇩🇪86:24:8e:15:09:13:3d:a7:4e:07:04:37 Apr 01, 2020 7:04:03 PM hudson.remoting.jnlp.Main$CuiListener status INFO: Connected 若出现如下错误:\nSEVERE: http://jenkins.luffy.com/tcpSlaveAgentListener/ appears to be publishing an invalid X-Instance-Identity. java.io.IOException: http://jenkins.luffy.com/tcpSlaveAgentListener/ appears to be publishing an invalid X-Instance-Identity.  at org.jenkinsci.remoting.engine.JnlpAgentEndpointResolver.resolve(JnlpAgentEndpointResolver.java:287)  at hudson.remoting.Engine.innerRun(Engine.java:694)  at hudson.remoting.Engine.run(Engine.java:519) 可以选择： 配置从节点 -\u0026gt; 高级 -\u0026gt; Tunnel连接位置，参考下图进行设置:\n  查看Jenkins节点列表，新节点已经处于可用状态\n  测试使用新节点执行任务\n  配置free项目\n  限制项目的运行节点 ，标签表达式选择172.21.51.68\n  立即构建\n  查看构建日志\nStarted by user admin Running as SYSTEM Building remotely on 172.21.51.68 in workspace /opt/jenkins_jobs/workspace/free-demo using credential gitlab-user Cloning the remote Git repository Cloning repository http://gitlab.luffy.com/root/myblog.git  \u0026gt; git init /opt/jenkins_jobs/workspace/free-demo # timeout=10  ...       Jenkins定制化容器 由于每次新部署Jenkins环境，均需要安装很多必要的插件，因此考虑把插件提前做到镜像中\nDockerfile\nFROMjenkinsci/blueocean:1.23.2LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34;## 用最新的插件列表文件替换默认插件文件COPY plugins.txt /usr/share/jenkins/ref/## 执行插件安装RUN /usr/local/bin/install-plugins.sh \u0026lt; /usr/share/jenkins/ref/plugins.txtplugins.txt\nace-editor:1.1 allure-jenkins-plugin:2.28.1 ant:1.10 antisamy-markup-formatter:1.6 apache-httpcomponents-client-4-api:4.5.10-1.0 authentication-tokens:1.3 ... get_plugin.sh\n admin:123456@localhost 需要替换成Jenkins的用户名、密码及访问地址\n #!/usr/bin/env bash curl -sSL \u0026#34;http://admin:123456@localhost:8080/pluginManager/api/xml?depth=1\u0026amp;xpath=/*/*/shortName|/*/*/version\u0026amp;wrapper=plugins\u0026#34; | perl -pe \u0026#39;s/.*?\u0026lt;shortName\u0026gt;([\\w-]+).*?\u0026lt;version\u0026gt;([^\u0026lt;]+)()(\u0026lt;\\/\\w+\u0026gt;)+/\\1:\\2\\n/g\u0026#39;|sed \u0026#39;s/ /:/\u0026#39; \u0026gt; plugins.txt ## 执行构建，定制jenkins容器 $ docker build . -t 172.21.51.67:5000/jenkins:v20200414 -f Dockerfile $ docker push 172.21.51.67:5000/jenkins:v20200414 至此，我们可以使用定制化的镜像启动jenkins服务\n## 删掉当前服务 $ kubectl delete -f jenkins-all.yaml  ## 删掉已挂载的数据 $ rm -rf /var/jenkins_home  ## 替换使用定制化镜像 $ sed -i \u0026#39;s#jenkinsci/blueocean#172.21.51.67:5000/jenkins:v20200404#g\u0026#39; jenkins-all.yaml  ## 重新创建服务 $ kubectl create -f jenkins-all.yaml 本章小结 自由风格项目弊端：\n 任务的完成需要在Jenkins端维护大量的配置 没法做版本控制 可读性、可移植性很差，不够优雅    流水线入门 官方文档\n为什么叫做流水线，和工厂产品的生产线类似，pipeline是从源码到发布到线上环境。关于流水线，需要知道的几个点：\n  重要的功能插件，帮助Jenkins定义了一套工作流框架；\n  Pipeline 的实现方式是一套 Groovy DSL（ 领域专用语言 ），所有的发布流程都可以表述为一段 Groovy 脚本；\n  将WebUI上需要定义的任务，以脚本代码的方式表述出来；\n  帮助jenkins实现持续集成CI（Continue Integration）和持续部署CD（Continue Deliver）的重要手段；\n  流水线基础语法 官方文档\n两种语法类型：\n Scripted Pipeline，脚本式流水线，最初支持的类型 Declarative Pipeline，声明式流水线，为Pipeline plugin在2.5版本之后新增的一种脚本类型，后续Open Blue Ocean所支持的类型。与原先的Scripted Pipeline一样，都可以用来编写脚本。Declarative Pipeline 是后续Open Blue Ocean所支持的类型，写法简单，支持内嵌Scripted Pipeline代码  为与BlueOcean脚本编辑器兼容，通常建议使用Declarative Pipeline的方式进行编写,从jenkins社区的动向来看，很明显这种语法结构也会是未来的趋势。\n脚本示例 pipeline {  agent {label \u0026#39;172.21.51.68\u0026#39;}  environment {  PROJECT = \u0026#39;myblog\u0026#39;  }  stages {  stage(\u0026#39;Checkout\u0026#39;) {  steps {  checkout scm  }  }  stage(\u0026#39;Build\u0026#39;) {  steps {  sh \u0026#39;make\u0026#39;  }  }  stage(\u0026#39;Test\u0026#39;){  steps {  sh \u0026#39;make check\u0026#39;  junit \u0026#39;reports/**/*.xml\u0026#39;  }  }  stage(\u0026#39;Deploy\u0026#39;) {  steps {  sh \u0026#39;make publish\u0026#39;  }  }  } \tpost {  success {  echo \u0026#39;Congratulations!\u0026#39;  } \tfailure {  echo \u0026#39;Oh no!\u0026#39;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 脚本解释：   checkout步骤为检出代码; scm是一个特殊变量，指示checkout步骤克隆触发此Pipeline运行的特定修订\n  agent：指明使用哪个agent节点来执行任务，定义于pipeline顶层或者stage内部\n  any，可以使用任意可用的agent来执行\n  label，在提供了标签的 Jenkins 环境中可用的代理上执行流水线或阶段。 例如: agent { label 'my-defined-label' }，最常见的使用方式\n  none，当在 pipeline 块的顶部没有全局代理， 该参数将会被分配到整个流水线的运行中并且每个 stage 部分都需要包含他自己的 agent 部分。比如: agent none\n  docker， 使用给定的容器执行流水线或阶段。 在指定的节点中，通过运行容器来执行任务\nagent {  docker {  image \u0026#39;maven:3-alpine\u0026#39;  label \u0026#39;my-defined-label\u0026#39;  args \u0026#39;-v /tmp:/tmp\u0026#39;  } }     options: 允许从流水线内部配置特定于流水线的选项。\n buildDiscarder , 为最近的流水线运行的特定数量保存组件和控制台输出。例如: options { buildDiscarder(logRotator(numToKeepStr: '10')) } disableConcurrentBuilds ,不允许同时执行流水线。 可被用来防止同时访问共享资源等。 例如: options { disableConcurrentBuilds() } timeout ,设置流水线运行的超时时间, 在此之后，Jenkins将中止流水线。例如: options { timeout(time: 1, unit: 'HOURS') } retry，在失败时, 重新尝试整个流水线的指定次数。 For example: options { retry(3) }    environment: 指令制定一个 键-值对序列，该序列将被定义为所有步骤的环境变量\n  stages: 包含一系列一个或多个 stage指令, stages 部分是流水线描述的大部分\u0026quot;work\u0026quot; 的位置。 建议 stages 至少包含一个 stage 指令用于连续交付过程的每个离散部分,比如构建, 测试, 和部署。\npipeline {  agent any  stages {  stage(\u0026#39;Example\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  }  }  } }   steps: 在给定的 stage 指令中执行的定义了一系列的一个或多个steps。\n  post: 定义一个或多个steps ，这些阶段根据流水线或阶段的完成情况而运行post 支持以下 post-condition 块中的其中之一: always, changed, failure, success, unstable, 和 aborted。\n always, 无论流水线或阶段的完成状态如何，都允许在 post 部分运行该步骤 changed, 当前流水线或阶段的完成状态与它之前的运行不同时，才允许在 post 部分运行该步骤 failure, 当前流水线或阶段的完成状态为\u0026quot;failure\u0026quot;，才允许在 post 部分运行该步骤, 通常web UI是红色 success, 当前流水线或阶段的完成状态为\u0026quot;success\u0026quot;，才允许在 post 部分运行该步骤, 通常web UI是蓝色或绿色 unstable, 当前流水线或阶段的完成状态为\u0026quot;unstable\u0026quot;，才允许在 post 部分运行该步骤, 通常由于测试失败,代码违规等造成。通常web UI是黄色 aborted， 只有当前流水线或阶段的完成状态为\u0026quot;aborted\u0026quot;，才允许在 post 部分运行该步骤, 通常由于流水线被手动的aborted。通常web UI是灰色    创建pipeline示意：\n新建任务 -\u0026gt; 流水线\njenkins/pipelines/p1.yaml\npipeline {  agent {label \u0026#39;172.21.51.68\u0026#39;}  environment {  PROJECT = \u0026#39;myblog\u0026#39;  }  stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;check\u0026#39;) {  steps {  checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/master\u0026#39;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;gitlab-user\u0026#39;, url: \u0026#39;http://gitlab.luffy.com/root/myblog.git\u0026#39;]]])  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  sh \u0026#39;docker build . -t myblog:latest -f Dockerfile\u0026#39;  }  }  stage(\u0026#39;send-msg\u0026#39;) {  steps {  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=67e81175c6ebacb1307e83f62680f36fbcf4524e8f43971cf2fb2049bc58723d\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;我就是我, 是不一样的烟火\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  }  } } 点击“立即构建”，同样的，我们可以配置触发器，使用webhook的方式接收项目的push事件，\n 构建触发器选择 Build when a change is pushed to GitLab. 生成 Secret token 配置gitlab，创建webhook，发送test push events测试  Blue Ocean: 官方文档\n我们需要知道的几点：\n 是一个插件， 旨在为Pipeline提供丰富的体验 ； 连续交付（CD）Pipeline的复杂可视化，允许快速和直观地了解Pipeline的状态； 目前支持的类型仅针对于Pipeline，尚不能替代Jenkins 经典版UI  思考：\n 每个项目都把大量的pipeline脚本写在Jenkins端，对于谁去维护及维护成本是一个问题 没法做版本控制    Jenkinsflie Jenkins Pipeline 提供了一套可扩展的工具，用于将“简单到复杂”的交付流程实现为“持续交付即代码”。Jenkins Pipeline 的定义通常被写入到一个文本文件（称为 Jenkinsfile ）中，该文件可以被放入项目的源代码控制库中。\n演示1：使用Jenkinsfile管理pipeline  在项目中新建Jenkinsfile文件，拷贝已有script内容 配置pipeline任务，流水线定义为Pipeline Script from SCM 执行push 代码测试  Jenkinsfile:\njenkins/pipelines/p2.yaml\npipeline {  agent { label \u0026#39;172.21.51.68\u0026#39;}   stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;check\u0026#39;) {  steps {  checkout([$class: \u0026#39;GitSCM\u0026#39;, branches: [[name: \u0026#39;*/master\u0026#39;]], doGenerateSubmoduleConfigurations: false, extensions: [], submoduleCfg: [], userRemoteConfigs: [[credentialsId: \u0026#39;gitlab-user\u0026#39;, url: \u0026#39;http://gitlab.luffy.com/root/myblog.git\u0026#39;]]])  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker build . -t myblog:latest\u0026#39;}  }  }  stage(\u0026#39;send-msg\u0026#39;) {  steps {  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=67e81175c6ebacb1307e83f62680f36fbcf4524e8f43971cf2fb2049bc58723d\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;我就是我, 是不一样的烟火\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  }  } } 演示2：优化及丰富流水线内容   优化代码检出阶段\n由于目前已经配置了使用git仓库地址，且使用SCM来检测项目，因此代码检出阶段完全没有必要再去指定一次\n  构建镜像的tag使用git的commit id\n  增加post阶段的消息通知，丰富通知内容\n  配置webhook，实现myblog代码推送后，触发Jenkinsfile任务执行\n  jenkins/pipelines/p3.yaml\npipeline {  agent { label \u0026#39;172.21.51.68\u0026#39;}   stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;check\u0026#39;) {  steps {  checkout scm  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker build . -t myblog:${GIT_COMMIT}\u0026#39;}  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=67e81175c6ebacb1307e83f62680f36fbcf4524e8f43971cf2fb2049bc58723d\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;😄👍构建成功👍😄\\n 关键字：luffy\\n 项目名称: ${JOB_BASE_NAME}\\n Commit Id: ${GIT_COMMIT}\\n 构建地址：${RUN_DISPLAY_URL}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=67e81175c6ebacb1307e83f62680f36fbcf4524e8f43971cf2fb2049bc58723d\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;😖❌构建失败❌😖\\n 关键字：luffy\\n 项目名称: ${JOB_BASE_NAME}\\n Commit Id: ${GIT_COMMIT}\\n 构建地址：${RUN_DISPLAY_URL}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 演示3：使用k8s部署服务   新建deploy目录，将k8s所需的文件放到deploy目录中\n  将镜像地址改成模板，在pipeline中使用新构建的镜像进行替换\n  执行kubectl apply -f deploy应用更改，需要配置kubectl认证\n$ scp -r k8s-master:/root/.kube /root   jenkins/pipelines/p4.yaml\npipeline {  agent { label \u0026#39;172.21.51.68\u0026#39;}   environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/myblog\u0026#34;  }   stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;check\u0026#39;) {  steps {  checkout scm  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker build . -t ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  }  stage(\u0026#39;push-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker push ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${IMAGE_REPO}:${GIT_COMMIT}#g\u0026#39; deploy/*\u0026#34;  timeout(time: 1, unit: \u0026#39;MINUTES\u0026#39;) {  sh \u0026#34;kubectl apply -f deploy/\u0026#34;  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=67e81175c6ebacb1307e83f62680f36fbcf4524e8f43971cf2fb2049bc58723d\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;😄👍构建成功👍😄\\n 关键字：myblog\\n 项目名称: ${JOB_BASE_NAME}\\n Commit Id: ${GIT_COMMIT}\\n 构建地址：${RUN_DISPLAY_URL}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=67e81175c6ebacb1307e83f62680f36fbcf4524e8f43971cf2fb2049bc58723d\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;😖❌构建失败❌😖\\n 关键字：luffy\\n 项目名称: ${JOB_BASE_NAME}\\n Commit Id: ${GIT_COMMIT}\\n 构建地址：${RUN_DISPLAY_URL}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 演示4：使用凭据管理敏感信息 上述Jenkinsfile中存在的问题是敏感信息使用明文，暴漏在代码中，如何管理流水线中的敏感信息（包含账号密码），之前我们在对接gitlab的时候，需要账号密码，已经使用过凭据来管理这类敏感信息，同样的，我们可以使用凭据来存储钉钉的token信息，那么，创建好凭据后，如何在Jenkinsfile中获取已有凭据的内容？\nJenkins 的声明式流水线语法有一个 credentials() 辅助方法（在environment 指令中使用），它支持 secret 文本，带密码的用户名，以及 secret 文件凭据。\n下面的流水线代码片段展示了如何创建一个使用带密码的用户名凭据的环境变量的流水线。\n在该示例中，带密码的用户名凭据被分配了环境变量，用来使你的组织或团队以一个公用账户访问 Bitbucket 仓库；这些凭据已在 Jenkins 中配置了凭据 ID jenkins-bitbucket-common-creds。\n当在 environment 指令中设置凭据环境变量时：\nenvironment { BITBUCKET_COMMON_CREDS = credentials(\u0026#39;jenkins-bitbucket-common-creds\u0026#39;) } 这实际设置了下面的三个环境变量：\n BITBUCKET_COMMON_CREDS - 包含一个以冒号分隔的用户名和密码，格式为 username:password。 BITBUCKET_COMMON_CREDS_USR - 附加的一个仅包含用户名部分的变量。 BITBUCKET_COMMON_CREDS_PSW - 附加的一个仅包含密码部分的变量。  pipeline {  agent {  // 此处定义 agent 的细节  }  environment {  //顶层流水线块中使用的 environment 指令将适用于流水线中的所有步骤。  BITBUCKET_COMMON_CREDS = credentials(\u0026#39;jenkins-bitbucket-common-creds\u0026#39;)  }  stages {  stage(\u0026#39;Example stage 1\u0026#39;) {  //在一个 stage 中定义的 environment 指令只会将给定的环境变量应用于 stage 中的步骤。  environment {  BITBUCKET_COMMON_CREDS = credentials(\u0026#39;another-credential-id\u0026#39;)  }  steps {  //  }  }  stage(\u0026#39;Example stage 2\u0026#39;) {  steps {  //  }  }  } } 因此对Jenkinsfile做改造：\njenkins/pipelines/p5.yaml\npipeline {  agent { label \u0026#39;172.21.51.68\u0026#39;}   environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/myblog\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  }   stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  echo \u0026#39;Hello World\u0026#39;  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;check\u0026#39;) {  steps {  checkout scm  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker build . -t ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  }  stage(\u0026#39;push-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker push ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${IMAGE_REPO}:${GIT_COMMIT}#g\u0026#39; deploy/*\u0026#34;  timeout(time: 1, unit: \u0026#39;MINUTES\u0026#39;) {  sh \u0026#34;kubectl apply -f deploy/\u0026#34;  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;😄👍构建成功👍😄\\n 关键字：luffy\\n 项目名称: ${JOB_BASE_NAME}\\n Commit Id: ${GIT_COMMIT}\\n 构建地址：${RUN_DISPLAY_URL}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;, \u0026#34;text\u0026#34;: { \u0026#34;content\u0026#34;: \u0026#34;😖❌构建失败❌😖\\n 关键字：luffy\\n 项目名称: ${JOB_BASE_NAME}\\n Commit Id: ${GIT_COMMIT}\\n 构建地址：${RUN_DISPLAY_URL}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 本章小结 上面我们已经通过Jenkinsfile完成了最简单的项目的构建和部署，那么我们来思考目前的方式：\n 目前都是在项目的单一分支下进行操作，企业内一般会使用feature、develop、release、master等多个分支来管理整个代码提交流程，如何根据不同的分支来做构建？ 构建视图中如何区分不同的分支? 如何不配置webhook的方式实现构建？ 如何根据不同的分支选择发布到不同的环境(开发、测试、生产)？    多分支流水线 官方示例\n我们简化一下流程，假如使用develop分支作为开发分支，master分支作为集成测试分支，看一下如何使用多分支流水线来管理。\n演示1：多分支流水线的使用  提交develop分支：  $ git checkout -b develop $ git push --set-upstream origin develop  禁用pipeline项目\n  Jenkins端创建多分支流水线项目\n 增加git分支源 发现标签 根据名称过滤，develop|master|v.* 高级克隆，设置浅克隆    保存后，会自动检索项目中所有存在Jenkinsfile文件的分支和标签，若匹配我们设置的过滤正则表达式，则会添加到多分支的构建视图中。所有添加到视图中的分支和标签，会默认执行一次构建任务。\n演示2：美化消息通知内容  添加构建阶段记录 使用markdown格式，添加构建分支消息  jenkins/pipelines/p6.yaml\npipeline {  agent { label \u0026#39;172.21.51.68\u0026#39;}   environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/myblog\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  TAB_STR = \u0026#34;\\n \\n\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026#34;  }   stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  script{  sh \u0026#34;git log --oneline -n 1 \u0026gt; gitlog.file\u0026#34;  env.GIT_LOG = readFile(\u0026#34;gitlog.file\u0026#34;).trim()  }  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;checkout\u0026#39;) {  steps {  checkout scm  script{  env.BUILD_TASKS = env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker build . -t ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;push-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker push ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${IMAGE_REPO}:${GIT_COMMIT}#g\u0026#39; deploy/*\u0026#34;  timeout(time: 1, unit: \u0026#39;MINUTES\u0026#39;) {  sh \u0026#34;kubectl apply -f deploy/\u0026#34;  }  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😄👍 构建成功 👍😄 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${GIT_BRANCH} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😖❌ 构建失败 ❌😖 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${GIT_BRANCH} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 演示3：通知gitlab构建状态 Jenkins端做了构建，可以通过gitlab通过的api将构建状态通知过去，作为开发人员发起Merge Request或者合并Merge Request的依据之一。\n注意一定要指定gitLabConnection(\u0026lsquo;gitlab\u0026rsquo;)，不然没法认证到Gitlab端\njenkins/pipelines/p7.yaml\npipeline {  agent { label \u0026#39;172.21.51.68\u0026#39;}   options { \tbuildDiscarder(logRotator(numToKeepStr: \u0026#39;10\u0026#39;)) \tdisableConcurrentBuilds() \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}   environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/demo/myblog\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  TAB_STR = \u0026#34;\\n \\n\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026#34;  }   stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  script{  sh \u0026#34;git log --oneline -n 1 \u0026gt; gitlog.file\u0026#34;  env.GIT_LOG = readFile(\u0026#34;gitlog.file\u0026#34;).trim()  }  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;checkout\u0026#39;) {  steps {  checkout scm  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS = env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker build . -t ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;push-image\u0026#39;) {  steps {  retry(2) { sh \u0026#39;docker push ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${IMAGE_REPO}:${GIT_COMMIT}#g\u0026#39; deploy/*\u0026#34;  timeout(time: 1, unit: \u0026#39;MINUTES\u0026#39;) {  sh \u0026#34;kubectl apply -f deploy/\u0026#34;  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😄👍 构建成功 👍😄 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😖❌ 构建失败 ❌😖 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 我们可以访问gitlab，然后找到commit记录，查看同步状态\n提交merge request，也可以查看到相关的任务状态，可以作为项目owner合并代码的依据之一：\n本章小节 优势:\n 根据分支展示, 视图人性化 自动检测各分支的变更  思考：\n Jenkins的slave端，没有任务的时候处于闲置状态，slave节点多的话造成资源浪费 是否可以利用kubernetes的Pod来启动slave，动态slave pod来执行构建任务    工具集成与Jenkinsfile实践篇  Jenkins如何对接kubernetes集群 使用kubernetes的Pod-Template来作为动态的agent执行Jenkins任务 如何制作agent容器实现不同类型的业务的集成 集成代码扫描、docker镜像自动构建、k8s服务部署、自动化测试  集成Kubernetes 插件安装及配置 插件官方文档\n  [系统管理] -\u0026gt; [插件管理] -\u0026gt; [搜索kubernetes]-\u0026gt;直接安装\n若安装失败，请先更新bouncycastle API Plugin并重新启动Jenkins\n  [系统管理] -\u0026gt; [系统配置] -\u0026gt; [Add a new cloud]\n  配置地址信息\n Kubernetes 地址: https://kubernetes.default（或者https://172.21.51.67:6443） Kubernetes 命名空间：jenkins 服务证书不用写（我们在安装Jenkins的时候已经指定过serviceAccount），均使用默认 连接测试，成功会提示：Connection test successful Jenkins地址：http://jenkins:8080 Jenkins 通道 ：jenkins:50000    配置Pod Template\n 名称：jnlp-slave 命名空间：jenkins 标签列表：jnlp-slave，作为agent的label选择用 连接 Jenkins 的超时时间（秒） ：300，设置连接jenkins超时时间 节点选择器：agent=true 工作空间卷：选择hostpath，设置/opt/jenkins_jobs/,注意需要设置chown -R 1000:1000 /opt/jenkins_jobs/权限，否则Pod没有权限    演示动态slave pod # 为准备运行jnlp-slave-agent的pod的节点打上label $ kubectl label node k8s-slave1 agent=true  ### 回放一次多分支流水线develop分支 agent { label \u0026#39;jnlp-slave\u0026#39;} 执行任务，会下载默认的jnlp-slave镜像，地址为jenkins/inbound-agent:4.3-4，我们可以先在k8s-master节点拉取下来该镜像：\n$ docker pull jenkins/inbound-agent:4.3-4 保存jenkinsfile提交后，会出现报错，因为我们的agent已经不再是宿主机，而是Pod中的容器内，报错如下：\n因此我们需要将用到的命令行工具集成到Pod的容器内，但是思考如下问题：\n 目前是用的jnlp的容器，是java的环境，我们在此基础上需要集成很多工具，能不能创建一个新的容器，让新容器来做具体的任务，jnlp-slave容器只用来负责连接jenkins-master 针对不同的构建环境（java、python、go、nodejs），可以制作不同的容器，来执行对应的任务    Pod-Template中容器镜像的制作 为解决上述问题，我们制作一个tools镜像，集成常用的工具，来完成常见的构建任务，需要注意的几点：\n 使用alpine基础镜像，自身体积比较小 替换国内安装源 为了使用docker，安装了docker 为了克隆代码，安装git 为了后续做python的测试等任务，安装python环境 为了在容器中调用kubectl的命令，拷贝了kubectl的二进制文件 为了认证kubectl，需要在容器内部生成.kube目录及config文件  $ mkdir tools; $ cd tools; $ cp `which kubectl` . $ cp ~/.kube/config . Dockerfile\njenkins/custom-images/tools/Dockerfile\nFROMalpineLABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34;USERrootRUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; \\  apk update \u0026amp;\u0026amp; \\  apk add --no-cache openrc docker git curl tar gcc g++ make \\  bash shadow openjdk8 python2 python2-dev py-pip python3-dev openssl-dev libffi-dev \\  libstdc++ harfbuzz nss freetype ttf-freefont \u0026amp;\u0026amp; \\  mkdir -p /root/.kube \u0026amp;\u0026amp; \\  usermod -a -G docker rootCOPY config /root/.kube/RUN rm -rf /var/cache/apk/* #-----------------安装 kubectl--------------------#COPY kubectl /usr/local/bin/RUN chmod +x /usr/local/bin/kubectl# ------------------------------------------------#执行镜像构建并推送到仓库中：\n$ docker build . -t 172.21.51.67:5000/devops/tools:v1 $ docker push 172.21.51.67:5000/devops/tools:v1 我们可以直接使用该镜像做测试：\n## 启动临时镜像做测试 $ docker run --rm -ti 172.21.51.67:5000/devops/tools:v1 bash # / git clone http://xxxxxx.git # / kubectl get no # / python3 #/ docker  ## 重新挂载docker的sock文件 docker run -v /var/run/docker.sock:/var/run/docker.sock --rm -ti 172.21.51.67:5000/devops/tools:v1 bash 实践通过Jenkinsfile实现demo项目自动发布到kubenetes环境 更新Jenkins中的PodTemplate，添加tools镜像，注意同时要先添加名为jnlp的container，因为我们是使用自定义的PodTemplate覆盖掉默认的模板：\n在卷栏目，添加卷，Host Path Volume，不然在容器中使用docker会提示docker服务未启动\ntools容器做好后，我们需要对Jenkinsfile做如下调整：\njenkins/pipelines/p8.yaml\npipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}   options { \tbuildDiscarder(logRotator(numToKeepStr: \u0026#39;10\u0026#39;)) \tdisableConcurrentBuilds() \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}   environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/myblog\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  TAB_STR = \u0026#34;\\n \\n\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026#34;  }   stages {  stage(\u0026#39;printenv\u0026#39;) {  steps {  script{  sh \u0026#34;git log --oneline -n 1 \u0026gt; gitlog.file\u0026#34;  env.GIT_LOG = readFile(\u0026#34;gitlog.file\u0026#34;).trim()  }  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS = env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  retry(2) { sh \u0026#39;docker build . -t ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;push-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  retry(2) { sh \u0026#39;docker push ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${IMAGE_REPO}:${GIT_COMMIT}#g\u0026#39; deploy/*\u0026#34;  timeout(time: 1, unit: \u0026#39;MINUTES\u0026#39;) {  sh \u0026#34;kubectl apply -f deploy/\u0026#34;  }  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😄👍 构建成功 👍😄 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😖❌ 构建失败 ❌😖 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } }   集成sonarQube实现代码扫描 Sonar可以从以下七个维度检测代码质量，而作为开发人员至少需要处理前5种代码质量问题。\n 不遵循代码标准 sonar可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具规范代码编写。 潜在的缺陷 sonar可以通过PMD,CheckStyle,Findbugs等等代码规则检测工具检 测出潜在的缺陷。 糟糕的复杂度分布 文件、类、方法等，如果复杂度过高将难以改变，这会使得开发人员 难以理解它们, 且如果没有自动化的单元测试，对于程序中的任何组件的改变都将可能导致需要全面的回归测试。 重复 显然程序中包含大量复制粘贴的代码是质量低下的，sonar可以展示 源码中重复严重的地方。 注释不足或者过多 没有注释将使代码可读性变差，特别是当不可避免地出现人员变动 时，程序的可读性将大幅下降 而过多的注释又会使得开发人员将精力过多地花费在阅读注释上，亦违背初衷。 缺乏单元测试 sonar可以很方便地统计并展示单元测试覆盖率。 糟糕的设计 通过sonar可以找出循环，展示包与包、类与类之间的相互依赖关系，可以检测自定义的架构规则 通过sonar可以管理第三方的jar包，可以利用LCOM4检测单个任务规则的应用情况， 检测耦合。  sonarqube架构简介  CS架构  sonarqube scanner sonarqube server   SonarQube Scanner 扫描仪在本地执行代码扫描任务 执行完后，将分析报告被发送到SonarQube服务器进行处理 SonarQube服务器处理和存储分析报告导致SonarQube数据库，并显示结果在UI中  sonarqube on kubernetes环境搭建  资源文件准备  sonar/sonar.yaml\n 和gitlab共享postgres数据库 使用ingress地址 sonar.luffy.com 进行访问 使用initContainers进行系统参数调整  apiVersion: v1 kind: Service metadata:  name: sonarqube  namespace: jenkins  labels:  app: sonarqube spec:  ports:  - name: sonarqube  port: 9000  targetPort: 9000  protocol: TCP  selector:  app: sonarqube --- apiVersion: apps/v1 kind: Deployment metadata:  namespace: jenkins  name: sonarqube  labels:  app: sonarqube spec:  replicas: 1  selector:  matchLabels:  app: sonarqube  template:  metadata:  labels:  app: sonarqube  spec:  nodeSelector:  sonar: \u0026#34;true\u0026#34;  initContainers:  - command:  - /sbin/sysctl  - -w  - vm.max_map_count=262144  image: alpine:3.6  imagePullPolicy: IfNotPresent  name: elasticsearch-logging-init  resources: {}  securityContext:  privileged: true  containers:  - name: sonarqube  image: 172.21.51.67:5000/sonarqube:7.9-community  ports:  - containerPort: 9000  env:  - name: SONARQUBE_JDBC_USERNAME  valueFrom:  secretKeyRef:  name: gitlab-secret  key: postgres.user.root  - name: SONARQUBE_JDBC_PASSWORD  valueFrom:  secretKeyRef:  name: gitlab-secret  key: postgres.pwd.root  - name: SONARQUBE_JDBC_URL  value: \u0026#34;jdbc:postgresql://postgres:5432/sonar\u0026#34;  livenessProbe:  httpGet:  path: /sessions/new  port: 9000  initialDelaySeconds: 60  periodSeconds: 30  readinessProbe:  httpGet:  path: /sessions/new  port: 9000  initialDelaySeconds: 60  periodSeconds: 30  failureThreshold: 6  resources:  limits:  cpu: 2000m  memory: 4096Mi  requests:  cpu: 300m  memory: 512Mi --- apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: sonarqube  namespace: jenkins spec:  rules:  - host: sonar.luffy.com  http:  paths:  - backend:  serviceName: sonarqube  servicePort: 9000  path: / status:  loadBalancer: {}  sonarqube服务端安装\n# 创建sonar数据库 $ kubectl -n jenkins exec -ti postgres-5859dc6f58-mgqz9 bash #/ psql  # create database sonar;  ## 创建sonarqube服务器 $ kubectl create -f sonar.yaml  ## 配置本地hosts解析 172.21.51.67 sonar.luffy.com  ## 访问sonarqube，初始用户名密码为 admin/admin $ curl http://sonar.luffy.com   sonar-scanner的安装\n下载地址： https://binaries.sonarsource.com/Distribution/sonar-scanner-cli/sonar-scanner-cli-4.2.0.1873-linux.zip。该地址比较慢，可以在网盘下载（https://pan.baidu.com/s/1SiEhWyHikTiKl5lEMX1tJg 提取码: tqb9）。\n  演示sonar代码扫描功能\n  在项目根目录中准备配置文件 sonar-project.properties\nsonar.projectKey=myblog sonar.projectName=myblog # if you want disabled the DTD verification for a proxy problem for example, true by default sonar.coverage.dtdVerification=false # JUnit like test report, default value is test.xml sonar.sources=blog,myblog   配置sonarqube服务器地址\n由于sonar-scanner需要将扫描结果上报给sonarqube服务器做质量分析，因此我们需要在sonar-scanner中配置sonarqube的服务器地址：\n在集群宿主机中测试，先配置一下hosts文件，然后配置sonar的地址：\n$ cat /etc/hosts 172.21.51.67 sonar.luffy.com  $ cat sonar-scanner/conf/sonar-scanner.properties #----- Default SonarQube server #sonar.host.url=http://localhost:9000 sonar.host.url=http://sonar.luffy.com #----- Default source code encoding #sonar.sourceEncoding=UTF-8      - 为了使所有的pod都可以通过`sonar.luffy.com`访问，可以配置coredns的静态解析 ```powershell hosts { 172.21.51.67 jenkins.luffy.com gitlab.luffy.com sonar.luffy.com fallthrough }   执行扫描\n## 在项目的根目录下执行 $ /opt/sonar-scanner-4.0.0.1744-linux/bin/sonar-scanner -X   sonarqube界面查看结果\n登录sonarqube界面查看结果，Quality Gates说明\n  插件安装及配置   集成到tools容器中\n由于我们的代码拉取、构建任务均是在tools容器中进行，因此我们需要把scanner集成到我们的tools容器中，又因为scanner是一个cli客户端，因此我们直接把包解压好，拷贝到tools容器内部，配置一下PATH路径即可，注意两点：\n  直接在在tools镜像中配置http://sonar.luffy.com\n  由于tools已经集成了java环境，因此可以直接剔除scanner自带的jre\n  删掉sonar-scanner/jre目录\n  修改sonar-scanner/bin/sonar-scanner\nuse_embedded_jre=false\n    $ cd tools $ cp -r /opt/sonar-scanner-4.0.0.1744-linux/ sonar-scanner ## sonar配置，由于我们是在Pod中使用，也可以直接配置：sonar.host.url=http://sonarqube:9000 $ cat sonar-scanner/conf/sonar-scanner.properties #----- Default SonarQube server sonar.host.url=http://sonar.luffy.com  #----- Default source code encoding #sonar.sourceEncoding=UTF-8  $ rm -rf sonar-scanner/jre $ vi sonar-scanner/bin/sonar-scanner ... use_embedded_jre=false ... Dockerfile\njenkins/custom-images/tools/Dockerfile2\nFROMalpineLABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34;USERrootRUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; \\  apk update \u0026amp;\u0026amp; \\  apk add --no-cache openrc docker git curl tar gcc g++ make \\  bash shadow openjdk8 python2 python2-dev py-pip python3-dev openssl-dev libffi-dev \\  libstdc++ harfbuzz nss freetype ttf-freefont \u0026amp;\u0026amp; \\  mkdir -p /root/.kube \u0026amp;\u0026amp; \\  usermod -a -G docker rootCOPY config /root/.kube/RUN rm -rf /var/cache/apk/*#-----------------安装 kubectl--------------------#COPY kubectl /usr/local/bin/RUN chmod +x /usr/local/bin/kubectl# ------------------------------------------------##---------------安装 sonar-scanner-----------------#COPY sonar-scanner /usr/lib/sonar-scannerRUN ln -s /usr/lib/sonar-scanner/bin/sonar-scanner /usr/local/bin/sonar-scanner \u0026amp;\u0026amp; chmod +x /usr/local/bin/sonar-scannerENV SONAR_RUNNER_HOME=/usr/lib/sonar-scanner# ------------------------------------------------#  重新构建镜像，并推送到仓库：\n $ docker build . -t 172.21.51.67:5000/devops/tools:v2  $ docker push 172.21.51.67:5000/devops/tools:v2   修改Jenkins PodTemplate\n为了在新的构建任务中可以拉取v2版本的tools镜像，需要更新PodTemplate\n  安装并配置sonar插件\n由于sonarqube的扫描的结果需要进行Quality Gates的检测，那么我们在容器中执行完代码扫描任务后，如何知道本次扫描是否通过了Quality Gates，那么就需要借助于sonarqube实现的jenkins的插件。\n  安装插件\n插件中心搜索sonarqube，直接安装\n  配置插件\n系统管理-\u0026gt;系统配置-\u0026gt; SonarQube servers -\u0026gt;Add SonarQube\n  Name：sonarqube\n  Server URL：http://sonar.luffy.com\n  Server authentication token\n① 登录sonarqube -\u0026gt; My Account -\u0026gt; Security -\u0026gt; Generate Token\n② 登录Jenkins，添加全局凭据，类型为Secret text\n    如何在jenkinsfile中使用\n我们在 https://jenkins.io/doc/pipeline/steps/sonar/ 官方介绍中可以看到：\n    Jenkinsfile集成sonarqube演示 jenkins/pipelines/p9.yaml\npipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}   options { \tbuildDiscarder(logRotator(numToKeepStr: \u0026#39;10\u0026#39;)) \tdisableConcurrentBuilds() \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}   environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/myblog\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  TAB_STR = \u0026#34;\\n \\n\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026#34;  }   stages {  stage(\u0026#39;git-log\u0026#39;) {  steps {  script{  sh \u0026#34;git log --oneline -n 1 \u0026gt; gitlog.file\u0026#34;  env.GIT_LOG = readFile(\u0026#34;gitlog.file\u0026#34;).trim()  }  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS = env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;CI\u0026#39;){  failFast true  parallel {  stage(\u0026#39;Unit Test\u0026#39;) {  steps {  echo \u0026#34;Unit Test Stage Skip...\u0026#34;  }  }  stage(\u0026#39;Code Scan\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  withSonarQubeEnv(\u0026#39;sonarqube\u0026#39;) {  sh \u0026#39;sonar-scanner -X\u0026#39;  sleep 3  }  script {  timeout(1) {  def qg = waitForQualityGate(\u0026#39;sonarqube\u0026#39;)  if (qg.status != \u0026#39;OK\u0026#39;) {  error \u0026#34;未通过Sonarqube的代码质量阈检查，请及时修改！failure: ${qg.status}\u0026#34;  }  }  }  }  }  }  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  retry(2) { sh \u0026#39;docker build . -t ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;push-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  retry(2) { sh \u0026#39;docker push ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${IMAGE_REPO}:${GIT_COMMIT}#g\u0026#39; deploy/*\u0026#34;  timeout(time: 1, unit: \u0026#39;MINUTES\u0026#39;) {  sh \u0026#34;kubectl apply -f deploy/\u0026#34;  }  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😄👍 构建成功 👍😄 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😖❌ 构建失败 ❌😖 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } }   集成RobotFramework实现验收测试 一个基于Python语言，用于验收测试和验收测试驱动开发（ATDD）的通用测试自动化框架，提供了一套特定的语法，并且有非常丰富的测试库 。\nrobot用例简介 robot/robot.txt\n*** Settings *** Library RequestsLibrary Library SeleniumLibrary  *** Variables *** ${demo_url} http://myblog.luffy/admin  *** Test Cases *** api  [Tags] critical  Create Session api ${demo_url}  ${alarm_system_info} RequestsLibrary.Get Request api /  log ${alarm_system_info.status_code}  log ${alarm_system_info.content}  should be true ${alarm_system_info.status_code} == 200  ui  [Tags] critical  ${chrome_options} = Evaluate sys.modules[\u0026#39;selenium.webdriver\u0026#39;].ChromeOptions() sys, selenium.webdriver  Call Method ${chrome_options} add_argument headless  Call Method ${chrome_options} add_argument no-sandbox  ${options}= Call Method ${chrome_options} to_capabilities  Open Browser ${demo_url}/ browser=chrome desired_capabilities=${options}  sleep 2s  Capture Page Screenshot  Page Should Contain Django  close browser # 使用tools镜像启动容器，来验证手动使用robotframework来做验收测试 $ docker run --rm -ti 172.21.51.67:5000/devops/tools:v2 bash bash-5.0# apk add chromium chromium-chromedriver $ cat requirements.txt robotframework robotframework-seleniumlibrary robotframework-databaselibrary robotframework-requests  #pip安装必要的软件包 $ pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txt  #使用robot命令做测试 $ robot -d artifacts/ robot.txt 与tools工具镜像集成 FROM alpine LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34; USER root  RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories \u0026amp;\u0026amp; \\  apk update \u0026amp;\u0026amp; \\  apk add --no-cache openrc docker git curl tar gcc g++ make \\  bash shadow openjdk8 python2 python2-dev py-pip python3-dev openssl-dev libffi-dev \\  libstdc++ harfbuzz nss freetype ttf-freefont chromium chromium-chromedriver \u0026amp;\u0026amp; \\  mkdir -p /root/.kube \u0026amp;\u0026amp; \\  usermod -a -G docker root   COPY config /root/.kube/  COPY requirements.txt /  RUN pip install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txt   RUN rm -rf /var/cache/apk/* \u0026amp;\u0026amp; \\  rm -rf ~/.cache/pip  #-----------------安装 kubectl--------------------# COPY kubectl /usr/local/bin/ RUN chmod +x /usr/local/bin/kubectl # ------------------------------------------------#  #---------------安装 sonar-scanner-----------------# COPY sonar-scanner /usr/lib/sonar-scanner RUN ln -s /usr/lib/sonar-scanner/bin/sonar-scanner /usr/local/bin/sonar-scanner \u0026amp;\u0026amp; chmod +x /usr/local/bin/sonar-scanner ENV SONAR_RUNNER_HOME=/usr/lib/sonar-scanner # ------------------------------------------------# $ docker build . -t 172.21.51.67:5000/devops/tools:v3  $ docker push 172.21.51.67:5000/devops/tools:v3 更新Jenkins中kubernetes中的containers template\n插件安装及配置 为什么要安装robot插件？\n  安装robotFramework\n 插件中心搜索robotframework，直接安装 tools集成robot命令（之前已经安装）    与jenkinsfile的集成\n container(\u0026#39;tools\u0026#39;) {  sh \u0026#39;robot -i critical -d artifacts/ robot.txt || echo ok\u0026#39;  echo \u0026#34;R ${currentBuild.result}\u0026#34;  step([  $class : \u0026#39;RobotPublisher\u0026#39;,  outputPath: \u0026#39;artifacts/\u0026#39;,  outputFileName : \u0026#34;output.xml\u0026#34;,  disableArchiveOutput : false,  passThreshold : 80,  unstableThreshold: 20.0,  onlyCritical : true,  otherFiles : \u0026#34;*.png\u0026#34;  ])  echo \u0026#34;R ${currentBuild.result}\u0026#34;  archiveArtifacts artifacts: \u0026#39;artifacts/*\u0026#39;, fingerprint: true  }   实践通过Jenkinsfile实现demo项目的验收测试 python-demo项目添加robot.txt文件：\njenkins/pipelines/p10.yaml\npipeline {  agent { label \u0026#39;jnlp-slave\u0026#39;}   options { \tbuildDiscarder(logRotator(numToKeepStr: \u0026#39;10\u0026#39;)) \tdisableConcurrentBuilds() \ttimeout(time: 20, unit: \u0026#39;MINUTES\u0026#39;) \tgitLabConnection(\u0026#39;gitlab\u0026#39;) \t}   environment {  IMAGE_REPO = \u0026#34;172.21.51.67:5000/myblog\u0026#34;  DINGTALK_CREDS = credentials(\u0026#39;dingTalk\u0026#39;)  TAB_STR = \u0026#34;\\n \\n\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026amp;nbsp;\u0026#34;  }   stages {  stage(\u0026#39;git-log\u0026#39;) {  steps {  script{  sh \u0026#34;git log --oneline -n 1 \u0026gt; gitlog.file\u0026#34;  env.GIT_LOG = readFile(\u0026#34;gitlog.file\u0026#34;).trim()  }  sh \u0026#39;printenv\u0026#39;  }  }  stage(\u0026#39;checkout\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  checkout scm  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS = env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;CI\u0026#39;){  failFast true  parallel {  stage(\u0026#39;Unit Test\u0026#39;) {  steps {  echo \u0026#34;Unit Test Stage Skip...\u0026#34;  }  }  stage(\u0026#39;Code Scan\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  withSonarQubeEnv(\u0026#39;sonarqube\u0026#39;) {  sh \u0026#39;sonar-scanner -X\u0026#39;  sleep 3  }  script {  timeout(1) {  def qg = waitForQualityGate(\u0026#39;sonarqube\u0026#39;)  if (qg.status != \u0026#39;OK\u0026#39;) {  error \u0026#34;未通过Sonarqube的代码质量阈检查，请及时修改！failure: ${qg.status}\u0026#34;  }  }  }  }  }  }  }  }  stage(\u0026#39;build-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  retry(2) { sh \u0026#39;docker build . -t ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;push-image\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  retry(2) { sh \u0026#39;docker push ${IMAGE_REPO}:${GIT_COMMIT}\u0026#39;}  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;deploy\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  sh \u0026#34;sed -i \u0026#39;s#{{IMAGE_URL}}#${IMAGE_REPO}:${GIT_COMMIT}#g\u0026#39; deploy/*\u0026#34;  timeout(time: 1, unit: \u0026#39;MINUTES\u0026#39;) {  sh \u0026#34;kubectl apply -f deploy/;sleep 20;\u0026#34;  }  }  updateGitlabCommitStatus(name: env.STAGE_NAME, state: \u0026#39;success\u0026#39;)  script{  env.BUILD_TASKS += env.STAGE_NAME + \u0026#34;√...\u0026#34; + env.TAB_STR  }  }  }  stage(\u0026#39;Accept Test\u0026#39;) {  steps {  container(\u0026#39;tools\u0026#39;) {  sh \u0026#39;robot -i critical -d artifacts/ robot.txt|| echo ok\u0026#39;  echo \u0026#34;R ${currentBuild.result}\u0026#34;  step([  $class : \u0026#39;RobotPublisher\u0026#39;,  outputPath: \u0026#39;artifacts/\u0026#39;,  outputFileName : \u0026#34;output.xml\u0026#34;,  disableArchiveOutput : false,  passThreshold : 80,  unstableThreshold: 20.0,  onlyCritical : true,  otherFiles : \u0026#34;*.png\u0026#34;  ])  echo \u0026#34;R ${currentBuild.result}\u0026#34;  archiveArtifacts artifacts: \u0026#39;artifacts/*\u0026#39;, fingerprint: true  }  }  }  }  post {  success {  echo \u0026#39;Congratulations!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😄👍 构建成功 👍😄 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  failure {  echo \u0026#39;Oh no!\u0026#39;  sh \u0026#34;\u0026#34;\u0026#34; curl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=${DINGTALK_CREDS_PSW}\u0026#39; \\ -H \u0026#39;Content-Type: application/json\u0026#39; \\ -d \u0026#39;{ \u0026#34;msgtype\u0026#34;: \u0026#34;markdown\u0026#34;, \u0026#34;markdown\u0026#34;: { \u0026#34;title\u0026#34;:\u0026#34;myblog\u0026#34;, \u0026#34;text\u0026#34;: \u0026#34;😖❌ 构建失败 ❌😖 \\n**项目名称**：luffy \\n**Git log**: ${GIT_LOG} \\n**构建分支**: ${BRANCH_NAME} \\n**构建地址**：${RUN_DISPLAY_URL} \\n**构建任务**：${BUILD_TASKS}\u0026#34; } }\u0026#39; \u0026#34;\u0026#34;\u0026#34;  }  always {  echo \u0026#39;I will always say Hello again!\u0026#39;  }  } } 在Jenkins中查看robot的构建结果。\n  小结 思路：\n 讲解最基础的Jenkins的使用 Pipeline流水线的使用 Jenkinsfile的使用 多分支流水线的使用 与Kubernetes集成，动态jnlp slave pod的使用 与sonarqube集成，实现代码扫描 与Robotframework集成，实现验收测试  问题：\n Jenkinsfile过于冗长 多个项目配置Jenkinsfile，存在很多重复内容 没有实现根据不同分支来部署到不同的环境 Java项目的构建 k8s部署后，采用等待的方式执行后续步骤，不合理  ","permalink":"https://iblog.zone/archives/%E4%BB%8E%E9%9B%B6%E5%BC%80%E5%A7%8B%E6%9E%84%E5%BB%BA%E5%9F%BA%E4%BA%8Ekubernetes%E7%9A%84devops%E5%B9%B3%E5%8F%B0/","summary":"基于Kubernetes的DevOps平台实践 持续集成工具：\n Jenkins gitlabci Tekton  本章基于k8s集群部署gitlab、sonarQube、Jenkins等工具，并把上述工具集成到Jenkins中，以Django项目和SpringBoot项目为例，通过多分支流水线及Jenkinsfile实现项目代码提交到不同的仓库分支，实现自动代码扫描、单元测试、docker容器构建、k8s服务的自动部署。\n DevOps、CI、CD介绍 Jenkins、sonarQube、gitlab的快速部署 Jenkins初体验 流水线入门及Jenkinsfile使用 Jenkins与Kubernetes的集成 sonarQube代码扫描与Jenkins的集成 实践Django项目的基于Jenkinsfile实现开发、测试环境的CI/CD  DevOps、CI、CD介绍 Continuous Integration (CI) / Continuous Delivery (CD)\n软件交付流程\n一个软件从零开始到最终交付，大概包括以下几个阶段：规划、编码、构建、测试、发布、部署和维护，基于这些阶段，我们的软件交付模型大致经历了几个阶段：\n瀑布式流程 前期需求确立之后，软件开发人员花费数周和数月编写代码，把所有需求一次性开发完，然后将代码交给QA（质量保障）团队进行测试，然后将最终的发布版交给运维团队去部署。瀑布模型，简单来说，就是等一个阶段所有工作完成之后，再进入下一个阶段。这种模式的问题也很明显，产品迭代周期长，灵活性差。一个周期动辄几周几个月，适应不了当下产品需要快速迭代的场景。\n敏捷开发 任务由大拆小，开发、测试协同工作，注重开发敏捷，不重视交付敏捷\nDevOps 开发、测试、运维协同工作, 持续开发+持续交付。\n我们是否可以认为DevOps = 提倡开发、测试、运维协同工作来实现持续开发、持续交付的一种软件交付模式？\n大家想一下为什么最初的开发模式没有直接进入DevOps的时代？\n原因是：沟通成本。\n各角色人员去沟通协作的时候都是手动去做，交流靠嘴，靠人去指挥，很显然会出大问题。所以说不能认为DevOps就是一种交付模式，因为解决不了沟通协作成本，这种模式就不具备可落地性。\n那DevOps时代如何解决角色之间的成本问题？DevOps的核心就是自动化。自动化的能力靠什么来支撑，工具和技术。\nDevOps工具链\n靠这些工具和技术，才实现了自动化流程，进而解决了协作成本，使得devops具备了可落地性。因此我们可以大致给devops一个定义：\ndevops = 提倡开发、测试、运维协同工作来实现持续开发、持续交付的一种软件交付模式 + 基于工具和技术支撑的自动化流程的落地实践。\n因此devops不是某一个具体的技术，而是一种思想+自动化能力，来使得构建、测试、发布软件能够更加地便捷、频繁和可靠的落地实践。本次课程核心内容就是要教会大家如何利用工具和技术来实现完整的DevOps平台的建设。我们主要使用的工具有：\n gitlab，代码仓库，企业内部使用最多的代码版本管理工具。 Jenkins， 一个可扩展的持续集成引擎，用于自动化各种任务，包括构建、测试和部署软件。 robotFramework， 基于Python的自动化测试框架 sonarqube，代码质量管理平台 maven，java包构建管理工具 Kubernetes Docker  Jenkins初体验 Kubernetes环境中部署jenkins 其他部署方式\n注意点：\n 第一次启动很慢 因为后面Jenkins会与kubernetes集群进行集成，会需要调用kubernetes集群的api，因此安装的时候创建了ServiceAccount并赋予了cluster-admin的权限 默认部署到jenkins=true的节点 初始化容器来设置权限 ingress来外部访问 数据存储通过hostpath挂载到宿主机中  jenkins/jenkins-all.","title":"从零开始构建基于Kubernetes的Devops平台"},{"content":"一、问题排查 由于资源问题，我们很多服务都共用一台机器，某天突发发现vpn登录不正常，连接后闪断频繁，登录机器查看，iowait特别高\n通过iotop命令查看发现是mysql进程占用高，造成系统卡顿\n二、问题处理 进入mysql，使用show full processlist 可以看到所有链接的情况，但是大多链接的 state 其实是 Sleep 的，这种的其实是空闲状态，没有太多查看价值\n我们要观察的是有问题的，所以可以进行过滤：\n-- 查询非 Sleep 状态的链接，按消耗时间倒序展示，自己加条件过滤 select id, db, user, host, command, time, state, info from information_schema.processlist where command != \u0026#39;Sleep\u0026#39; order by time desc; 这样就过滤出来哪些是正在干活的，然后按照消耗时间倒叙展示，排在最前面的，极大可能就是有问题的链接了，然后查看 info 一列，就能看到具体执行的什么 SQL 语句了，针对分析\n展示列解释：\n id - 线程ID，可以用：kill id; 杀死一个线程，很有用 db - 数据库 user - 用户 host - 连库的主机IP command - 当前执行的命令，比如最常见的：Sleep，Query，Connect 等 time - 消耗时间，单位秒，很有用 state - 执行状态，比如：Sending data，Sorting for group，Creating tmp table，Locked等等，很有用，其他状态可以看看本文最后的参考文章 info - 执行的SQL语句，很有用  kill 使用 上面提到的 线程ID 是可以通过 kill 杀死的；所以上面基本上可以把有问题的执行语句找出来，然后就可以 kill 掉了，那么一个一个来 kill 么？\n-- 查询执行时间超过2分钟的线程，然后拼接成 kill 语句 select concat(\u0026#39;kill \u0026#39;, id, \u0026#39;;\u0026#39;) from information_schema.processlist where command != \u0026#39;Sleep\u0026#39; and time \u0026gt; 2*60 order by time desc; 在下一步我就不用说了吧，把拼接 kill 的执行结果跑一遍就搞定了\n这个有时候非常好用，谁用谁知道，还有个快速的办法，使用以下命令杀死所有进程\nmysql -e \u0026#34;show full processlist;\u0026#34; -ss | awk \u0026#39;{print \u0026#34;KILL \u0026#34;$1\u0026#34;;\u0026#34;}\u0026#39;| mysql 常见问题 一些问题会导致连锁反应，而且不太好定位，有时候以为是慢查询，很可能是大多时间是在等在CPU、内存资源的释放，所以有时候同一个查询消耗的时间有时候差异很大\n总结了一些常见问题：\n CPU报警：很可能是 SQL 里面有较多的计算导致的 连接数超高：很可能是有慢查询，然后导致很多的查询在排队，排查问题的时候可以看到”事发现场“类似的 SQL 语句一大片，那么有可能是没有索引或者索引不好使，可以用：explain 分析一下 SQL 语句  ","permalink":"https://iblog.zone/archives/linux-iowait%E9%AB%98%E9%97%AE%E9%A2%98%E6%8E%92%E6%9F%A5%E5%8F%8A%E5%A4%84%E7%90%86/","summary":"一、问题排查 由于资源问题，我们很多服务都共用一台机器，某天突发发现vpn登录不正常，连接后闪断频繁，登录机器查看，iowait特别高\n通过iotop命令查看发现是mysql进程占用高，造成系统卡顿\n二、问题处理 进入mysql，使用show full processlist 可以看到所有链接的情况，但是大多链接的 state 其实是 Sleep 的，这种的其实是空闲状态，没有太多查看价值\n我们要观察的是有问题的，所以可以进行过滤：\n-- 查询非 Sleep 状态的链接，按消耗时间倒序展示，自己加条件过滤 select id, db, user, host, command, time, state, info from information_schema.processlist where command != \u0026#39;Sleep\u0026#39; order by time desc; 这样就过滤出来哪些是正在干活的，然后按照消耗时间倒叙展示，排在最前面的，极大可能就是有问题的链接了，然后查看 info 一列，就能看到具体执行的什么 SQL 语句了，针对分析\n展示列解释：\n id - 线程ID，可以用：kill id; 杀死一个线程，很有用 db - 数据库 user - 用户 host - 连库的主机IP command - 当前执行的命令，比如最常见的：Sleep，Query，Connect 等 time - 消耗时间，单位秒，很有用 state - 执行状态，比如：Sending data，Sorting for group，Creating tmp table，Locked等等，很有用，其他状态可以看看本文最后的参考文章 info - 执行的SQL语句，很有用  kill 使用 上面提到的 线程ID 是可以通过 kill 杀死的；所以上面基本上可以把有问题的执行语句找出来，然后就可以 kill 掉了，那么一个一个来 kill 么？","title":"Linux iowait高问题排查及处理"},{"content":"第四天 Kubernetes集群的日志及监控 k8s日志收集架构 https://kubernetes.io/docs/concepts/cluster-administration/logging/\n总体分为三种方式：\n 使用在每个节点上运行的节点级日志记录代理。 在应用程序的 pod 中，包含专门记录日志的 sidecar 容器。 将日志直接从应用程序中推送到日志记录后端。  使用节点级日志代理 容器日志驱动：\nhttps://docs.docker.com/config/containers/logging/configure/\n查看当前的docker主机的驱动：\n$ docker info --format \u0026#39;{{.LoggingDriver}}\u0026#39; json-file格式，docker会默认将标准和错误输出保存为宿主机的文件，路径为：\n/var/lib/docker/containers/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log\n并且可以设置日志轮转：\n{  \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;,  \u0026#34;log-opts\u0026#34;: {  \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34;,  \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34;,  \u0026#34;labels\u0026#34;: \u0026#34;production_status\u0026#34;,  \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;  } } 优势：\n 部署方便，使用DaemonSet类型控制器来部署agent即可 对业务应用的影响最小，没有侵入性  劣势:\n 只能收集标准和错误输出，对于容器内的文件日志，暂时收集不到  使用 sidecar 容器和日志代理   方式一：sidecar 容器将应用程序日志传送到自己的标准输出。 思路：在pod中启动一个sidecar容器，把容器内的日志文件吐到标准输出，由宿主机中的日志收集agent进行采集。\n$ cat count-pod.yaml apiVersion: v1 kind: Pod metadata:  name: counter spec:  containers:  - name: count  image: busybox  args:  - /bin/sh  - -c  - \u0026gt;  i=0;  while true;  do  echo \u0026#34;$i: $(date)\u0026#34; \u0026gt;\u0026gt; /var/log/1.log;  echo \u0026#34;$(date)INFO $i\u0026#34; \u0026gt;\u0026gt; /var/log/2.log;  i=$((i+1));  sleep 1;  done  volumeMounts:  - name: varlog  mountPath: /var/log  - name: count-log-1  image: busybox  args: [/bin/sh, -c, \u0026#39;tail -n+1 -f /var/log/1.log\u0026#39;]  volumeMounts:  - name: varlog  mountPath: /var/log  - name: count-log-2  image: busybox  args: [/bin/sh, -c, \u0026#39;tail -n+1 -f /var/log/2.log\u0026#39;]  volumeMounts:  - name: varlog  mountPath: /var/log  volumes:  - name: varlog  emptyDir: {}  $ kubectl create -f counter-pod.yaml $ kubectl logs -f counter -c count-log-1 优势：\n 可以实现容器内部日志收集 对业务应用的侵入性不大  劣势：\n 每个业务pod都需要做一次改造 增加了一次日志的写入，对磁盘使用率有一定影响    方式二：sidecar 容器运行一个日志代理，配置该日志代理以便从应用容器收集日志。 思路：直接在业务Pod中使用sidecar的方式启动一个日志收集的组件（比如fluentd），这样日志收集可以将容器内的日志当成本地文件来进行收取。\n优势：不用往宿主机存储日志，本地日志完全可以收集\n劣势：每个业务应用额外启动一个日志agent，带来额外的资源损耗\n  从应用中直接暴露日志目录 企业日志方案选型 目前来讲，最建议的是采用节点级的日志代理。\n方案一：自研方案，实现一个自研的日志收集agent，大致思路：\n 针对容器的标准输出及错误输出，使用常规的方式，监听宿主机中的容器输出路径即可 针对容器内部的日志文件  在容器内配置统一的环境变量，比如LOG_COLLECT_FILES，指定好容器内待收集的日志目录及文件 agent启动的时候挂载docker.sock文件及磁盘的根路径 监听docker的容器新建、删除事件，通过docker的api，查出容器的存储、环境变量、k8s属性等信息 配置了LOG_COLLECT_FILES环境变量的容器，根据env中的日志路径找到主机中对应的文件路径，然后生成收集的配置文件 agent与开源日志收集工具（Fluentd或者filebeat等）配合，agent负责下发配置到收集工具中并对进程做reload    方案二：日志使用开源的Agent进行收集（EFK方案），适用范围广，可以满足绝大多数日志收集、展示的需求。\n实践使用EFK实现业务日志收集 EFK架构工作流程   Elasticsearch\n一个开源的分布式、Restful 风格的搜索和数据分析引擎，它的底层是开源库Apache Lucene。它可以被下面这样准确地形容：\n 一个分布式的实时文档存储，每个字段可以被索引与搜索； 一个分布式实时分析搜索引擎； 能胜任上百个服务节点的扩展，并支持 PB 级别的结构化或者非结构化数据。    Kibana\nKibana是一个开源的分析和可视化平台，设计用于和Elasticsearch一起工作。可以通过Kibana来搜索，查看，并和存储在Elasticsearch索引中的数据进行交互。也可以轻松地执行高级数据分析，并且以各种图标、表格和地图的形式可视化数据。\n  Fluentd\n一个针对日志的收集、处理、转发系统。通过丰富的插件系统，可以收集来自于各种系统或应用的日志，转化为用户指定的格式后，转发到用户所指定的日志存储系统之中。\nFluentd 通过一组给定的数据源抓取日志数据，处理后（转换成结构化的数据格式）将它们转发给其他服务，比如 Elasticsearch、对象存储、kafka等等。Fluentd 支持超过300个日志存储和分析服务，所以在这方面是非常灵活的。主要运行步骤如下\n 首先 Fluentd 从多个日志源获取数据 结构化并且标记这些数据 然后根据匹配的标签将数据发送到多个目标服务    Fluentd精讲 Fluentd架构 为什么推荐使用fluentd作为k8s体系的日志收集工具？\n  云原生：https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch\n  将日志文件JSON化\n  可插拔架构设计\n  极小的资源占用\n基于C和Ruby语言， 30-40MB，13,000 events/second/core\n  极强的可靠性\n 基于内存和本地文件的缓存 强大的故障转移    fluentd事件流的生命周期及指令配置 https://docs.fluentd.org/v/0.12/quickstart/life-of-a-fluentd-event\nInput -\u0026gt; filter 1 -\u0026gt; ... -\u0026gt; filter N -\u0026gt; Buffer -\u0026gt; Output 启动命令\n$ fluentd -c fluent.conf 指令介绍：\n  source ，数据源，对应Input 通过使用 source 指令，来选择和配置所需的输入插件来启用 Fluentd 输入源， source 把事件提交到 fluentd 的路由引擎中。使用type来区分不同类型的数据源。如下配置可以监听指定文件的追加输入：\n\u0026lt;source\u0026gt;  @type tail  path /var/log/httpd-access.log  pos_file /var/log/td-agent/httpd-access.log.pos  tag myapp.access  format apache2 \u0026lt;/source\u0026gt;   filter，Event processing pipeline（事件处理流）\nfilter 可以串联成 pipeline，对数据进行串行处理，最终再交给 match 输出。 如下可以对事件内容进行处理：\n\u0026lt;source\u0026gt;  @type http  port 9880 \u0026lt;/source\u0026gt;  \u0026lt;filter myapp.access\u0026gt;  @type record_transformer  \u0026lt;record\u0026gt;  host_param “#{Socket.gethostname}”  \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt; filter 获取数据后，调用内置的 @type record_transformer 插件，在事件的 record 里插入了新的字段 host_param，然后再交给 match 输出。\n  label指令\n可以在 source 里指定 @label，这个 source 所触发的事件就会被发送给指定的 label 所包含的任务，而不会被后续的其他任务获取到。\n\u0026lt;source\u0026gt;  @type forward \u0026lt;/source\u0026gt;  \u0026lt;source\u0026gt; ### 这个任务指定了 label 为 @SYSTEM ### 会被发送给 \u0026lt;label @SYSTEM\u0026gt; ### 而不会被发送给下面紧跟的 filter 和 match  @type tail  @label @SYSTEM  path /var/log/httpd-access.log  pos_file /var/log/td-agent/httpd-access.log.pos  tag myapp.access  format apache2 \u0026lt;/source\u0026gt;  \u0026lt;filter access.**\u0026gt;  @type record_transformer  \u0026lt;record\u0026gt;  # …  \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt;  \u0026lt;match **\u0026gt;  @type elasticsearch  # … \u0026lt;/match\u0026gt;  \u0026lt;label @SYSTEM\u0026gt;  ### 将会接收到上面 @type tail 的 source event  \u0026lt;filter var.log.middleware.**\u0026gt;  @type grep  # …  \u0026lt;/filter\u0026gt;   \u0026lt;match **\u0026gt;  @type s3  # …  \u0026lt;/match\u0026gt; \u0026lt;/label\u0026gt;   match，匹配输出\n查找匹配 “tags” 的事件，并处理它们。match 命令的最常见用法是将事件输出到其他系统（因此，与 match 命令对应的插件称为 “输出插件”）\n\u0026lt;source\u0026gt;  @type http  port 9880 \u0026lt;/source\u0026gt;  \u0026lt;filter myapp.access\u0026gt;  @type record_transformer  \u0026lt;record\u0026gt;  host_param “#{Socket.gethostname}”  \u0026lt;/record\u0026gt; \u0026lt;/filter\u0026gt;  \u0026lt;match myapp.access\u0026gt;  @type file  path /var/log/fluent/access \u0026lt;/match\u0026gt;   事件的结构：\ntime：事件的处理时间\ntag：事件的来源，在fluentd.conf中配置\nrecord：真实的日志内容，json对象\n比如，下面这条原始日志：\n192.168.0.1 - - [28/Feb/2013:12:00:00 +0900] \u0026#34;GET / HTTP/1.1\u0026#34; 200 777 经过fluentd 引擎处理完后的样子可能是：\n2020-07-16 08:40:35 +0000 apache.access: {\u0026#34;user\u0026#34;:\u0026#34;-\u0026#34;,\u0026#34;method\u0026#34;:\u0026#34;GET\u0026#34;,\u0026#34;code\u0026#34;:200,\u0026#34;size\u0026#34;:777,\u0026#34;host\u0026#34;:\u0026#34;192.168.0.1\u0026#34;,\u0026#34;path\u0026#34;:\u0026#34;/\u0026#34;} fluentd的buffer事件缓冲模型 Input -\u0026gt; filter 1 -\u0026gt; ... -\u0026gt; filter N -\u0026gt; Buffer -\u0026gt; Output 因为每个事件数据量通常很小，考虑数据传输效率、稳定性等方面的原因，所以基本不会每条事件处理完后都会立马写入到output端，因此fluentd建立了缓冲模型，模型中主要有两个概念：\n buffer_chunk：事件缓冲块，用来存储本地已经处理完待发送至目的端的事件，可以设置每个块的大小。 buffer_queue：存储chunk的队列，可以设置长度  可以设置的参数，主要有：\n buffer_type，缓冲类型，可以设置file或者memory buffer_chunk_limit，每个chunk块的大小，默认8MB buffer_queue_limit ，chunk块队列的最大长度，默认256 flush_interval ，flush一个chunk的时间间隔 retry_limit ，chunk块发送失败重试次数，默认17次，之后就丢弃该chunk数据 retry_wait ，重试发送chunk数据的时间间隔，默认1s，第2次失败再发送的话，间隔2s，下次4秒，以此类推  大致的过程为：\n随着fluentd事件的不断生成并写入chunk，缓存块持变大，当缓存块满足buffer_chunk_limit大小或者新的缓存块诞生超过flush_interval时间间隔后，会推入缓存queue队列尾部，该队列大小由buffer_queue_limit决定。\n每次有新的chunk入列，位于队列最前部的chunk块会立即写入配置的存储后端，比如配置的是kafka，则立即把数据推入kafka中。\n比较理想的情况是每次有新的缓存块进入缓存队列，则立马会被写入到后端，同时，新缓存块也持续入列，但是入列的速度不会快于出列的速度，这样基本上缓存队列处于空的状态，队列中最多只有一个缓存块。\n但是实际情况考虑网络等因素，往往缓存块被写入后端存储的时候会出现延迟或者写入失败的情况，当缓存块写入后端失败时，该缓存块还会留在队列中，等retry_wait时间后重试发送，当retry的次数达到retry_limit后，该缓存块被销毁（数据被丢弃）。\n此时缓存队列持续有新的缓存块进来，如果队列中存在很多未及时写入到后端存储的缓存块的话，当队列长度达到buffer_queue_limit大小，则新的事件被拒绝，fluentd报错，error_class=Fluent::Plugin::Buffer::BufferOverflowError error=\u0026ldquo;buffer space has too many data\u0026rdquo;。\n还有一种情况是网络传输缓慢的情况，若每3秒钟会产生一个新块，但是写入到后端时间却达到了30s钟，队列长度为100，那么每个块出列的时间内，又有新的10个块进来，那么队列很快就会被占满，导致异常出现。\n实践一：实现业务应用日志的收集及字段解析 目标：收集容器内的nginx应用的access.log日志，并解析日志字段为JSON格式，原始日志的格式为：\n$ tail -f access.log ... 53.49.146.149 1561620585.973 0.005 502 [27/Jun/2019:15:29:45 +0800] 178.73.215.171 33337 GET https 收集并处理成：\n{  \u0026#34;serverIp\u0026#34;: \u0026#34;53.49.146.149\u0026#34;,  \u0026#34;timestamp\u0026#34;: \u0026#34;1561620585.973\u0026#34;,  \u0026#34;respondTime\u0026#34;: \u0026#34;0.005\u0026#34;,  \u0026#34;httpCode\u0026#34;: \u0026#34;502\u0026#34;,  \u0026#34;eventTime\u0026#34;: \u0026#34;27/Jun/2019:15:29:45 +0800\u0026#34;,  \u0026#34;clientIp\u0026#34;: \u0026#34;178.73.215.171\u0026#34;,  \u0026#34;clientPort\u0026#34;: \u0026#34;33337\u0026#34;,  \u0026#34;method\u0026#34;: \u0026#34;GET\u0026#34;,  \u0026#34;protocol\u0026#34;: \u0026#34;https\u0026#34; } 思路：\n 配置fluent.conf  使用@tail插件通过监听access.log文件 用filter实现对nginx日志格式解析   启动fluentd服务 手动追加内容至access.log文件 观察本地输出内容是否符合预期  fluent.conf\n\u0026lt;source\u0026gt; \t@type tail \t@label @nginx_access \tpath /fluentd/access.log \tpos_file /fluentd/nginx_access.posg \ttag nginx_access \tformat none \t@log_level trace \u0026lt;/source\u0026gt; \u0026lt;label @nginx_access\u0026gt;  \u0026lt;filter nginx_access\u0026gt;  @type parser \tkey_name message \tformat /(?\u0026lt;serverIp\u0026gt;[^ ]*) (?\u0026lt;timestamp\u0026gt;[^ ]*) (?\u0026lt;respondTime\u0026gt;[^ ]*) (?\u0026lt;httpCode\u0026gt;[^ ]*) \\[(?\u0026lt;eventTime\u0026gt;[^\\]]*)\\] (?\u0026lt;clientIp\u0026gt;[^ ]*) (?\u0026lt;clientPort\u0026gt;[^ ]*) (?\u0026lt;method\u0026gt;[^ ]*) (?\u0026lt;protocol\u0026gt;[^ ]*)/  \u0026lt;/filter\u0026gt;  \u0026lt;match nginx_access\u0026gt;  @type stdout  \u0026lt;/match\u0026gt; \u0026lt;/label\u0026gt; 启动服务，追加文件内容：\n$ docker run -u root --rm -ti 172.21.51.67:5000/fluentd_elasticsearch/fluentd:v2.5.2 sh / # cd /fluentd/ / # touch access.log / # fluentd -c /fluentd/etc/fluent.conf / # echo \u0026#39;53.49.146.149 1561620585.973 0.005 502 [27/Jun/2019:15:29:45 +0800] 178.73.215.171 33337 GET https\u0026#39; \u0026gt;\u0026gt;/fluentd/access.log 使用该网站进行正则校验： http://fluentular.herokuapp.com\n实践二：使用ruby实现日志字段的转换及自定义处理 \u0026lt;source\u0026gt; \t@type tail \t@label @nginx_access \tpath /fluentd/access.log \tpos_file /fluentd/nginx_access.posg \ttag nginx_access \tformat none \t@log_level trace \u0026lt;/source\u0026gt; \u0026lt;label @nginx_access\u0026gt;  \u0026lt;filter nginx_access\u0026gt;  @type parser \tkey_name message \tformat /(?\u0026lt;serverIp\u0026gt;[^ ]*) (?\u0026lt;timestamp\u0026gt;[^ ]*) (?\u0026lt;respondTime\u0026gt;[^ ]*) (?\u0026lt;httpCode\u0026gt;[^ ]*) \\[(?\u0026lt;eventTime\u0026gt;[^\\]]*)\\] (?\u0026lt;clientIp\u0026gt;[^ ]*) (?\u0026lt;clientPort\u0026gt;[^ ]*) (?\u0026lt;method\u0026gt;[^ ]*) (?\u0026lt;protocol\u0026gt;[^ ]*)/  \u0026lt;/filter\u0026gt;  \u0026lt;filter nginx_access\u0026gt; \t@type record_transformer \tenable_ruby  \u0026lt;record\u0026gt; \thost_name \u0026#34;#{Socket.gethostname}\u0026#34;  my_key \u0026#34;my_val\u0026#34;  tls ${record[\u0026#34;protocol\u0026#34;].index(\u0026#34;https\u0026#34;) ? \u0026#34;true\u0026#34; : \u0026#34;false\u0026#34;}  \u0026lt;/record\u0026gt;  \u0026lt;/filter\u0026gt;  \u0026lt;match nginx_access\u0026gt;  @type stdout  \u0026lt;/match\u0026gt; \u0026lt;/label\u0026gt; ConfigMap的配置文件挂载使用场景 开始之前，我们先来回顾一下，configmap的常用的挂载场景。\n场景一：单文件挂载到空目录 假如业务应用有一个配置文件，名为 application-1.conf，如果想将此配置挂载到pod的/etc/application/目录中。\napplication-1.conf的内容为：\n$ cat application-1.conf name: \u0026#34;application\u0026#34; platform: \u0026#34;linux\u0026#34; purpose: \u0026#34;demo\u0026#34; company: \u0026#34;luffy\u0026#34; version: \u0026#34;v2.1.0\u0026#34; 该配置文件在k8s中可以通过configmap来管理，通常我们有如下两种方式来管理配置文件：\n  通过kubectl命令行来生成configmap\n# 通过文件直接创建 $ kubectl -n default create configmap application-config --from-file=application-1.conf  # 会生成配置文件，查看内容，configmap的key为文件名字 $ kubectl -n default get cm application-config -oyaml   通过yaml文件直接创建\n$ cat application-config.yaml apiVersion: v1 kind: ConfigMap metadata:  name: application-config  namespace: default data:  application-1.conf: |  name: \u0026#34;application\u0026#34;  platform: \u0026#34;linux\u0026#34;  purpose: \u0026#34;demo\u0026#34;  company: \u0026#34;luffy\u0026#34;  version: \u0026#34;v2.1.0\u0026#34;  # 创建configmap $ kubectl create -f application-config.yaml   准备一个demo-deployment.yaml文件，挂载上述configmap到/etc/application/中\n$ cat demo-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: demo  namespace: default spec:  selector:  matchLabels:  app: demo  template:  metadata:  labels:  app: demo  spec:  volumes:  - configMap:  name: application-config  name: config  containers:  - name: nginx  image: nginx:alpine  imagePullPolicy: IfNotPresent  volumeMounts:  - mountPath: \u0026#34;/etc/application\u0026#34;  name: config 创建并查看：\n$ kubectl create -f demo-deployment.yaml 修改configmap文件的内容，观察pod中是否自动感知变化：\n$ kubectl edit cm application-config  整个configmap文件直接挂载到pod中，若configmap变化，pod会自动感知并拉取到pod内部。\n但是pod内的进程不会自动重启，所以很多服务会实现一个内部的reload接口，用来加载最新的配置文件到进程中。\n 场景二：多文件挂载 假如有多个配置文件，都需要挂载到pod内部，且都在一个目录中\n$ cat application-1.conf name: \u0026#34;application-1\u0026#34; platform: \u0026#34;linux\u0026#34; purpose: \u0026#34;demo\u0026#34; company: \u0026#34;luffy\u0026#34; version: \u0026#34;v2.1.0\u0026#34; $ cat application-2.conf name: \u0026#34;application-2\u0026#34; platform: \u0026#34;linux\u0026#34; purpose: \u0026#34;demo\u0026#34; company: \u0026#34;luffy\u0026#34; version: \u0026#34;v2.1.0\u0026#34; 同样可以使用两种方式创建：\n$ kubectl delete cm application-config  $ kubectl create cm application-config --from-file=application-1.conf --from-file=application-2.conf  $ kubectl get cm application-config -oyaml 观察Pod已经自动获取到最新的变化\n$ kubectl exec demo-55c649865b-gpkgk ls /etc/application/ application-1.conf application-2.conf 此时，是挂载到pod内的空目录中/etc/application，假如想挂载到pod已存在的目录中，比如：\n$ kubectl exec demo-55c649865b-gpkgk ls /etc/profile.d color_prompt locale 更改deployment的挂载目录：\n$ cat demo-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: demo  namespace: default spec:  selector:  matchLabels:  app: demo  template:  metadata:  labels:  app: demo  spec:  volumes:  - configMap:  name: application-config  name: config  containers:  - name: nginx  image: nginx:alpine  imagePullPolicy: IfNotPresent  volumeMounts:  - mountPath: \u0026#34;/etc/profile.d\u0026#34;  name: config 重建pod\n$ kubectl apply -f demo-deployment.yaml  # 查看pod内的/etc/profile.d目录，发现已有文件被覆盖 $ kubectl exec demo-77d685b9f7-68qz7 ls /etc/profile.d application-1.conf application-2.conf 场景三 挂载子路径 实现多个配置文件，可以挂载到pod内的不同的目录中。比如：\n application-1.conf挂载到/etc/application/ application-2.conf挂载到/etc/profile.d  configmap保持不变，修改deployment文件：\n$ cat demo-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: demo  namespace: default spec:  selector:  matchLabels:  app: demo  template:  metadata:  labels:  app: demo  spec:  volumes:  - name: config  configMap:  name: application-config  items:  - key: application-1.conf  path: application1  - key: application-2.conf  path: application2  containers:  - name: nginx  image: nginx:alpine  imagePullPolicy: IfNotPresent  volumeMounts:  - mountPath: \u0026#34;/etc/application/application-1.conf\u0026#34;  name: config  subPath: application1  - mountPath: \u0026#34;/etc/profile.d/application-2.conf\u0026#34;  name: config  subPath: application2 测试挂载：\n$ kubectl apply -f demo-deployment.yaml  $ kubectl exec demo-78489c754-shjhz ls /etc/application application-1.conf  $ kubectl exec demo-78489c754-shjhz ls /etc/profile.d/ application-2.conf color_prompt locale  使用subPath挂载到Pod内部的文件，不会自动感知原有ConfigMap的变更\n 部署es服务 部署分析  es生产环境是部署es集群，通常会使用statefulset进行部署 es默认使用elasticsearch用户启动进程，es的数据目录是通过宿主机的路径挂载，因此目录权限被主机的目录权限覆盖，因此可以利用initContainer容器在es进程启动之前把目录的权限修改掉，注意init container要用特权模式启动。 若希望使用helm部署，参考 https://github.com/helm/charts/tree/master/stable/elasticsearch  使用StatefulSet管理有状态服务 使用Deployment创建多副本的pod的情况：\napiVersion: apps/v1 kind: Deployment metadata:  name: nginx-deployment  namespace: default  labels:  app: nginx-deployment spec:  replicas: 3  selector:  matchLabels:  app: nginx-deployment  template:  metadata:  labels:  app: nginx-deployment  spec:  containers:  - name: nginx  image: nginx:alpine  ports:  - containerPort: 80 使用StatefulSet创建多副本pod的情况：\napiVersion: apps/v1 kind: StatefulSet metadata:  name: nginx-statefulset  namespace: default  labels:  app: nginx-sts spec:  replicas: 3  serviceName: \u0026#34;nginx\u0026#34;  selector:  matchLabels:  app: nginx-sts  template:  metadata:  labels:  app: nginx-sts  spec:  containers:  - name: nginx  image: nginx:alpine  ports:  - containerPort: 80 无头服务Headless Service\nkind: Service apiVersion: v1 metadata:  name: nginx  namespace: default spec:  selector:  app: nginx-sts  ports:  - protocol: TCP  port: 80  targetPort: 80  clusterIP: None $ kubectl -n default exec -ti nginx-statefulset-0 sh / # curl nginx-statefulset-2.nginx 部署并验证 es-config.yaml\napiVersion: v1 kind: ConfigMap metadata:  name: es-config  namespace: logging data:  elasticsearch.yml: |cluster.name: \u0026#34;luffy-elasticsearch\u0026#34; node.name: \u0026#34;${POD_NAME}\u0026#34; network.host: 0.0.0.0 discovery.seed_hosts: \u0026#34;es-svc-headless\u0026#34; cluster.initial_master_nodes: \u0026#34;elasticsearch-0,elasticsearch-1,elasticsearch-2\u0026#34; es-svc-headless.yaml\napiVersion: v1 kind: Service metadata:  name: es-svc-headless  namespace: logging  labels:  k8s-app: elasticsearch spec:  selector:  k8s-app: elasticsearch  clusterIP: None  ports:  - name: in  port: 9300  protocol: TCP es-statefulset.yaml\napiVersion: apps/v1 kind: StatefulSet metadata:  name: elasticsearch  namespace: logging  labels:  k8s-app: elasticsearch spec:  replicas: 3  serviceName: es-svc-headless  selector:  matchLabels:  k8s-app: elasticsearch  template:  metadata:  labels:  k8s-app: elasticsearch  spec:  initContainers:  - command:  - /sbin/sysctl  - -w  - vm.max_map_count=262144  image: alpine:3.6  imagePullPolicy: IfNotPresent  name: elasticsearch-logging-init  resources: {}  securityContext:  privileged: true  - name: fix-permissions  image: alpine:3.6  command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown -R 1000:1000 /usr/share/elasticsearch/data\u0026#34;]  securityContext:  privileged: true  volumeMounts:  - name: es-data-volume  mountPath: /usr/share/elasticsearch/data  containers:  - name: elasticsearch  image: 172.21.51.67:5000/elasticsearch/elasticsearch:7.4.2  env:  - name: POD_NAME  valueFrom:  fieldRef:  fieldPath: metadata.name  resources:  limits:  cpu: \u0026#39;1\u0026#39;  memory: 2Gi  requests:  cpu: \u0026#39;1\u0026#39;  memory: 2Gi  ports:  - containerPort: 9200  name: db  protocol: TCP  - containerPort: 9300  name: transport  protocol: TCP  volumeMounts:  - name: es-config-volume  mountPath: /usr/share/elasticsearch/config/elasticsearch.yml  subPath: elasticsearch.yml  - name: es-data-volume  mountPath: /usr/share/elasticsearch/data  volumes:  - name: es-config-volume  configMap:  name: es-config  items:  - key: elasticsearch.yml  path: elasticsearch.yml  volumeClaimTemplates:  - metadata:  name: es-data-volume  spec:  accessModes: [\u0026#34;ReadWriteOnce\u0026#34;]  storageClassName: \u0026#34;nfs\u0026#34;  resources:  requests:  storage: 5Gi es-svc.yaml\napiVersion: v1 kind: Service metadata:  name: es-svc  namespace: logging  labels:  k8s-app: elasticsearch spec:  selector:  k8s-app: elasticsearch  ports:  - name: out  port: 9200  protocol: TCP $ kubectl create namespace logging  ## 部署服务 $ kubectl create -f es-config.yaml $ kubectl create -f es-svc-headless.yaml $ kubectl create -f es-sts.yaml $ kubectl create -f es-svc.yaml  ## 等待片刻，查看一下es的pod部署到了k8s-slave1节点，状态变为running $ kubectl -n logging get po -o wide NAME READY STATUS RESTARTS AGE IP elasticsearch-0 1/1 Running 0 15m 10.244.0.126 elasticsearch-1 1/1 Running 0 15m 10.244.0.127 elasticsearch-2 1/1 Running 0 15m 10.244.0.128 # 然后通过curl命令访问一下服务，验证es是否部署成功 $ kubectl -n logging get svc es-svc ClusterIP 10.104.226.175 \u0026lt;none\u0026gt; 9200/TCP 2s es-svc-headless ClusterIP None \u0026lt;none\u0026gt; 9300/TCP 32m $ curl 10.104.226.175:9200 {  \u0026#34;name\u0026#34; : \u0026#34;elasticsearch-2\u0026#34;,  \u0026#34;cluster_name\u0026#34; : \u0026#34;luffy-elasticsearch\u0026#34;,  \u0026#34;cluster_uuid\u0026#34; : \u0026#34;7FDIACx9T-2ajYcB5qp4hQ\u0026#34;,  \u0026#34;version\u0026#34; : {  \u0026#34;number\u0026#34; : \u0026#34;7.4.2\u0026#34;,  \u0026#34;build_flavor\u0026#34; : \u0026#34;default\u0026#34;,  \u0026#34;build_type\u0026#34; : \u0026#34;docker\u0026#34;,  \u0026#34;build_hash\u0026#34; : \u0026#34;2f90bbf7b93631e52bafb59b3b049cb44ec25e96\u0026#34;,  \u0026#34;build_date\u0026#34; : \u0026#34;2019-10-28T20:40:44.881551Z\u0026#34;,  \u0026#34;build_snapshot\u0026#34; : false,  \u0026#34;lucene_version\u0026#34; : \u0026#34;8.2.0\u0026#34;,  \u0026#34;minimum_wire_compatibility_version\u0026#34; : \u0026#34;6.8.0\u0026#34;,  \u0026#34;minimum_index_compatibility_version\u0026#34; : \u0026#34;6.0.0-beta1\u0026#34;  },  \u0026#34;tagline\u0026#34; : \u0026#34;You Know, for Search\u0026#34; 部署kibana 部署分析   kibana需要暴露web页面给前端使用，因此使用ingress配置域名来实现对kibana的访问\n  kibana为无状态应用，直接使用Deployment来启动\n  kibana需要访问es，直接利用k8s服务发现访问此地址即可，http://es-svc:9200\n  部署并验证 efk/kibana.yaml\napiVersion: apps/v1 kind: Deployment metadata:  name: kibana  namespace: logging  labels:  app: kibana spec:  selector:  matchLabels:  app: \u0026#34;kibana\u0026#34;  template:  metadata:  labels:  app: kibana  spec:  containers:  - name: kibana  image: 172.21.51.67:5000/kibana/kibana:7.4.2  resources:  limits:  cpu: 1000m  requests:  cpu: 100m  env:  - name: ELASTICSEARCH_HOSTS  value: http://es-svc:9200  - name: SERVER_NAME  value: kibana-logging  - name: SERVER_REWRITEBASEPATH  value: \u0026#34;false\u0026#34;  ports:  - containerPort: 5601 --- apiVersion: v1 kind: Service metadata:  name: kibana  namespace: logging  labels:  app: kibana spec:  ports:  - port: 5601  protocol: TCP  targetPort: 5601  type: ClusterIP  selector:  app: kibana --- apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: kibana  namespace: logging spec:  rules:  - host: kibana.luffy.com  http:  paths:  - path: /  backend:  serviceName: kibana  servicePort: 5601 $ kubectl create -f kibana.yaml deployment.apps/kibana created service/kibana created ingress/kibana created  ## 配置域名解析 kibana.luffy.com，并访问服务进行验证，若可以访问，说明连接es成功 Fluentd服务部署 部署分析  fluentd为日志采集服务，kubernetes集群的每个业务节点都有日志产生，因此需要使用daemonset的模式进行部署 为进一步控制资源，会为daemonset指定一个选择标签，fluentd=true来做进一步过滤，只有带有此标签的节点才会部署fluentd 日志采集，需要采集哪些目录下的日志，采集后发送到es端，因此需要配置的内容比较多，我们选择使用configmap的方式把配置文件整个挂载出来  部署服务 efk/fluentd-es-config-main.yaml\napiVersion: v1 data:  fluent.conf: |-# This is the root config file, which only includes components of the actual configuration # # Do not collect fluentd\u0026#39;s own logs to avoid infinite loops. \u0026lt;match fluent.**\u0026gt; @type null \u0026lt;/match\u0026gt; @include /fluentd/etc/config.d/*.conf kind: ConfigMap metadata:  labels:  addonmanager.kubernetes.io/mode: Reconcile  name: fluentd-es-config-main  namespace: logging 配置文件，fluentd-config.yaml，注意点：\n 数据源source的配置，k8s会默认把容器的标准和错误输出日志重定向到宿主机中 默认集成了 kubernetes_metadata_filter 插件，来解析日志格式，得到k8s相关的元数据，raw.kubernetes match输出到es端的flush配置  efk/fluentd-configmap.yaml\nkind: ConfigMap apiVersion: v1 metadata:  name: fluentd-config  namespace: logging  labels:  addonmanager.kubernetes.io/mode: Reconcile data:  containers.input.conf: |-\u0026lt;source\u0026gt; @id fluentd-containers.log @type tail path /var/log/containers/*.log pos_file /var/log/es-containers.log.pos time_format %Y-%m-%dT%H:%M:%S.%NZ localtime tag raw.kubernetes.* format json read_from_head false \u0026lt;/source\u0026gt; # Detect exceptions in the log output and forward them as one log entry. # https://github.com/GoogleCloudPlatform/fluent-plugin-detect-exceptions \u0026lt;match raw.kubernetes.**\u0026gt; @id raw.kubernetes @type detect_exceptions remove_tag_prefix raw message log stream stream multiline_flush_interval 5 max_bytes 500000 max_lines 1000 \u0026lt;/match\u0026gt;  output.conf: |-# Enriches records with Kubernetes metadata \u0026lt;filter kubernetes.**\u0026gt; @type kubernetes_metadata \u0026lt;/filter\u0026gt; \u0026lt;match **\u0026gt; @id elasticsearch @type elasticsearch @log_level info include_tag_key true hosts elasticsearch-0.es-svc-headless:9200,elasticsearch-1.es-svc-headless:9200,elasticsearch-2.es-svc-headless:9200 #port 9200 logstash_format true #index_name kubernetes-%Y.%m.%d request_timeout 30s \u0026lt;buffer\u0026gt; @type file path /var/log/fluentd-buffers/kubernetes.system.buffer flush_mode interval retry_type exponential_backoff flush_thread_count 2 flush_interval 5s retry_forever retry_max_interval 30 chunk_limit_size 2M queue_limit_length 8 overflow_action block \u0026lt;/buffer\u0026gt; \u0026lt;/match\u0026gt; daemonset定义文件，fluentd.yaml，注意点：\n 需要配置rbac规则，因为需要访问k8s api去根据日志查询元数据 需要将/var/log/containers/目录挂载到容器中 需要将fluentd的configmap中的配置文件挂载到容器内 想要部署fluentd的节点，需要添加fluentd=true的标签  efk/fluentd.yaml\napiVersion: v1 kind: ServiceAccount metadata:  name: fluentd-es  namespace: logging  labels:  k8s-app: fluentd-es  kubernetes.io/cluster-service: \u0026#34;true\u0026#34;  addonmanager.kubernetes.io/mode: Reconcile --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: fluentd-es  labels:  k8s-app: fluentd-es  kubernetes.io/cluster-service: \u0026#34;true\u0026#34;  addonmanager.kubernetes.io/mode: Reconcile rules: - apiGroups:  - \u0026#34;\u0026#34;  resources:  - \u0026#34;namespaces\u0026#34;  - \u0026#34;pods\u0026#34;  verbs:  - \u0026#34;get\u0026#34;  - \u0026#34;watch\u0026#34;  - \u0026#34;list\u0026#34; --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: fluentd-es  labels:  k8s-app: fluentd-es  kubernetes.io/cluster-service: \u0026#34;true\u0026#34;  addonmanager.kubernetes.io/mode: Reconcile subjects: - kind: ServiceAccount  name: fluentd-es  namespace: logging  apiGroup: \u0026#34;\u0026#34; roleRef:  kind: ClusterRole  name: fluentd-es  apiGroup: \u0026#34;\u0026#34; --- apiVersion: apps/v1 kind: DaemonSet metadata:  labels:  addonmanager.kubernetes.io/mode: Reconcile  k8s-app: fluentd-es  name: fluentd-es  namespace: logging spec:  selector:  matchLabels:  k8s-app: fluentd-es  template:  metadata:  labels:  k8s-app: fluentd-es  spec:  containers:  - env:  - name: FLUENTD_ARGS  value: --no-supervisor -q  image: 172.21.51.67:5000/fluentd_elasticsearch/fluentd:v2.5.2  imagePullPolicy: IfNotPresent  name: fluentd-es  resources:  limits:  memory: 500Mi  requests:  cpu: 100m  memory: 200Mi  volumeMounts:  - mountPath: /var/log  name: varlog  - mountPath: /var/lib/docker/containers  name: varlibdockercontainers  readOnly: true  - mountPath: /fluentd/etc/config.d  name: config-volume  - mountPath: /fluentd/etc/fluent.conf  name: config-volume-main  subPath: fluent.conf  nodeSelector:  fluentd: \u0026#34;true\u0026#34;  securityContext: {}  serviceAccount: fluentd-es  serviceAccountName: fluentd-es  volumes:  - hostPath:  path: /var/log  type: \u0026#34;\u0026#34;  name: varlog  - hostPath:  path: /var/lib/docker/containers  type: \u0026#34;\u0026#34;  name: varlibdockercontainers  - configMap:  defaultMode: 420  name: fluentd-config  name: config-volume  - configMap:  defaultMode: 420  items:  - key: fluent.conf  path: fluent.conf  name: fluentd-es-config-main  name: config-volume-main ## 给slave1打上标签，进行部署fluentd日志采集服务 $ kubectl label node k8s-slave1 fluentd=true $ kubectl label node k8s-slave2 fluentd=true  # 创建服务 $ kubectl create -f fluentd-es-config-main.yaml configmap/fluentd-es-config-main created $ kubectl create -f fluentd-configmap.yaml configmap/fluentd-config created $ kubectl create -f fluentd.yaml serviceaccount/fluentd-es created clusterrole.rbac.authorization.k8s.io/fluentd-es created clusterrolebinding.rbac.authorization.k8s.io/fluentd-es created daemonset.extensions/fluentd-es created  ## 然后查看一下pod是否已经在k8s-slave1 $ kubectl -n logging get po -o wide NAME READY STATUS RESTARTS AGE elasticsearch-logging-0 1/1 Running 0 123m fluentd-es-246pl 1/1 Running 0 2m2s kibana-944c57766-ftlcw 1/1 Running 0 50m 上述是简化版的k8s日志部署收集的配置，完全版的可以提供 https://github.com/kubernetes/kubernetes/tree/master/cluster/addons/fluentd-elasticsearch 来查看。\nEFK功能验证 验证思路 在slave节点中启动服务，同时往标准输出中打印测试日志，到kibana中查看是否可以收集\n创建测试容器 efk/test-pod.yaml\napiVersion: v1 kind: Pod metadata:  name: counter spec:  nodeSelector:  fluentd: \u0026#34;true\u0026#34;  containers:  - name: count  image: alpine:3.6  args: [/bin/sh, -c,  \u0026#39;i=0; while true; do echo \u0026#34;$i: $(date)\u0026#34;; i=$((i+1)); sleep 1; done\u0026#39;] $ kubectl get po NAME READY STATUS RESTARTS AGE counter 1/1 Running 0 6s 配置kibana 登录kibana界面，按照截图的顺序操作：\n也可以通过其他元数据来过滤日志数据，比如可以单击任何日志条目以查看其他元数据，如容器名称，Kubernetes 节点，命名空间等，比如kubernetes.pod_name : counter\n到这里，我们就在 Kubernetes 集群上成功部署了 EFK ，要了解如何使用 Kibana 进行日志数据分析，可以参考 Kibana 用户指南文档：https://www.elastic.co/guide/en/kibana/current/index.html\nPrometheus实现k8s集群的服务监控 Prometheus 是一个开源监控系统，它本身已经成为了云原生中指标监控的事实标准 。\nk8s集群监控体系演变史 第一版本：Cadvisor+InfluxDB+Grafana\n只能从主机维度进行采集，没有Namespace、Pod等维度的汇聚功能\n第二版本： Heapster+InfluxDB+Grafana\nheapster负责调用各node中的cadvisor接口，对数据进行汇总，然后导到InfluxDB ， 可以从cluster，node，pod的各个层面提供详细的资源使用情况。\n第三版本：Metrics-Server + Prometheus\nk8s对监控接口进行了标准化，主要分了三类：\n  Resource Metrics\n对应的接口是 metrics.k8s.io，主要的实现就是 metrics-server，它提供的是资源的监控，比较常见的是节点级别、pod 级别、namespace 级别、class 级别。这类的监控指标都可以通过 metrics.k8s.io 这个接口获取到\n  Custom Metrics\n对应的接口是 custom.metrics.k8s.io，主要的实现是 Prometheus， 它提供的是资源监控和自定义监控，资源监控和上面的资源监控其实是有覆盖关系的。\n自定义监控指的是：比如应用上面想暴露一个类似像在线人数，或者说调用后面的这个数据库的 MySQL 的慢查询。这些其实都是可以在应用层做自己的定义的，然后并通过标准的 Prometheus 的 client，暴露出相应的 metrics，然后再被 Prometheus 进行采集\n  External Metrics\n对应的接口是 external.metrics.k8s.io。主要的实现厂商就是各个云厂商的 provider，通过这个 provider 可以通过云资源的监控指标\n  Prometheus架构  Prometheus Server ，监控、告警平台核心，抓取目标端监控数据，生成聚合数据，存储时间序列数据 exporter，由被监控的对象提供，提供API暴漏监控对象的指标，供prometheus 抓取  node-exporter blackbox-exporter redis-exporter mysql-exporter custom-exporter \u0026hellip;   pushgateway，提供一个网关地址，外部数据可以推送到该网关，prometheus也会从该网关拉取数据 Alertmanager，接收Prometheus发送的告警并对于告警进行一系列的处理后发送给指定的目标 Grafana：配置数据源，图标方式展示数据  Prometheus安装 基于go开发， https://github.com/prometheus/prometheus\n若使用docker部署直接启动镜像即可：\n$ docker run --name prometheus -d -p 127.0.0.1:9090:9090 prom/prometheus 我们想制作Prometheus的yaml文件，可以先启动容器进去看一下默认的启动命令：\n$ docker run -d --name tmp -p 127.0.0.1:9090:9090 prom/prometheus:v2.19.2 $ docker exec -ti tmp sh #/ ps aux #/ cat /etc/prometheus/prometheus.yml global:  scrape_interval: 15s # Set the scrape interval to every 15 seconds. Default is every 1 minute.  evaluation_interval: 15s # Evaluate rules every 15 seconds. The default is every 1 minute.  # scrape_timeout is set to the global default (10s).  # Alertmanager configuration alerting:  alertmanagers:  - static_configs:  - targets:  # - alertmanager:9093  # Load rules once and periodically evaluate them according to the global \u0026#39;evaluation_interval\u0026#39;. rule_files:  # - \u0026#34;first_rules.yml\u0026#34;  # - \u0026#34;second_rules.yml\u0026#34;  # A scrape configuration containing exactly one endpoint to scrape: # Here it\u0026#39;s Prometheus itself. exporter scrape_configs:  # The job name is added as a label `job=\u0026lt;job_name\u0026gt;` to any timeseries scraped from this config.  - job_name: \u0026#39;prometheus\u0026#39;   # metrics_path defaults to \u0026#39;/metrics\u0026#39;  # scheme defaults to \u0026#39;http\u0026#39;.   static_configs:  - targets: [\u0026#39;localhost:9090\u0026#39;]   本例中，使用k8s来部署，所需的资源清单如下：\n# 创建新的命名空间 monitor，存储prometheus相关资源 $ cat prometheus-namespace.yaml apiVersion: v1 kind: Namespace metadata:  name: monitor  # 需要准备配置文件，因此使用configmap的形式保存 $ cat prometheus-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: prometheus-config  namespace: monitor data:  prometheus.yml: |  global:  scrape_interval: 15s  evaluation_interval: 15s  scrape_configs:  - job_name: \u0026#39;prometheus\u0026#39;  static_configs:  - targets: [\u0026#39;localhost:9090\u0026#39;]   # prometheus的资源文件 # 出现Prometheus数据存储权限问题，因为Prometheus内部使用nobody启动进程，挂载数据目录后权限为root，因此使用initContainer进行目录权限修复： $ cat prometheus-deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: prometheus  namespace: monitor  labels:  app: prometheus spec:  selector:  matchLabels:  app: prometheus  template:  metadata:  labels:  app: prometheus  spec:  serviceAccountName: prometheus  nodeSelector:  app: prometheus  initContainers:  - name: \u0026#34;change-permission-of-directory\u0026#34;  image: busybox  command: [\u0026#34;/bin/sh\u0026#34;]  args: [\u0026#34;-c\u0026#34;, \u0026#34;chown -R 65534:65534 /prometheus\u0026#34;]  securityContext:  privileged: true  volumeMounts:  - mountPath: \u0026#34;/etc/prometheus\u0026#34;  name: config-volume  - mountPath: \u0026#34;/prometheus\u0026#34;  name: data  containers:  - image: prom/prometheus:v2.19.2  name: prometheus  args:  - \u0026#34;--config.file=/etc/prometheus/prometheus.yml\u0026#34;  - \u0026#34;--storage.tsdb.path=/prometheus\u0026#34; # 指定tsdb数据路径  - \u0026#34;--web.enable-lifecycle\u0026#34; # 支持热更新，直接执行localhost:9090/-/reload立即生效  - \u0026#34;--web.console.libraries=/usr/share/prometheus/console_libraries\u0026#34;  - \u0026#34;--web.console.templates=/usr/share/prometheus/consoles\u0026#34;  ports:  - containerPort: 9090  name: http  volumeMounts:  - mountPath: \u0026#34;/etc/prometheus\u0026#34;  name: config-volume  - mountPath: \u0026#34;/prometheus\u0026#34;  name: data  resources:  requests:  cpu: 100m  memory: 512Mi  limits:  cpu: 100m  memory: 512Mi  volumes:  - name: data  hostPath:  path: /data/prometheus/  - configMap:  name: prometheus-config  name: config-volume  # rbac,prometheus会调用k8s api做服务发现进行抓取指标 $ cat prometheus-rbac.yaml apiVersion: v1 kind: ServiceAccount metadata:  name: prometheus  namespace: monitor --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRole metadata:  name: prometheus rules: - apiGroups:  - \u0026#34;\u0026#34;  resources:  - nodes  - services  - endpoints  - pods  - nodes/proxy  verbs:  - get  - list  - watch - apiGroups:  - \u0026#34;extensions\u0026#34;  resources:  - ingresses  verbs:  - get  - list  - watch - apiGroups:  - \u0026#34;\u0026#34;  resources:  - configmaps  - nodes/metrics  verbs:  - get - nonResourceURLs:  - /metrics  verbs:  - get --- apiVersion: rbac.authorization.k8s.io/v1beta1 kind: ClusterRoleBinding metadata:  name: prometheus roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: prometheus subjects: - kind: ServiceAccount  name: prometheus  namespace: monitor   # 提供Service，为Ingress使用 $ cat prometheus-svc.yaml apiVersion: v1 kind: Service metadata:  name: prometheus  namespace: monitor  labels:  app: prometheus spec:  selector:  app: prometheus  type: ClusterIP  ports:  - name: web  port: 9090  targetPort: http  $ cat prometheus-ingress.yaml apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: prometheus  namespace: monitor spec:  rules:  - host: prometheus.luffy.com  http:  paths:  - path: /  backend:  serviceName: prometheus  servicePort: 9090 部署上述资源：\n# 命名空间 $ kubectl create prometheus-namespace.yaml  # 给node打上label $ kubectl label node k8s-slave1 app=prometheus  #部署configmap $ kubectl create -f prometheus-configmap.yaml  # rbac $ kubectl create -f prometheus-rbac.yaml  # deployment $ kubectl create -f prometheus-deployment.yaml  # service $ kubectl create -f prometheus-svc.yaml  # ingress $ kubectl create -f prometheus-ingress.yaml  # 访问测试 $ kubectl -n monitor get ingress 理解时间序列数据库（TSDB） # http://localhost:9090/metrics $ kubectl -n monitor get po -o wide prometheus-dcb499cbf-fxttx 1/1 Running 0 13h 10.244.1.132 k8s-slave1  $ curl http://10.244.1.132:9090/metrics ... # HELP promhttp_metric_handler_requests_total Total number of scrapes by HTTP status code. # TYPE promhttp_metric_handler_requests_total counter promhttp_metric_handler_requests_total{code=\u0026#34;200\u0026#34;} 149 promhttp_metric_handler_requests_total{code=\u0026#34;500\u0026#34;} 0 promhttp_metric_handler_requests_total{code=\u0026#34;503\u0026#34;} 0 tsdb（Time Series Database）\n其中#号开头的两行分别为：\n HELP开头说明该行为指标的帮助信息，通常解释指标的含义 TYPE开头是指明了指标的类型  counter 计数器 guage 测量器 histogram 柱状图 summary 采样点分位图统计    其中非#开头的每一行表示当前采集到的一个监控样本：\n promhttp_metric_handler_requests_total表明了当前指标的名称 大括号中的标签则反映了当前样本的一些特征和维度 浮点数则是该监控样本的具体值。  每次采集到的数据都会被Prometheus以time-series（时间序列）的方式保存到内存中，定期刷新到硬盘。如下所示，可以将time-series理解为一个以时间为X轴的数字矩阵：\n ^  │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026#34;cpu0\u0026#34;,mode=\u0026#34;idle\u0026#34;}  │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026#34;cpu0\u0026#34;,mode=\u0026#34;system\u0026#34;}  │ . . . . . . . . . . . . . . . . . . node_load1{}  │ . . . . . . . . . . . . . . . . . .  v  \u0026lt;------------------ 时间 ----------------\u0026gt; 在time-series中的每一个点称为一个样本（sample），样本由以下三部分组成：\n 指标(metric)：metric name和描述当前样本特征的labelsets; 时间戳(timestamp)：一个精确到毫秒的时间戳; 样本值(value)： 一个float64的浮点型数据表示当前样本的值。  在形式上，所有的指标(Metric)都通过如下格式标示：\n\u0026lt;metric name\u0026gt;{\u0026lt;label name\u0026gt;=\u0026lt;label value\u0026gt;, ...}  指标的名称(metric name)可以反映被监控样本的含义（比如，http_request_total - 表示当前系统接收到的HTTP请求总量）。 标签(label)反映了当前样本的特征维度，通过这些维度Prometheus可以对样本数据进行过滤，聚合等。  Prometheus：定期去Tragets列表拉取监控数据，存储到TSDB中，并且提供指标查询、分析的语句和接口。\n添加监控目标 无论是业务应用还是k8s系统组件，只要提供了metrics api，并且该api返回的数据格式满足标准的Prometheus数据格式要求即可。\n其实，很多组件已经为了适配Prometheus采集指标，添加了对应的/metrics api，比如\nCoreDNS：\n$ kubectl -n kube-system get po -owide|grep coredns coredns-58cc8c89f4-nshx2 1/1 Running 6 22d 10.244.0.20 coredns-58cc8c89f4-t9h2r 1/1 Running 7 22d 10.244.0.21  $ curl 10.244.0.20:9153/metrics 修改target配置：\n$ cat prometheus-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: prometheus-config  namespace: monitor data:  prometheus.yml: |  global:  scrape_interval: 15s  scrape_timeout: 15s  scrape_configs:  - job_name: \u0026#39;prometheus\u0026#39;  static_configs:  - targets: [\u0026#39;localhost:9090\u0026#39;]  - job_name: \u0026#39;coredns\u0026#39;  static_configs:  - targets: [\u0026#39;10.96.0.10:9153\u0026#39;]  $ kubectl apply -f prometheus-configmap.yaml  # 重建pod生效 $ kubectl -n monitor delete po prometheus-dcb499cbf-fxttx 常用监控对象的指标采集 对于集群的监控一般我们需要考虑以下几个方面：\n 内部系统组件的状态：比如 kube-apiserver、kube-scheduler、kube-controller-manager、kubedns/coredns 等组件的详细运行状态 Kubernetes 节点的监控：比如节点的 cpu、load、disk、memory 等指标 业务容器指标的监控（容器CPU、内存、磁盘等） 编排级的 metrics：比如 Deployment 的状态、资源请求、调度和 API 延迟等数据指标  监控kube-apiserver apiserver自身也提供了/metrics 的api来提供监控数据，\n$ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 23d  $ curl -k -H \u0026#34;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InhXcmtaSG5ZODF1TVJ6dUcycnRLT2c4U3ZncVdoVjlLaVRxNG1wZ0pqVmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1xNXBueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImViZDg2ODZjLWZkYzAtNDRlZC04NmZlLTY5ZmE0ZTE1YjBmMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbiJ9.iEIVMWg2mHPD88GQ2i4uc_60K4o17e39tN0VI_Q_s3TrRS8hmpi0pkEaN88igEKZm95Qf1qcN9J5W5eqOmcK2SN83Dd9dyGAGxuNAdEwi0i73weFHHsjDqokl9_4RGbHT5lRY46BbIGADIphcTeVbCggI6T_V9zBbtl8dcmsd-lD_6c6uC2INtPyIfz1FplynkjEVLapp_45aXZ9IMy76ljNSA8Uc061Uys6PD3IXsUD5JJfdm7lAt0F7rn9SdX1q10F2lIHYCMcCcfEpLr4Vkymxb4IU4RCR8BsMOPIO_yfRVeYZkG4gU2C47KwxpLsJRrTUcUXJktSEPdeYYXf9w\u0026#34; https://172.21.51.67:6443/metrics 可以通过手动配置如下job来试下对apiserver服务的监控，\n$ cat prometheus-configmap.yaml ...  - job_name: \u0026#39;kubernetes-apiserver\u0026#39;  static_configs:  - targets: [\u0026#39;10.96.0.1\u0026#39;]  scheme: https  tls_config:  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  insecure_skip_verify: true  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 监控集群节点基础指标 node_exporter https://github.com/prometheus/node_exporter\n分析：\n 每个节点都需要监控，因此可以使用DaemonSet类型来管理node_exporter 添加节点的容忍配置 挂载宿主机中的系统文件信息  apiVersion: apps/v1 kind: DaemonSet metadata:  name: node-exporter  namespace: monitor  labels:  app: node-exporter spec:  selector:  matchLabels:  app: node-exporter  template:  metadata:  labels:  app: node-exporter  spec:  hostPID: true  hostIPC: true  hostNetwork: true  nodeSelector:  kubernetes.io/os: linux  containers:  - name: node-exporter  image: prom/node-exporter:v1.0.1  args:  - --web.listen-address=$(HOSTIP):9100  - --path.procfs=/host/proc  - --path.sysfs=/host/sys  - --path.rootfs=/host/root  - --collector.filesystem.ignored-mount-points=^/(dev|proc|sys|var/lib/docker/.+)($|/)  - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|sysfs|tracefs)$  ports:  - containerPort: 9100  env:  - name: HOSTIP  valueFrom:  fieldRef:  fieldPath: status.hostIP  resources:  requests:  cpu: 150m  memory: 180Mi  limits:  cpu: 150m  memory: 180Mi  securityContext:  runAsNonRoot: true  runAsUser: 65534  volumeMounts:  - name: proc  mountPath: /host/proc  - name: sys  mountPath: /host/sys  - name: root  mountPath: /host/root  mountPropagation: HostToContainer  readOnly: true  tolerations:  - operator: \u0026#34;Exists\u0026#34;  volumes:  - name: proc  hostPath:  path: /proc  - name: dev  hostPath:  path: /dev  - name: sys  hostPath:  path: /sys  - name: root  hostPath:  path: / 创建node-exporter服务\n$ kubectl create -f node-exporter.yaml  $ kubectl -n monitor get po 问题来了，如何添加到Prometheus的target中？\n 配置一个Service，后端挂载node-exporter的服务，把Service的地址配置到target中  带来新的问题，target中无法直观的看到各节点node-exporter的状态   把每个node-exporter的服务都添加到target列表中  带来新的问题，集群节点的增删，都需要手动维护列表 target列表维护量随着集群规模增加    Prometheus的服务发现与Relabeling 之前已经给Prometheus配置了RBAC，有读取node的权限，因此Prometheus可以去调用Kubernetes API获取node信息，所以Prometheus通过与 Kubernetes API 集成，提供了内置的服务发现分别是：Node、Service、Pod、Endpoints、Ingress\n配置job即可：\n - job_name: \u0026#39;kubernetes-sd-node-exporter\u0026#39;  kubernetes_sd_configs:  - role: node 重建查看效果：\n$ kubectl apply -f prometheus-configmap.yaml $ kubectl -n monitor delete po prometheus-dcb499cbf-6cwlg 默认访问的地址是http://node-ip/10250/metrics，10250是kubelet API的服务端口，说明Prometheus的node类型的服务发现模式，默认是和kubelet的10250绑定的，而我们是期望使用node-exporter作为采集的指标来源，因此需要把访问的endpoint替换成http://node-ip:9100/metrics。\n在真正抓取数据前，Prometheus提供了relabeling的能力。怎么理解？\n查看Target的Label列，可以发现，每个target对应会有很多Before Relabeling的标签，这些__开头的label是系统内部使用，不会存储到样本的数据里，但是，我们在查看数据的时候，可以发现，每个数据都有两个默认的label，即：\nprometheus_notifications_dropped_total{instance=\u0026#34;localhost:9090\u0026#34;,job=\u0026#34;prometheus\u0026#34;}\tinstance的值其实则取自于__address__\n这种发生在采集样本数据之前，对Target实例的标签进行重写的机制在Prometheus被称为Relabeling。\n因此，利用relabeling的能力，只需要将__address__替换成node_exporter的服务地址即可。\n - job_name: \u0026#39;kubernetes-sd-node-exporter\u0026#39;  kubernetes_sd_configs:  - role: node  relabel_configs:  - source_labels: [__address__]  regex: \u0026#39;(.*):10250\u0026#39;  replacement: \u0026#39;${1}:9100\u0026#39;  target_label: __address__  action: replace 再次更新Prometheus服务后，查看targets列表及node-exporter提供的指标，node_load1\n使用cadvisor实现容器监控指标的采集（废弃） cAdvisor 指标访问路径为 https://10.96.0.1/api/v1/nodes/\u0026lt;node_name\u0026gt;/proxy/metrics/cadvisor\nhttps://10.96.0.1/api/v1/nodes/k8s-master/proxy/metrics/cadvisor https://10.96.0.1/api/v1/nodes/k8s-slave1/proxy/metrics/cadvisor https://10.96.0.1/api/v1/nodes/k8s-slave2/proxy/metrics/cadvisor 分析：\n 每个节点都需要做替换，可以利用Prometheus服务发现中 node这种role   - job_name: \u0026#39;kubernetes-sd-cadvisor\u0026#39;  kubernetes_sd_configs:  - role: node 默认添加的target列表为：__schema__://__address__ __metrics_path__\nhttp://172.21.51.67:10250/metrics http://172.21.51.68:10250/metrics http://172.21.51.69:10250/metrics   抓取的地址是相同的，可以用10.96.0.1做固定值进行替换__address__\n - job_name: \u0026#39;kubernetes-sd-cadvisor\u0026#39;  kubernetes_sd_configs:  - role: node  relabel_configs:  - target_label: __address__  replacement: 10.96.0.1  action: replace 目前为止，替换后的样子：\nhttp://10.96.0.1/metrics http://10.96.0.1/metrics http://10.96.0.1/metrics   需要把找到node-name，来做动态替换__metrics_path__\n - job_name: \u0026#39;kubernetes-sd-cadvisor\u0026#39;  kubernetes_sd_configs:  - role: node  relabel_configs:  - target_label: __address__  replacement: 10.96.0.1  action: replace  - source_labels: [__meta_kubernetes_node_name]  regex: (.+)  target_label: __metrics_path__  replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor 目前为止，替换后的样子：\nhttp://10.96.0.1/api/v1/nodes/k8s-master/proxy/metrics/cadvisor http://10.96.0.1/api/v1/nodes/k8s-slave1/proxy/metrics/cadvisor http://10.96.0.1/api/v1/nodes/k8s-slave2/proxy/metrics/cadvisor   加上api-server的认证信息\n - job_name: \u0026#39;kubernetes-sd-cadvisor\u0026#39;  kubernetes_sd_configs:  - role: node  scheme: https  tls_config:  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  insecure_skip_verify: true  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  relabel_configs:  - target_label: __address__  replacement: 10.96.0.1  - source_labels: [__meta_kubernetes_node_name]  regex: (.+)  target_label: __metrics_path__  replacement: /api/v1/nodes/${1}/proxy/metrics/cadvisor   重新应用配置，然后重建Prometheus的pod。查看targets列表，查看cadvisor指标，比如container_cpu_system_seconds_total，container_memory_usage_bytes\n综上，利用node类型，可以实现对daemonset类型服务的目标自动发现以及监控数据抓取。\n使用cadvisor实现容器指标的采集（新） 目前cAdvisor集成到了kubelet组件内 ，因此可以通过kubelet的接口实现容器指标的采集，具体的API为:\nhttps://\u0026lt;node-ip\u0026gt;:10250/metrics/cadvisor # node上的cadvisor采集到的容器指标 https://\u0026lt;node-ip\u0026gt;:10250/metrics # node上的kubelet的指标数据  # 可以通过curl -k -H \u0026#34;Authorization: Bearer xxxx\u0026#34; https://xxxx/xx查看 因此，针对容器指标来讲，我们期望的采集target是：\nhttps://172.21.51.67:10250/metrics/cadvisor https://172.21.51.68:10250/metrics/cadvisor https://172.21.51.69:10250/metrics/cadvisor 即每个node节点都需要去采集数据，联想到prometheus的服务发现中的node类型，因此，配置：\n - job_name: \u0026#39;kubernetes-sd-cadvisor\u0026#39;  kubernetes_sd_configs:  - role: node 默认添加的target列表为：__schema__://__address__ __metrics_path__\nhttp://172.21.51.67:10250/metrics http://172.21.51.68:10250/metrics http://172.21.51.69:10250/metrics 和期望值不同的是__schema__和__metrics_path__，针对__metrics_path__可以使用relabel修改：\n relabel_configs:  - target_label: __metrics_path__  replacement: /metrics/cadvisor 针对__schema__：\n - job_name: \u0026#39;kubernetes-sd-cadvisor\u0026#39;  kubernetes_sd_configs:  - role: node  scheme: https  tls_config:  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  insecure_skip_verify: true  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token  relabel_configs:  - target_label: __metrics_path__  replacement: /metrics/cadvisor 重新应用配置，然后重建Prometheus的pod。查看targets列表，查看cadvisor指标，比如container_cpu_system_seconds_total，container_memory_usage_bytes\n综上，利用node类型，可以实现对daemonset类型服务的目标自动发现以及监控数据抓取。\n补充：\n若想采集kubelet的指标：\n - job_name: \u0026#39;kubernetes-sd-kubelet\u0026#39;  kubernetes_sd_configs:  - role: node  scheme: https  tls_config:  ca_file: /var/run/secrets/kubernetes.io/serviceaccount/ca.crt  insecure_skip_verify: true  bearer_token_file: /var/run/secrets/kubernetes.io/serviceaccount/token 集群Service服务的监控指标采集 比如集群中存在100个业务应用，每个业务应用都需要被Prometheus监控。\n每个服务是不是都需要手动添加配置？有没有更好的方式？\n - job_name: \u0026#39;kubernetes-sd-endpoints\u0026#39;  kubernetes_sd_configs:  - role: endpoints 添加到Prometheus配置中进行测试：\n$ kubectl apply -f prometheus-configmap.yaml $ kubectl -n monitor delete po prometheus-dcb499cbf-4h9qj 此使的Target列表中，kubernetes-sd-endpoints下出现了N多条数据，\n可以发现，实际上endpoint这个类型，目标是去抓取整个集群中所有的命名空间的Endpoint列表，然后使用默认的/metrics进行数据抓取，我们可以通过查看集群中的所有ep列表来做对比：\n$ kubectl get endpoints --all-namespaces 但是实际上并不是每个服务都已经实现了/metrics监控的，也不是每个实现了/metrics接口的服务都需要注册到Prometheus中，因此，我们需要一种方式对需要采集的服务实现自主可控。这就需要利用relabeling中的keep功能。\n我们知道，relabel的作用对象是target的Before Relabling标签，比如说，假如通过如下定义:\n- job_name: \u0026#39;kubernetes-sd-endpoints\u0026#39;  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__keep_this_service__]  action: keep  regex: “true” 那么就可以实现target的Before Relabling中若存在__keep_this_service__，且值为true的话，则会加入到kubernetes-endpoints这个target中，否则就会被删除。\n因此可以为我们期望被采集的服务，加上对应的Prometheus的label即可。\n问题来了，怎么加？\n查看coredns的metrics类型Before Relabling中的值，可以发现，存在如下类型的Prometheus的标签：\n__meta_kubernetes_service_annotation_prometheus_io_scrape=\u0026#34;true\u0026#34; __meta_kubernetes_service_annotation_prometheus_io_port=\u0026#34;9153\u0026#34; 这些内容是如何生成的呢，查看coredns对应的服务属性：\n$ kubectl -n kube-system get service kube-dns -oyaml apiVersion: v1 kind: Service metadata:  annotations:  prometheus.io/port: \u0026#34;9153\u0026#34;  prometheus.io/scrape: \u0026#34;true\u0026#34;  creationTimestamp: \u0026#34;2020-06-28T17:05:35Z\u0026#34;  labels:  k8s-app: kube-dns  kubernetes.io/cluster-service: \u0026#34;true\u0026#34;  kubernetes.io/name: KubeDNS  name: kube-dns  namespace: kube-system  ... 发现存在annotations声明，因此，可以联想到二者存在对应关系，Service的定义中的annotations里的特殊字符会被转换成Prometheus中的label中的下划线。\n我们即可以使用如下配置，来定义服务是否要被抓取监控数据。\n- job_name: \u0026#39;kubernetes-sd-endpoints\u0026#39;  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]  action: keep  regex: true 这样的话，我们只需要为服务定义上如下的声明，即可实现Prometheus自动采集数据\n annotations: \tprometheus.io/scrape: \u0026#34;true\u0026#34; 有些时候，我们业务应用提供监控数据的path地址并不一定是/metrics，如何实现兼容？\n同样的思路，我们知道，Prometheus会默认使用Before Relabling中的__metrics_path作为采集路径，因此，我们再自定义一个annotation，prometheus.io/path\n annotations: \tprometheus.io/scrape: \u0026#34;true\u0026#34; \tprometheus.io/path: \u0026#34;/path/to/metrics\u0026#34; 这样，Prometheus端会自动生成如下标签：\n__meta_kubernetes_service_annotation_prometheus_io_path=\u0026#34;/path/to/metrics\u0026#34; 我们只需要在relabel_configs中用该标签的值，去重写__metrics_path__的值即可。因此：\n- job_name: \u0026#39;kubernetes-sd-endpoints\u0026#39;  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]  action: keep  regex: true  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]  action: replace  target_label: __metrics_path__  regex: (.+) 有些时候，业务服务的metrics是独立的端口，比如coredns，业务端口是53，监控指标采集端口是9153，这种情况，如何处理？\n很自然的，我们会想到通过自定义annotation来处理，\n annotations: \tprometheus.io/scrape: \u0026#34;true\u0026#34; \tprometheus.io/path: \u0026#34;/path/to/metrics\u0026#34; \tprometheus.io/port: \u0026#34;9153\u0026#34; 如何去替换？\n我们知道Prometheus默认使用Before Relabeling中的__address__进行作为服务指标采集的地址，但是该地址的格式通常是这样的\n__address__=\u0026#34;10.244.0.20:53\u0026#34; __address__=\u0026#34;10.244.0.21\u0026#34; 我们的目标是将如下两部分拼接在一起：\n 10.244.0.20 prometheus.io/port定义的值，即__meta_kubernetes_service_annotation_prometheus_io_port的值  因此，需要使用正则规则取出上述两部分：\n - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]  action: replace  target_label: __address__  regex: ([^:]+)(?::\\d+)?;(\\d+)  replacement: $1:$2 需要注意的几点：\n __address__中的:53有可能不存在，因此，使用()?的匹配方式进行 表达式中，三段()我们只需要第一和第三段，不需要中间括号部分的内容，因此使用?:的方式来做非获取匹配，即可以匹配内容，但是不会被记录到$1,$2这种变量中 多个source_labels中间默认使用;号分割，因此匹配的时候需要注意添加;号  此外，还可以将before relabeling 中的更多常用的字段取出来添加到目标的label中，比如：\n - source_labels: [__meta_kubernetes_namespace]  action: replace  target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]  action: replace  target_label: kubernetes_name  - source_labels: [__meta_kubernetes_pod_name]  action: replace  target_label: kubernetes_pod_name 因此，目前的relabel的配置如下：\n - job_name: \u0026#39;kubernetes-sd-endpoints\u0026#39;  kubernetes_sd_configs:  - role: endpoints  relabel_configs:  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_scrape]  action: keep  regex: true  - source_labels: [__meta_kubernetes_service_annotation_prometheus_io_path]  action: replace  target_label: __metrics_path__  regex: (.+)  - source_labels: [__address__, __meta_kubernetes_service_annotation_prometheus_io_port]  action: replace  target_label: __address__  regex: ([^:]+)(?::\\d+)?;(\\d+)  replacement: $1:$2  - source_labels: [__meta_kubernetes_namespace]  action: replace  target_label: kubernetes_namespace  - source_labels: [__meta_kubernetes_service_name]  action: replace  target_label: kubernetes_name  - source_labels: [__meta_kubernetes_pod_name]  action: replace  target_label: kubernetes_pod_name 验证一下：\n更新configmap并重启Prometheus服务，查看target列表。\nkube-state-metrics监控 已经有了cadvisor，容器运行的指标已经可以获取到，但是下面这种情况却无能为力：\n 我调度了多少个replicas？现在可用的有几个？ 多少个Pod是running/stopped/terminated状态？ Pod重启了多少次？  而这些则是kube-state-metrics提供的内容，它基于client-go开发，轮询Kubernetes API，并将Kubernetes的结构化信息转换为metrics。因此，需要借助于kube-state-metrics来实现。\n指标类别包括：\n CronJob Metrics DaemonSet Metrics Deployment Metrics Job Metrics LimitRange Metrics Node Metrics PersistentVolume Metrics PersistentVolumeClaim Metrics Pod Metrics  kube_pod_info kube_pod_owner kube_pod_status_phase kube_pod_status_ready kube_pod_status_scheduled kube_pod_container_status_waiting kube_pod_container_status_terminated_reason \u0026hellip;   Pod Disruption Budget Metrics ReplicaSet Metrics ReplicationController Metrics ResourceQuota Metrics Service Metrics StatefulSet Metrics Namespace Metrics Horizontal Pod Autoscaler Metrics Endpoint Metrics Secret Metrics ConfigMap Metrics  部署： https://github.com/kubernetes/kube-state-metrics#kubernetes-deployment\n$ wget https://github.com/kubernetes/kube-state-metrics/archive/v1.9.7.tar.gz  $ tar zxf v1.9.7.tar.gz $ cp -r kube-state-metrics-1.9.7/examples/standard/ .  $ ll standard/ total 20 -rw-r--r-- 1 root root 377 Jul 24 06:12 cluster-role-binding.yaml -rw-r--r-- 1 root root 1651 Jul 24 06:12 cluster-role.yaml -rw-r--r-- 1 root root 1069 Jul 24 06:12 deployment.yaml -rw-r--r-- 1 root root 193 Jul 24 06:12 service-account.yaml -rw-r--r-- 1 root root 406 Jul 24 06:12 service.yaml  # 替换namespace为monitor $ sed -i \u0026#39;s/namespace: kube-system/namespace: monitor/g\u0026#39; standard/*  $ kubectl create -f standard/ clusterrolebinding.rbac.authorization.k8s.io/kube-state-metrics created clusterrole.rbac.authorization.k8s.io/kube-state-metrics created deployment.apps/kube-state-metrics created serviceaccount/kube-state-metrics created service/kube-state-metrics created 如何添加到Prometheus监控target中？\n$ cat standard/service.yaml apiVersion: v1 kind: Service metadata:  annotations:  prometheus.io/scrape: \u0026#34;true\u0026#34;  prometheus.io/port: \u0026#34;8080\u0026#34;  labels:  app.kubernetes.io/name: kube-state-metrics  app.kubernetes.io/version: v1.9.7  name: kube-state-metrics  namespace: monitor spec:  clusterIP: None  ports:  - name: http-metrics  port: 8080  targetPort: http-metrics  - name: telemetry  port: 8081  targetPort: telemetry  selector:  app.kubernetes.io/name: kube-state-metrics  $ kubectl apply -f standard/service.yaml 查看target列表，观察是否存在kube-state-metrics的target。\nkube_pod_container_status_running\nkube_deployment_status_replicas\nGrafana 可视化面板，功能齐全的度量仪表盘和图形编辑器，支持 Graphite、zabbix、InfluxDB、Prometheus、OpenTSDB、Elasticsearch 等作为数据源，比 Prometheus 自带的图表展示功能强大太多，更加灵活，有丰富的插件，功能更加强大。\n安装 注意点：\n 使用最新版本的镜像 https://github.com/grafana/grafana 通过环境变量设置管理员账户密码  GF_SECURITY_ADMIN_USER GF_SECURITY_ADMIN_PASSWORD   通过设置securityContext的方式让grafana进程使用root启动 数据挂载到本地 配置ingress暴露访问入口  $ cat grafana-all.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: grafana  namespace: monitor spec:  selector:  matchLabels:  app: grafana  template:  metadata:  labels:  app: grafana  spec:  volumes:  - name: storage  hostPath:  path: /data/grafana/  nodeSelector:  app: prometheus  securityContext:  runAsUser: 0  containers:  - name: grafana  image: grafana/grafana:7.1.1  imagePullPolicy: IfNotPresent  ports:  - containerPort: 3000  name: grafana  env:  - name: GF_SECURITY_ADMIN_USER  value: admin  - name: GF_SECURITY_ADMIN_PASSWORD  value: admin  readinessProbe:  failureThreshold: 10  httpGet:  path: /api/health  port: 3000  scheme: HTTP  initialDelaySeconds: 60  periodSeconds: 10  successThreshold: 1  timeoutSeconds: 30  livenessProbe:  failureThreshold: 3  httpGet:  path: /api/health  port: 3000  scheme: HTTP  periodSeconds: 10  successThreshold: 1  timeoutSeconds: 1  resources:  limits:  cpu: 150m  memory: 512Mi  requests:  cpu: 150m  memory: 512Mi  volumeMounts:  - mountPath: /var/lib/grafana  name: storage --- apiVersion: v1 kind: Service metadata:  name: grafana  namespace: monitor spec:  type: ClusterIP  ports:  - port: 3000  selector:  app: grafana  --- apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: grafana  namespace: monitor spec:  rules:  - host: grafana.luffy.com  http:  paths:  - path: /  backend:  serviceName: grafana  servicePort: 3000 配置数据源：\n URL：http://prometheus:9090  如何丰富Grafana监控面板：\n 导入dashboard 安装相应的插件 自定义监控面板  导入Dashboard的配置 dashboard： https://grafana.com/grafana/dashboards\n Node Exporter https://grafana.com/grafana/dashboards/8919 Prometheus： https://grafana.com/grafana/dashboards/8588  DevOpsProdigy KubeGraf插件的使用 除了直接导入Dashboard，我们还可以通过安装插件的方式获得，Configuration -\u0026gt; Plugins可以查看已安装的插件，通过 官方插件列表 我们可以获取更多可用插件。\nKubernetes相关的插件：\n grafana-kubernetes-app devopsprodigy-kubegraf-app  DevOpsProdigy KubeGraf 是一个非常优秀的 Grafana Kubernetes 插件，是 Grafana 官方的 Kubernetes 插件的升级版本，该插件可以用来可视化和分析 Kubernetes 集群的性能，通过各种图形直观的展示了 Kubernetes 集群的主要服务的指标和特征，还可以用于检查应用程序的生命周期和错误日志。\n# 进入grafana容器内部执行安装 $ kubectl -n monitor exec -ti grafana-594f447d6c-jmjsw bash bash-5.0# grafana-cli plugins install devopsprodigy-kubegraf-app 1.4.1 installing devopsprodigy-kubegraf-app @ 1.4.1 from: https://grafana.com/api/plugins/devopsprodigy-kubegraf-app/versions/1.4.1/download into: /var/lib/grafana/plugins  ✔ Installed devopsprodigy-kubegraf-app successfully  Restart grafana after installing plugins . \u0026lt;service grafana-server restart\u0026gt;  bash-5.0# grafana-cli plugins install grafana-piechart-panel installing grafana-piechart-panel @ 1.5.0 from: https://grafana.com/api/plugins/grafana-piechart-panel/versions/1.5.0/download into: /var/lib/grafana/plugins  ✔ Installed grafana-piechart-panel successfully  Restart grafana after installing plugins . \u0026lt;service grafana-server restart\u0026gt;  # 也可以下载离线包进行安装  # 重建pod生效 $ kubectl -n monitor delete po grafana-594f447d6c-jmjsw 登录grafana界面，Configuration -\u0026gt; Plugins 中找到安装的插件，点击插件进入插件详情页面，点击 [Enable]按钮启用插件，点击 Set up your first k8s-cluster 创建一个新的 Kubernetes 集群:\n  Name：luffy-k8s\n  URL：https://kubernetes.default:443\n  Access：使用默认的Server(default)\n  Skip TLS Verify：勾选，跳过证书合法性校验\n  Auth：勾选TLS Client Auth以及With CA Cert，勾选后会下面有三块证书内容需要填写，内容均来自~/.kube/config文件，需要对文件中的内容做一次base64 解码\n CA Cert：使用config文件中的certificate-authority-data对应的内容 Client Cert：使用config文件中的client-certificate-data对应的内容 Client Key：使用config文件中的client-key-data对应的内容    自定义监控面板 通用的监控需求基本上都可以使用第三方的Dashboard来解决，对于业务应用自己实现的指标的监控面板，则需要我们手动进行创建。\n调试Panel：直接输入Metrics，查询数据。\n如，输入node_load1来查看集群节点最近1分钟的平均负载，直接保存即可生成一个panel\n如何根据字段过滤，实现联动效果？\n比如想实现根据集群节点名称进行过滤，可以通过如下方式：\n  设置 -\u0026gt; Variables -\u0026gt; Add Variable，添加一个变量node，\n Name：node Label：选择节点 Data Source：Prometheus Query：kube_node_info，可以在页面下方的Preview of values查看到当前变量的可选值 Regex：/.*node=\\\u0026quot;(.+?)\\\u0026quot;.*/ Refresh：On Dashboard Load Multi-value：true Include All Options：true    修改Metrics，$node和变量名字保持一致，意思为自动读取当前设置的节点的名字\nnode_load1{instance=~\u0026#34;$node\u0026#34;}   再添加一个面板，使用如下的表达式：\n100-avg(irate(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;,instance=~\u0026#34;$node\u0026#34;}[5m])) by (instance)*100 Metrics指标类型与PromQL TSDB的样本分布示意图：\n ^  │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026#34;cpu0\u0026#34;,mode=\u0026#34;idle\u0026#34;}  │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026#34;cpu0\u0026#34;,mode=\u0026#34;system\u0026#34;}  │ . . . . . . . . . . . . . . . . . . node_load1{}  │ . . . . . . . . . . . . . . . . . . node_cpu_seconds_total{...}  v  \u0026lt;------------------ 时间 ----------------\u0026gt; Guage类型：\n$ kubectl -n monitor get po -o wide |grep k8s-master node-exporter-ld6sq 1/1 Running 0 4d3h 172.21.51.67 k8s-master $ curl -s 172.21.51.67:9100/metrics |grep node_load1 # HELP node_load1 1m load average. # TYPE node_load1 gauge node_load1 0.18 # HELP node_load15 15m load average. # TYPE node_load15 gauge node_load15 0.37 Gauge类型的指标侧重于反应系统的当前状态。\n 这类指标的样本数据可增可减。 常见指标如：node_memory_MemAvailable_bytes（可用内存大小）、node_load1（系统平均负载）  Guage类型的数据，通常直接查询就会有比较直观的业务含义，比如：\n node_load5 node_memory_MemAvailable_bytes  我们也会对这类数据做简单的处理，比如：\n 过滤其中某些节点 对指标进行数学运算  这就是PromQL提供的能力，可以对收集到的数据做聚合、计算等处理。\nPromQL（ Prometheus Query Language ）是Prometheus自定义的一套强大的数据查询语言，除了使用监控指标作为查询关键字以为，还内置了大量的函数，帮助用户进一步对时序数据进行处理。\n比如：\n  只显示k8s-master节点的平均负载\nnode_load1{instance=\u0026#34;k8s-master\u0026#34;}   显示除了k8s-master节点外的其他节点的平均负载\nnode_load1{instance!=\u0026#34;k8s-master\u0026#34;}   正则匹配\nnode_load1{instance=~\u0026#34;k8s-master|k8s-slave1\u0026#34;}   集群各节点系统内存使用率\n(node_memory_MemTotal_bytes - node_memory_MemFree_bytes) / node_memory_MemTotal_bytes   counter类型：\n$ curl -s 172.21.51.67:9100/metrics |grep node_cpu_seconds_total # HELP node_cpu_seconds_total Seconds the cpus spent in each mode. # TYPE node_cpu_seconds_total counter node_cpu_seconds_total{cpu=\u0026#34;0\u0026#34;,mode=\u0026#34;idle\u0026#34;} 294341.02 node_cpu_seconds_total{cpu=\u0026#34;0\u0026#34;,mode=\u0026#34;iowait\u0026#34;} 120.78 node_cpu_seconds_total{cpu=\u0026#34;0\u0026#34;,mode=\u0026#34;irq\u0026#34;} 0 node_cpu_seconds_total{cpu=\u0026#34;0\u0026#34;,mode=\u0026#34;nice\u0026#34;} 0.13 node_cpu_seconds_total{cpu=\u0026#34;0\u0026#34;,mode=\u0026#34;softirq\u0026#34;} 1263.29 counter类型的指标其工作方式和计数器一样，只增不减（除非系统发生重置）。常见的监控指标，如http_requests_total，node_cpu_seconds_total都是Counter类型的监控指标。\n通常计数器类型的指标，名称后面都以_total结尾。我们通过理解CPU利用率的PromQL表达式来讲解Counter指标类型的使用。\n各节点CPU的平均使用率表达式：\n(1- sum(increase(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[2m])) by (instance) / sum(increase(node_cpu_seconds_total{}[2m])) by (instance)) * 100 分析：\nnode_cpu_seconds_total的指标含义是统计系统运行以来，CPU资源分配的时间总数，单位为秒，是累加的值。比如，直接运行该指标：\nnode_cpu_seconds_total # 显示的是所有节点、所有CPU核心、在各种工作模式下分配的时间总和 其中mode的值和我们平常在系统中执行top命令看到的CPU显示的信息一致：\n每个mode对应的含义如下：\n user(us) 表示用户态空间或者说是用户进程(running user space processes)使用CPU所耗费的时间。这是日常我们部署的应用所在的层面，最常见常用。 system(sy) 表示内核态层级使用CPU所耗费的时间。分配内存、IO操作、创建子进程……都是内核操作。这也表明，当IO操作频繁时，System参数会很高。 steal(st) 当运行在虚拟化环境中，花费在其它 OS 中的时间（基于虚拟机监视器 hypervisor 的调度）；可以理解成由于虚拟机调度器将 cpu 时间用于其它 OS 了，故当前 OS 无法使用 CPU 的时间。 softirq(si) 从系统启动开始，累计到当前时刻，软中断时间 irq(hi) 从系统启动开始，累计到当前时刻，硬中断时间 nice(ni) 从系统启动开始，累计到当前时刻， 低优先级(低优先级意味着进程 nice 值小于 0)用户态的进程所占用的CPU时间 iowait(wa) 从系统启动开始，累计到当前时刻，IO等待时间 idle(id) 从系统启动开始，累计到当前时刻，除IO等待时间以外的其它等待时间，亦即空闲时间  我们通过指标拿到的各核心cpu分配的总时长数据，都是瞬时的数据，如何转换成 CPU的利用率？\n先来考虑如何我们如何计算CPU利用率，假如我的k8s-master节点是4核CPU，我们来考虑如下场景：\n 过去1分钟内每个CPU核心处于idle状态的时长，假如分别为 :  cpu0：20s cpu1：30s cpu2：50s cpu3：40s   则四个核心总共可分配的时长是 4*60=240s 实际空闲状态的总时长为20+30+50+40=140s 那么我们可以计算出过去1分钟k8s-master节点的CPU利用率为 (1- 140/240) * 100 = 41.7%  因此，我们只需要使用PromQL取出上述过程中的值即可：\n# 过滤出当前时间点idle的时长 node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}  # 使用[1m]取出1分钟区间内的样本值,注意，1m区间要大于prometheus设置的抓取周期，此处会将周期内所以的样本值取出 node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[1m]  # 使用increase方法，获取该区间内idle状态的增量值,即1分钟内，mode=\u0026#34;idle\u0026#34;状态增加的时长 increase(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[1m])  # 由于是多个cpu核心，因此需要做累加，使用sum函数 sum(increase(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[1m]))  # 由于是多台机器，因此，需要按照instance的值进行分组累加，使用by关键字做分组,这样就获得了1分钟内，每个节点上 所有CPU核心idle状态的增量时长，即前面示例中的”20+30+50+40=140s“ sum(increase(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[1m])) by (instance)  # 去掉mode=idle的过滤条件，即可获取1分钟内，所有状态的cpu获得的增量总时长，即4*60=240s sum(increase(node_cpu_seconds_total{}[1m])) by (instance)  # 最终的语句 (1- sum(increase(node_cpu_seconds_total{mode=\u0026#34;idle\u0026#34;}[1m])) by (instance) / sum(increase(node_cpu_seconds_total{}[1m])) by (instance)) * 100 除此之外，还会经常看到irate和rate方法的使用：\nirate() 是基于最后两个数据点计算一个时序指标在一个范围内的每秒递增率 ，举个例子：\n# 1min内，k8s-master节点的idle状态的cpu分配时长增量值 increase(node_cpu_seconds_total{instance=\u0026#34;k8s-master\u0026#34;,mode=\u0026#34;idle\u0026#34;}[1m])  {cpu=\u0026#34;0\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t56.5 {cpu=\u0026#34;1\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t56.04 {cpu=\u0026#34;2\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t56.6 {cpu=\u0026#34;3\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t56.5  #以第一条数据为例，说明过去的1分钟，k8s-master节点的第一个CPU核心，有56.5秒的时长是出于idle状态的  # 1min内，k8s-master节点的idle状态的cpu分配每秒的速率 irate(node_cpu_seconds_total{instance=\u0026#34;k8s-master\u0026#34;,mode=\u0026#34;idle\u0026#34;}[1m]) {cpu=\u0026#34;0\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.934 {cpu=\u0026#34;1\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.932 {cpu=\u0026#34;2\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.933 {cpu=\u0026#34;3\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.936 # 该值如何计算的？ # irate会取出样本中的最后两个点来作为增长依据，然后做差值计算，并且除以两个样本间的数据时长，也就是说，我们设置2m,5m取出来的值是一样的，因为只会计算最后两个样本差。 # 以第一条数据为例，表示用irate计算出来的结果是，过去的两分钟内，cpu平均每秒钟有0.934秒的时间是处于idle状态的   # rate会1min内第一个和最后一个样本值为依据，计算方式和irate保持一致 rate(node_cpu_seconds_total{instance=\u0026#34;k8s-master\u0026#34;,mode=\u0026#34;idle\u0026#34;}[1m]) {cpu=\u0026#34;0\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.933 {cpu=\u0026#34;1\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.940 {cpu=\u0026#34;2\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.935 {cpu=\u0026#34;3\u0026#34;,instance=\u0026#34;k8s-master\u0026#34;,job=\u0026#34;kubernetes-sd-node-exporter\u0026#34;,mode=\u0026#34;idle\u0026#34;}\t0.937 因此rate的值，相对来讲更平滑，因为计算的是时间段内的平均，更适合于用作告警。\nAlertmanager Alertmanager是一个独立的告警模块。\n  接收Prometheus等客户端发来的警报\n  通过分组、删除重复等处理，并将它们通过路由发送给正确的接收器；\n  告警方式可以按照不同的规则发送给不同的模块负责人。Alertmanager支持Email, Slack，等告警方式, 也可以通过webhook接入钉钉等国内IM工具。\n  如果集群主机的内存使用率超过80%，且该现象持续了2分钟？想实现这样的监控告警，如何做？\n从上图可得知设置警报和通知的主要步骤是：\n  安装和配置 Alertmanager\n  配置Prometheus与Alertmanager对话\n  在Prometheus中创建警报规则\n  安装 Alertmanager， https://github.com/prometheus/alertmanager#install\n./alertmanager --config.file=config.yml alertmanager.yml配置文件格式：\n$ cat alertmanager-config.yml apiVersion: v1 data:  config.yml: |  global:  # 当alertmanager持续多长时间未接收到告警后标记告警状态为 resolved  resolve_timeout: 5m  # 配置邮件发送信息  smtp_smarthost: \u0026#39;smtp.163.com:25\u0026#39;  smtp_from: \u0026#39;earlene163@163.com\u0026#39;  smtp_auth_username: \u0026#39;earlene163@163.com\u0026#39;  smtp_auth_password: \u0026#39;qzpm10\u0026#39;  smtp_require_tls: false  # 所有报警信息进入后的根路由，用来设置报警的分发策略  route:  # 接收到的报警信息里面有许多alertname=NodeLoadHigh 这样的标签的报警信息将会批量被聚合到一个分组里面  group_by: [\u0026#39;alertname\u0026#39;]  # 当一个新的报警分组被创建后，需要等待至少 group_wait 时间来初始化通知，如果在等待时间内当前group接收到了新的告警，这些告警将会合并为一个通知向receiver发送  group_wait: 30s   # 相同的group发送告警通知的时间间隔  group_interval: 30s  # 如果一个报警信息已经发送成功了，等待 repeat_interval 时间来重新发送  repeat_interval: 1m   # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器  receiver: default   # 上面所有的属性都由所有子路由继承，并且可以在每个子路由上进行覆盖。  routes:  - {}  # 配置告警接收者的信息  receivers:  - name: \u0026#39;default\u0026#39;  email_configs:  - to: \u0026#39;654147123@qq.com\u0026#39;  send_resolved: true # 接受告警恢复的通知 kind: ConfigMap metadata:  name: alertmanager  namespace: monitor 主要配置的作用：\n global: 全局配置，包括报警解决后的超时时间、SMTP 相关配置、各种渠道通知的 API 地址等等。 route: 用来设置报警的分发策略，它是一个树状结构，按照深度优先从左向右的顺序进行匹配。 receivers: 配置告警消息接受者信息，例如常用的 email、wechat、slack、webhook 等消息通知方式。  配置文件：\n$ kubectl create -f alertmanager-config.yml 其他资源清单文件:\n$ cat alertmanager-all.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: alertmanager  namespace: monitor  labels:  app: alertmanager spec:  selector:  matchLabels:  app: alertmanager  template:  metadata:  labels:  app: alertmanager  spec:  volumes:  - name: config  configMap:  name: alertmanager  containers:  - name: alertmanager  image: prom/alertmanager:v0.21.0  imagePullPolicy: IfNotPresent  args:  - \u0026#34;--config.file=/etc/alertmanager/config.yml\u0026#34;  - \u0026#34;--log.level=debug\u0026#34;  ports:  - containerPort: 9093  name: http  volumeMounts:  - mountPath: \u0026#34;/etc/alertmanager\u0026#34;  name: config  resources:  requests:  cpu: 100m  memory: 256Mi  limits:  cpu: 100m  memory: 256Mi --- apiVersion: v1 kind: Service metadata:  name: alertmanager  namespace: monitor spec:  type: ClusterIP  ports:  - port: 9093  selector:  app: alertmanager  --- apiVersion: extensions/v1beta1 kind: Ingress metadata:  name: alertmanager  namespace: monitor spec:  rules:  - host: alertmanager.luffy.com  http:  paths:  - path: /  backend:  serviceName: alertmanager  servicePort: 9093 配置Prometheus与Alertmanager对话 是否告警是由Prometheus进行判断的，若有告警产生，Prometheus会将告警push到Alertmanager，因此，需要在Prometheus端配置alertmanager的地址：\n alerting:  alertmanagers:  - static_configs:  - targets:  - alertmanager:9093 因此，修改Prometheus的配置文件，然后重新加载pod\n# 编辑prometheus-configmap.yaml配置，添加alertmanager内容 $ vim prometheus-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: prometheus-config  namespace: monitor data:  prometheus.yml: |  global:  scrape_interval: 30s  evaluation_interval: 30s  alerting:  alertmanagers:  - static_configs:  - targets:  - alertmanager:9093 ...   $ kubectl apply -f prometheus-configmap.yaml  # 现在已经有监控数据了，因此使用prometheus提供的reload的接口，进行服务重启  # 查看配置文件是否已经自动加载到pod中 $ kubectl -n monitor get po -o wide prometheus-dcb499cbf-pljfn 1/1 Running 0 47h 10.244.1.167  $ kubectl -n monitor exec -ti prometheus-dcb499cbf-pljfn cat /etc/prometheus/prometheus.yml |grep alertmanager  # 使用软加载的方式， $ curl -X POST 10.244.1.167:9090/-/reload 配置报警规则 目前Prometheus与Alertmanager已经连通，接下来我们可以针对收集到的各类指标配置报警规则，一旦满足报警规则的设置，则Prometheus将报警信息推送给Alertmanager，进而转发到我们配置的邮件中。\n在哪里配置？同样是在prometheus-configmap中：\n$ vim prometheus-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: prometheus-config  namespace: monitor data:  prometheus.yml: |  global:  scrape_interval: 30s  evaluation_interval: 30s  alerting:  alertmanagers:  - static_configs:  - targets:  - alertmanager:9093  # Load rules once and periodically evaluate them according to the global \u0026#39;evaluation_interval\u0026#39;.  rule_files:  - /etc/prometheus/alert_rules.yml  # - \u0026#34;first_rules.yml\u0026#34;  # - \u0026#34;second_rules.yml\u0026#34;  scrape_configs:  - job_name: \u0026#39;prometheus\u0026#39;  static_configs:  - targets: [\u0026#39;localhost:9090\u0026#39;] ... rules.yml我们同样使用configmap的方式挂载到prometheus容器内部，因此只需要在已有的configmap中加一个数据项目\n$ vim prometheus-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: prometheus-config  namespace: monitor data:  prometheus.yml: |  global:  scrape_interval: 30s  evaluation_interval: 30s  alerting:  alertmanagers:  - static_configs:  - targets:  - alertmanager:9093  # Load rules once and periodically evaluate them according to the global \u0026#39;evaluation_interval\u0026#39;.  rule_files:  - /etc/prometheus/alert_rules.yml  # - \u0026#34;first_rules.yml\u0026#34;  # - \u0026#34;second_rules.yml\u0026#34;  scrape_configs:  - job_name: \u0026#39;prometheus\u0026#39;  static_configs:  - targets: [\u0026#39;localhost:9090\u0026#39;] ... # 省略中间部分  alert_rules.yml: |  groups:  - name: node_metrics  rules:  - alert: NodeLoad  expr: node_load15 \u0026lt; 1  for: 2m  annotations:  summary: \u0026#34;{{$labels.instance}}: Low node load detected\u0026#34;  description: \u0026#34;{{$labels.instance}}: node load is below 1 (current value is: {{ $value }}\u0026#34; 告警规则的几个要素：\n group.name：告警分组的名称，一个组下可以配置一类告警规则，比如都是物理节点相关的告警 alert：告警规则的名称 expr：是用于进行报警规则 PromQL 查询语句，expr通常是布尔表达式，可以让Prometheus根据计算的指标值做 true or false 的判断 for：评估等待时间（Pending Duration），用于表示只有当触发条件持续一段时间后才发送告警，在等待期间新产生的告警状态为pending，屏蔽掉瞬时的问题，把焦点放在真正有持续影响的问题上 labels：自定义标签，允许用户指定额外的标签列表，把它们附加在告警上，可以用于后面做路由判断，通知到不同的终端，通常被用于添加告警级别的标签 annotations：指定了另一组标签，它们不被当做告警实例的身份标识，它们经常用于存储一些额外的信息，用于报警信息的展示之类的  规则配置中，支持模板的方式，其中：\n  {{$labels}}可以获取当前指标的所有标签，支持{{$labels.instance}}或者{{$labels.job}}这种形式\n  {{ $value }}可以获取当前计算出的指标值\n  更新配置并软重启，并查看Prometheus报警规则。\n一个报警信息在生命周期内有下面3种状态：\n inactive: 表示当前报警信息处于非活动状态，即不满足报警条件 pending: 表示在设置的阈值时间范围内被激活了，即满足报警条件，但是还在观察期内 firing: 表示超过设置的阈值时间被激活了，即满足报警条件，且报警触发时间超过了观察期，会发送到Alertmanager端  对于已经 pending 或者 firing 的告警，Prometheus 也会将它们存储到时间序列ALERTS{}中。当然我们也可以通过表达式去查询告警实例：\nALERTS{} 查看Alertmanager日志：\nlevel=warn ts=2020-07-28T13:43:59.430Z caller=notify.go:674 component=dispatcher receiver=email integration=email[0] msg=\u0026#34;Notify attempt failed, will retry later\u0026#34; attempts=1 err=\u0026#34;*email.loginAuth auth: 550 User has no permission\u0026#34; 说明告警已经推送到Alertmanager端了，但是邮箱登录的时候报错，这是因为邮箱默认没有开启第三方客户端登录。因此需要登录163邮箱设置SMTP服务允许客户端登录。\n自定义webhook实现告警消息的推送 目前官方内置的第三方通知集成包括：邮件、 即时通讯软件（如Slack、Hipchat）、移动应用消息推送(如Pushover)和自动化运维工具（例如：Pagerduty、Opsgenie、Victorops）。可以在alertmanager的管理界面中查看到。\n每一个receiver具有一个全局唯一的名称，并且对应一个或者多个通知方式：\nname: \u0026lt;string\u0026gt; email_configs:  [ - \u0026lt;email_config\u0026gt;, ... ] hipchat_configs:  [ - \u0026lt;hipchat_config\u0026gt;, ... ] slack_configs:  [ - \u0026lt;slack_config\u0026gt;, ... ] opsgenie_configs:  [ - \u0026lt;opsgenie_config\u0026gt;, ... ] webhook_configs:  [ - \u0026lt;webhook_config\u0026gt;, ... ] 如果想实现告警消息推送给企业常用的即时聊天工具，如钉钉或者企业微信，如何配置？\nAlertmanager的通知方式中还可以支持Webhook，通过这种方式开发者可以实现更多个性化的扩展支持。\n# 警报接收者 receivers: #ops  - name: \u0026#39;demo-webhook\u0026#39;  webhook_configs:  - send_resolved: true  url: http://demo-webhook/alert/send 当我们配置了上述webhook地址，则当告警路由到demo-webhook时，alertmanager端会向webhook地址推送POST请求：\n$ curl -X POST -d\u0026#34;$demoAlerts\u0026#34; http://demo-webhook/alert/send $ echo $demoAlerts {  \u0026#34;version\u0026#34;: \u0026#34;4\u0026#34;,  \u0026#34;groupKey\u0026#34;: \u0026lt;string\u0026gt;, alerts (e.g. to deduplicate) ,  \u0026#34;status\u0026#34;: \u0026#34;\u0026lt;resolved|firing\u0026gt;\u0026#34;,  \u0026#34;receiver\u0026#34;: \u0026lt;string\u0026gt;,  \u0026#34;groupLabels\u0026#34;: \u0026lt;object\u0026gt;,  \u0026#34;commonLabels\u0026#34;: \u0026lt;object\u0026gt;,  \u0026#34;commonAnnotations\u0026#34;: \u0026lt;object\u0026gt;,  \u0026#34;externalURL\u0026#34;: \u0026lt;string\u0026gt;, // backlink to the Alertmanager.  \u0026#34;alerts\u0026#34;:  [{  \u0026#34;labels\u0026#34;: \u0026lt;object\u0026gt;,  \u0026#34;annotations\u0026#34;: \u0026lt;object\u0026gt;,  \u0026#34;startsAt\u0026#34;: \u0026#34;\u0026lt;rfc3339\u0026gt;\u0026#34;,  \u0026#34;endsAt\u0026#34;: \u0026#34;\u0026lt;rfc3339\u0026gt;\u0026#34;  }] } 因此，假如我们想把报警消息自动推送到钉钉群聊，只需要：\n 实现一个webhook，部署到k8s集群  接收POST请求，将Alertmanager传过来的数据做解析，调用dingtalk的API，实现消息推送   配置alertmanager的receiver为webhook地址  如何给钉钉群聊发送消息？ 钉钉机器人\n钉钉群聊机器人设置：\n每个群聊机器人在创建的时候都会生成唯一的一个访问地址：\nhttps://oapi.dingtalk.com/robot/send?access_token=e54f616718798e32d1e2ff1af5b095c37501878f816bdab2daf66d390633843a 这样，我们就可以使用如下方式来模拟给群聊机器人发送请求，实现消息的推送：\ncurl \u0026#39;https://oapi.dingtalk.com/robot/send?access_token=e54f616718798e32d1e2ff1af5b095c37501878f816bdab2daf66d390633843a\u0026#39; \\  -H \u0026#39;Content-Type: application/json\u0026#39; \\  -d \u0026#39;{\u0026#34;msgtype\u0026#34;: \u0026#34;text\u0026#34;,\u0026#34;text\u0026#34;: {\u0026#34;content\u0026#34;: \u0026#34;我就是我, 是不一样的烟火\u0026#34;}}\u0026#39; https://gitee.com/agagin/prometheus-webhook-dingtalk\n镜像地址：timonwong/prometheus-webhook-dingtalk:master\n二进制运行：\n$ ./prometheus-webhook-dingtalk --config.file=config.yml 假如使用如下配置：\ntargets:  webhook_dev:  url: https://oapi.dingtalk.com/robot/send?access_token=e54f616718798e32d1e2ff1af5b095c37501878f816bdab2daf66d390633843a  webhook_ops:  url: https://oapi.dingtalk.com/robot/send?access_token=d4e7b72eab6d1b2245bc0869d674f627dc187577a3ad485d9c1d131b7d67b15b 则prometheus-webhook-dingtalk启动后会自动支持如下API的POST访问：\nhttp://locahost:8060/dingtalk/webhook_dev/send http://localhost:8060/dingtalk/webhook_ops/send 这样可以使用一个prometheus-webhook-dingtalk来实现多个钉钉群的webhook地址\n部署prometheus-webhook-dingtalk，从Dockerfile可以得知需要注意的点：\n 默认使用配置文件/etc/prometheus-webhook-dingtalk/config.yml，可以通过configmap挂载 该目录下还有模板文件，因此需要使用subpath的方式挂载 部署Service，作为Alertmanager的默认访问，服务端口默认8060  配置文件：\n$ cat webhook-dingtalk-configmap.yaml apiVersion: v1 data:  config.yml: |  targets:  webhook_dev:  url: https://oapi.dingtalk.com/robot/send?access_token=e54f616718798e32d1e2ff1af5b095c37501878f816bdab2daf66d390633843a kind: ConfigMap metadata:  name: webhook-dingtalk-config  namespace: monitor Deployment和Service\n$ cat webhook-dingtalk-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: webhook-dingtalk  namespace: monitor spec:  selector:  matchLabels:  app: webhook-dingtalk  template:  metadata:  labels:  app: webhook-dingtalk  spec:  containers:  - name: webhook-dingtalk  image: timonwong/prometheus-webhook-dingtalk:master  imagePullPolicy: IfNotPresent  volumeMounts:  - mountPath: \u0026#34;/etc/prometheus-webhook-dingtalk/config.yml\u0026#34;  name: config  subPath: config.yml  ports:  - containerPort: 8060  name: http  resources:  requests:  cpu: 50m  memory: 100Mi  limits:  cpu: 50m  memory: 100Mi  volumes:  - name: config  configMap:  name: webhook-dingtalk-config  items:  - key: config.yml  path: config.yml --- apiVersion: v1 kind: Service metadata:  name: webhook-dingtalk  namespace: monitor spec:  selector:  app: webhook-dingtalk  ports:  - name: hook  port: 8060  targetPort: http 创建：\n$ kubectl create -f webhook-dingtalk-configmap.yaml $ kubectl create -f webhook-dingtalk-deploy.yaml  # 查看日志，可以得知当前的可用webhook日志 $ kubectl -n monitor logs -f webhook-dingtalk-f7f5589c9-qglkd ... file=/etc/prometheus-webhook-dingtalk/config.yml msg=\u0026#34;Completed loading of configuration file\u0026#34; level=info ts=2020-07-30T14:05:40.963Z caller=main.go:117 component=configuration msg=\u0026#34;Loading templates\u0026#34; templates= ts=2020-07-30T14:05:40.963Z caller=main.go:133 component=configuration msg=\u0026#34;Webhook urls for prometheus alertmanager\u0026#34; urls=\u0026#34;http://localhost:8060/dingtalk/webhook_dev/send http://localhost:8060/dingtalk/webhook_ops/send\u0026#34; level=info ts=2020-07-30T14:05:40.963Z caller=web.go:210 component=web msg=\u0026#34;Start listening for connections\u0026#34; address=:8060 修改Alertmanager路由及webhook配置：\n$ cat alertmanager-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: alertmanager  namespace: monitor data:  config.yml: |-  global:  # 当alertmanager持续多长时间未接收到告警后标记告警状态为 resolved  resolve_timeout: 5m  # 配置邮件发送信息  smtp_smarthost: \u0026#39;smtp.163.com:25\u0026#39;  smtp_from: \u0026#39;earlene163@163.com\u0026#39;  smtp_auth_username: \u0026#39;earlene163@163.com\u0026#39;  # 注意这里不是邮箱密码，是邮箱开启第三方客户端登录后的授权码  smtp_auth_password: \u0026#39;GXIWNXKMMEVMNHAJ\u0026#39;  smtp_require_tls: false  # 所有报警信息进入后的根路由，用来设置报警的分发策略  route:  # 按照告警名称分组  group_by: [\u0026#39;alertname\u0026#39;]  # 当一个新的报警分组被创建后，需要等待至少 group_wait 时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。  group_wait: 30s   # 相同的group之间发送告警通知的时间间隔  group_interval: 30s   # 如果一个报警信息已经发送成功了，等待 repeat_interval 时间来重新发送他们，不同类型告警发送频率需要具体配置  repeat_interval: 10m   # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器  receiver: default   # 路由树，默认继承global中的配置，并且可以在每个子路由上进行覆盖。  routes:  - {}  receivers:  - name: \u0026#39;default\u0026#39;  email_configs:  - to: \u0026#39;654147123@qq.com\u0026#39;  send_resolved: true # 接受告警恢复的通知  webhook_configs:  - send_resolved: true  url: http://webhook-dingtalk:8060/dingtalk/webhook_dev/send 验证钉钉消息是否正常收到。\n基于Label的动态告警处理 真实的场景中，我们往往期望可以给告警设置级别，而且可以实现不同的报警级别可以由不同的receiver接收告警消息。\nAlertmanager中路由负责对告警信息进行分组匹配，并向告警接收器发送通知。告警接收器可以通过以下形式进行配置：\nroutes: - receiver: ops  group_wait: 10s  match:  severity: critical - receiver: dev  group_wait: 10s  match_re:  severity: normal|middle receivers:  - ops  ...  - dev  ...  - \u0026lt;receiver\u0026gt; ... 因此可以为了更全面的感受报警的逻辑，我们再添加两个报警规则：\n alert_rules.yml: |  groups:  - name: node_metrics  rules:  - alert: NodeLoad  expr: node_load15 \u0026lt; 1  for: 2m  labels:  severity: normal  annotations:  summary: \u0026#34;{{$labels.instance}}: Low node load detected\u0026#34;  description: \u0026#34;{{$labels.instance}}: node load is below 1 (current value is: {{ $value }}\u0026#34;  - alert: NodeMemoryUsage  expr: (node_memory_MemTotal_bytes - (node_memory_MemFree_bytes + node_memory_Buffers_bytes + node_memory_Cached_bytes)) / node_memory_MemTotal_bytes * 100 \u0026gt; 40  for: 2m  labels:  severity: critical  annotations:  summary: \u0026#34;{{$labels.instance}}: High Memory usage detected\u0026#34;  description: \u0026#34;{{$labels.instance}}: Memory usage is above 40% (current value is: {{ $value }}\u0026#34;  - name: targets_status  rules:  - alert: TargetStatus  expr: up == 0  for: 1m  labels:  severity: critical  annotations:  summary: \u0026#34;{{$labels.instance}}: prometheus target down\u0026#34;  description: \u0026#34;{{$labels.instance}}: prometheus target down，job is {{$labels.job}}\u0026#34; 我们为不同的报警规则设置了不同的标签，如severity: critical，针对规则中的label，来配置alertmanager路由规则，实现转发给不同的接收者。\n$ cat alertmanager-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: alertmanager  namespace: monitor data:  config.yml: |-  global:  # 当alertmanager持续多长时间未接收到告警后标记告警状态为 resolved  resolve_timeout: 5m  # 配置邮件发送信息  smtp_smarthost: \u0026#39;smtp.163.com:25\u0026#39;  smtp_from: \u0026#39;earlene163@163.com\u0026#39;  smtp_auth_username: \u0026#39;earlene163@163.com\u0026#39;  # 注意这里不是邮箱密码，是邮箱开启第三方客户端登录后的授权码  smtp_auth_password: \u0026#39;RMAOPQVHKLPYFVHZ\u0026#39;  smtp_require_tls: false  # 所有报警信息进入后的根路由，用来设置报警的分发策略  route:  # 按照告警名称分组  group_by: [\u0026#39;alertname\u0026#39;]  # 当一个新的报警分组被创建后，需要等待至少 group_wait 时间来初始化通知，这种方式可以确保您能有足够的时间为同一分组来获取多个警报，然后一起触发这个报警信息。  group_wait: 30s   # 相同的group之间发送告警通知的时间间隔  group_interval: 30s   # 如果一个报警信息已经发送成功了，等待 repeat_interval 时间来重新发送他们，不同类型告警发送频率需要具体配置  repeat_interval: 1m   # 默认的receiver：如果一个报警没有被一个route匹配，则发送给默认的接收器  receiver: default   # 路由树，默认继承global中的配置，并且可以在每个子路由上进行覆盖。  routes:  - receiver: critical_alerts  group_wait: 10s  match:  severity: critical  - receiver: normal_alerts  group_wait: 10s  match_re:  severity: normal|middle  receivers:  - name: \u0026#39;default\u0026#39;  email_configs:  - to: \u0026#39;654147123@qq.com\u0026#39;  send_resolved: true # 接受告警恢复的通知  - name: \u0026#39;critical_alerts\u0026#39;  webhook_configs:  - send_resolved: true  url: http://webhook-dingtalk:8060/dingtalk/webhook_ops/send  - name: \u0026#39;normal_alerts\u0026#39;  webhook_configs:  - send_resolved: true  url: http://webhook-dingtalk:8060/dingtalk/webhook_dev/send 再配置一个钉钉机器人，修改webhook-dingtalk的配置，添加webhook_ops的配置：\n$ cat webhook-dingtalk-configmap.yaml apiVersion: v1 data:  config.yml: |  targets:  webhook_dev:  url: https://oapi.dingtalk.com/robot/send?access_token=e54f616718798e32d1e2ff1af5b095c37501878f816bdab2daf66d390633843a  webhook_ops:  url: https://oapi.dingtalk.com/robot/send?access_token=5a68888fbecde75b1832ff024d7374e51f2babd33f1078e5311cdbb8e2c00c3a kind: ConfigMap metadata:  name: webhook-dingtalk-config  namespace: monitor 设置webhook-dingtalk开启lifecycle\n分别更新Prometheus和Alertmanager配置，查看报警的发送。\n抑制和静默 前面我们知道，告警的group(分组)功能通过把多条告警数据聚合，有效的减少告警的频繁发送。除此之外，Alertmanager还支持Inhibition(抑制) 和 Silences(静默)，帮助我们抑制或者屏蔽报警。\n  Inhibition 抑制\n抑制是当出现其它告警的时候压制当前告警的通知，可以有效的防止告警风暴。\n比如当机房出现网络故障时，所有服务都将不可用而产生大量服务不可用告警，但这些警告并不能反映真实问题在哪，真正需要发出的应该是网络故障告警。当出现网络故障告警的时候，应当抑制服务不可用告警的通知。\n在Alertmanager配置文件中，使用inhibit_rules定义一组告警的抑制规则：\ninhibit_rules:  [ - \u0026lt;inhibit_rule\u0026gt; ... ] 每一条抑制规则的具体配置如下：\ntarget_match:  [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt;, ... ] target_match_re:  [ \u0026lt;labelname\u0026gt;: \u0026lt;regex\u0026gt;, ... ]  source_match:  [ \u0026lt;labelname\u0026gt;: \u0026lt;labelvalue\u0026gt;, ... ] source_match_re:  [ \u0026lt;labelname\u0026gt;: \u0026lt;regex\u0026gt;, ... ]  [ equal: \u0026#39;[\u0026#39; \u0026lt;labelname\u0026gt;, ... \u0026#39;]\u0026#39; ] 当已经发送的告警通知匹配到target_match或者target_match_re规则，当有新的告警规则如果满足source_match或者定义的匹配规则，并且已发送的告警与新产生的告警中equal定义的标签完全相同，则启动抑制机制，新的告警不会发送。\n例如，定义如下抑制规则：\n- source_match:  alertname: NodeDown  severity: critical  target_match:  severity: critical  equal:  - node 如当集群中的某一个主机节点异常宕机导致告警NodeDown被触发，同时在告警规则中定义了告警级别severity=critical。由于主机异常宕机，该主机上部署的所有服务，中间件会不可用并触发报警。根据抑制规则的定义，如果有新的告警级别为severity=critical，并且告警中标签node的值与NodeDown告警的相同，则说明新的告警是由NodeDown导致的，则启动抑制机制停止向接收器发送通知。\n演示：实现如果 NodeMemoryUsage 报警触发，则抑制NodeLoad指标规则引起的报警。\n inhibit_rules:  - source_match:  alertname: NodeMemoryUsage  severity: critical  target_match:  severity: normal  equal:  - instance   Silences： 静默\n简单直接的在指定时段关闭告警。静默通过匹配器（Matcher）来配置，类似于路由树。警告进入系统的时候会检查它是否匹配某条静默规则，如果是则该警告的通知将忽略。 静默规则在Alertmanager的 Web 界面里配置。\n  一条告警产生后，还要经过 Alertmanager 的分组、抑制处理、静默处理、去重处理和降噪处理最后再发送给接收者。这个过程中可能会因为各种原因会导致告警产生了却最终没有进行通知，可以通过下图了解整个告警的生命周期：\nhttps://github.com/liyongxin/prometheus-webhook-wechat\n自定义指标实现业务伸缩 Kubernetes Metrics API体系回顾 前面章节，我们讲过基于CPU和内存的HPA，即利用metrics-server及HPA，可以实现业务服务可以根据pod的cpu和内存进行弹性伸缩。\nk8s对监控接口进行了标准化：\n  Resource Metrics\n对应的接口是 metrics.k8s.io，主要的实现就是 metrics-server\n  Custom Metrics\n对应的接口是 custom.metrics.k8s.io，主要的实现是 Prometheus， 它提供的是资源监控和自定义监控\n  安装完metrics-server后，利用kube-aggregator的功能，实现了metrics api的注册。可以通过如下命令\n$ kubectl api-versions ... metrics.k8s.io/v1beta1 HPA通过使用该API获取监控的CPU和内存资源：\n# 查询nodes节点的cpu和内存数据 $ kubectl get --raw=\u0026#34;/apis/metrics.k8s.io/v1beta1/nodes\u0026#34;|jq  $ kubectl get --raw=\u0026#34;/apis/metrics.k8s.io/v1beta1/pods\u0026#34;|jq  # 若本机没有安装jq命令，可以参考如下方式进行安装 $ wget http://dl.fedoraproject.org/pub/epel/epel-release-latest-7.noarch.rpm $ rpm -ivh epel-release-latest-7.noarch.rpm $ yum install -y jq 同样，为了实现通用指标的采集，需要部署Prometheus Adapter，来提供custom.metrics.k8s.io，作为HPA获取通用指标的入口。\nAdapter安装对接 项目地址为： https://github.com/DirectXMan12/k8s-prometheus-adapter\n$ git clone https://github.com/DirectXMan12/k8s-prometheus-adapter.git  # 最新release版本v0.7.0，代码切换到v0.7.0分支 $ git checkout v0.7.0 查看部署说明 https://github.com/DirectXMan12/k8s-prometheus-adapter/tree/v0.7.0/deploy\n  镜像使用官方提供的v0.7.0最新版 https://hub.docker.com/r/directxman12/k8s-prometheus-adapter/tags\n  准备证书\n$ export PURPOSE=serving $ openssl req -x509 -sha256 -new -nodes -days 365 -newkey rsa:2048 -keyout ${PURPOSE}.key -out ${PURPOSE}.crt -subj \u0026#34;/CN=ca\u0026#34;  $ kubectl -n monitor create secret generic cm-adapter-serving-certs --from-file=./serving.crt --from-file=./serving.key  # 查看证书 $ kubectl -n monitor describe secret cm-adapter-serving-certs   准备资源清单\n$ mkdir yamls $ cp manifests/custom-metrics-apiserver-deployment.yaml yamls/ $ cp manifests/custom-metrics-apiserver-service.yaml yamls/ $ cp manifests/custom-metrics-apiservice.yaml yamls/  $ cd yamls # 新建rbac文件 $ vi custom-metrics-apiserver-rbac.yaml kind: ServiceAccount apiVersion: v1 metadata:  name: custom-metrics-apiserver  namespace: monitor --- apiVersion: rbac.authorization.k8s.io/v1 kind: ClusterRoleBinding metadata:  name: custom-metrics-resource-cluster-admin roleRef:  apiGroup: rbac.authorization.k8s.io  kind: ClusterRole  name: cluster-admin subjects: - kind: ServiceAccount  name: custom-metrics-apiserver  namespace: monitor  # 新建配置文件 $ vi custom-metrics-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: adapter-config  namespace: monitor data:  config.yaml: |  rules:  - {}   替换命名空间\n# 资源清单文件默认用的命名空间是custom-metrics，替换为本例中使用的monitor $ sed -i \u0026#39;s/namespace: custom-metrics/namespace: monitor/g\u0026#39; yamls/*   配置adapter对接的Prometheus地址\n# 由于adapter会和Prometheus交互，因此需要配置对接的Prometheus地址 # 替换掉28行：yamls/custom-metrics-apiserver-deployment.yaml 中的--prometheus-url $ vim yamls/custom-metrics-apiserver-deployment.yaml ...  18 spec:  19 serviceAccountName: custom-metrics-apiserver  20 containers:  21 - name: custom-metrics-apiserver  22 image: directxman12/k8s-prometheus-adapter-amd64  23 args:  24 - --secure-port=6443  25 - --tls-cert-file=/var/run/serving-cert/serving.crt  26 - --tls-private-key-file=/var/run/serving-cert/serving.key  27 - --logtostderr=true  28 - --prometheus-url=http://prometheus:9090/  29 - --metrics-relist-interval=1m  30 - --v=10  31 - --config=/etc/adapter/config.yaml ...   部署服务\n$ kubectl create -f yamls/   验证一下：\n$ kubectl api-versions custom.metrics.k8s.io/v1beta1  $ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 |jq {  \u0026#34;kind\u0026#34;: \u0026#34;APIResourceList\u0026#34;,  \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,  \u0026#34;groupVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;,  \u0026#34;resources\u0026#34;: [] } 通用指标示例程序部署 为了演示效果，我们新建一个deployment来模拟业务应用。\n$ cat custom-metrics-demo.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: custom-metrics-demo spec:  selector:  matchLabels:  app: custom-metrics-demo  template:  metadata:  labels:  app: custom-metrics-demo  spec:  containers:  - name: custom-metrics-demo  image: cnych/nginx-vts:v1.0  resources:  limits:  cpu: 50m  requests:  cpu: 50m  ports:  - containerPort: 80  name: http 部署：\n$ kubectl create -f custom-metrics-demo.yaml  $ kubectl get po -o wide custom-metrics-demo-95b5bc949-xpppl 1/1 Running 0 65s 10.244.1.194  $ curl 10.244.1.194/status/format/prometheus ... nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;1xx\u0026#34;} 0 nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;2xx\u0026#34;} 8 nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;3xx\u0026#34;} 0 nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;4xx\u0026#34;} 0 nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;5xx\u0026#34;} 0 nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;} 8 ... 注册为Prometheus的target：\n$ cat custom-metrics-demo-svc.yaml apiVersion: v1 kind: Service metadata:  name: custom-metrics-demo  annotations:  prometheus.io/scrape: \u0026#34;true\u0026#34;  prometheus.io/port: \u0026#34;80\u0026#34;  prometheus.io/path: \u0026#34;/status/format/prometheus\u0026#34; spec:  ports:  - port: 80  targetPort: 80  name: http  selector:  app: custom-metrics-demo  type: ClusterIP 自动注册为Prometheus的采集Targets。\n通常web类的应用，会把每秒钟的请求数作为业务伸缩的指标依据。\n实践：\n使用案例应用custom-metrics-demo，如果custom-metrics-demo最近2分钟内每秒钟的请求数超过10次，则自动扩充业务应用的副本数。\n  配置自定义指标\n告诉Adapter去采集转换哪些指标，Adapter支持转换的指标，才可以作为HPA的依据\n  配置HPA规则\napiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: nginx-custom-hpa  namespace: default spec:  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: custom-metrics-demo  minReplicas: 1  maxReplicas: 3  metrics:  - type: Pods  pods:  metricName: nginx_vts_server_requests_per_second  targetAverageValue: 10   Adapter配置自定义指标 思考：\n前面讲CPU的平均使用率的采集，其实是通过node_cpu_seconds_total指标计算得到的。\n ^  │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026#34;cpu0\u0026#34;,mode=\u0026#34;idle\u0026#34;}  │ . . . . . . . . . . . . . . . . . . . node_cpu{cpu=\u0026#34;cpu0\u0026#34;,mode=\u0026#34;system\u0026#34;}  │ . . . . . . . . . . . . . . . . . . node_load1{}  │ . . . . . . . . . . . . . . . . . . node_cpu_seconds_total{...}  v  \u0026lt;------------------ 时间 ----------------\u0026gt; 同样，如果想获得每个业务应用最近2分钟内每秒的访问次数，也是根据总数来做计算，因此，需要使用业务自定义指标nginx_vts_server_requests_total，配合rate方法即可获取每秒钟的请求数。\nrate(nginx_vts_server_requests_total[2m])  # 如查询有多条数据，需做汇聚，需要使用sum sum(rate(nginx_vts_server_requests_total[2m])) by(kubernetes_pod_name)   自定义指标可以配置多个，因此，需要将规则使用数组来配置\nrules: - {}   告诉Adapter，哪些自定义指标可以使用\nrules: - seriesQuery: \u0026#39;nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}\u0026#39; seriesQuery是PromQL语句，和直接用nginx_vts_server_requests_total查询到的结果一样，凡是seriesQuery可以查询到的指标，都可以用作自定义指标\n  告诉Adapter，指标中的标签和k8s中的资源对象的关联关系\nrules: - seriesQuery: \u0026#39;nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}\u0026#39;  resources:  overrides:  kubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;}  kubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;} 我们查询到的可用指标格式为：\nnginx_vts_server_requests_total{code=\u0026#34;1xx\u0026#34;,host=\u0026#34;*\u0026#34;,instance=\u0026#34;10.244.1.194:80\u0026#34;,job=\u0026#34;kubernetes-sd-endpoints\u0026#34;,kubernetes_name=\u0026#34;custom-metrics-demo\u0026#34;,kubernetes_namespace=\u0026#34;default\u0026#34;,kubernetes_pod_name=\u0026#34;custom-metrics-demo-95b5bc949-xpppl\u0026#34;}\t由于HPA在调用Adapter接口的时候，告诉Adapter的是查询哪个命名空间下的哪个Pod的指标，因此，Adapter在去查询的时候，需要做一层适配转换（因为并不是每个prometheus查询到的结果中都是叫做kubernetes_namespace和kubernetes_pod_name）\n  指定自定义的指标名称，供HPA配置使用\nrules: - seriesQuery: \u0026#39;nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}\u0026#39;  resources:  overrides:  kubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;}  kubernetes_pod_name: {resource: \u0026#34;pods\u0026#34;}  name:  matches: \u0026#34;^(.*)_total\u0026#34;  as: \u0026#34;${1}_per_second\u0026#34; 因为Adapter转换完之后的指标含义为：每秒钟的请求数。因此提供指标名称，该配置根据正则表达式做了匹配替换，转换完后的指标名称为：nginx_vts_server_requests_per_second，HPA规则中可以直接配置该名称。\n  告诉Adapter如何获取最终的自定义指标值\nrules: - seriesQuery: \u0026#39;nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}\u0026#39;  resources:  overrides:  kubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;}  kubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;}  name:  matches: \u0026#34;^(.*)_total\u0026#34;  as: \u0026#34;${1}_per_second\u0026#34;  metricsQuery: \u0026#39;sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[2m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)\u0026#39; 我们最终期望的写法可能是这样：\nsum(rate(nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;,kubernetes_namespace=\u0026#34;default\u0026#34;}[2m])) by (kubernetes_pod_name) 但是Adapter提供了更简单的写法：\nsum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[2m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)  Series: 指标名称 LabelMatchers: 指标查询的label GroupBy: 结果分组，针对HPA过来的查询，都会匹配成kubernetes_pod_name    更新Adapter的配置：\n$ vi custom-metrics-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: adapter-config  namespace: monitor data:  config.yaml: |  rules:  - seriesQuery: \u0026#39;nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}\u0026#39;  seriesFilters: []  resources:  overrides:  kubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;}  kubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;}  name:  matches: \u0026#34;^(.*)_total\u0026#34;  as: \u0026#34;${1}_per_second\u0026#34;  metricsQuery: (sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[1m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)) 需要更新configmap并重启adapter服务：\n$ kubectl apply -f custom-metrics-configmap.yaml  $ kubectl -n monitor delete po custom-metrics-apiserver-c689ff947-zp8gq 再次查看可用的指标数据：\n$ kubectl get --raw /apis/custom.metrics.k8s.io/v1beta1 |jq {  \u0026#34;kind\u0026#34;: \u0026#34;APIResourceList\u0026#34;,  \u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,  \u0026#34;groupVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;,  \u0026#34;resources\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;namespaces/nginx_vts_server_requests_per_second\u0026#34;,  \u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;namespaced\u0026#34;: false,  \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,  \u0026#34;verbs\u0026#34;: [  \u0026#34;get\u0026#34;  ]  },  {  \u0026#34;name\u0026#34;: \u0026#34;pods/nginx_vts_server_requests_per_second\u0026#34;,  \u0026#34;singularName\u0026#34;: \u0026#34;\u0026#34;,  \u0026#34;namespaced\u0026#34;: true,  \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,  \u0026#34;verbs\u0026#34;: [  \u0026#34;get\u0026#34;  ]  }  ] } 我们发现有两个可用的resources，引用官方的一段解释：\nNotice that we get an entry for both \u0026#34;pods\u0026#34; and \u0026#34;namespaces\u0026#34; -- the adapter exposes the metric on each resource that we\u0026#39;ve associated the metric with (and all namespaced resources must be associated with a namespace), and will fill in the \u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt; section with the appropriate label depending on which we ask for.  We can now connect to $KUBERNETES/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second, and we should see $ kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second\u0026#34; | jq {  \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,  \u0026#34;apiVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;,  \u0026#34;metadata\u0026#34;: {  \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second\u0026#34;  },  \u0026#34;items\u0026#34;: [  {  \u0026#34;describedObject\u0026#34;: {  \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;,  \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;,  \u0026#34;name\u0026#34;: \u0026#34;custom-metrics-demo-95b5bc949-xpppl\u0026#34;,  \u0026#34;apiVersion\u0026#34;: \u0026#34;/v1\u0026#34;  },  \u0026#34;metricName\u0026#34;: \u0026#34;nginx_vts_server_requests_per_second\u0026#34;,  \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-02T04:07:06Z\u0026#34;,  \u0026#34;value\u0026#34;: \u0026#34;133m\u0026#34;,  \u0026#34;selector\u0026#34;: null  }  ] } 其中133m等于0.133，即当前指标查询每秒钟请求数为0.133次\nhttps://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config-walkthrough.md\nhttps://github.com/DirectXMan12/k8s-prometheus-adapter/blob/master/docs/config.md\n配置HPA实现自定义指标的业务伸缩 $ cat hpa-custom-metrics.yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: nginx-custom-hpa spec:  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: custom-metrics-demo  minReplicas: 1  maxReplicas: 3  metrics:  - type: Pods  pods:  metricName: nginx_vts_server_requests_per_second  targetAverageValue: 10  $ kubectl create -f hpa-custom-metrics.yaml  $ kubectl get hpa 注意metricName为自定义的指标名称。\n使用ab命令压测custom-metrics-demo服务，观察hpa的变化：\n$ kubectl get svc -o wide custom-metrics-demo ClusterIP 10.104.110.245 \u0026lt;none\u0026gt; 80/TCP 16h  $ ab -n1000 -c 5 http://10.104.110.245:80/ 观察hpa变化:\n$ kubectl describe hpa nginx-custom-hpa 查看adapter日志：\n$ kubectl -n monitor logs --tail=100 -f custom-metrics-apiserver-c689ff947-m5vlr ... I0802 04:43:58.404559 1 httplog.go:90] GET /apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second?labelSelector=app%3Dcustom-metrics-demo: (20.713209ms) 200 [kube-controller-manager/v1.16.0 (linux/amd64) kubernetes/2bd9643/system:serviceaccount:kube-system:horizontal-pod-autoscaler 172.21.51.67:60626] 实际的请求：\nhttp://prometheus:9090/api/v1/query?query=%28sum%28rate%28nginx_vts_server_requests_per_second%7Bkubernetes_namespace%3D%22default%22%2Ckubernetes_pod_name%3D~%22custom-metrics-demo-95b5bc949-9vd8q%7Ccustom-metrics-demo-95b5bc949-qrpnp%22%2Cjob%3D%22kubernetes-sd-endpoints%22%7D%5B1m%5D%29%29+by+%28kubernetes_pod_name  I1028 08:56:05.289421 1 api.go:74] GET http://prometheus:9090/api/v1/query?query=%28sum%28rate%28nginx_vts_server_requests_total%7Bkubernetes_namespace%3D%22default%22%2Ckubernetes_pod_name%3D~%22custom-metrics-demo-95b5bc949-9vd8q%7Ccustom-metrics-demo-95b5bc949-qrpnp%22%7D%5B1m%5D%29%29+by+%28kubernetes_pod_name%29%29\u0026amp;time=1603875365.284 200 OK 补充：coredns通用指标的hpa\n添加指标：\n$ cat custom-metrics-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: adapter-config  namespace: monitor data:  config.yaml: |rules: - seriesQuery: \u0026#39;nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}\u0026#39; seriesFilters: [] resources: overrides: kubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;} kubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;} name: as: \u0026#34;nginx_vts_server_requests_per_second\u0026#34; metricsQuery: (sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;}[1m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)) - seriesQuery: \u0026#39;coredns_dns_request_count_total{job=\u0026#34;kubernetes-sd-endpoints\u0026#34;}\u0026#39; seriesFilters: [] resources: overrides: kubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;} kubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;} name: as: \u0026#34;coredns_dns_request_count_total_1minute\u0026#34; metricsQuery: (sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,job=\u0026#34;kubernetes-sd-endpoints\u0026#34;}[1m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)) coredns的hpa文件：\n$ cat coredns-hpa.yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: coredns-hpa  namespace: kube-system spec:  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: coredns  minReplicas: 2  maxReplicas: 3  metrics:  - type: Pods  pods:  metricName: coredns_dns_request_count_total_1minute  targetAverageValue: 1 hpa会拿pod、namespace去通过adaptor提供的api查询指标数据：\nsum(rate(coredns_dns_request_count_total{kubernetes_namespace=\u0026#34;default\u0026#34;,kubernetes_pod_name=~\u0026#34;custom-metrics-demo-95b5bc949-9vd8q|custom-metrics-demo-95b5bc949-qrpnp\u0026#34;,job=\u0026#34;kubernetes-sd-endpoints\u0026#34;}[1m])) by (kubernetes_pod_name) 追加：Adapter查询数据和直接查询Prometheus数据不一致（相差4倍）的问题。\n$ vi custom-metrics-configmap.yaml apiVersion: v1 kind: ConfigMap metadata:  name: adapter-config  namespace: monitor data:  config.yaml: |  rules:  - seriesQuery: \u0026#39;nginx_vts_server_requests_total{host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}\u0026#39;  seriesFilters: []  resources:  overrides:  kubernetes_namespace: {resource: \u0026#34;namespace\u0026#34;}  kubernetes_pod_name: {resource: \u0026#34;pod\u0026#34;}  name:  matches: \u0026#34;^(.*)_total\u0026#34;  as: \u0026#34;${1}_per_second\u0026#34;  metricsQuery: (sum(rate(\u0026lt;\u0026lt;.Series\u0026gt;\u0026gt;{\u0026lt;\u0026lt;.LabelMatchers\u0026gt;\u0026gt;,host=\u0026#34;*\u0026#34;,code=\u0026#34;total\u0026#34;}[1m])) by (\u0026lt;\u0026lt;.GroupBy\u0026gt;\u0026gt;)) 查询验证：\n$ kubectl get --raw \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/*/nginx_vts_server_requests_per_second\u0026#34; | jq {  \u0026#34;kind\u0026#34;: \u0026#34;MetricValueList\u0026#34;,  \u0026#34;apiVersion\u0026#34;: \u0026#34;custom.metrics.k8s.io/v1beta1\u0026#34;,  \u0026#34;metadata\u0026#34;: {  \u0026#34;selfLink\u0026#34;: \u0026#34;/apis/custom.metrics.k8s.io/v1beta1/namespaces/default/pods/%2A/nginx_vts_server_requests_per_second\u0026#34;  },  \u0026#34;items\u0026#34;: [  {  \u0026#34;describedObject\u0026#34;: {  \u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;,  \u0026#34;namespace\u0026#34;: \u0026#34;default\u0026#34;,  \u0026#34;name\u0026#34;: \u0026#34;custom-metrics-demo-95b5bc949-xpppl\u0026#34;,  \u0026#34;apiVersion\u0026#34;: \u0026#34;/v1\u0026#34;  },  \u0026#34;metricName\u0026#34;: \u0026#34;nginx_vts_server_requests_per_second\u0026#34;,  \u0026#34;timestamp\u0026#34;: \u0026#34;2020-08-02T04:07:06Z\u0026#34;,  \u0026#34;value\u0026#34;: \u0026#34;133m\u0026#34;,  \u0026#34;selector\u0026#34;: null  }  ] } ","permalink":"https://iblog.zone/archives/kubernetes%E9%9B%86%E7%BE%A4%E7%9A%84%E6%97%A5%E5%BF%97%E5%8F%8A%E7%9B%91%E6%8E%A7/","summary":"第四天 Kubernetes集群的日志及监控 k8s日志收集架构 https://kubernetes.io/docs/concepts/cluster-administration/logging/\n总体分为三种方式：\n 使用在每个节点上运行的节点级日志记录代理。 在应用程序的 pod 中，包含专门记录日志的 sidecar 容器。 将日志直接从应用程序中推送到日志记录后端。  使用节点级日志代理 容器日志驱动：\nhttps://docs.docker.com/config/containers/logging/configure/\n查看当前的docker主机的驱动：\n$ docker info --format \u0026#39;{{.LoggingDriver}}\u0026#39; json-file格式，docker会默认将标准和错误输出保存为宿主机的文件，路径为：\n/var/lib/docker/containers/\u0026lt;container-id\u0026gt;/\u0026lt;container-id\u0026gt;-json.log\n并且可以设置日志轮转：\n{  \u0026#34;log-driver\u0026#34;: \u0026#34;json-file\u0026#34;,  \u0026#34;log-opts\u0026#34;: {  \u0026#34;max-size\u0026#34;: \u0026#34;10m\u0026#34;,  \u0026#34;max-file\u0026#34;: \u0026#34;3\u0026#34;,  \u0026#34;labels\u0026#34;: \u0026#34;production_status\u0026#34;,  \u0026#34;env\u0026#34;: \u0026#34;os,customer\u0026#34;  } } 优势：\n 部署方便，使用DaemonSet类型控制器来部署agent即可 对业务应用的影响最小，没有侵入性  劣势:\n 只能收集标准和错误输出，对于容器内的文件日志，暂时收集不到  使用 sidecar 容器和日志代理   方式一：sidecar 容器将应用程序日志传送到自己的标准输出。 思路：在pod中启动一个sidecar容器，把容器内的日志文件吐到标准输出，由宿主机中的日志收集agent进行采集。\n$ cat count-pod.yaml apiVersion: v1 kind: Pod metadata:  name: counter spec:  containers:  - name: count  image: busybox  args:  - /bin/sh  - -c  - \u0026gt;  i=0;  while true;  do  echo \u0026#34;$i: $(date)\u0026#34; \u0026gt;\u0026gt; /var/log/1.","title":"Kubernetes集群的日志及监控"},{"content":"第三天 Kubernetes进阶实践 本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。\n ETCD数据的访问 kube-scheduler调度策略实践  预选与优选流程 生产中常用的调度配置实践   k8s集群网络模型  CNI介绍及集群网络选型 Flannel网络模型的实现  vxlan Backend hostgw Backend     集群认证与授权  APIServer安全控制模型 Kubectl的认证授权 RBAC kubelet的认证授权 Service Account   使用Helm管理复杂应用的部署  Helm工作原理详解 Helm的模板开发 实战：使用Helm部署Harbor仓库   kubernetes对接分部式存储  pv、pvc介绍 k8s集群如何使用cephfs作为分布式存储后端 利用storageClass实现动态存储卷的管理 实战：使用分部署存储实现有状态应用的部署   本章知识梳理及回顾  ETCD常用操作 拷贝etcdctl命令行工具：\n$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看etcd集群的成员节点：\n$ export ETCDCTL_API=3 $ etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key member list -w table  $ alias etcdctl=\u0026#39;etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key\u0026#39;  $ etcdctl member list -w table 查看etcd集群节点状态：\n$ etcdctl endpoint status -w table  $ etcdctl endpoint health -w table 设置key值:\n$ etcdctl put luffy 1 $ etcdctl get luffy 查看所有key值：\n$ etcdctl get / --prefix --keys-only 查看具体的key对应的数据：\n$ etcdctl get /registry/pods/jenkins/sonar-postgres-7fc5d748b6-gtmsb 添加定时任务做数据快照（重要！）\n$ etcdctl snapshot save `hostname`-etcd_`date +%Y%m%d%H%M`.db 恢复快照：\n  停止etcd和apiserver\n  移走当前数据目录\n$ mv /var/lib/etcd/ /tmp   恢复快照\n$ etcdctl snapshot restore `hostname`-etcd_`date +%Y%m%d%H%M`.db --data-dir=/var/lib/etcd/   集群恢复\nhttps://github.com/etcd-io/etcd/blob/master/Documentation/op-guide/recovery.md\n  Kubernetes调度 为何要控制Pod应该如何调度  集群中有些机器的配置高（SSD，更好的内存等），我们希望核心的服务（比如说数据库）运行在上面 某两个服务的网络传输很频繁，我们希望它们最好在同一台机器上 \u0026hellip;\u0026hellip;  Kubernetes Scheduler 的作用是将待调度的 Pod 按照一定的调度算法和策略绑定到集群中一个合适的 Worker Node 上，并将绑定信息写入到 etcd 中，之后目标 Node 中 kubelet 服务通过 API Server 监听到 Scheduler 产生的 Pod 绑定事件获取 Pod 信息，然后下载镜像启动容器。\n调度的过程 Scheduler 提供的调度流程分为预选 (Predicates) 和优选 (Priorities) 两个步骤：\n 预选，K8S会遍历当前集群中的所有 Node，筛选出其中符合要求的 Node 作为候选 优选，K8S将对候选的 Node 进行打分  经过预选筛选和优选打分之后，K8S选择分数最高的 Node 来运行 Pod，如果最终有多个 Node 的分数最高，那么 Scheduler 将从当中随机选择一个 Node 来运行 Pod。\n预选：\n优选：\nCordon $ kubectl cordon k8s-slave2 $ kubectl drain k8s-slave2 NodeSelector label是kubernetes中一个非常重要的概念，用户可以非常灵活的利用 label 来管理集群中的资源，POD 的调度可以根据节点的 label 进行特定的部署。\n查看节点的label：\n$ kubectl get nodes --show-labels 为节点打label：\n$ kubectl label node k8s-master disktype=ssd 当 node 被打上了相关标签后，在调度的时候就可以使用这些标签了，只需要在spec 字段中添加nodeSelector字段，里面是我们需要被调度的节点的 label。\n... spec:  hostNetwork: true # 声明pod的网络模式为host模式，效果通docker run --net=host  volumes:  - name: mysql-data  hostPath:  path: /opt/mysql/data  nodeSelector: # 使用节点选择器将Pod调度到指定label的节点  component: mysql  containers:  - name: mysql  image: 192.168.136.10:5000/demo/mysql:5.7 ... nodeAffinity 节点亲和性 ， 比上面的nodeSelector更加灵活，它可以进行一些简单的逻辑组合，不只是简单的相等匹配 。分为两种，硬策略和软策略。\nrequiredDuringSchedulingIgnoredDuringExecution ： 硬策略，如果没有满足条件的节点的话，就不断重试直到满足条件为止，简单说就是你必须满足我的要求，不然我就不会调度Pod。\npreferredDuringSchedulingIgnoredDuringExecution：软策略，如果你没有满足调度要求的节点的话，Pod就会忽略这条规则，继续完成调度过程，说白了就是满足条件最好了，没有满足就忽略掉的策略。\n#要求 Pod 不能运行在128和132两个节点上，如果有节点满足disktype=ssd或者sas的话就优先调度到这类节点上 ... spec:  containers:  - name: demo  image: 192.168.136.10:5000/demo/myblog:v1  ports:  - containerPort: 8002  affinity:  nodeAffinity:  requiredDuringSchedulingIgnoredDuringExecution:  nodeSelectorTerms:  - matchExpressions:  - key: kubernetes.io/hostname  operator: NotIn  values:  - 172.21.51.698  - 192.168.136.132   preferredDuringSchedulingIgnoredDuringExecution:  - weight: 1  preference:  matchExpressions:  - key: disktype  operator: In  values:  - ssd  - sas ... 这里的匹配逻辑是 label 的值在某个列表中，现在Kubernetes提供的操作符有下面的几种：\n In：label 的值在某个列表中 NotIn：label 的值不在某个列表中 Gt：label 的值大于某个值 Lt：label 的值小于某个值 Exists：某个 label 存在 DoesNotExist：某个 label 不存在  如果nodeSelectorTerms下面有多个选项的话，满足任何一个条件就可以了；如果matchExpressions有多个选项的话，则必须同时满足这些条件才能正常调度 Pod\n污点（Taints）与容忍（tolerations） 对于nodeAffinity无论是硬策略还是软策略方式，都是调度 Pod 到预期节点上，而Taints恰好与之相反，如果一个节点标记为 Taints ，除非 Pod 也被标识为可以容忍污点节点，否则该 Taints 节点不会被调度Pod。\nTaints(污点)是Node的一个属性，设置了Taints(污点)后，因为有了污点，所以Kubernetes是不会将Pod调度到这个Node上的。于是Kubernetes就给Pod设置了个属性Tolerations(容忍)，只要Pod能够容忍Node上的污点，那么Kubernetes就会忽略Node上的污点，就能够(不是必须)把Pod调度过去。\n场景一：私有云服务中，某业务使用GPU进行大规模并行计算。为保证性能，希望确保该业务对服务器的专属性，避免将普通业务调度到部署GPU的服务器。\n场景二：用户希望把 Master 节点保留给 Kubernetes 系统组件使用，或者把一组具有特殊资源预留给某些 Pod，则污点就很有用了，Pod 不会再被调度到 taint 标记过的节点。taint 标记节点举例如下：\n设置污点：\n$ kubectl taint node [node_name] key=value:[effect]  其中[effect] 可取值： [ NoSchedule | PreferNoSchedule | NoExecute ]  NoSchedule：一定不能被调度。  PreferNoSchedule：尽量不要调度。  NoExecute：不仅不会调度，还会驱逐Node上已有的Pod。  示例：kubectl taint node k8s-slave1 smoke=true:NoSchedule 去除污点：\n去除指定key及其effect：  kubectl taint nodes [node_name] key:[effect]- #这里的key不用指定value   去除指定key所有的effect:  kubectl taint nodes node_name key-   示例：  kubectl taint node k8s-master smoke=true:NoSchedule  kubectl taint node k8s-master smoke:NoExecute-  kubectl taint node k8s-master smoke- 污点演示：\n## 给k8s-slave1打上污点，smoke=true:NoSchedule $ kubectl taint node k8s-slave1 smoke=true:NoSchedule $ kubectl taint node k8s-slave2 drunk=true:NoSchedule   ## 扩容myblog的Pod，观察新Pod的调度情况 $ kuebctl -n luffy scale deploy myblog --replicas=3 $ kubectl -n luffy get po -w ## pending Pod容忍污点示例：myblog/deployment/deploy-myblog-taint.yaml\n... spec:  containers:  - name: demo  image: 192.168.136.10:5000/demo/myblog:v1  tolerations: #设置容忍性  - key: \u0026#34;smoke\u0026#34;  operator: \u0026#34;Equal\u0026#34; #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal  value: \u0026#34;true\u0026#34;  effect: \u0026#34;NoSchedule\u0026#34;  - key: \u0026#34;drunk\u0026#34;  operator: \u0026#34;Exists\u0026#34; #如果操作符为Exists，那么value属性可省略,不指定operator，默认为Equal  #意思是这个Pod要容忍的有污点的Node的key是smoke Equal true,效果是NoSchedule，  #tolerations属性下各值必须使用引号，容忍的值都是设置Node的taints时给的值。 $ kubectl apply -f deploy-myblog-taint.yaml spec:  containers:  - name: demo  image: 192.168.136.10:5000/demo/myblog  tolerations:  - operator: \u0026#34;Exists\u0026#34; 验证NoExecute效果\nKubernetes集群的网络实现 CNI介绍及集群网络选型, CSI\n容器网络接口（Container Network Interface），实现kubernetes集群的Pod网络通信及管理。包括：\n CNI Plugin负责给容器配置网络，它包括两个基本的接口： 配置网络: AddNetwork(net NetworkConfig, rt RuntimeConf) (types.Result, error) 清理网络: DelNetwork(net NetworkConfig, rt RuntimeConf) error IPAM Plugin负责给容器分配IP地址，主要实现包括host-local和dhcp。  以上两种插件的支持，使得k8s的网络可以支持各式各样的管理模式，当前在业界也出现了大量的支持方案，其中比较流行的比如flannel、calico等。\nkubernetes配置了cni网络插件后，其容器网络创建流程为：\n kubelet先创建pause容器生成对应的network namespace 调用网络driver，因为配置的是CNI，所以会调用CNI相关代码，识别CNI的配置目录为/etc/cni/net.d CNI driver根据配置调用具体的CNI插件，二进制调用，可执行文件目录为/opt/cni/bin,项目 CNI插件给pause容器配置正确的网络，pod中其他的容器都是用pause的网络  可以在此查看社区中的CNI实现，https://github.com/containernetworking/cni\n通用类型：flannel、calico等，部署使用简单\n其他：根据具体的网络环境及网络需求选择，比如\n 公有云机器，可以选择厂商与网络插件的定制Backend，如AWS、阿里、腾讯针对flannel均有自己的插件，也有AWS ECS CNI 私有云厂商，比如Vmware NSX-T等 网络性能等，MacVlan  Flannel网络模型实现剖析 flannel实现overlay，underlay网络通常有多种实现：\n udp vxlan host-gw \u0026hellip;  不特殊指定的话，默认会使用vxlan技术作为Backend，可以通过如下查看：\n$ kubectl -n kube-system exec kube-flannel-ds-amd64-cb7hs cat /etc/kube-flannel/net-conf.json {  \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;,  \u0026#34;Backend\u0026#34;: {  \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34;  } } vxlan介绍及点对点通信的实现 VXLAN 全称是虚拟可扩展的局域网（ Virtual eXtensible Local Area Network），它是一种 overlay 技术，通过三层的网络来搭建虚拟的二层网络。\n它创建在原来的 IP 网络（三层）上，只要是三层可达（能够通过 IP 互相通信）的网络就能部署 vxlan。在每个端点上都有一个 vtep 负责 vxlan 协议报文的封包和解包，也就是在虚拟报文上封装 vtep 通信的报文头部。物理网络上可以创建多个 vxlan 网络，这些 vxlan 网络可以认为是一个隧道，不同节点的虚拟机能够通过隧道直连。每个 vxlan 网络由唯一的 VNI 标识，不同的 vxlan 可以不相互影响。\n VTEP（VXLAN Tunnel Endpoints）：vxlan 网络的边缘设备，用来进行 vxlan 报文的处理（封包和解包）。vtep 可以是网络设备（比如交换机），也可以是一台机器（比如虚拟化集群中的宿主机） VNI（VXLAN Network Identifier）：VNI 是每个 vxlan 的标识，一共有 2^24 = 16,777,216，一般每个 VNI 对应一个租户，也就是说使用 vxlan 搭建的公有云可以理论上可以支撑千万级别的租户  演示：在k8s-slave1和k8s-slave2两台机器间，利用vxlan的点对点能力，实现虚拟二层网络的通信\nk8s-slave1节点：\n# 创建vTEP设备，对端指向k8s-slave2节点，指定VNI及underlay网络使用的网卡 $ ip link add vxlan20 type vxlan id 20 remote 172.21.51.69 dstport 4789 dev eth0  $ ip -d link show vxlan20  # 启动设备 $ ip link set vxlan20 up  # 设置ip地址  ip addr add 10.0.136.11/24 dev vxlan20 k8s-slave2节点：\n# 创建VTEP设备，对端指向k8s-slave1节点，指定VNI及underlay网络使用的网卡 $ ip link add vxlan20 type vxlan id 20 remote 172.21.51.68 dstport 4789 dev eth0  # 启动设备 $ ip link set vxlan20 up  # 设置ip地址 $ ip addr add 10.0.136.12/24 dev vxlan20 在k8s-slave1节点：\n$ ping 10.0.136.12 隧道是一个逻辑上的概念，在 vxlan 模型中并没有具体的物理实体想对应。隧道可以看做是一种虚拟通道，vxlan 通信双方（图中的虚拟机）认为自己是在直接通信，并不知道底层网络的存在。从整体来说，每个 vxlan 网络像是为通信的虚拟机搭建了一个单独的通信通道，也就是隧道。\n实现的过程：\n虚拟机的报文通过 vtep 添加上 vxlan 以及外部的报文层，然后发送出去，对方 vtep 收到之后拆除 vxlan 头部然后根据 VNI 把原始报文发送到目的虚拟机。\n# 查看k8s-slave1主机路由 $ route -n 10.0.136.0 0.0.0.0 255.255.255.0 U 0 0 0 vxlan20  # 到了vxlan的设备后， $ ip -d link show vxlan20  vxlan id 20 remote 172.21.51.69 dev eth0 srcport 0 0 dstport 4789 ...  # 查看fdb地址表，主要由MAC地址、VLAN号、端口号和一些标志域等信息组成,vtep 对端地址为 172.21.51.69，换句话说，如果接收到的报文添加上 vxlan 头部之后都会发到 172.21.51.69 $ bridge fdb show|grep vxlan20 00:00:00:00:00:00 dev vxlan20 dst 172.21.51.69 via eth0 self permanent 在k8s-slave2机器抓包，查看vxlan封装后的包:\n# 在k8s-slave2机器执行 $ tcpdump -i eth0 host 172.21.51.68 -w vxlan.cap  # 在k8s-slave1机器执行 $ ping 10.0.136.12 使用wireshark分析ICMP类型的数据包\n跨主机容器网络的通信 思考：容器网络模式下，vxlan设备该接在哪里？\n基本的保证：目的容器的流量要通过vtep设备进行转发！\n演示：利用vxlan实现跨主机容器网络通信\n为了不影响已有的网络，因此创建一个新的网桥，创建容器接入到新的网桥来演示效果\n在k8s-slave1节点：\n$ docker network ls  # 创建新网桥，指定cidr段 $ docker network create --subnet 172.18.0.0/16 network-luffy $ docker network ls  # 新建容器，接入到新网桥 $ docker run -d --name vxlan-test --net network-luffy --ip 172.18.0.2 nginx:alpine  $ docker exec vxlan-test ifconfig  $ brctl show network-luffy 在k8s-slave2节点：\n# 创建新网桥，指定cidr段 $ docker network create --subnet 172.18.0.0/16 network-luffy  # 新建容器，接入到新网桥 $ docker run -d --name vxlan-test --net network-luffy --ip 172.18.0.3 nginx:alpine 此时执行ping测试：\n$ docker exec vxlan-test ping 172.18.0.3 分析：数据到了网桥后，出不去。结合前面的示例，因此应该将流量由vtep设备转发，联想到网桥的特性，接入到桥中的端口，会由网桥负责转发数据，因此，相当于所有容器发出的数据都会经过到vxlan的端口，vxlan将流量转到对端的vtep端点，再次由网桥负责转到容器中。\nk8s-slave1节点：\n# 删除旧的vtep $ ip link del vxlan20  # 新建vtep $ ip link add vxlan_docker type vxlan id 100 remote 172.21.51.69 dstport 4789 dev eth0 $ ip link set vxlan_docker up # 不用设置ip，因为目标是可以转发容器的数据即可  # 接入到网桥中 $ brctl addif br-904603a72dcd vxlan_docker k8s-slave2节点：\n# 删除旧的vtep $ ip link del vxlan20  # 新建vtep $ ip link add vxlan_docker type vxlan id 100 remote 172.21.51.68 dstport 4789 dev eth0 $ ip link set vxlan_docker up # 不用设置ip，因为目标是可以转发容器的数据即可  # 接入到网桥中 $ brctl addif br-c6660fe2dc53 vxlan_docker 再次执行ping测试：\n$ docker exec vxlan-test ping 172.18.0.3 Flannel的vxlan实现精讲 思考：k8s集群的网络环境和手动实现的跨主机的容器通信有哪些差别？\n CNI要求，集群中的每个Pod都必须分配唯一的Pod IP k8s集群内的通信不是vxlan点对点通信，因为集群内的所有节点之间都需要互联  没法创建点对点的vxlan模型    flannel如何为每个节点分配Pod地址段：\n$ kubectl -n kube-system exec kube-flannel-ds-amd64-cb7hs cat /etc/kube-flannel/net-conf.json {  \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;,  \u0026#34;Backend\u0026#34;: {  \u0026#34;Type\u0026#34;: \u0026#34;vxlan\u0026#34;  } }  #查看节点的pod ip [root@k8s-master bin]# kd get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog-5d9ff54d4b-4rftt 1/1 Running 1 33h 10.244.2.19 k8s-slave2 myblog-5d9ff54d4b-n447p 1/1 Running 1 33h 10.244.1.32 k8s-slave1  #查看k8s-slave1主机分配的地址段 $ cat /run/flannel/subnet.env FLANNEL_NETWORK=10.244.0.0/16 FLANNEL_SUBNET=10.244.1.1/24 FLANNEL_MTU=1450 FLANNEL_IPMASQ=true  # kubelet启动容器的时候就可以按照本机的网段配置来为pod设置IP地址 vtep的设备在哪：\n$ ip -d link show flannel.1 # 没有remote ip，非点对点 Pod的流量如何转到vtep设备中\n$ brctl show cni0  # 每个Pod都会使用Veth pair来实现流量转到cni0网桥  $ route -n 10.244.0.0 10.244.0.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.1.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 10.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 vtep封包的时候，如何拿到目的vetp端的IP及MAC信息\n# flanneld启动的时候会需要配置--iface=eth0,通过该配置可以将网卡的ip及Mac信息存储到ETCD中， # 这样，flannel就知道所有的节点分配的IP段及vtep设备的IP和MAC信息，而且所有节点的flanneld都可以感知到节点的添加和删除操作，就可以动态的更新本机的转发配置 演示跨主机Pod通信的流量详细过程：\n$ kubectl -n luffy get po -o wide myblog-5d9ff54d4b-4rftt 1/1 Running 1 25h 10.244.2.19 k8s-slave2 myblog-5d9ff54d4b-n447p 1/1 Running 1 25h 10.244.1.32 k8s-slave1  $ kubectl -n luffy exec myblog-5d9ff54d4b-n447p -- ping 10.244.2.19 -c 2 PING 10.244.2.19 (10.244.2.19) 56(84) bytes of data. 64 bytes from 10.244.2.19: icmp_seq=1 ttl=62 time=0.480 ms 64 bytes from 10.244.2.19: icmp_seq=2 ttl=62 time=1.44 ms  --- 10.244.2.19 ping statistics --- 2 packets transmitted, 2 received, 0% packet loss, time 1001ms rtt min/avg/max/mdev = 0.480/0.961/1.443/0.482 ms  # 查看路由 $ kubectl -n luffy exec myblog-5d9ff54d4b-n447p -- route -n Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 10.244.1.1 0.0.0.0 UG 0 0 0 eth0 10.244.0.0 10.244.1.1 255.255.0.0 UG 0 0 0 eth0 10.244.1.0 0.0.0.0 255.255.255.0 U 0 0 0 eth0  # 查看k8s-slave1 的veth pair 和网桥 $ brctl show bridge name bridge id STP enabled interfaces cni0 8000.6a9a0b341d88 no veth048cc253  veth76f8e4ce  vetha4c972e1 # 流量到了cni0后，查看slave1节点的route $ route -n Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.136.2 0.0.0.0 UG 100 0 0 eth0 10.0.136.0 0.0.0.0 255.255.255.0 U 0 0 0 vxlan20 10.244.0.0 10.244.0.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.1.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 10.244.2.0 10.244.2.0 255.255.255.0 UG 0 0 0 flannel.1 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.136.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0  # 流量转发到了flannel.1网卡，查看该网卡，其实是vtep设备 $ ip -d link show flannel.1 4: flannel.1: \u0026lt;BROADCAST,MULTICAST,UP,LOWER_UP\u0026gt; mtu 1450 qdisc noqueue state UNKNOWN mode DEFAULT group default  link/ether 8a:2a:89:4d:b0:31 brd ff:ff:ff:ff:ff:ff promiscuity 0  vxlan id 1 local 172.21.51.68 dev eth0 srcport 0 0 dstport 8472 nolearning ageing 300 noudpcsum noudp6zerocsumtx noudp6zerocsumrx addrgenmode eui64 numtxqueues 1 numrxqueues 1 gso_max_size 65536 gso_max_segs 65535  # 该转发到哪里，通过etcd查询数据，然后本地缓存，流量不用走多播发送 $ bridge fdb show dev flannel.1 a6:64:a0:a5:83:55 dst 192.168.136.10 self permanent 86:c2:ad:4e:47:20 dst 172.21.51.69 self permanent  # 对端的vtep设备接收到请求后做解包，取出源payload内容，查看k8s-slave2的路由 $ route -n Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.136.2 0.0.0.0 UG 100 0 0 eth0 10.0.136.0 0.0.0.0 255.255.255.0 U 0 0 0 vxlan20 10.244.0.0 10.244.0.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.1.0 10.244.1.0 255.255.255.0 UG 0 0 0 flannel.1 10.244.2.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.136.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0  #根据路由规则转发到cni0网桥,然后由网桥转到具体的Pod中 实际的请求图：\n k8s-slave1 节点中的 pod-a（10.244.2.19）当中的 IP 包通过 pod-a 内的路由表被发送到eth0，进一步通过veth pair转到宿主机中的网桥 cni0 到达 cni0 当中的 IP 包通过匹配节点 k8s-slave1 的路由表发现通往 10.244.2.19 的 IP 包应该交给 flannel.1 接口 flannel.1 作为一个 VTEP 设备，收到报文后将按照 VTEP 的配置进行封包，第一次会查询ETCD，知道10.244.2.19的vtep设备是k8s-slave2机器，IP地址是172.21.51.69，拿到MAC 地址进行 VXLAN 封包。 通过节点 k8s-slave2 跟 k8s-slave1之间的网络连接，VXLAN 包到达 k8s-slave2 的 eth0 接口 通过端口 8472，VXLAN 包被转发给 VTEP 设备 flannel.1 进行解包 解封装后的 IP 包匹配节点 k8s-slave2 当中的路由表（10.244.2.0），内核将 IP 包转发给cni0 cni0将 IP 包转发给连接在 cni0 上的 pod-b  利用host-gw模式提升集群网络性能 vxlan模式适用于三层可达的网络环境，对集群的网络要求很宽松，但是同时由于会通过VTEP设备进行额外封包和解包，因此给性能带来了额外的开销。\n网络插件的目的其实就是将本机的cni0网桥的流量送到目的主机的cni0网桥。实际上有很多集群是部署在同一二层网络环境下的，可以直接利用二层的主机当作流量转发的网关。这样的话，可以不用进行封包解包，直接通过路由表去转发流量。\n为什么三层可达的网络不直接利用网关转发流量？\n内核当中的路由规则，网关必须在跟主机当中至少一个 IP 处于同一网段。 由于k8s集群内部各节点均需要实现Pod互通，因此，也就意味着host-gw模式需要整个集群节点都在同一二层网络内。 修改flannel的网络后端：\n$ kubectl edit cm kube-flannel-cfg -n kube-system ... net-conf.json: |  {  \u0026#34;Network\u0026#34;: \u0026#34;10.244.0.0/16\u0026#34;,  \u0026#34;Backend\u0026#34;: {  \u0026#34;Type\u0026#34;: \u0026#34;host-gw\u0026#34;  }  } kind: ConfigMap ... 重建Flannel的Pod\n$ kubectl -n kube-system get po |grep flannel kube-flannel-ds-amd64-5dgb8 1/1 Running 0 15m kube-flannel-ds-amd64-c2gdc 1/1 Running 0 14m kube-flannel-ds-amd64-t2jdd 1/1 Running 0 15m  $ kubectl -n kube-system delete po kube-flannel-ds-amd64-5dgb8 kube-flannel-ds-amd64-c2gdc kube-flannel-ds-amd64-t2jdd  # 等待Pod新启动后，查看日志，出现Backend type: host-gw字样 $ kubectl -n kube-system logs -f kube-flannel-ds-amd64-4hjdw I0704 01:18:11.916374 1 kube.go:126] Waiting 10m0s for node controller to sync I0704 01:18:11.916579 1 kube.go:309] Starting kube subnet manager I0704 01:18:12.917339 1 kube.go:133] Node controller sync successful I0704 01:18:12.917848 1 main.go:247] Installing signal handlers I0704 01:18:12.918569 1 main.go:386] Found network config - Backend type: host-gw I0704 01:18:13.017841 1 main.go:317] Wrote subnet file to /run/flannel/subnet.env 查看节点路由表：\n$ route -n Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 192.168.136.2 0.0.0.0 UG 100 0 0 eth0 10.244.0.0 0.0.0.0 255.255.255.0 U 0 0 0 cni0 10.244.1.0 172.21.51.68 255.255.255.0 UG 0 0 0 eth0 10.244.2.0 172.21.51.69 255.255.255.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 docker0 192.168.136.0 0.0.0.0 255.255.255.0 U 100 0 0 eth0  k8s-slave1 节点中的 pod-a（10.244.2.19）当中的 IP 包通过 pod-a 内的路由表被发送到eth0，进一步通过veth pair转到宿主机中的网桥 cni0 到达 cni0 当中的 IP 包通过匹配节点 k8s-slave1 的路由表发现通往 10.244.2.19 的 IP 包应该使用172.21.51.69这个网关进行转发 包到达k8s-slave2节点（172.21.51.69）节点的eth0网卡，根据该节点的路由规则，转发给cni0网卡 cni0将 IP 包转发给连接在 cni0 上的 pod-b  Kubernetes认证与授权 APIServer安全控制   Authentication：身份认证\n 这个环节它面对的输入是整个http request，负责对来自client的请求进行身份校验，支持的方法包括:  basic auth client证书验证（https双向验证） jwt token(用于serviceaccount)   APIServer启动时，可以指定一种Authentication方法，也可以指定多种方法。如果指定了多种方法，那么APIServer将会逐个使用这些方法对客户端请求进行验证， 只要请求数据通过其中一种方法的验证，APIServer就会认为Authentication成功； 使用kubeadm引导启动的k8s集群，apiserver的初始配置中，默认支持client证书验证和serviceaccount两种身份验证方式。 证书认证通过设置--client-ca-file根证书以及--tls-cert-file和--tls-private-key-file来开启。 在这个环节，apiserver会通过client证书或 http header中的字段(比如serviceaccount的jwt token)来识别出请求的用户身份，包括”user”、”group”等，这些信息将在后面的authorization环节用到。    Authorization：鉴权，你可以访问哪些资源\n 这个环节面对的输入是http request context中的各种属性，包括：user、group、request path（比如：/api/v1、/healthz、/version等）、 request verb(比如：get、list、create等)。 APIServer会将这些属性值与事先配置好的访问策略(access policy）相比较。APIServer支持多种authorization mode，包括Node、RBAC、Webhook等。 APIServer启动时，可以指定一种authorization mode，也可以指定多种authorization mode，如果是后者，只要Request通过了其中一种mode的授权， 那么该环节的最终结果就是授权成功。在较新版本kubeadm引导启动的k8s集群的apiserver初始配置中，authorization-mode的默认配置是”Node,RBAC”。    Admission Control：准入控制，一个控制链(层层关卡)，用于拦截请求的一种方式。偏集群安全控制、管理方面。\n  为什么需要？\n认证与授权获取 http 请求 header 以及证书，无法通过body内容做校验。\nAdmission 运行在 API Server 的增删改查 handler 中，可以自然地操作 API resource\n  举个栗子\n  以NamespaceLifecycle为例， 该插件确保处于Termination状态的Namespace不再接收新的对象创建请求，并拒绝请求不存在的Namespace。该插件还可以防止删除系统保留的Namespace:default，kube-system，kube-public。\n  LimitRanger，若集群的命名空间设置了LimitRange对象，若Pod声明时未设置资源值，则按照LimitRange的定义来未Pod添加默认值\napiVersion: v1 kind: LimitRange metadata:  name: mem-limit-range  namespace: luffy spec:  limits:  - default:  memory: 512Mi  defaultRequest:  memory: 256Mi  type: Container --- apiVersion: v1 kind: Pod metadata:  name: default-mem-demo-2 spec:  containers:  - name: default-mem-demo-2-ctr  image: nginx:alpin   NodeRestriction， 此插件限制kubelet修改Node和Pod对象，这样的kubelets只允许修改绑定到Node的Pod API对象，以后版本可能会增加额外的限制 。开启Node授权策略后，默认会打开该项\n    怎么用？\nAPIServer启动时通过 --enable-admission-plugins --disable-admission-plugins 指定需要打开或者关闭的 Admission Controller\n  场景\n 自动注入sidecar容器或者initContainer容器 webhook admission，实现业务自定义的控制需求      kubectl的认证授权 kubectl的日志调试级别：\n   信息 描述     v=0 通常，这对操作者来说总是可见的。   v=1 当您不想要很详细的输出时，这个是一个合理的默认日志级别。   v=2 有关服务和重要日志消息的有用稳定状态信息，这些信息可能与系统中的重大更改相关。这是大多数系统推荐的默认日志级别。   v=3 关于更改的扩展信息。   v=4 调试级别信息。   v=6 显示请求资源。   v=7 显示 HTTP 请求头。   v=8 显示 HTTP 请求内容。   v=9 显示 HTTP 请求内容，并且不截断内容。    $ kubectl get nodes -v=7 I0329 20:20:08.633065 3979 loader.go:359] Config loaded from file /root/.kube/config I0329 20:20:08.633797 3979 round_trippers.go:416] GET https://192.168.136.10:6443/api/v1/nodes?limit=500 kubeadm init启动完master节点后，会默认输出类似下面的提示内容：\n... ... Your Kubernetes master has initialized successfully!  To start using your cluster, you need to run the following as a regular user:  mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/config ... ... 这些信息是在告知我们如何配置kubeconfig文件。按照上述命令配置后，master节点上的kubectl就可以直接使用$HOME/.kube/config的信息访问k8s cluster了。 并且，通过这种配置方式，kubectl也拥有了整个集群的管理员(root)权限。\n很多K8s初学者在这里都会有疑问：\n 当kubectl使用这种kubeconfig方式访问集群时，Kubernetes的kube-apiserver是如何对来自kubectl的访问进行身份验证(authentication)和授权(authorization)的呢？ 为什么来自kubectl的请求拥有最高的管理员权限呢？  查看/root/.kube/config文件：\n前面提到过apiserver的authentication支持通过tls client certificate、basic auth、token等方式对客户端发起的请求进行身份校验， 从kubeconfig信息来看，kubectl显然在请求中使用了tls client certificate的方式，即客户端的证书。\n证书base64解码：\n$ echo xxxxxxxxxxxxxx |base64 -d \u0026gt; kubectl.crt 说明在认证阶段，apiserver会首先使用--client-ca-file配置的CA证书去验证kubectl提供的证书的有效性,基本的方式 ：\n$ openssl verify -CAfile /etc/kubernetes/pki/ca.crt kubectl.crt kubectl.crt: OK 除了认证身份，还会取出必要的信息供授权阶段使用，文本形式查看证书内容：\n$ openssl x509 -in kubectl.crt -text Certificate:  Data:  Version: 3 (0x2)  Serial Number: 4736260165981664452 (0x41ba9386f52b74c4)  Signature Algorithm: sha256WithRSAEncryption  Issuer: CN=kubernetes  Validity  Not Before: Feb 10 07:33:39 2020 GMT  Not After : Feb 9 07:33:40 2021 GMT  Subject: O=system:masters, CN=kubernetes-admin  ... 认证通过后，提取出签发证书时指定的CN(Common Name),kubernetes-admin，作为请求的用户名 (User Name), 从证书中提取O(Organization)字段作为请求用户所属的组 (Group)，group = system:masters，然后传递给后面的授权模块。\nkubeadm在init初始引导集群启动过程中，创建了许多默认的RBAC规则， 在k8s有关RBAC的官方文档中，我们看到下面一些default clusterrole列表:\n其中第一个cluster-admin这个cluster role binding绑定了system:masters group，这和authentication环节传递过来的身份信息不谋而合。 沿着system:masters group对应的cluster-admin clusterrolebinding“追查”下去，真相就会浮出水面。\n我们查看一下这一binding：\n$ kubectl describe clusterrolebinding cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true Role:  Kind: ClusterRole  Name: cluster-admin Subjects:  Kind Name Namespace  ---- ---- ---------  Group system:masters 我们看到在kube-system名字空间中，一个名为cluster-admin的clusterrolebinding将cluster-admin cluster role与system:masters Group绑定到了一起， 赋予了所有归属于system:masters Group中用户cluster-admin角色所拥有的权限。\n我们再来查看一下cluster-admin这个role的具体权限信息：\n$ kubectl describe clusterrole cluster-admin Name: cluster-admin Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule:  Resources Non-Resource URLs Resource Names Verbs  --------- ----------------- -------------- -----  *.* [] [] [*]  [*] [] [*] 非资源类，如查看集群健康状态。\nRBAC Role-Based Access Control，基于角色的访问控制， apiserver启动参数添加\u0026ndash;authorization-mode=RBAC 来启用RBAC认证模式，kubeadm安装的集群默认已开启。官方介绍\n查看开启：\n# master节点查看apiserver进程 $ ps aux |grep apiserver RBAC模式引入了4个资源类型：\n  Role，角色\n一个Role只能授权访问单个namespace\n## 示例定义一个名为pod-reader的角色，该角色具有读取default这个命名空间下的pods的权限 kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:  namespace: default  name: pod-reader rules: - apiGroups: [\u0026#34;\u0026#34;] # \u0026#34;\u0026#34; indicates the core API group  resources: [\u0026#34;pods\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;]  ## apiGroups: \u0026#34;\u0026#34;,\u0026#34;apps\u0026#34;, \u0026#34;autoscaling\u0026#34;, \u0026#34;batch\u0026#34;, kubectl api-versions ## resources: \u0026#34;services\u0026#34;, \u0026#34;pods\u0026#34;,\u0026#34;deployments\u0026#34;... kubectl api-resources ## verbs: \u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;, \u0026#34;delete\u0026#34;, \u0026#34;exec\u0026#34;  ## https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.18/   ClusterRole\n一个ClusterRole能够授予和Role一样的权限，但是它是集群范围内的。\n## 定义一个集群角色，名为secret-reader，该角色可以读取所有的namespace中的secret资源 kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  # \u0026#34;namespace\u0026#34; omitted since ClusterRoles are not namespaced  name: secret-reader rules: - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;secrets\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;list\u0026#34;]  # User,Group,ServiceAccount   Rolebinding\n将role中定义的权限分配给用户和用户组。RoleBinding包含主题（users,groups,或service accounts）和授予角色的引用。对于namespace内的授权使用RoleBinding，集群范围内使用ClusterRoleBinding。\n## 定义一个角色绑定，将pod-reader这个role的权限授予给jane这个User，使得jane可以在读取default这个命名空间下的所有的pod数据 kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: read-pods  namespace: default subjects: - kind: User  #这里可以是User,Group,ServiceAccount  name: jane   apiGroup: rbac.authorization.k8s.io roleRef:  kind: Role #这里可以是Role或者ClusterRole,若是ClusterRole，则权限也仅限于rolebinding的内部  name: pod-reader # match the name of the Role or ClusterRole you wish to bind to  apiGroup: rbac.authorization.k8s.io 注意：rolebinding既可以绑定role，也可以绑定clusterrole，当绑定clusterrole的时候，subject的权限也会被限定于rolebinding定义的namespace内部，若想跨namespace，需要使用clusterrolebinding\n## 定义一个角色绑定，将dave这个用户和secret-reader这个集群角色绑定，虽然secret-reader是集群角色，但是因为是使用rolebinding绑定的，因此dave的权限也会被限制在development这个命名空间内 apiVersion: rbac.authorization.k8s.io/v1 # This role binding allows \u0026#34;dave\u0026#34; to read secrets in the \u0026#34;development\u0026#34; namespace. # You need to already have a ClusterRole named \u0026#34;secret-reader\u0026#34;. kind: RoleBinding metadata:  name: read-secrets  #  # The namespace of the RoleBinding determines where the permissions are granted.  # This only grants permissions within the \u0026#34;development\u0026#34; namespace.  namespace: development subjects: - kind: User  name: dave # Name is case sensitive  apiGroup: rbac.authorization.k8s.io - kind: ServiceAccount  name: dave # Name is case sensitive  namespace: luffy roleRef:  kind: ClusterRole  name: secret-reader  apiGroup: rbac.authorization.k8s.io 考虑一个场景： 如果集群中有多个namespace分配给不同的管理员，每个namespace的权限是一样的，就可以只定义一个clusterrole，然后通过rolebinding将不同的namespace绑定到管理员身上，否则就需要每个namespace定义一个Role，然后做一次rolebinding。\n  ClusterRolebingding\n允许跨namespace进行授权\napiVersion: rbac.authorization.k8s.io/v1 # This cluster role binding allows anyone in the \u0026#34;manager\u0026#34; group to read secrets in any namespace. kind: ClusterRoleBinding metadata:  name: read-secrets-global subjects: - kind: Group  name: manager # Name is case sensitive  apiGroup: rbac.authorization.k8s.io roleRef:  kind: ClusterRole  name: secret-reader  apiGroup: rbac.authorization.k8s.io   kubelet的认证授权 查看kubelet进程\n$ systemctl status kubelet ● kubelet.service - kubelet: The Kubernetes Node Agent  Loaded: loaded (/usr/lib/systemd/system/kubelet.service; enabled; vendor preset: disabled)  Drop-In: /usr/lib/systemd/system/kubelet.service.d  └─10-kubeadm.conf  Active: active (running) since Sun 2020-07-05 19:33:36 EDT; 1 day 12h ago  Docs: https://kubernetes.io/docs/  Main PID: 10622 (kubelet)  Tasks: 24  Memory: 60.5M  CGroup: /system.slice/kubelet.service  └─851 /usr/bin/kubelet --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf --kubeconfig=/etc/kubernetes/kubelet.conf 查看/etc/kubernetes/kubelet.conf，解析证书：\n$ echo xxxxx |base64 -d \u0026gt;kubelet.crt $ openssl x509 -in kubelet.crt -text Certificate:  Data:  Version: 3 (0x2)  Serial Number: 9059794385454520113 (0x7dbadafe23185731)  Signature Algorithm: sha256WithRSAEncryption  Issuer: CN=kubernetes  Validity  Not Before: Feb 10 07:33:39 2020 GMT  Not After : Feb 9 07:33:40 2021 GMT  Subject: O=system:nodes, CN=system:node:master-1 得到我们期望的内容：\nSubject: O=system:nodes, CN=system:node:k8s-master 我们知道，k8s会把O作为Group来进行请求，因此如果有权限绑定给这个组，肯定在clusterrolebinding的定义中可以找得到。因此尝试去找一下绑定了system:nodes组的clusterrolebinding\n$ kubectl get clusterrolebinding|awk \u0026#39;NR\u0026gt;1{print $1}\u0026#39;|xargs kubectl get clusterrolebinding -oyaml|grep -n10 system:nodes 98- roleRef: 99- apiGroup: rbac.authorization.k8s.io 100- kind: ClusterRole 101- name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient 102- subjects: 103- - apiGroup: rbac.authorization.k8s.io 104- kind: Group 105: name: system:nodes 106-- apiVersion: rbac.authorization.k8s.io/v1 107- kind: ClusterRoleBinding 108- metadata: 109- creationTimestamp: \u0026#34;2020-02-10T07:34:02Z\u0026#34; 110- name: kubeadm:node-proxier 111- resourceVersion: \u0026#34;213\u0026#34; 112- selfLink: /apis/rbac.authorization.k8s.io/v1/clusterrolebindings/kubeadm%3Anode-proxier  $ kubectl describe clusterrole system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Name: system:certificates.k8s.io:certificatesigningrequests:selfnodeclient Labels: kubernetes.io/bootstrapping=rbac-defaults Annotations: rbac.authorization.kubernetes.io/autoupdate: true PolicyRule:  Resources Non-Resource URLs Resource Names Verbs  --------- ----------------- -------------- -----  certificatesigningrequests.certificates.k8s.io/selfnodeclient [] [] [create] 结局有点意外，除了system:certificates.k8s.io:certificatesigningrequests:selfnodeclient外，没有找到system相关的rolebindings，显然和我们的理解不一样。 尝试去找资料，发现了这么一段 :\n   Default ClusterRole Default ClusterRoleBinding Description     system:kube-scheduler system:kube-scheduler user Allows access to the resources required by the schedulercomponent.   system:volume-scheduler system:kube-scheduler user Allows access to the volume resources required by the kube-scheduler component.   system:kube-controller-manager system:kube-controller-manager user Allows access to the resources required by the controller manager component. The permissions required by individual controllers are detailed in the controller roles.   system:node None Allows access to resources required by the kubelet, including read access to all secrets, and write access to all pod status objects. You should use the Node authorizer and NodeRestriction admission plugin instead of the system:node role, and allow granting API access to kubelets based on the Pods scheduled to run on them. The system:node role only exists for compatibility with Kubernetes clusters upgraded from versions prior to v1.8.   system:node-proxier system:kube-proxy user Allows access to the resources required by the kube-proxycomponent.    大致意思是说：之前会定义system:node这个角色，目的是为了kubelet可以访问到必要的资源，包括所有secret的读权限及更新pod状态的写权限。如果1.8版本后，是建议使用 Node authorizer and NodeRestriction admission plugin 来代替这个角色的。\n我们目前使用1.16，查看一下授权策略：\n$ ps axu|grep apiserver kube-apiserver --authorization-mode=Node,RBAC --enable-admission-plugins=NodeRestriction 查看一下官网对Node authorizer的介绍：\nNode authorization is a special-purpose authorization mode that specifically authorizes API requests made by kubelets.\nIn future releases, the node authorizer may add or remove permissions to ensure kubelets have the minimal set of permissions required to operate correctly.\nIn order to be authorized by the Node authorizer, kubelets must use a credential that identifies them as being in the system:nodes group, with a username of system:node:\u0026lt;nodeName\u0026gt;\nService Account及K8S Api调用 前面说，认证可以通过证书，也可以通过使用ServiceAccount（服务账户）的方式来做认证。大多数时候，我们在基于k8s做二次开发时都是选择通过ServiceAccount + RBAC 的方式。我们之前访问dashboard的时候，是如何做的？\n## 新建一个名为admin的serviceaccount，并且把名为cluster-admin的这个集群角色的权限授予新建的 #serviceaccount apiVersion: v1 kind: ServiceAccount metadata:  name: admin  namespace: kubernetes-dashboard --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata:  name: admin  annotations:  rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef:  kind: ClusterRole  name: cluster-admin  apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount  name: admin  namespace: kubernetes-dashboard 我们查看一下：\n$ kubectl -n kubernetes-dashboard get sa admin -o yaml apiVersion: v1 kind: ServiceAccount metadata:  creationTimestamp: \u0026#34;2020-04-01T11:59:21Z\u0026#34;  name: admin  namespace: kubernetes-dashboard  resourceVersion: \u0026#34;1988878\u0026#34;  selfLink: /api/v1/namespaces/kubernetes-dashboard/serviceaccounts/admin  uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f secrets: - name: admin-token-lfsrf 注意到serviceaccount上默认绑定了一个名为admin-token-lfsrf的secret，我们查看一下secret\n$ kubectl -n kubernetes-dashboard describe secret admin-token-lfsrf Name: admin-token-lfsrf Namespace: kubernetes-dashboard Labels: \u0026lt;none\u0026gt; Annotations: kubernetes.io/service-account.name: admin  kubernetes.io/service-account.uid: 639ecc3e-74d9-11ea-a59b-000c29dfd73f  Type: kubernetes.io/service-account-token Data ==== ca.crt: 1025 bytes namespace: 4 bytes token: eyJhbGciOiJSUzI1NiIsImtpZCI6IiJ9.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJkZW1vIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZWNyZXQubmFtZSI6ImFkbWluLXRva2VuLWxmc3JmIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQubmFtZSI6ImFkbWluIiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9zZXJ2aWNlLWFjY291bnQudWlkIjoiNjM5ZWNjM2UtNzRkOS0xMWVhLWE1OWItMDAwYzI5ZGZkNzNmIiwic3ViIjoic3lzdGVtOnNlcnZpY2VhY2NvdW50OmRlbW86YWRtaW4ifQ.ffGCU4L5LxTsMx3NcNixpjT6nLBi-pmstb4I-W61nLOzNaMmYSEIwAaugKMzNR-2VwM14WbuG04dOeO67niJeP6n8-ALkl-vineoYCsUjrzJ09qpM3TNUPatHFqyjcqJ87h4VKZEqk2qCCmLxB6AGbEHpVFkoge40vHs56cIymFGZLe53JZkhu3pwYuS4jpXytV30Ad-HwmQDUu_Xqcifni6tDYPCfKz2CZlcOfwqHeGIHJjDGVBKqhEeo8PhStoofBU6Y4OjObP7HGuTY-Foo4QindNnpp0QU6vSb7kiOiQ4twpayybH8PTf73dtdFt46UF6mGjskWgevgolvmO8A 演示role的权限：\n$ cat test-sa.yaml serviceaccount apiVersion: v1 kind: ServiceAccount metadata:  name: test  namespace: kubernetes-dashboard  --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata:  name: test  annotations:  rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef:  kind: ClusterRole  name: cluster-admin  apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount  name: test  namespace: kubernetes-dashboard curl演示\n$ curl -k -H \u0026#34;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InhXcmtaSG5ZODF1TVJ6dUcycnRLT2c4U3ZncVdoVjlLaVRxNG1wZ0pqVmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1xNXBueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImViZDg2ODZjLWZkYzAtNDRlZC04NmZlLTY5ZmE0ZTE1YjBmMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbiJ9.iEIVMWg2mHPD88GQ2i4uc_60K4o17e39tN0VI_Q_s3TrRS8hmpi0pkEaN88igEKZm95Qf1qcN9J5W5eqOmcK2SN83Dd9dyGAGxuNAdEwi0i73weFHHsjDqokl9_4RGbHT5lRY46BbIGADIphcTeVbCggI6T_V9zBbtl8dcmsd-lD_6c6uC2INtPyIfz1FplynkjEVLapp_45aXZ9IMy76ljNSA8Uc061Uys6PD3IXsUD5JJfdm7lAt0F7rn9SdX1q10F2lIHYCMcCcfEpLr4Vkymxb4IU4RCR8BsMOPIO_yfRVeYZkG4gU2C47KwxpLsJRrTUcUXJktSEPdeYYXf9w\u0026#34; https://192.168.136.10:6443/api/v1/namespaces/luffy/pods?limit=500 通过HPA实现业务应用的动态扩缩容 HPA控制器介绍 当系统资源过高的时候，我们可以使用如下命令来实现 Pod 的扩缩容功能\n$ kubectl -n luffy scale deployment myblog --replicas=2 但是这个过程是手动操作的。在实际项目中，我们需要做到是的是一个自动化感知并自动扩容的操作。Kubernetes 也为提供了这样的一个资源对象：Horizontal Pod Autoscaling（Pod 水平自动伸缩），简称HPA\n基本原理：HPA 通过监控分析控制器控制的所有 Pod 的负载变化情况来确定是否需要调整 Pod 的副本数量\nHPA的实现有两个版本：\n autoscaling/v1，只包含了根据CPU指标的检测，稳定版本 autoscaling/v2beta1，支持根据memory或者用户自定义指标进行伸缩  如何获取Pod的监控数据？\n k8s 1.8以下：使用heapster，1.11版本完全废弃 k8s 1.8以上：使用metric-server  思考：为什么之前用 heapster ，现在废弃了项目，改用 metric-server ？\nheapster时代，apiserver 会直接将metric请求通过apiserver proxy 的方式转发给集群内的 hepaster 服务，采用这种 proxy 方式是有问题的：\n  http://kubernetes_master_address/api/v1/namespaces/namespace_name/services/service_name[:port_name]/proxy   proxy只是代理请求，一般用于问题排查，不够稳定，且版本不可控\n  heapster的接口不能像apiserver一样有完整的鉴权以及client集成\n  pod 的监控数据是核心指标（HPA调度），应该和 pod 本身拥有同等地位，即 metric应该作为一种资源存在，如metrics.k8s.io 的形式，称之为 Metric Api\n  于是官方从 1.8 版本开始逐步废弃 heapster，并提出了上边 Metric api 的概念，而 metrics-server 就是这种概念下官方的一种实现，用于从 kubelet获取指标，替换掉之前的 heapster。\nMetrics Server 可以通过标准的 Kubernetes API 把监控数据暴露出来，比如获取某一Pod的监控数据：\nhttps://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces/\u0026lt;namespace-name\u0026gt;/pods/\u0026lt;pod-name\u0026gt;  # https://192.168.136.10:6443/api/v1/namespaces/luffy/pods?limit=500 目前的采集流程：\nMetric Server 官方介绍\n... Metric server collects metrics from the Summary API, exposed by Kubelet on each node.  Metrics Server registered in the main API server through Kubernetes aggregator, which was introduced in Kubernetes 1.7 ... 安装 官方代码仓库地址：https://github.com/kubernetes-sigs/metrics-server\nDepending on your cluster setup, you may also need to change flags passed to the Metrics Server container. Most useful flags:\n --kubelet-preferred-address-types - The priority of node address types used when determining an address for connecting to a particular node (default [Hostname,InternalDNS,InternalIP,ExternalDNS,ExternalIP]) --kubelet-insecure-tls - Do not verify the CA of serving certificates presented by Kubelets. For testing purposes only. --requestheader-client-ca-file - Specify a root certificate bundle for verifying client certificates on incoming requests.  $ wget https://github.com/kubernetes-sigs/metrics-server/releases/download/v0.3.6/components.yaml 修改args参数：\n...  84 containers:  85 - name: metrics-server  86 image: registry.aliyuncs.com/google_containers/metrics-server-amd64:v0.3.6  87 imagePullPolicy: IfNotPresent  88 args:  89 - --cert-dir=/tmp  90 - --secure-port=4443  91 - --kubelet-insecure-tls  92 - --kubelet-preferred-address-types=InternalIP ... 执行安装：\n$ kubectl create -f components.yaml  $ kubectl -n kube-system get pods  $ kubectl top nodes kubelet的指标采集 无论是 heapster还是 metric-server，都只是数据的中转和聚合，两者都是调用的 kubelet 的 api 接口获取的数据，而 kubelet 代码中实际采集指标的是 cadvisor 模块，你可以在 node 节点访问 10250 端口获取监控数据：\n Kubelet Summary metrics: https://127.0.0.1:10250/metrics，暴露 node、pod 汇总数据 Cadvisor metrics: https://127.0.0.1:10250/metrics/cadvisor，暴露 container 维度数据  调用示例：\n$ curl -k -H \u0026#34;Authorization: Bearer eyJhbGciOiJSUzI1NiIsImtpZCI6InhXcmtaSG5ZODF1TVJ6dUcycnRLT2c4U3ZncVdoVjlLaVRxNG1wZ0pqVmcifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1xNXBueiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6ImViZDg2ODZjLWZkYzAtNDRlZC04NmZlLTY5ZmE0ZTE1YjBmMCIsInN1YiI6InN5c3RlbTpzZXJ2aWNlYWNjb3VudDprdWJlcm5ldGVzLWRhc2hib2FyZDphZG1pbiJ9.iEIVMWg2mHPD88GQ2i4uc_60K4o17e39tN0VI_Q_s3TrRS8hmpi0pkEaN88igEKZm95Qf1qcN9J5W5eqOmcK2SN83Dd9dyGAGxuNAdEwi0i73weFHHsjDqokl9_4RGbHT5lRY46BbIGADIphcTeVbCggI6T_V9zBbtl8dcmsd-lD_6c6uC2INtPyIfz1FplynkjEVLapp_45aXZ9IMy76ljNSA8Uc061Uys6PD3IXsUD5JJfdm7lAt0F7rn9SdX1q10F2lIHYCMcCcfEpLr4Vkymxb4IU4RCR8BsMOPIO_yfRVeYZkG4gU2C47KwxpLsJRrTUcUXJktSEPdeYYXf9w\u0026#34; https://localhost:10250/metrics kubelet虽然提供了 metric 接口，但实际监控逻辑由内置的cAdvisor模块负责，早期的时候，cadvisor是单独的组件，从k8s 1.12开始，cadvisor 监听的端口在k8s中被删除，所有监控数据统一由Kubelet的API提供。\ncadvisor获取指标时实际调用的是 runc/libcontainer库，而libcontainer是对 cgroup文件 的封装，即 cadvsior也只是个转发者，它的数据来自于cgroup文件。\ncgroup文件中的值是监控数据的最终来源，如\n  mem usage的值，\n  对于docker容器来讲，来源于/sys/fs/cgroup/memory/docker/[containerId]/memory.usage_in_bytes\n  对于pod来讲，/sys/fs/cgroup/memory/kubepods/besteffort/pod[podId]/memory.usage_in_bytes或者\n/sys/fs/cgroup/memory/kubepods/burstable/pod[podId]/memory.usage_in_bytes\n    如果没限制内存，Limit = machine_mem，否则来自于 /sys/fs/cgroup/memory/docker/[id]/memory.limit_in_bytes\n  内存使用率 = memory.usage_in_bytes/memory.limit_in_bytes\n  Metrics数据流：\n思考：\nMetrics Server是独立的一个服务，只能服务内部实现自己的api，是如何做到通过标准的kubernetes 的API格式暴露出去的？\nkube-aggregator\nkube-aggregator聚合器及Metric-Server的实现 kube-aggregator是对 apiserver 的api的一种拓展机制，它允许开发人员编写一个自己的服务，并把这个服务注册到k8s的api里面，即扩展 API 。\n定义一个APIService对象：\napiVersion: apiregistration.k8s.io/v1 kind: APIService metadata:  name: v1beta1.luffy.k8s.io spec:  group: luffy.k8s.io  groupPriorityMinimum: 100  insecureSkipTLSVerify: true  service:  name: service-A  # 必须https访问  namespace: luffy  port: 443  version: v1beta1  versionPriority: 100 k8s会自动帮我们代理如下url的请求：\nproxyPath := \u0026#34;/apis/\u0026#34; + apiService.Spec.Group + \u0026#34;/\u0026#34; + apiService.Spec.Version 即：https://192.168.136.10:6443/apis/luffy.k8s.io/v1beta1/xxxx转到我们的service-A服务中，service-A中只需要实现 https://service-A/luffy.k8s.io/v1beta1/xxxx 即可。\n看下metric-server的实现：\n$ kubectl get apiservice NAME SERVICE AVAILABLE v1beta1.metrics.k8s.io kube-system/metrics-server\tTrue  $ kubectl get apiservice v1beta1.metrics.k8s.io -oyaml ... spec:  group: metrics.k8s.io  groupPriorityMinimum: 100  insecureSkipTLSVerify: true  service:  name: metrics-server  namespace: kube-system  port: 443  version: v1beta1  versionPriority: 100 ...  $ kubectl -n kube-system get svc metrics-server NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE metrics-server ClusterIP 10.110.111.146 \u0026lt;none\u0026gt; 443/TCP 11h  $ curl -k -H \u0026#34;Authorization: Bearer xxxx\u0026#34; https://10.110.111.146 {  \u0026#34;paths\u0026#34;: [  \u0026#34;/apis\u0026#34;,  \u0026#34;/apis/metrics.k8s.io\u0026#34;,  \u0026#34;/apis/metrics.k8s.io/v1beta1\u0026#34;,  \u0026#34;/healthz\u0026#34;,  \u0026#34;/healthz/healthz\u0026#34;,  \u0026#34;/healthz/log\u0026#34;,  \u0026#34;/healthz/ping\u0026#34;,  \u0026#34;/healthz/poststarthook/generic-apiserver-start-informers\u0026#34;,  \u0026#34;/metrics\u0026#34;,  \u0026#34;/openapi/v2\u0026#34;,  \u0026#34;/version\u0026#34;  ]  # https://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces/\u0026lt;namespace-name\u0026gt;/pods/\u0026lt;pod-name\u0026gt; #  $ curl -k -H \u0026#34;Authorization: Bearer xxxx\u0026#34; https://10.110.111.146/apis/metrics.k8s.io/v1beta1/namespaces/luffy/pods/myblog-5d9ff54d4b-4rftt  $ curl -k -H \u0026#34;Authorization: Bearer xxxx\u0026#34; https://192.168.136.10:6443/apis/metrics.k8s.io/v1beta1/namespaces/luffy/pods/myblog-5d9ff54d4b-4rftt HPA实践 基于CPU的动态伸缩 创建hpa对象：\n# 方式一 $ cat hpa-myblog.yaml apiVersion: autoscaling/v1 kind: HorizontalPodAutoscaler metadata:  name: hpa-myblog-cpu  namespace: luffy spec:  maxReplicas: 3  minReplicas: 1  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: myblog  targetCPUUtilizationPercentage: 10  # 方式二 $ kubectl -n luffy autoscale deployment myblog --cpu-percent=10 --min=1 --max=3  Deployment对象必须配置requests的参数，不然无法获取监控数据，也无法通过HPA进行动态伸缩\n 验证：\n$ yum -y install httpd-tools $ kubectl -n luffy get svc myblog myblog ClusterIP 10.104.245.225 \u0026lt;none\u0026gt; 80/TCP 6d18h  # 为了更快看到效果，先调整副本数为1 $ kubectl -n luffy scale deploy myblog --replicas=1  # 模拟1000个用户并发访问页面10万次 $ ab -n 100000 -c 1000 http://10.104.245.225/blog/index/  $ kubectl get hpa $ kubectl -n luffy get pods 压力降下来后，会有默认5分钟的scaledown的时间，可以通过controller-manager的如下参数设置：\n--horizontal-pod-autoscaler-downscale-stabilization  The value for this option is a duration that specifies how long the autoscaler has to wait before another downscale operation can be performed after the current one has completed. The default value is 5 minutes (5m0s). 是一个逐步的过程，当前的缩放完成后，下次缩放的时间间隔，比如从3个副本降低到1个副本，中间大概会等待2*5min = 10分钟\n基于内存的动态伸缩 创建hpa对象\n$ cat hpa-demo-mem.yaml apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: hpa-demo-mem  namespace: luffy spec:  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: hpa-demo-mem  minReplicas: 1  maxReplicas: 3  metrics:  - type: Resource  resource:  name: memory  targetAverageUtilization: 30 加压演示脚本：\n$ cat increase-mem-config.yaml apiVersion: v1 kind: ConfigMap metadata:  name: increase-mem-config  namespace: luffy data:  increase-mem.sh: |  #!/bin/bash   mkdir /tmp/memory  mount -t tmpfs -o size=40M tmpfs /tmp/memory  dd if=/dev/zero of=/tmp/memory/block  sleep 60  rm /tmp/memory/block  umount /tmp/memory  rmdir /tmp/memory 测试deployment：\n$ cat hpa-demo-mem-deploy.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: hpa-demo-mem  namespace: luffy spec:  selector:  matchLabels:  app: nginx  template:  metadata:  labels:  app: nginx  spec:  volumes:  - name: increase-mem-script  configMap:  name: increase-mem-config  containers:  - name: nginx  image: nginx:alpine  ports:  - containerPort: 80  volumeMounts:  - name: increase-mem-script  mountPath: /etc/script  resources:  requests:  memory: 50Mi  cpu: 50m  securityContext:  privileged: true 测试：\n$ kubectl create -f increase-mem-config.yaml $ kubectl create -f hpa-demo-mem.yaml $ kubectl create -f hpa-demo-mem-deploy.yaml  $ kubectl -n luffy exec -ti hpa-demo-mem-7fc75bf5c8-xx424 sh #/ sh /etc/script/increase-mem.sh   # 观察hpa及pod $ kubectl -n luffy get hpa $ kubectl -n luffy get po 基于自定义指标的动态伸缩 除了基于 CPU 和内存来进行自动扩缩容之外，我们还可以根据自定义的监控指标来进行。这个我们就需要使用 Prometheus Adapter，Prometheus 用于监控应用的负载和集群本身的各种指标，Prometheus Adapter 可以帮我们使用 Prometheus 收集的指标并使用它们来制定扩展策略，这些指标都是通过 APIServer 暴露的，而且 HPA 资源对象也可以很轻易的直接使用。\n架构图：\nkubernetes对接分部式存储 PV与PVC快速入门 k8s存储的目的就是保证Pod重建后，数据不丢失。简单的数据持久化的下述方式：\n  emptyDir\napiVersion: v1 kind: Pod metadata:  name: test-pd spec:  containers:  - image: k8s.gcr.io/test-webserver  name: webserver  volumeMounts:  - mountPath: /cache  name: cache-volume  - image: k8s.gcr.io/test-redis  name: redis  volumeMounts:  - mountPath: /data  name: cache-volume volumes:  - name: cache-volume  emptyDir: {}  Pod内的容器共享卷的数据 存在于Pod的生命周期，Pod销毁，数据丢失 Pod内的容器自动重建后，数据不会丢失    hostPath\napiVersion: v1 kind: Pod metadata:  name: test-pd spec:  containers:  - image: k8s.gcr.io/test-webserver  name: test-container  volumeMounts:  - mountPath: /test-pd  name: test-volume  volumes:  - name: test-volume  hostPath:  # directory location on host  path: /data  # this field is optional  type: Directory 通常配合nodeSelector使用\n  nfs存储\n...  volumes:  - name: redisdata  #卷名称  nfs: #使用NFS网络存储卷  server: 192.168.31.241 #NFS服务器地址  path: /data/redis  #NFS服务器共享的目录  readOnly: false #是否为只读 ...   volume支持的种类众多（参考 https://kubernetes.io/docs/concepts/storage/volumes/#types-of-volumes ），每种对应不同的存储后端实现，因此为了屏蔽后端存储的细节，同时使得Pod在使用存储的时候更加简洁和规范，k8s引入了两个新的资源类型，PV和PVC。\nPersistentVolume（持久化卷），是对底层的存储的一种抽象，它和具体的底层的共享存储技术的实现方式有关，比如 Ceph、GlusterFS、NFS 等，都是通过插件机制完成与共享存储的对接。如使用PV对接NFS存储：\napiVersion: v1 kind: PersistentVolume metadata:  name: nfs-pv spec:  capacity:  storage: 1Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain  nfs:  path: /data/k8s  server: 121.204.157.52  capacity，存储能力， 目前只支持存储空间的设置， 就是我们这里的 storage=1Gi，不过未来可能会加入 IOPS、吞吐量等指标的配置。 accessModes，访问模式， 是用来对 PV 进行访问模式的设置，用于描述用户应用对存储资源的访问权限，访问权限包括下面几种方式：  ReadWriteOnce（RWO）：读写权限，但是只能被单个节点挂载 ReadOnlyMany（ROX）：只读权限，可以被多个节点挂载 ReadWriteMany（RWX）：读写权限，可以被多个节点挂载     persistentVolumeReclaimPolicy，pv的回收策略, 目前只有 NFS 和 HostPath 两种类型支持回收策略  Retain（保留）- 保留数据，需要管理员手工清理数据 Recycle（回收）- 清除 PV 中的数据，效果相当于执行 rm -rf /thevolume/* Delete（删除）- 与 PV 相连的后端存储完成 volume 的删除操作，当然这常见于云服务商的存储服务，比如 ASW EBS。    因为PV是直接对接底层存储的，就像集群中的Node可以为Pod提供计算资源（CPU和内存）一样，PV可以为Pod提供存储资源。因此PV不是namespaced的资源，属于集群层面可用的资源。Pod如果想使用该PV，需要通过创建PVC挂载到Pod中。\nPVC全写是PersistentVolumeClaim（持久化卷声明），PVC 是用户存储的一种声明，创建完成后，可以和PV实现一对一绑定。对于真正使用存储的用户不需要关心底层的存储实现细节，只需要直接使用 PVC 即可。\napiVersion: v1 kind: PersistentVolumeClaim metadata:  name: pvc-nfs  namespace: default spec:  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 1Gi 然后Pod中通过如下方式去使用：\n...  spec:  containers:  - name: nginx  image: nginx:alpine  imagePullPolicy: IfNotPresent  ports:  - containerPort: 80  name: web  volumeMounts: #挂载容器中的目录到pvc nfs中的目录  - name: www  mountPath: /usr/share/nginx/html  volumes:  - name: www  persistentVolumeClaim: #指定pvc  claimName: pvc-nfs ... PV与PVC管理NFS存储卷实践 环境准备 服务端：121.204.157.52\n$ yum -y install nfs-utils rpcbind  # 共享目录 $ mkdir -p /data/k8s \u0026amp;\u0026amp; chmod 755 /data/k8s  $ echo \u0026#39;/data/k8s *(insecure,rw,sync,no_root_squash)\u0026#39;\u0026gt;\u0026gt;/etc/exports  $ systemctl enable rpcbind \u0026amp;\u0026amp; systemctl start rpcbind $ systemctl enable nfs \u0026amp;\u0026amp; systemctl start nfs 客户端：k8s集群slave节点\n$ yum -y install nfs-utils rpcbind $ mkdir /nfsdata $ mount -t nfs 121.204.157.52:/data/k8s /nfsdata PV与PVC演示 $ cat pv-nfs.yaml apiVersion: v1 kind: PersistentVolume metadata:  name: nfs-pv spec:  capacity:  storage: 1Gi  accessModes:  - ReadWriteOnce  persistentVolumeReclaimPolicy: Retain  nfs:  path: /data/k8s  server: 121.204.157.52  $ kubectl create -f pv-nfs.yaml  $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM STORAGECLASS nfs-pv 1Gi RWO Retain Available 一个 PV 的生命周期中，可能会处于4中不同的阶段：\n Available（可用）：表示可用状态，还未被任何 PVC 绑定 Bound（已绑定）：表示 PV 已经被 PVC 绑定 Released（已释放）：PVC 被删除，但是资源还未被集群重新声明 Failed（失败）： 表示该 PV 的自动回收失败  $ cat pvc.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata:  name: pvc-nfs  namespace: default spec:  accessModes:  - ReadWriteOnce  resources:  requests:  storage: 1Gi  $ kubectl create -f pvc.yaml $ kubectl get pvc NAME STATUS VOLUME CAPACITY ACCESS MODES STORAGECLASS AGE pvc-nfs Bound nfs-pv 1Gi RWO 3s $ kubectl get pv NAME CAPACITY ACCESS MODES RECLAIM POLICY STATUS CLAIM nfs-pv 1Gi RWO Retain Bound default/pvc-nfs  #访问模式，storage大小（pvc大小需要小于pv大小），以及 PV 和 PVC 的 storageClassName 字段必须一样，这样才能够进行绑定。  #PersistentVolumeController会不断地循环去查看每一个 PVC，是不是已经处于 Bound（已绑定）状态。如果不是，那它就会遍历所有的、可用的 PV，并尝试将其与未绑定的 PVC 进行绑定，这样，Kubernetes 就可以保证用户提交的每一个 PVC，只要有合适的 PV 出现，它就能够很快进入绑定状态。而所谓将一个 PV 与 PVC 进行“绑定”，其实就是将这个 PV 对象的名字，填在了 PVC 对象的 spec.volumeName 字段上。  # 查看nfs数据目录 $ ls /nfsdata 创建Pod挂载pvc\n$ cat deployment.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: nfs-pvc spec:  replicas: 1  selector: #指定Pod的选择器  matchLabels:  app: nginx  template:  metadata:  labels:  app: nginx  spec:  containers:  - name: nginx  image: nginx:alpine  imagePullPolicy: IfNotPresent  ports:  - containerPort: 80  name: web  volumeMounts: #挂载容器中的目录到pvc nfs中的目录  - name: www  mountPath: /usr/share/nginx/html  volumes:  - name: www  persistentVolumeClaim: #指定pvc  claimName: pvc-nfs   $ kubectl create -f deployment.yaml  # 查看容器/usr/share/nginx/html目录 storageClass实现动态挂载 创建pv及pvc过程是手动，且pv与pvc一一对应，手动创建很繁琐。因此，通过storageClass + provisioner的方式来实现通过PVC自动创建并绑定PV。\n部署： https://github.com/kubernetes-retired/external-storage\nprovisioner.yaml apiVersion: apps/v1 kind: Deployment metadata:  name: nfs-client-provisioner  labels:  app: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: default spec:  replicas: 1  selector:  matchLabels:  app: nfs-client-provisioner  strategy:  type: Recreate  selector:  matchLabels:  app: nfs-client-provisioner  template:  metadata:  labels:  app: nfs-client-provisioner  spec:  serviceAccountName: nfs-client-provisioner  containers:  - name: nfs-client-provisioner  image: quay.io/external_storage/nfs-client-provisioner:latest  volumeMounts:  - name: nfs-client-root  mountPath: /persistentvolumes  env:  - name: PROVISIONER_NAME  value: luffy.com/nfs  - name: NFS_SERVER  value: 172.21.51.55  - name: NFS_PATH   value: /data/k8s  volumes:  - name: nfs-client-root  nfs:  server: 172.21.51.55  path: /data/k8s rbac.yaml kind: ServiceAccount apiVersion: v1 metadata:  name: nfs-client-provisioner  namespace: nfs-provisioner --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: nfs-client-provisioner-runner  namespace: nfs-provisioner rules:  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;persistentvolumes\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;persistentvolumeclaims\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;]  - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;]  resources: [\u0026#34;storageclasses\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;events\u0026#34;]  verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: run-nfs-client-provisioner  namespace: nfs-provisioner subjects:  - kind: ServiceAccount  name: nfs-client-provisioner  namespace: nfs-provisioner roleRef:  kind: ClusterRole  name: nfs-client-provisioner-runner  apiGroup: rbac.authorization.k8s.io --- kind: Role apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: leader-locking-nfs-client-provisioner  namespace: nfs-provisioner rules:  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;endpoints\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;] --- kind: RoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: leader-locking-nfs-client-provisioner  namespace: nfs-provisioner subjects:  - kind: ServiceAccount  name: nfs-client-provisioner  # replace with namespace where provisioner is deployed  namespace: nfs-provisioner roleRef:  kind: Role  name: leader-locking-nfs-client-provisioner  apiGroup: rbac.authorization.k8s.io storage-class.yaml apiVersion: storage.k8s.io/v1 kind: StorageClass metadata:  name: nfs provisioner: luffy.com/nfs pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: test-claim2 spec:  accessModes:  - ReadWriteMany  resources:  requests:  storage: 1Mi  storageClassName: nfs 对接Ceph存储实践 ceph的安装及使用参考 http://docs.ceph.org.cn/start/intro/\n# CephFS需要使用两个Pool来分别存储数据和元数据 ceph osd pool create cephfs_data 128 ceph osd pool create cephfs_meta 128 ceph osd lspools  # 创建一个CephFS ceph fs new cephfs cephfs_meta cephfs_data  # 查看 ceph fs ls  # rados -p cephfs_meta ls storageClass实现动态挂载 创建pv及pvc过程是手动，且pv与pvc一一对应，手动创建很繁琐。因此，通过storageClass + provisioner的方式来实现通过PVC自动创建并绑定PV。\n比如，针对cephfs，可以创建如下类型的storageclass：\nkind: StorageClass apiVersion: storage.k8s.io/v1 metadata:  name: dynamic-cephfs provisioner: ceph.com/cephfs parameters:  monitors: 121.204.157.52:6789  adminId: admin  adminSecretName: ceph-admin-secret  adminSecretNamespace: \u0026#34;kube-system\u0026#34;  claimRoot: /volumes/kubernetes NFS，ceph-rbd，cephfs均提供了对应的provisioner\n部署cephfs-provisioner\n$ cat external-storage-cephfs-provisioner.yaml apiVersion: v1 kind: ServiceAccount metadata:  name: cephfs-provisioner  namespace: kube-system --- kind: ClusterRole apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: cephfs-provisioner rules:  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;persistentvolumes\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;delete\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;persistentvolumeclaims\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;update\u0026#34;]  - apiGroups: [\u0026#34;storage.k8s.io\u0026#34;]  resources: [\u0026#34;storageclasses\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;events\u0026#34;]  verbs: [\u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;endpoints\u0026#34;]  verbs: [\u0026#34;get\u0026#34;, \u0026#34;list\u0026#34;, \u0026#34;watch\u0026#34;, \u0026#34;create\u0026#34;, \u0026#34;update\u0026#34;, \u0026#34;patch\u0026#34;]  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;secrets\u0026#34;]  verbs: [\u0026#34;create\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;delete\u0026#34;] --- kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1 metadata:  name: cephfs-provisioner subjects:  - kind: ServiceAccount  name: cephfs-provisioner  namespace: kube-system roleRef:  kind: ClusterRole  name: cephfs-provisioner  apiGroup: rbac.authorization.k8s.io  --- apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata:  name: cephfs-provisioner  namespace: kube-system rules:  - apiGroups: [\u0026#34;\u0026#34;]  resources: [\u0026#34;secrets\u0026#34;]  verbs: [\u0026#34;create\u0026#34;, \u0026#34;get\u0026#34;, \u0026#34;delete\u0026#34;] --- apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata:  name: cephfs-provisioner  namespace: kube-system roleRef:  apiGroup: rbac.authorization.k8s.io  kind: Role  name: cephfs-provisioner subjects: - kind: ServiceAccount  name: cephfs-provisioner  namespace: kube-system  --- apiVersion: apps/v1 kind: Deployment metadata:  name: cephfs-provisioner  namespace: kube-system spec:  replicas: 1  selector:  matchLabels:  app: cephfs-provisioner  strategy:  type: Recreate  template:  metadata:  labels:  app: cephfs-provisioner  spec:  containers:  - name: cephfs-provisioner  image: \u0026#34;quay.io/external_storage/cephfs-provisioner:latest\u0026#34;  env:  - name: PROVISIONER_NAME  value: ceph.com/cephfs  imagePullPolicy: IfNotPresent  command:  - \u0026#34;/usr/local/bin/cephfs-provisioner\u0026#34;  args:  - \u0026#34;-id=cephfs-provisioner-1\u0026#34;  - \u0026#34;-disable-ceph-namespace-isolation=true\u0026#34;  serviceAccount: cephfs-provisioner 在ceph monitor机器中查看admin账户的key\n$ ceph auth ls $ ceph auth get-key client.admin AQAejeJbowvgMhAAsuloUOvepcj/TXEIoSrd7A== 创建secret\n$ echo -n AQAejeJbowvgMhAAsuloUOvepcj/TXEIoSrd7A==|base64 QVFBZWplSmJvd3ZnTWhBQXN1bG9VT3ZlcGNqL1RYRUlvU3JkN0E9PQ== $ cat ceph-admin-secret.yaml apiVersion: v1 data:  key: QVFBZWplSmJvd3ZnTWhBQXN1bG9VT3ZlcGNqL1RYRUlvU3JkN0E9PQ== kind: Secret metadata:  name: ceph-admin-secret  namespace: kube-system type: Opaque 创建storageclass\n$ cat cephfs-storage-class.yaml kind: StorageClass apiVersion: storage.k8s.io/v1 metadata:  name: dynamic-cephfs provisioner: ceph.com/cephfs parameters:  monitors: 36.111.140.31:6789  adminId: admin  adminSecretName: ceph-admin-secret  adminSecretNamespace: \u0026#34;kube-system\u0026#34;  claimRoot: /volumes/kubernetes 动态pvc验证及实现分析 使用流程： 创建pvc，指定storageclass和存储大小，即可实现动态存储。\n创建pvc测试自动生成pv\n$ cat cephfs-pvc-test.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: cephfs-claim spec:  accessModes:  - ReadWriteOnce  storageClassName: dynamic-cephfs  resources:  requests:  storage: 2Gi  $ kubectl create -f cephfs-pvc-test.yaml  $ kubectl get pv pvc-2abe427e-7568-442d-939f-2c273695c3db 2Gi RWO Delete Bound default/cephfs-claim dynamic-cephfs 1s 创建Pod使用pvc挂载cephfs数据盘\n$ cat test-pvc-cephfs.yaml apiVersion: v1 kind: Pod metadata:  name: nginx-pod  labels:  name: nginx-pod spec:  containers:  - name: nginx-pod  image: nginx:alpine  ports:  - name: web  containerPort: 80  volumeMounts:  - name: cephfs  mountPath: /usr/share/nginx/html  volumes:  - name: cephfs  persistentVolumeClaim:  claimName: cephfs-claim  $ kubectl create -f test-pvc-cephfs.yaml 我们所说的容器的持久化，实际上应该理解为宿主机中volume的持久化，因为Pod是支持销毁重建的，所以只能通过宿主机volume持久化，然后挂载到Pod内部来实现Pod的数据持久化。\n宿主机上的volume持久化，因为要支持数据漂移，所以通常是数据存储在分布式存储中，宿主机本地挂载远程存储（NFS，Ceph，OSS），这样即使Pod漂移也不影响数据。\nk8s的pod的挂载盘通常的格式为：\n/var/lib/kubelet/pods/\u0026lt;Pod的ID\u0026gt;/volumes/kubernetes.io~\u0026lt;Volume类型\u0026gt;/\u0026lt;Volume名字\u0026gt; 查看nginx-pod的挂载盘，\n$ df -TH /var/lib/kubelet/pods/61ba43c5-d2e9-4274-ac8c-008854e4fa8e/volumes/kubernetes.io~cephfs/pvc-2abe427e-7568-442d-939f-2c273695c3db/  $ findmnt /var/lib/kubelet/pods/61ba43c5-d2e9-4274-ac8c-008854e4fa8e/volumes/kubernetes.io~cephfs/pvc-2abe427e-7568-442d-939f-2c273695c3db/  36.111.140.31:6789:/volumes/kubernetes/kubernetes/kubernetes-dynamic-pvc-ffe3d84d-c433-11ea-b347-6acc3cf3c15f 使用Helm3管理复杂应用的部署 认识Helm   为什么有helm？\n  Helm是什么？\nkubernetes的包管理器，“可以将Helm看作Linux系统下的apt-get/yum”。\n 对于应用发布者而言，可以通过Helm打包应用，管理应用依赖关系，管理应用版本并发布应用到软件仓库。 对于使用者而言，使用Helm后不用需要了解Kubernetes的Yaml语法并编写应用部署文件，可以通过Helm下载并在kubernetes上安装需要的应用。  除此以外，Helm还提供了kubernetes上的软件部署，删除，升级，回滚应用的强大功能。\n  Helm的版本\n  helm2\nC/S架构，helm通过Tiller与k8s交互\n  helm3\n  从安全性和易用性方面考虑，移除了Tiller服务端，helm3直接使用kubeconfig文件鉴权访问APIServer服务器\n  由二路合并升级成为三路合并补丁策略（ 旧的配置，线上状态，新的配置 ）\nhelm install very_important_app ./very_important_app 这个应用的副本数量设置为 3 。现在，如果有人不小心执行了 kubectl edit 或：\nkubectl scale -replicas=0 deployment/very_important_app 然后，团队中的某个人发现 very_important_app 莫名其妙宕机了，尝试执行命令：\nhelm rollback very_important_app 在 Helm 2 中，这个操作将比较旧的配置与新的配置，然后生成一个更新补丁。由于，误操作的人仅修改了应用的线上状态（旧的配置并未更新）。Helm 在回滚时，什么事情也不会做。因为旧的配置与新的配置没有差别（都是 3 个副本）。然后，Helm 不执行回滚，副本数继续保持为 0\n  移除了helm serve本地repo仓库\n  创建应用时必须指定名字（或者\u0026ndash;generate-name随机生成）\n      Helm的重要概念\n chart，应用的信息集合，包括各种对象的配置模板、参数定义、依赖关系、文档说明等 Repoistory，chart仓库，存储chart的地方，并且提供了一个该 Repository 的 Chart 包的清单文件以供查询。Helm 可以同时管理多个不同的 Repository。 release， 当 chart 被安装到 kubernetes 集群，就生成了一个 release ， 是 chart 的运行实例，代表了一个正在运行的应用    helm 是包管理工具，包就是指 chart，helm 能够：\n 从零创建chart 与仓库交互，拉取、保存、更新 chart 在kubernetes集群中安装、卸载 release 更新、回滚、测试 release  安装与快速入门实践 下载最新的稳定版本：https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz\n更多版本可以参考： https://github.com/helm/helm/releases\n# k8s-master节点 $ wget https://get.helm.sh/helm-v3.2.4-linux-amd64.tar.gz $ tar -zxf helm-v3.2.4-linux-amd64.tar.gz  $ cp linux-amd64/helm /usr/local/bin/  # 验证安装 $ helm version version.BuildInfo{Version:\u0026#34;v3.2.4\u0026#34;, GitCommit:\u0026#34;0ad800ef43d3b826f31a5ad8dfbb4fe05d143688\u0026#34;, GitTreeState:\u0026#34;clean\u0026#34;, GoVersion:\u0026#34;go1.13.12\u0026#34;} $ helm env  # 添加仓库 $ helm repo add stable http://mirror.azure.cn/kubernetes/charts/ # 同步最新charts信息到本地 $ helm repo update 快速入门实践：\n示例一：使用helm安装mysql应用\n# helm 搜索chart包 $ helm search repo mysql  # 从仓库安装 $ helm install mysql stable/mysql  $ helm ls $ kubectl get all  # 从chart仓库中把chart包下载到本地 $ helm pull stable/mysql $ tree mysql 示例二：新建nginx的chart并安装\n$ helm create nginx  # 从本地安装 $ helm install nginx ./nginx  # 安装到别的命名空间luffy $ helm -n luffy install ./nginx  # 查看 $ helm ls $ helm -n luffy ls  # $ kubectl get all $ kubectl -n luffy get all Chart的模板语法及开发 nginx的chart实现分析 格式：\n$ tree nginx/ nginx/ ├── charts # 存放子chart ├── Chart.yaml # 该chart的全局定义信息 ├── templates # chart运行所需的资源清单模板，用于和values做渲染 │ ├── deployment.yaml │ ├── _helpers.tpl # 定义全局的命名模板，方便在其他模板中引入使用 │ ├── hpa.yaml │ ├── ingress.yaml │ ├── NOTES.txt # helm安装完成后终端的提示信息 │ ├── serviceaccount.yaml │ ├── service.yaml │ └── tests │ └── test-connection.yaml └── values.yaml # 模板使用的默认值信息 很明显，资源清单都在templates中，数据来源于values.yaml，安装的过程就是将模板与数据融合成k8s可识别的资源清单，然后部署到k8s环境中。\n分析模板文件的实现：\n  引用命名模板并传递作用域\n{{ include \u0026#34;nginx.fullname\u0026#34; . }} include从_helpers.tpl中引用命名模板，并传递顶级作用域.\n  内置对象\n.Values .Release.Name  Release：该对象描述了 release 本身的相关信息，它内部有几个对象：  Release.Name：release 名称 Release.Namespace：release 安装到的命名空间 Release.IsUpgrade：如果当前操作是升级或回滚，则该值为 true Release.IsInstall：如果当前操作是安装，则将其设置为 true Release.Revision：release 的 revision 版本号，在安装的时候，值为1，每次升级或回滚都会增加 Reelase.Service：渲染当前模板的服务，在 Helm 上，实际上该值始终为 Helm   Values：从 values.yaml 文件和用户提供的 values 文件传递到模板的 Values 值 Chart：获取 Chart.yaml 文件的内容，该文件中的任何数据都可以访问，例如 {{ .Chart.Name }}-{{ .Chart.Version}} 可以渲染成 mychart-0.1.0    模板定义\n{{- define \u0026#34;nginx.fullname\u0026#34; -}} {{- if .Values.fullnameOverride }} {{- .Values.fullnameOverride | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- $name := default .Chart.Name .Values.nameOverride }} {{- if contains $name .Release.Name }} {{- .Release.Name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- else }} {{- printf \u0026#34;%s-%s\u0026#34; .Release.Name $name | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }} {{- end }} {{- end }} {{- end }}   {{- 去掉左边的空格及换行，-}} 去掉右侧的空格及换行\n  示例\napiVersion: v1 kind: ConfigMap metadata:  name: {{ .Release.Name }}-configmap data:  myvalue: \u0026#34;Hello World\u0026#34;  drink: {{ .Values.favorite.drink | default \u0026#34;tea\u0026#34; | quote }}  food: {{ .Values.favorite.food | upper | quote }}  {{ if eq .Values.favorite.drink \u0026#34;coffee\u0026#34; }}  mug: true  {{ end }} 渲染完后是：\napiVersion: v1 kind: ConfigMap metadata:  name: mychart-1575971172-configmap data:  myvalue: \u0026#34;Hello World\u0026#34;  drink: \u0026#34;coffee\u0026#34;  food: \u0026#34;PIZZA\u0026#34;   mug: true     管道及方法\n  trunc表示字符串截取，63作为参数传递给trunc方法，trimSuffix表示去掉-后缀\n{{- .Values.fullnameOverride | trunc 63 | trimSuffix \u0026#34;-\u0026#34; }}   nindent表示前面的空格数\n selector:  matchLabels:  {{- include \u0026#34;nginx.selectorLabels\u0026#34; . | nindent 6 }}   lower表示将内容小写，quote表示用双引号引起来\nvalue: {{ include \u0026#34;mytpl\u0026#34; . | lower | quote }}     条件判断语句每个if对应一个end\n{{- if .Values.fullnameOverride }} ... {{- else }} ... {{- end }} 通常用来根据values.yaml中定义的开关来控制模板中的显示：\n{{- if not .Values.autoscaling.enabled }}  replicas: {{ .Values.replicaCount }} {{- end }}   定义变量，模板中可以通过变量名字去引用\n{{- $name := default .Chart.Name .Values.nameOverride }}   遍历values的数据\n {{- with .Values.nodeSelector }}  nodeSelector:  {{- toYaml . | nindent 8 }}  {{- end }} toYaml处理值中的转义及特殊字符， \u0026ldquo;kubernetes.io/role\u0026rdquo;=master ， name=\u0026ldquo;value1,value2\u0026rdquo; 类似的情况\n  default设置默认值\nimage: \u0026#34;{{ .Values.image.repository }}:{{ .Values.image.tag | default .Chart.AppVersion }}\u0026#34;   Helm template\nhpa.yaml\n{{- if .Values.autoscaling.enabled }} apiVersion: autoscaling/v2beta1 kind: HorizontalPodAutoscaler metadata:  name: {{ include \u0026#34;nginx.fullname\u0026#34; . }}  labels:  {{- include \u0026#34;nginx.labels\u0026#34; . | nindent 4 }} spec:  scaleTargetRef:  apiVersion: apps/v1  kind: Deployment  name: {{ include \u0026#34;nginx.fullname\u0026#34; . }}  minReplicas: {{ .Values.autoscaling.minReplicas }}  maxReplicas: {{ .Values.autoscaling.maxReplicas }}  metrics:  {{- if .Values.autoscaling.targetCPUUtilizationPercentage }}  - type: Resource  resource:  name: cpu  targetAverageUtilization: {{ .Values.autoscaling.targetCPUUtilizationPercentage }}  {{- end }}  {{- if .Values.autoscaling.targetMemoryUtilizationPercentage }}  - type: Resource  resource:  name: memory  targetAverageUtilization: {{ .Values.autoscaling.targetMemoryUtilizationPercentage }}  {{- end }} {{- end }} 创建应用的时候赋值  set的方式  # 改变副本数和resource值 $ helm install nginx-2 ./nginx --set replicaCount=2 --set resources.limits.cpu=200m --set resources.limits.memory=256Mi   value文件的方式\n$ cat nginx-values.yaml resources:  limits:  cpu: 100m  memory: 128Mi  requests:  cpu: 100m  memory: 128Mi autoscaling:  enabled: true  minReplicas: 1  maxReplicas: 3  targetCPUUtilizationPercentage: 80 ingress:  enabled: true  hosts:  - host: chart-example.luffy.com  paths:  - /  $ helm install -f nginx-values.yaml nginx-3 ./nginx   更多语法参考：\nhttps://helm.sh/docs/topics/charts/\n部署mysql失败的问题\n实战：使用Helm部署Harbor镜像及chart仓库 harbor踩坑部署 架构 https://github.com/goharbor/harbor/wiki/Architecture-Overview-of-Harbor\n Core，核心组件  API Server，接收处理用户请求 Config Manager ：所有系统的配置，比如认证、邮件、证书配置等 Project Manager：项目管理 Quota Manager ：配额管理 Chart Controller：chart管理 Replication Controller ：镜像副本控制器，可以与不同类型的仓库实现镜像同步  Distribution (docker registry) Docker Hub \u0026hellip;   Scan Manager ：扫描管理，引入第三方组件，进行镜像安全扫描 Registry Driver ：镜像仓库驱动，目前使用docker registry   Job Service，执行异步任务，如同步镜像信息 Log Collector，统一日志收集器，收集各模块日志 GC Controller Chart Museum，chart仓库服务，第三方 Docker Registry，镜像仓库服务 kv-storage，redis缓存服务，job service使用，存储job metadata local/remote storage，存储服务，比较镜像存储 SQL Database，postgresl，存储用户、项目等元数据  通常用作企业级镜像仓库服务，实际功能强大很多。\n组件众多，因此使用helm部署\n# 添加harbor chart仓库 $ helm repo add harbor https://helm.goharbor.io  # 搜索harbor的chart $ helm search repo harbor  # 不知道如何部署，因此拉到本地 $ helm pull harbor/harbor --version 1.4.1 创建pvc\n$ kubectl create namespace harbor $ cat harbor-pvc.yaml kind: PersistentVolumeClaim apiVersion: v1 metadata:  name: harbor-pvc  namespace: harbor spec:  accessModes:  - ReadWriteOnce  storageClassName: dynamic-cephfs  resources:  requests:  storage: 20Gi 修改harbor配置：\n 开启ingress访问 externalURL，web访问入口，和ingress的域名相同 持久化，使用PVC对接的cephfs harborAdminPassword: \u0026ldquo;Harbor12345\u0026rdquo;，管理员默认账户 admin/Harbor12345 开启chartmuseum clair和trivy漏洞扫描组件，暂不启用  helm创建：\n# 使用本地chart安装 $ helm install harbor ./harbor -n harbor 踩坑一：redis持久化数据目录权限导致无法登录\nredis数据目录，/var/lib/redis，需要设置redis的用户及用户组权限\n initContainers:  - name: \u0026#34;change-permission-of-directory\u0026#34;  image: {{ .Values.redis.internal.image.repository }}:{{ .Values.redis.internal.image.tag }}  imagePullPolicy: {{ .Values.imagePullPolicy }}  command: [\u0026#34;/bin/sh\u0026#34;]  args: [\u0026#34;-c\u0026#34;, \u0026#34;chown -R 999:999 /var/lib/redis\u0026#34;]  securityContext:  runAsUser: 0  volumeMounts:  - name: data  mountPath: /var/lib/redis  subPath: {{ $redis.subPath }} 踩坑二：registry组件的镜像存储目录权限导致镜像推送失败\nregistry的镜像存储目录，需要设置registry用户的用户及用户组，不然镜像推送失败\n initContainers:  - name: \u0026#34;change-permission-of-directory\u0026#34;  securityContext:  runAsUser: 0  image: {{ .Values.registry.registry.image.repository }}:{{ .Values.registry.registry.image.tag }}  imagePullPolicy: {{ .Values.imagePullPolicy }}  command: [\u0026#34;/bin/sh\u0026#34;]  args: [\u0026#34;-c\u0026#34;, \u0026#34;chown -R 10000:10000 {{ .Values.persistence.imageChartStorage.filesystem.rootdirectory }}\u0026#34;]  volumeMounts:  - name: registry-data  mountPath: {{ .Values.persistence.imageChartStorage.filesystem.rootdirectory }}  subPath: {{ .Values.persistence.persistentVolumeClaim.registry.subPath }} 踩坑三：chartmuseum存储目录权限，导致chart推送失败\n initContainers:  - name: \u0026#34;change-permission-of-directory\u0026#34;  image: {{ .Values.chartmuseum.image.repository }}:{{ .Values.chartmuseum.image.tag }}  imagePullPolicy: {{ .Values.imagePullPolicy }}  command: [\u0026#34;/bin/sh\u0026#34;]  args: [\u0026#34;-c\u0026#34;, \u0026#34;chown -R 10000:10000 /chart_storage\u0026#34;]  securityContext:  runAsUser: 0  volumeMounts:  - name: chartmuseum-data  mountPath: /chart_storage  subPath: {{ .Values.persistence.persistentVolumeClaim.chartmuseum.subPath }} 更新内容后，执行更新release\n$ helm upgrade harbor -n harbor ./ 推送镜像到Harbor仓库 配置hosts及docker非安全仓库：\n$ cat /etc/hosts ... 192.168.136.10 k8s-master core.harbor.domain ...  $ cat /etc/docker/daemon.json {  \u0026#34;insecure-registries\u0026#34;: [  \u0026#34;192.168.136.10:5000\u0026#34;,  \u0026#34;core.harbor.domain\u0026#34;  ],  \u0026#34;registry-mirrors\u0026#34; : [  \u0026#34;https://8xpk5wnt.mirror.aliyuncs.com\u0026#34;  ] }  # $ systemctl restart docker  # 使用账户密码登录admin/Harbor12345 $ docker login core.harbor.domain  $ docker tag nginx:alpine core.harbor.domain/library/nginx:alpine $ docker push core.harbor.domain/library/nginx:alpine 推送chart到Harbor仓库 helm3默认没有安装helm push插件，需要手动安装。插件地址 https://github.com/chartmuseum/helm-push\n安装插件：\n$ helm plugin install https://github.com/chartmuseum/helm-push 离线安装：\n$ helm plugin install ./helm-push 添加repo\n$ helm repo add myharbor https://core.harbor.domain/chartrepo/library # x509错误  # 添加证书信任，根证书为配置给ingress使用的证书 $ kubectl get secret harbor-harbor-ingress -n harbor -o jsonpath=\u0026#34;{.data.ca\\.crt}\u0026#34; | base64 -d \u0026gt;harbor.ca.crt  $ cp harbor.ca.crt /etc/pki/ca-trust/source/anchors $ update-ca-trust enable; update-ca-trust extract  # 再次添加 $ helm repo add myharbor https://core.harbor.domain/chartrepo/library --ca-file=harbor.ca.crt  $ helm repo ls 推送chart到仓库：\n$ helm push harbor myharbor --ca-file=harbor.ca.crt -u admin -p Harbor12345 查看harbor仓库的chart\n课程小结 使用k8s的进阶内容。\n  学习k8s在etcd中数据的存储，掌握etcd的基本操作命令\n  理解k8s调度的过程，预选及优先。影响调度策略的设置\n  Flannel网络的原理学习，了解网络的流向，帮助定位问题\n  认证与授权，掌握kubectl、kubelet、rbac及二次开发如何调度API\n  利用HPA进行业务动态扩缩容，通过metrics-server了解整个k8s的监控体系\n  PV + PVC\n  Helm\n  ","permalink":"https://iblog.zone/archives/kubernetes%E8%BF%9B%E9%98%B6%E5%AE%9E%E8%B7%B5/","summary":"第三天 Kubernetes进阶实践 本章介绍Kubernetes的进阶内容，包含Kubernetes集群调度、CNI插件、认证授权安全体系、分布式存储的对接、Helm的使用等，让学员可以更加深入的学习Kubernetes的核心内容。\n ETCD数据的访问 kube-scheduler调度策略实践  预选与优选流程 生产中常用的调度配置实践   k8s集群网络模型  CNI介绍及集群网络选型 Flannel网络模型的实现  vxlan Backend hostgw Backend     集群认证与授权  APIServer安全控制模型 Kubectl的认证授权 RBAC kubelet的认证授权 Service Account   使用Helm管理复杂应用的部署  Helm工作原理详解 Helm的模板开发 实战：使用Helm部署Harbor仓库   kubernetes对接分部式存储  pv、pvc介绍 k8s集群如何使用cephfs作为分布式存储后端 利用storageClass实现动态存储卷的管理 实战：使用分部署存储实现有状态应用的部署   本章知识梳理及回顾  ETCD常用操作 拷贝etcdctl命令行工具：\n$ docker exec -ti etcd_container which etcdctl $ docker cp etcd_container:/usr/local/bin/etcdctl /usr/bin/etcdctl 查看etcd集群的成员节点：\n$ export ETCDCTL_API=3 $ etcdctl --endpoints=https://[127.","title":"Kubernetes进阶实践"},{"content":"第二天 Kubernetes落地实践之旅 本章学习kubernetes的架构及工作流程，重点介绍如何使用Workload管理业务应用的生命周期，实现服务不中断的滚动更新，通过服务发现和集群内负载均衡来实现集群内部的服务间访问，并通过ingress实现外部使用域名访问集群内部的服务。\n学习过程中会逐步对Django项目做k8s改造，从零开始编写所需的资源文件。通过本章的学习，学员会掌握高可用k8s集群的搭建，同时Django demo项目已经可以利用k8s的控制器、服务发现、负载均衡、配置管理等特性来实现生命周期的管理。\n纯容器模式的问题  业务容器数量庞大，哪些容器部署在哪些节点，使用了哪些端口，如何记录、管理，需要登录到每台机器去管理？ 跨主机通信，多个机器中的容器之间相互调用如何做，iptables规则手动维护？ 跨主机容器间互相调用，配置如何写？写死固定IP+端口？ 如何实现业务高可用？多个容器对外提供服务如何实现负载均衡？ 容器的业务中断了，如何可以感知到，感知到以后，如何自动启动新的容器? 如何实现滚动升级保证业务的连续性？ \u0026hellip;\u0026hellip;  容器调度管理平台 Docker Swarm Mesos Google Kubernetes\n2017年开始Kubernetes凭借强大的容器集群管理功能, 逐步占据市场,目前在容器编排领域一枝独秀\nhttps://kubernetes.io/\n架构图 分布式系统，两类角色：管理节点和工作节点\n核心组件   ETCD：分布式高性能键值数据库,存储整个集群的所有元数据\n  ApiServer: API服务器,集群资源访问控制入口,提供restAPI及安全访问控制\n  Scheduler：调度器,负责把业务容器调度到最合适的Node节点\n  Controller Manager：控制器管理,确保集群资源按照期望的方式运行\n Replication Controller Node controller ResourceQuota Controller Namespace Controller ServiceAccount Controller Token Controller Service Controller Endpoints Controller    kubelet：运行在每个节点上的主要的“节点代理”，脏活累活\n pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理. 容器监控：kubelet 会监控所在节点的资源使用情况，并定时向 master 报告，资源使用数据都是通过 cAdvisor 获取的。知道整个集群所有节点的资源情况，对于 pod 的调度和正常运行至关重要    kube-proxy：维护节点中的iptables或者ipvs规则\n  kubectl: 命令行接口，用于对 Kubernetes 集群运行命令 https://kubernetes.io/zh/docs/reference/kubectl/\n  工作流程  用户准备一个资源文件（记录了业务应用的名称、镜像地址等信息），通过调用APIServer执行创建Pod APIServer收到用户的Pod创建请求，将Pod信息写入到etcd中 调度器通过list-watch的方式，发现有新的pod数据，但是这个pod还没有绑定到某一个节点中 调度器通过调度算法，计算出最适合该pod运行的节点，并调用APIServer，把信息更新到etcd中 kubelet同样通过list-watch方式，发现有新的pod调度到本机的节点了，因此调用容器运行时，去根据pod的描述信息，拉取镜像，启动容器，同时生成事件信息 同时，把容器的信息、事件及状态也通过APIServer写入到etcd中  架构设计的几点思考  系统各个组件分工明确(APIServer是所有请求入口，CM是控制中枢，Scheduler主管调度，而Kubelet负责运行)，配合流畅，整个运行机制一气呵成。 除了配置管理和持久化组件ETCD，其他组件并不保存数据。意味除ETCD外其他组件都是无状态的。因此从架构设计上对kubernetes系统高可用部署提供了支撑。 同时因为组件无状态，组件的升级，重启，故障等并不影响集群最终状态，只要组件恢复后就可以从中断处继续运行。 各个组件和kube-apiserver之间的数据推送都是通过list-watch机制来实现。  实践\u0026ndash;集群安装 k8s集群主流安装方式对比分析  minikube 二进制安装 kubeadm等安装工具  kubeadm https://kubernetes.io/zh/docs/reference/setup-tools/kubeadm/kubeadm/\n《Kubernetes安装手册（非高可用版）》\n核心组件 静态Pod的方式：\n## etcd、apiserver、controller-manager、kube-scheduler $ kubectl -n kube-system get po systemd服务方式：\n$ systemctl status kubelet kubectl：二进制命令行工具\n理解集群资源 组件是为了支撑k8s平台的运行，安装好的软件。\n资源是如何去使用k8s的能力的定义。比如，k8s可以使用Pod来管理业务应用，那么Pod就是k8s集群中的一类资源，集群中的所有资源可以提供如下方式查看：\n$ kubectl api-resources 如何理解namespace：\n命名空间，集群内一个虚拟的概念，类似于资源池的概念，一个池子里可以有各种资源类型，绝大多数的资源都必须属于某一个namespace。集群初始化安装好之后，会默认有如下几个namespace：\n$ kubectl get namespaces NAME STATUS AGE default Active 84m kube-node-lease Active 84m kube-public Active 84m kube-system Active 84m kubernetes-dashboard Active 71m  所有NAMESPACED的资源，在创建的时候都需要指定namespace，若不指定，默认会在default命名空间下 相同namespace下的同类资源不可以重名，不同类型的资源可以重名 不同namespace下的同类资源可以重名 通常在项目使用的时候，我们会创建带有业务含义的namespace来做逻辑上的整合  kubectl的使用 类似于docker，kubectl是命令行工具，用于与APIServer交互，内置了丰富的子命令，功能极其强大。 https://kubernetes.io/docs/reference/kubectl/overview/\n$ kubectl -h $ kubectl get -h $ kubectl create -h $ kubectl create namespace -h 实践\u0026ndash;使用k8s管理业务应用 最小调度单元 Pod docker调度的是容器，在k8s集群中，最小的调度单元是Pod（豆荚）\n为什么引入Pod   与容器引擎解耦\nDocker、Rkt。平台设计与引擎的具体的实现解耦\n  多容器共享网络|存储|进程 空间, 支持的业务场景更加灵活\n  使用yaml格式定义Pod myblog/one-pod/pod.yaml\napiVersion: v1 kind: Pod metadata:  name: myblog  namespace: luffy  labels:  component: myblog spec:  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  env:  - name: MYSQL_HOST  # 指定root用户的用户名  value: \u0026#34;127.0.0.1\u0026#34;  - name: MYSQL_PASSWD  value: \u0026#34;123456\u0026#34;  ports:  - containerPort: 8002  - name: mysql  image: 172.21.51.67:5000/mysql:5.7-utf8  ports:  - containerPort: 3306  env:  - name: MYSQL_ROOT_PASSWORD  value: \u0026#34;123456\u0026#34;  - name: MYSQL_DATABASE  value: \u0026#34;myblog\u0026#34; { \t\u0026#34;apiVersion\u0026#34;: \u0026#34;v1\u0026#34;,\t\t\u0026#34;kind\u0026#34;: \u0026#34;Pod\u0026#34;, \t\u0026#34;metadata\u0026#34;: { \t\u0026#34;name\u0026#34;: \u0026#34;myblog\u0026#34;,  \u0026#34;namespace\u0026#34;: \u0026#34;luffy\u0026#34;,  \u0026#34;labels\u0026#34;: {  \u0026#34;component\u0026#34;: \u0026#34;myblog\u0026#34;  } \t}, \t\u0026#34;spec\u0026#34;: { \t\u0026#34;containers\u0026#34;: [ \t{ \t\u0026#34;name\u0026#34;: \u0026#34;myblog\u0026#34;, \t\u0026#34;image\u0026#34;: \u0026#34;172.21.51.67:5000/myblog\u0026#34;,  \u0026#34;env\u0026#34;: [  {  \u0026#34;name\u0026#34;: \u0026#34;MYSQL_HOST\u0026#34;,  \u0026#34;value\u0026#34;: \u0026#34;127.0.0.1\u0026#34;  },  {  \u0026#34;name\u0026#34;: \u0026#34;MYSQL_PASSWD\u0026#34;,  \u0026#34;value\u0026#34;: \u0026#34;123456\u0026#34;  }  ], \t\u0026#34;ports\u0026#34;: [ \t{ \t\u0026#34;containerPort\u0026#34;: 8002 \t} \t] \t},  {  \u0026#34;name\u0026#34;: \u0026#34;mysql\u0026#34;,  ... \t} \t] \t} }    apiVersion 含义     alpha 进入K8s功能的早期候选版本，可能包含Bug，最终不一定进入K8s   beta 已经过测试的版本，最终会进入K8s，但功能、对象定义可能会发生变更。   stable 可安全使用的稳定版本   v1 stable 版本之后的首个版本，包含了更多的核心对象   apps/v1 使用最广泛的版本，像Deployment、ReplicaSets都已进入该版本    资源类型与apiVersion对照表\n   Kind apiVersion     ClusterRoleBinding rbac.authorization.k8s.io/v1   ClusterRole rbac.authorization.k8s.io/v1   ConfigMap v1   CronJob batch/v1beta1   DaemonSet extensions/v1beta1   Node v1   Namespace v1   Secret v1   PersistentVolume v1   PersistentVolumeClaim v1   Pod v1   Deployment v1、apps/v1、apps/v1beta1、apps/v1beta2   Service v1   Ingress extensions/v1beta1   ReplicaSet apps/v1、apps/v1beta2   Job batch/v1   StatefulSet apps/v1、apps/v1beta1、apps/v1beta2    快速获得资源和版本\n$ kubectl explain pod $ kubectl explain Pod.apiVersion 创建和访问Pod ## 创建namespace, namespace是逻辑上的资源池 $ kubectl create namespace luffy  ## 使用指定文件创建Pod $ kubectl create -f pod.yaml  ## 查看pod，可以简写po ## 所有的操作都需要指定namespace，如果是在default命名空间下，则可以省略 $ kubectl -n luffy get pods -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 3m 10.244.1.146 k8s-slave1  ## 使用Pod Ip访问服务,3306和8002 $ curl 10.244.1.146:8002/blog/index/  ## 进入容器,执行初始化, 不必到对应的主机执行docker exec $ kubectl -n luffy exec -ti myblog -c myblog bash / # env / # python3 manage.py migrate $ kubectl -n luffy exec -ti myblog -c mysql bash / # mysql -p123456  ## 再次访问服务,3306和8002 $ curl 10.244.1.146:8002/blog/index/ Infra容器 登录k8s-slave1节点\n$ docker ps -a |grep myblog ## 发现有三个容器 ## 其中包含mysql和myblog程序以及Infra容器 ## 为了实现Pod内部的容器可以通过localhost通信，每个Pod都会启动Infra容器，然后Pod内部的其他容器的网络空间会共享该Infra容器的网络空间(Docker网络的container模式)，Infra容器只需要hang住网络空间，不需要额外的功能，因此资源消耗极低。  ## 登录master节点，查看pod内部的容器ip均相同，为pod ip $ kubectl -n luffy exec -ti myblog -c myblog bash / # ifconfig $ kubectl -n luffy exec -ti myblog -c mysql bash / # ifconfig pod容器命名: k8s_\u0026lt;container_name\u0026gt;_\u0026lt;pod_name\u0026gt;_\u0026lt;namespace\u0026gt;_\u0026lt;random_string\u0026gt;\n查看pod详细信息 ## 查看pod调度节点及pod_ip $ kubectl -n luffy get pods -o wide ## 查看完整的yaml $ kubectl -n luffy get po myblog -o yaml ## 查看pod的明细信息及事件 $ kubectl -n luffy describe pod myblog Troubleshooting and Debugging #进入Pod内的容器 $ kubectl -n \u0026lt;namespace\u0026gt; exec \u0026lt;pod_name\u0026gt; -c \u0026lt;container_name\u0026gt; -ti /bin/sh  #查看Pod内容器日志,显示标准或者错误输出日志 $ kubectl -n \u0026lt;namespace\u0026gt; logs -f \u0026lt;pod_name\u0026gt; -c \u0026lt;container_name\u0026gt; 更新服务版本 $ kubectl apply -f demo-pod.yaml 删除Pod服务 #根据文件删除 $ kubectl delete -f demo-pod.yaml  #根据pod_name删除 $ kubectl -n \u0026lt;namespace\u0026gt; delete pod \u0026lt;pod_name\u0026gt; Pod数据持久化 若删除了Pod，由于mysql的数据都在容器内部，会造成数据丢失，因此需要数据进行持久化。\n  定点使用hostpath挂载，nodeSelector定点\nmyblog/one-pod/pod-with-volume.yaml\napiVersion: v1 kind: Pod metadata:  name: myblog  namespace: luffy  labels:  component: myblog spec:  volumes:  - name: mysql-data  hostPath:  path: /opt/mysql/data  nodeSelector: # 使用节点选择器将Pod调度到指定label的节点  component: mysql  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  env:  - name: MYSQL_HOST  # 指定root用户的用户名  value: \u0026#34;127.0.0.1\u0026#34;  - name: MYSQL_PASSWD  value: \u0026#34;123456\u0026#34;  ports:  - containerPort: 8002  - name: mysql  image: 172.21.51.67:5000/mysql:5.7-utf8  ports:  - containerPort: 3306  env:  - name: MYSQL_ROOT_PASSWORD  value: \u0026#34;123456\u0026#34;  - name: MYSQL_DATABASE  value: \u0026#34;myblog\u0026#34;  volumeMounts:  - name: mysql-data  mountPath: /var/lib/mysql 保存文件为pod-with-volume.yaml，执行创建\n## 若存在旧的同名服务，先删除掉，后创建 $ kubectl -n luffy delete pod myblog ## 创建 $ kubectl create -f pod-with-volume.yaml  ## 此时pod状态Pending $ kubectl -n luffy get po NAME READY STATUS RESTARTS AGE myblog 0/2 Pending 0 32s  ## 查看原因，提示调度失败，因为节点不满足node selector $ kubectl -n luffy describe po myblog Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Warning FailedScheduling 12s (x2 over 12s) default-scheduler 0/3 nodes are available: 3 node(s) didn\u0026#39;t match node selector.  ## 为节点打标签 $ kubectl label node k8s-slave1 component=mysql  ## 再次查看，已经运行成功 $ kubectl -n luffy get po NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 3m54s 10.244.1.150 k8s-slave1  ## 到k8s-slave1节点，查看/opt/mysql/data $ ll /opt/mysql/data/ total 188484 -rw-r----- 1 polkitd input 56 Mar 29 09:20 auto.cnf -rw------- 1 polkitd input 1676 Mar 29 09:20 ca-key.pem -rw-r--r-- 1 polkitd input 1112 Mar 29 09:20 ca.pem drwxr-x--- 2 polkitd input 8192 Mar 29 09:20 sys ...  ## 执行migrate，创建数据库表，然后删掉pod，再次创建后验证数据是否存在 $ kubectl -n luffy exec -ti myblog python3 manage.py migrate  ## 访问服务，正常 $ curl 10.244.1.150:8002/blog/index/  ## 删除pod $ kubectl delete -f pod-with-volume.yaml  ## 再次创建Pod $ kubectl create -f pod-with-volume.yaml  ## 查看pod ip并访问服务 $ kubectl -n luffy get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 2/2 Running 0 7s 10.244.1.151 k8s-slave1  ## 未重新做migrate，服务正常 $ curl 10.244.1.151:8002/blog/index/   使用PV+PVC连接分布式存储解决方案\n ceph glusterfs nfs    服务健康检查 检测容器服务是否健康的手段，若不健康，会根据设置的重启策略（restartPolicy）进行操作，两种检测机制可以分别单独设置，若不设置，默认认为Pod是健康的。\n两种机制：\n  LivenessProbe探针 存活性探测：用于判断容器是否存活，即Pod是否为running状态，如果LivenessProbe探针探测到容器不健康，则kubelet将kill掉容器，并根据容器的重启策略是否重启，如果一个容器不包含LivenessProbe探针，则Kubelet认为容器的LivenessProbe探针的返回值永远成功。\n...  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  livenessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒  periodSeconds: 10 # 执行探测的频率  timeoutSeconds: 2\t# 探测超时时间 ...   ReadinessProbe探针 可用性探测：用于判断容器是否正常提供服务，即容器的Ready是否为True，是否可以接收请求，如果ReadinessProbe探测失败，则容器的Ready将为False， Endpoint Controller 控制器将此Pod的Endpoint从对应的service的Endpoint列表中移除，不再将任何请求调度此Pod上，直到下次探测成功。（剔除此pod不参与接收请求不会将流量转发给此Pod）。\n...  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  readinessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10  timeoutSeconds: 2  periodSeconds: 10 ...   三种类型：\n exec：通过执行命令来检查服务是否正常，返回值为0则表示容器健康 httpGet方式：通过发送http请求检查服务是否正常，返回200-399状态码则表明容器健康 tcpSocket：通过容器的IP和Port执行TCP检查，如果能够建立TCP连接，则表明容器健康  示例：\n完整文件路径 myblog/one-pod/pod-with-healthcheck.yaml\n containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  env:  - name: MYSQL_HOST  # 指定root用户的用户名  value: \u0026#34;127.0.0.1\u0026#34;  - name: MYSQL_PASSWD  value: \u0026#34;123456\u0026#34;  ports:  - containerPort: 8002  livenessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒  periodSeconds: 10 # 执行探测的频率  timeoutSeconds: 2\t# 探测超时时间  readinessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10  timeoutSeconds: 2  periodSeconds: 10  initialDelaySeconds：容器启动后第一次执行探测时需要等待多少秒。 periodSeconds：执行探测的频率。默认是10秒，最小1秒。 timeoutSeconds：探测超时时间。默认1秒，最小1秒。 successThreshold：探测失败后，最少连续探测成功多少次才被认定为成功。默认是1。 failureThreshold：探测成功后，最少连续探测失败多少次才被认定为失败。默认是3，最小值是1。  K8S将在Pod开始启动10s(initialDelaySeconds)后利用HTTP访问8002端口的/blog/index/，如果超过2s或者返回码不在200~399内，则健康检查失败\n重启策略 Pod的重启策略（RestartPolicy）应用于Pod内的所有容器，并且仅在Pod所处的Node上由kubelet进行判断和重启操作。当某个容器异常退出或者健康检查失败时，kubelet将根据RestartPolicy的设置来进行相应的操作。 Pod的重启策略包括Always、OnFailure和Never，默认值为Always。\n Always：当容器进程退出后，由kubelet自动重启该容器； OnFailure：当容器终止运行且退出码不为0时，由kubelet自动重启该容器； Never：不论容器运行状态如何，kubelet都不会重启该容器。  演示重启策略：\napiVersion: v1 kind: Pod metadata:  name: test-restart-policy spec:  restartPolicy: OnFailure  containers:  - name: busybox  image: busybox  args:  - /bin/sh  - -c  - sleep 10 \u0026amp;\u0026amp; exit 0  使用默认的重启策略，即 restartPolicy: Always ，无论容器是否是正常退出，都会自动重启容器 使用OnFailure的策略时  如果把exit 1，去掉，即让容器的进程正常退出的话，则不会重启 只有非正常退出状态才会重启   使用Never时，退出了就不再重启  可以看出，若容器正常退出，Pod的状态会是Completed，非正常退出，状态为CrashLoopBackOff\n镜像拉取策略 spec:  containers:  - name: myblog  image: 172.21.51.67:5000/demo/myblog  imagePullPolicy: IfNotPresent 设置镜像的拉取策略，默认为IfNotPresent\n Always，总是拉取镜像，即使本地有镜像也从仓库拉取 IfNotPresent ，本地有则使用本地镜像，本地没有则去仓库拉取 Never，只使用本地镜像，本地没有则报错  Pod资源限制 为了保证充分利用集群资源，且确保重要容器在运行周期内能够分配到足够的资源稳定运行，因此平台需要具备\nPod的资源限制的能力。 对于一个pod来说，资源最基础的2个的指标就是：CPU和内存。\nKubernetes提供了个采用requests和limits 两种类型参数对资源进行预分配和使用限制。\n完整文件路径：myblog/one-pod/pod-with-resourcelimits.yaml\n...  containers:  - name: myblog  image: 172.21.51.67:5000/myblog  env:  - name: MYSQL_HOST  # 指定root用户的用户名  value: \u0026#34;127.0.0.1\u0026#34;  - name: MYSQL_PASSWD  value: \u0026#34;123456\u0026#34;  ports:  - containerPort: 8002  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m ... requests：\n 容器使用的最小资源需求,作用于schedule阶段，作为容器调度时资源分配的判断依赖 只有当前节点上可分配的资源量 \u0026gt;= request 时才允许将容器调度到该节点 request参数不限制容器的最大可使用资源 requests.cpu被转成docker的\u0026ndash;cpu-shares参数，与cgroup cpu.shares功能相同 (无论宿主机有多少个cpu或者内核，\u0026ndash;cpu-shares选项都会按照比例分配cpu资源） requests.memory没有对应的docker参数，仅作为k8s调度依据  limits：\n 容器能使用资源的最大值 设置为0表示对使用的资源不做限制, 可无限的使用 当pod 内存超过limit时，会被oom 当cpu超过limit时，不会被kill，但是会限制不超过limit值 limits.cpu会被转换成docker的–cpu-quota参数。与cgroup cpu.cfs_quota_us功能相同 limits.memory会被转换成docker的–memory参数。用来限制容器使用的最大内存  对于 CPU，我们知道计算机里 CPU 的资源是按“时间片”的方式来进行分配的，系统里的每一个操作都需要 CPU 的处理，所以，哪个任务要是申请的 CPU 时间片越多，那么它得到的 CPU 资源就越多。\n然后还需要了解下 CGroup 里面对于 CPU 资源的单位换算：\n1 CPU = 1000 millicpu（1 Core = 1000m） 这里的 m 就是毫、毫核的意思，Kubernetes 集群中的每一个节点可以通过操作系统的命令来确认本节点的 CPU 内核数量，然后将这个数量乘以1000，得到的就是节点总 CPU 总毫数。比如一个节点有四核，那么该节点的 CPU 总毫量为 4000m。\ndocker run命令和 CPU 限制相关的所有选项如下：\n   选项 描述     --cpuset-cpus=\u0026quot;\u0026quot; 允许使用的 CPU 集，值可以为 0-3,0,1   -c,--cpu-shares=0 CPU 共享权值（相对权重）   cpu-period=0 限制 CPU CFS 的周期，范围从 100ms~1s，即[1000, 1000000]   --cpu-quota=0 限制 CPU CFS 配额，必须不小于1ms，即 \u0026gt;= 1000，绝对限制    docker run -it --cpu-period=50000 --cpu-quota=25000 ubuntu:16.04 /bin/bash 将 CFS 调度的周期设为 50000，将容器在每个周期内的 CPU 配额设置为 25000，表示该容器每 50ms 可以得到 50% 的 CPU 运行时间。\n 注意：若内存使用超出限制，会引发系统的OOM机制，因CPU是可压缩资源，不会引发Pod退出或重建\n yaml优化 目前完善后的yaml，myblog/one-pod/pod-completed.yaml\napiVersion: v1 kind: Pod metadata:  name: myblog  namespace: luffy  labels:  component: myblog spec:  volumes:  - name: mysql-data  hostPath:  path: /opt/mysql/data  nodeSelector: # 使用节点选择器将Pod调度到指定label的节点  component: mysql  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  env:  - name: MYSQL_HOST  # 指定root用户的用户名  value: \u0026#34;127.0.0.1\u0026#34;  - name: MYSQL_PASSWD  value: \u0026#34;123456\u0026#34;  ports:  - containerPort: 8002  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  livenessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒  periodSeconds: 15 # 执行探测的频率  timeoutSeconds: 2\t# 探测超时时间  readinessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10  timeoutSeconds: 2  periodSeconds: 15  - name: mysql  image: 172.21.51.67:5000/mysql:5.7-utf8  ports:  - containerPort: 3306  env:  - name: MYSQL_ROOT_PASSWORD  value: \u0026#34;123456\u0026#34;  - name: MYSQL_DATABASE  value: \u0026#34;myblog\u0026#34;  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  readinessProbe:  tcpSocket:  port: 3306  initialDelaySeconds: 5  periodSeconds: 10  livenessProbe:  tcpSocket:  port: 3306  initialDelaySeconds: 15  periodSeconds: 20  volumeMounts:  - name: mysql-data  mountPath: /var/lib/mysql 为什么要优化\n 考虑真实的使用场景，像数据库这类中间件，是作为公共资源，为多个项目提供服务，不适合和业务容器绑定在同一个Pod中，因为业务容器是经常变更的，而数据库不需要频繁迭代 yaml的环境变量中存在敏感信息（账号、密码），存在安全隐患  解决问题一，需要拆分yaml\nmyblog/two-pod/mysql.yaml\napiVersion: v1 kind: Pod metadata:  name: mysql  namespace: luffy  labels:  component: mysql spec:  hostNetwork: true\t# 声明pod的网络模式为host模式，效果同docker run --net=host  volumes:  - name: mysql-data  hostPath:  path: /opt/mysql/data  nodeSelector: # 使用节点选择器将Pod调度到指定label的节点  component: mysql  containers:  - name: mysql  image: 172.21.51.67:5000/mysql:5.7-utf8  ports:  - containerPort: 3306  env:  - name: MYSQL_ROOT_PASSWORD  value: \u0026#34;123456\u0026#34;  - name: MYSQL_DATABASE  value: \u0026#34;myblog\u0026#34;  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  readinessProbe:  tcpSocket:  port: 3306  initialDelaySeconds: 5  periodSeconds: 10  livenessProbe:  tcpSocket:  port: 3306  initialDelaySeconds: 15  periodSeconds: 20  volumeMounts:  - name: mysql-data  mountPath: /var/lib/mysql myblog.yaml\napiVersion: v1 kind: Pod metadata:  name: myblog  namespace: luffy  labels:  component: myblog spec:  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  imagePullPolicy: IfNotPresent  env:  - name: MYSQL_HOST  # 指定root用户的用户名  value: \u0026#34;172.21.51.68\u0026#34;  - name: MYSQL_PASSWD  value: \u0026#34;123456\u0026#34;  ports:  - containerPort: 8002  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  livenessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒  periodSeconds: 15 # 执行探测的频率  timeoutSeconds: 2\t# 探测超时时间  readinessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10  timeoutSeconds: 2  periodSeconds: 15 创建测试\n## 先删除旧pod $ kubectl -n luffy delete po myblog  ## 分别创建mysql和myblog $ kubectl create -f mysql.yaml $ kubectl create -f myblog.yaml  ## 查看pod，注意mysqlIP为宿主机IP，因为网络模式为host $ kubectl -n luffy get po -o wide NAME READY STATUS RESTARTS AGE IP NODE myblog 1/1 Running 0 41s 10.244.1.152 k8s-slave1 mysql 1/1 Running 0 52s 172.21.51.68 k8s-slave1  ## 访问myblog服务正常 $ curl 10.244.1.152:8002/blog/index/ 解决问题二，环境变量中敏感信息带来的安全隐患\n为什么要统一管理环境变量\n 环境变量中有很多敏感的信息，比如账号密码，直接暴漏在yaml文件中存在安全性问题 团队内部一般存在多个项目，这些项目直接存在配置相同环境变量的情况，因此可以统一维护管理 对于开发、测试、生产环境，由于配置均不同，每套环境部署的时候都要修改yaml，带来额外的开销  k8s提供两类资源，configMap和Secret，可以用来实现业务配置的统一管理， 允许将配置文件与镜像文件分离，以使容器化的应用程序具有可移植性 。\n  configMap，通常用来管理应用的配置文件或者环境变量，myblog/two-pod/configmap.yaml\napiVersion: v1 kind: ConfigMap metadata:  name: myblog  namespace: luffy data:  MYSQL_HOST: \u0026#34;172.21.51.68\u0026#34;  MYSQL_PORT: \u0026#34;3306\u0026#34; 创建并查看configMap：\n$ kubectl create -f configmap.yaml $ kubectl -n luffy get cm myblog -oyaml 或者可以使用命令的方式，从文件中创建，比如：\nconfigmap.txt\n$ cat configmap.txt MYSQL_HOST=172.21.51.68 MYSQL_PORT=3306 $ kubectl create configmap myblog --from-env-file=configmap.txt   Secret，管理敏感类的信息，默认会base64编码存储，有三种类型\n Service Account ：用来访问Kubernetes API，由Kubernetes自动创建，并且会自动挂载到Pod的/run/secrets/kubernetes.io/serviceaccount目录中；创建ServiceAccount后，Pod中指定serviceAccount后，自动创建该ServiceAccount对应的secret； Opaque ： base64编码格式的Secret，用来存储密码、密钥等； kubernetes.io/dockerconfigjson ：用来存储私有docker registry的认证信息。  myblog/two-pod/secret.yaml\napiVersion: v1 kind: Secret metadata:  name: myblog  namespace: luffy type: Opaque data:  MYSQL_USER: cm9vdA==\t#注意加-n参数， echo -n root|base64  MYSQL_PASSWD: MTIzNDU2 创建并查看：\n$ kubectl create -f secret.yaml $ kubectl -n luffy get secret 如果不习惯这种方式，可以通过如下方式：\n$ cat secret.txt MYSQL_USER=root MYSQL_PASSWD=123456 $ kubectl -n luffy create secret generic myblog --from-env-file=secret.txt   修改后的mysql的yaml，资源路径：myblog/two-pod/mysql-with-config.yaml\n... spec:  containers:  - name: mysql  image: 172.21.51.67:5000/mysql:5.7-utf8  env:  - name: MYSQL_USER  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_USER  - name: MYSQL_ROOT_PASSWORD  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_PASSWD  - name: MYSQL_DATABASE  value: \u0026#34;myblog\u0026#34; ... 整体修改后的myblog的yaml，资源路径：myblog/two-pod/myblog-with-config.yaml\napiVersion: v1 kind: Pod metadata:  name: myblog  namespace: luffy  labels:  component: myblog spec:  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  imagePullPolicy: IfNotPresent  env:  - name: MYSQL_HOST  valueFrom:  configMapKeyRef:  name: myblog  key: MYSQL_HOST  - name: MYSQL_PORT  valueFrom:  configMapKeyRef:  name: myblog  key: MYSQL_PORT  - name: MYSQL_USER  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_USER  - name: MYSQL_PASSWD  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_PASSWD  ports:  - containerPort: 8002  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  livenessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒  periodSeconds: 15 # 执行探测的频率  timeoutSeconds: 2\t# 探测超时时间  readinessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10  timeoutSeconds: 2  periodSeconds: 15 在部署不同的环境时，pod的yaml无须再变化，只需要在每套环境中维护一套ConfigMap和Secret即可。但是注意configmap和secret不能跨namespace使用，且更新后，pod内的env不会自动更新，重建后方可更新。\n如何编写资源yaml   拿来主义，从机器中已有的资源中拿\n$ kubectl -n kube-system get po,deployment,ds   学会在官网查找， https://kubernetes.io/docs/home/\n  从kubernetes-api文档中查找， https://kubernetes.io/docs/reference/generated/kubernetes-api/v1.16/#pod-v1-core\n  kubectl explain 查看具体字段含义\n  pod状态与生命周期 Pod的状态如下表所示：\n   状态值 描述     Pending API Server已经创建该Pod，等待调度器调度   ContainerCreating 拉取镜像启动容器中   Running Pod内容器均已创建，且至少有一个容器处于运行状态、正在启动状态或正在重启状态   Succeeded|Completed Pod内所有容器均已成功执行退出，且不再重启   Failed|Error Pod内所有容器均已退出，但至少有一个容器退出为失败状态   CrashLoopBackOff Pod内有容器启动失败，比如配置文件丢失导致主进程启动失败   Unknown 由于某种原因无法获取该Pod的状态，可能由于网络通信不畅导致    生命周期示意图：\n启动和关闭示意：\n初始化容器：\n 验证业务应用依赖的组件是否均已启动 修改目录的权限 调整系统参数  ...  initContainers:  - command:  - /sbin/sysctl  - -w  - vm.max_map_count=262144  image: alpine:3.6  imagePullPolicy: IfNotPresent  name: elasticsearch-logging-init  resources: {}  securityContext:  privileged: true  - name: fix-permissions  image: alpine:3.6  command: [\u0026#34;sh\u0026#34;, \u0026#34;-c\u0026#34;, \u0026#34;chown -R 1000:1000 /usr/share/elasticsearch/data\u0026#34;]  securityContext:  privileged: true  volumeMounts:  - name: elasticsearch-logging  mountPath: /usr/share/elasticsearch/data ... 验证Pod生命周期：\napiVersion: v1 kind: Pod metadata:  name: demo-start-stop  namespace: luffy  labels:  component: demo-start-stop spec:  initContainers:  - name: init  image: busybox  command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): INIT \u0026gt;\u0026gt; /loap/timing\u0026#39;]  volumeMounts:  - mountPath: /loap  name: timing  containers:  - name: main  image: busybox  command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): START \u0026gt;\u0026gt; /loap/timing; sleep 10; echo $(date +%s): END \u0026gt;\u0026gt; /loap/timing;\u0026#39;]  volumeMounts:  - mountPath: /loap   name: timing  livenessProbe:  exec:  command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): LIVENESS \u0026gt;\u0026gt; /loap/timing\u0026#39;]  readinessProbe:  exec:  command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): READINESS \u0026gt;\u0026gt; /loap/timing\u0026#39;]  lifecycle:  postStart:  exec:  command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): POST-START \u0026gt;\u0026gt; /loap/timing\u0026#39;]  preStop:  exec:  command: [\u0026#39;sh\u0026#39;, \u0026#39;-c\u0026#39;, \u0026#39;echo $(date +%s): PRE-STOP \u0026gt;\u0026gt; /loap/timing\u0026#39;]  volumes:  - name: timing  hostPath:  path: /tmp/loap 创建pod测试：\n$ kubectl create -f demo-pod-start.yaml  ## 查看demo状态 $ kubectl -n luffy get po -o wide -w  ## 查看调度节点的/tmp/loap/timing $ cat /tmp/loap/timing 1585424708: INIT 1585424746: START 1585424746: POST-START 1585424754: READINESS 1585424756: LIVENESS 1585424756: END  须主动杀掉 Pod 才会触发 pre-stop hook，如果是 Pod 自己 Down 掉，则不会执行 pre-stop hook\n 小结  实现k8s平台与特定的容器运行时解耦，提供更加灵活的业务部署方式，引入了Pod概念 k8s使用yaml格式定义资源文件，yaml中Map与List的语法，与json做类比 通过kubectl create | get | exec | logs | delete 等操作k8s资源，必须指定namespace 每启动一个Pod，为了实现网络空间共享，会先创建Infra容器，并把其他容器网络加入该容器 通过livenessProbe和readinessProbe实现Pod的存活性和就绪健康检查 通过requests和limit分别限定容器初始资源申请与最高上限资源申请 Pod通过initContainer和lifecycle分别来执行初始化、pod启动和删除时候的操作，使得功能更加全面和灵活 编写yaml讲究方法，学习k8s，养成从官方网站查询知识的习惯  做了哪些工作：\n 定义Pod.yaml，将myblog和mysql打包在同一个Pod中，使用myblog使用localhost访问mysql mysql数据持久化，为myblog业务应用添加了健康检查和资源限制 将myblog与mysql拆分，使用独立的Pod管理 yaml文件中的环境变量存在账号密码明文等敏感信息，使用configMap和Secret来统一配置，优化部署  只使用Pod, 面临的问题:\n 业务应用启动多个副本 Pod重建后IP会变化，外部如何访问Pod服务 运行业务Pod的某个节点挂了，可以自动帮我把Pod转移到集群中的可用节点启动起来 我的业务应用功能是收集节点监控数据,需要把Pod运行在k8集群的各个节点上  Pod控制器 Workload (工作负载) 控制器又称工作负载是用于实现管理pod的中间层，确保pod资源符合预期的状态，pod的资源出现故障时，会尝试 进行重启，当根据重启策略无效，则会重新新建pod的资源。\n ReplicaSet: 代用户创建指定数量的pod副本数量，确保pod副本数量符合预期状态，并且支持滚动式自动扩容和缩容功能 Deployment：工作在ReplicaSet之上，用于管理无状态应用，目前来说最好的控制器。支持滚动更新和回滚功能，提供声明式配置 DaemonSet：用于确保集群中的每一个节点只运行特定的pod副本，通常用于实现系统级后台任务。比如EFK服务 Job：只要完成就立即退出，不需要重启或重建 Cronjob：周期性任务控制，不需要持续后台运行 StatefulSet：管理有状态应用  Deployment myblog/deployment/deploy-mysql.yaml\napiVersion: apps/v1 kind: Deployment metadata:  name: mysql  namespace: luffy spec:  replicas: 1\t#指定Pod副本数  selector:\t#指定Pod的选择器  matchLabels:  app: mysql  template:  metadata:  labels:\t#给Pod打label  app: mysql  spec:  volumes:  - name: mysql-data  hostPath:  path: /opt/mysql/data  nodeSelector: # 使用节点选择器将Pod调度到指定label的节点  component: mysql  containers:  - name: mysql  image: 172.21.51.67:5000/mysql:5.7-utf8  ports:  - containerPort: 3306  env:  - name: MYSQL_USER  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_USER  - name: MYSQL_ROOT_PASSWORD  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_PASSWD  - name: MYSQL_DATABASE  value: \u0026#34;myblog\u0026#34;  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  readinessProbe:  tcpSocket:  port: 3306  initialDelaySeconds: 5  periodSeconds: 10  livenessProbe:  tcpSocket:  port: 3306  initialDelaySeconds: 15  periodSeconds: 20  volumeMounts:  - name: mysql-data  mountPath: /var/lib/mysql deploy-myblog.yaml:\napiVersion: apps/v1 kind: Deployment metadata:  name: myblog  namespace: luffy spec:  replicas: 1\t#指定Pod副本数  selector:\t#指定Pod的选择器  matchLabels:  app: myblog  template:  metadata:  labels:\t#给Pod打label  app: myblog  spec:  containers:  - name: myblog  image: 172.21.51.67:5000/myblog:v1  imagePullPolicy: IfNotPresent  env:  - name: MYSQL_HOST  valueFrom:  configMapKeyRef:  name: myblog  key: MYSQL_HOST  - name: MYSQL_PORT  valueFrom:  configMapKeyRef:  name: myblog  key: MYSQL_PORT  - name: MYSQL_USER  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_USER  - name: MYSQL_PASSWD  valueFrom:  secretKeyRef:  name: myblog  key: MYSQL_PASSWD  ports:  - containerPort: 8002  resources:  requests:  memory: 100Mi  cpu: 50m  limits:  memory: 500Mi  cpu: 100m  livenessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10 # 容器启动后第一次执行探测是需要等待多少秒  periodSeconds: 15 # 执行探测的频率  timeoutSeconds: 2\t# 探测超时时间  readinessProbe:  httpGet:  path: /blog/index/  port: 8002  scheme: HTTP  initialDelaySeconds: 10  timeoutSeconds: 2  periodSeconds: 15 创建Deployment $ kubectl create -f deploy.yaml 查看Deployment # kubectl api-resources $ kubectl -n luffy get deploy NAME READY UP-TO-DATE AVAILABLE AGE myblog 1/1 1 1 2m22s mysql 1/1 1 1 2d11h   * `NAME` 列出了集群中 Deployments 的名称。  * `READY`显示当前正在运行的副本数/期望的副本数。  * `UP-TO-DATE`显示已更新以实现期望状态的副本数。  * `AVAILABLE`显示应用程序可供用户使用的副本数。  * `AGE` 显示应用程序运行的时间量。  # 查看pod $ kubectl -n luffy get po NAME READY STATUS RESTARTS AGE myblog-7c96c9f76b-qbbg7 1/1 Running 0 109s mysql-85f4f65f99-w6jkj 1/1 Running 0 2m28s  # 查看replicaSet $ kubectl -n luffy get rs 副本保障机制 controller实时检测pod状态，并保障副本数一直处于期望的值。\n## 删除pod，观察pod状态变化 $ kubectl -n luffy delete pod myblog-7c96c9f76b-qbbg7  # 观察pod $ kubectl get pods -o wide  ## 设置两个副本, 或者通过kubectl -n luffy edit deploy myblog的方式，最好通过修改文件，然后apply的方式，这样yaml文件可以保持同步 $ kubectl -n luffy scale deploy myblog --replicas=2 deployment.extensions/myblog scaled  # 观察pod $ kubectl get pods -o wide NAME READY STATUS RESTARTS AGE myblog-7c96c9f76b-qbbg7 1/1 Running 0 11m myblog-7c96c9f76b-s6brm 1/1 Running 0 55s mysql-85f4f65f99-w6jkj 1/1 Running 0 11m Pod驱逐策略 K8S 有个特色功能叫 pod eviction，它在某些场景下如节点 NotReady，或者资源不足时，把 pod 驱逐至其它节点，这也是出于业务保护的角度去考虑的。\n Kube-controller-manager: 周期性检查所有节点状态，当节点处于 NotReady 状态超过一段时间后，驱逐该节点上所有 pod。    pod-eviction-timeout：NotReady 状态节点超过该时间后，执行驱逐，默认 5 min，适用于k8s 1.13版本之前\n  1.13版本后，集群开启 TaintBasedEvictions 与TaintNodesByCondition 功能，即taint-based-evictions，即节点若失联或者出现各种异常情况，k8s会自动为node打上污点，同时为pod默认添加如下容忍设置：\n tolerations:  - effect: NoExecute  key: node.kubernetes.io/not-ready  operator: Exists  tolerationSeconds: 300  - effect: NoExecute  key: node.kubernetes.io/unreachable  operator: Exists  tolerationSeconds: 300 即各pod可以独立设置驱逐容忍时间。\n    Kubelet: 周期性检查本节点资源，当资源不足时，按照优先级驱逐部分 pod  memory.available：节点可用内存 nodefs.available：节点根盘可用存储空间 nodefs.inodesFree：节点inodes可用数量 imagefs.available：镜像存储盘的可用空间 imagefs.inodesFree：镜像存储盘的inodes可用数量    服务更新 修改服务，重新打tag模拟服务更新。\n更新方式：\n  修改yaml文件，使用kubectl apply -f deploy-myblog.yaml来应用更新\n  kubectl -n luffy edit deploy myblog在线更新\n  kubectl -n luffy set image deploy myblog myblog=172.21.51.67:5000/myblog:v2 --record\n  修改文件测试：\n$ vi mybolg/blog/template/index.html  $ docker build . -t 172.21.51.67:5000/myblog:v2 -f Dockerfile $ docker push 172.21.51.67:5000/myblog:v2 更新策略 ... spec:  replicas: 2\t#指定Pod副本数  selector:\t#指定Pod的选择器  matchLabels:  app: myblog  strategy:  rollingUpdate:  maxSurge: 1  maxUnavailable: 25%  type: RollingUpdate\t#指定更新方式为滚动更新，默认策略，通过get deploy yaml查看  ... 策略控制：\n maxSurge：最大激增数, 指更新过程中, 最多可以比replicas预先设定值多出的pod数量, 可以为固定值或百分比,默认为desired Pods数的25%。计算时向上取整(比如3.4，取4)，更新过程中最多会有replicas + maxSurge个pod maxUnavailable： 指更新过程中, 最多有几个pod处于无法服务状态 , 可以为固定值或百分比，默认为desired Pods数的25%。计算时向下取整(比如3.6，取3)  在Deployment rollout时，需要保证Available(Ready) Pods数不低于 desired pods number - maxUnavailable; 保证所有的非异常状态Pods数不多于 desired pods number + maxSurge。\nreplicas=3\nrunning状态pod最大不超过3+1=4个，\nrunning状态的Pod数不低于3-0=3个\n 先新增一个v2版本的pod，目前3个v1版本+1个v2版本，共4个pod 删掉一个v1版本的pod，目前2个v1版本+1个v2版本，共3个pod 先新增一个v2版本的pod，目前2个v1版本+2个v2版本，共4个pod 删掉一个v1版本的pod，目前1个v1版本+2个v2版本，共3个pod 先新增一个v2版本的pod，目前1个v1版本+3个v2版本，共4个pod 删掉一个v1版本的pod，目前0个v1版本+3个v2版本，共3个pod  以myblog为例，使用默认的策略，更新过程:\n maxSurge 25%，2个实例，向上取整，则maxSurge为1，意味着最多可以有2+1=3个Pod，那么此时会新创建1个ReplicaSet，RS-new，把副本数置为1，此时呢，副本控制器就去创建这个新的Pod 同时，maxUnavailable是25%，副本数2*25%，向下取整，则为0，意味着，滚动更新的过程中，不能有少于2个可用的Pod，因此，旧的Replica（RS-old）会先保持不动，等RS-new管理的Pod状态Ready后，此时已经有3个Ready状态的Pod了，那么由于只要保证有2个可用的Pod即可，因此，RS-old的副本数会有2个变成1个，此时，会删掉一个旧的Pod 删掉旧的Pod的时候，由于总的Pod数量又变成2个了，因此，距离最大的3个还有1个Pod可以创建，所以，RS-new把管理的副本数由1改成2，此时又会创建1个新的Pod，等RS-new管理了2个Pod都ready后，那么就可以把RS-old的副本数由1置为0了，这样就完成了滚动更新  #查看滚动更新事件 $ kubectl -n luffy describe deploy myblog ... Events:  Type Reason Age From Message  ---- ------ ---- ---- -------  Normal ScalingReplicaSet 11s deployment-controller Scaled up replica set myblog-6cf56fc848 to 1  Normal ScalingReplicaSet 11s deployment-controller Scaled down replica set myblog-6fdcf98f9 to 1  Normal ScalingReplicaSet 11s deployment-controller Scaled up replica set myblog-6cf56fc848 to 2  Normal ScalingReplicaSet 6s deployment-controller Scaled down replica set myblog-6fdcf98f9 to 0 $ kubectl get rs NAME DESIRED CURRENT READY AGE myblog-6cf56fc848 2 2 2 16h myblog-6fdcf98f9 0 0 0 16h 服务回滚 通过滚动升级的策略可以平滑的升级Deployment，若升级出现问题，需要最快且最好的方式回退到上一次能够提供正常工作的版本。为此K8S提供了回滚机制。\nrevision：更新应用时，K8S都会记录当前的版本号，即为revision，当升级出现问题时，可通过回滚到某个特定的revision，默认配置下，K8S只会保留最近的几个revision，可以通过Deployment配置文件中的spec.revisionHistoryLimit属性增加revision数量，默认是10。\n查看当前：\n$ kubectl -n luffy rollout history deploy myblog ##CHANGE-CAUSE为空 $ kubectl delete -f deploy-myblog.yaml ## 方便演示到具体效果，删掉已有deployment 记录回滚：\n$ kubectl create -f deploy-myblog.yaml --record  $ kubectl -n luffy set image deploy myblog myblog=172.21.51.67:5000/myblog:v2 --record=true 查看deployment更新历史：\n$ kubectl -n luffy rollout history deploy myblog deployment.extensions/myblog REVISION CHANGE-CAUSE 1 kubectl create --filename=deploy-myblog.yaml --record=true 2 kubectl set image deploy myblog myblog=172.21.51.67:5000/demo/myblog:v1 --record=true 回滚到具体的REVISION:\n$ kubectl -n luffy rollout undo deploy myblog --to-revision=1 deployment.extensions/myblog rolled back  # 访问应用测试 Kubernetes服务访问之Service 通过以前的学习，我们已经能够通过Deployment来创建一组Pod来提供具有高可用性的服务。虽然每个Pod都会分配一个单独的Pod IP，然而却存在如下两个问题：\n Pod IP仅仅是集群内可见的虚拟IP，外部无法访问。 Pod IP会随着Pod的销毁而消失，当ReplicaSet对Pod进行动态伸缩时，Pod IP可能随时随地都会变化，这样对于我们访问这个服务带来了难度。  Service 负载均衡之Cluster IP service是一组pod的服务抽象，相当于一组pod的LB，负责将请求分发给对应的pod。service会为这个LB提供一个IP，一般称为cluster IP 。使用Service对象，通过selector进行标签选择，找到对应的Pod:\nmyblog/deployment/svc-myblog.yaml\napiVersion: v1 kind: Service metadata:  name: myblog  namespace: luffy spec:  ports:  - port: 80  protocol: TCP  targetPort: 8002  selector:  app: myblog  type: ClusterIP 操作演示：\n## 别名 $ alias kd=\u0026#39;kubectl -n luffy\u0026#39;  ## 创建服务 $ kd create -f svc-myblog.yaml $ kd get po --show-labels NAME READY STATUS RESTARTS AGE LABELS myblog-5c97d79cdb-jn7km 1/1 Running 0 6m5s app=myblog mysql-85f4f65f99-w6jkj 1/1 Running 0 176m app=mysql  $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 7m50s  $ kd describe svc myblog Name: myblog Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=myblog Type: ClusterIP IP: 10.99.174.93 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 8002/TCP Endpoints: 10.244.0.68:8002 Session Affinity: None Events: \u0026lt;none\u0026gt;  ## 扩容myblog服务 $ kd scale deploy myblog --replicas=2 deployment.extensions/myblog scaled  ## 再次查看 $ kd describe svc myblog Name: myblog Namespace: demo Labels: \u0026lt;none\u0026gt; Annotations: \u0026lt;none\u0026gt; Selector: app=myblog Type: ClusterIP IP: 10.99.174.93 Port: \u0026lt;unset\u0026gt; 80/TCP TargetPort: 8002/TCP Endpoints: 10.244.0.68:8002,10.244.1.158:8002 Session Affinity: None Events: \u0026lt;none\u0026gt; Service与Pod如何关联:\nservice对象创建的同时，会创建同名的endpoints对象，若服务设置了readinessProbe, 当readinessProbe检测失败时，endpoints列表中会剔除掉对应的pod_ip，这样流量就不会分发到健康检测失败的Pod中\n$ kd get endpoints myblog NAME ENDPOINTS AGE myblog 10.244.0.68:8002,10.244.1.158:8002 7m Service Cluster-IP如何访问:\n$ kd get svc myblog NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 13m $ curl 10.99.174.93/blog/index/ 为mysql服务创建service：\napiVersion: v1 kind: Service metadata:  name: mysql  namespace: luffy spec:  ports:  - port: 3306  protocol: TCP  targetPort: 3306  selector:  app: mysql  type: ClusterIP 访问mysql：\n$ kd get svc mysql mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 3s $ curl 10.108.214.84:3306 目前使用hostNetwork部署，通过宿主机ip+port访问，弊端：\n 服务使用hostNetwork，使得宿主机的端口大量暴漏，存在安全隐患 容易引发端口冲突  服务均属于k8s集群，尽可能使用k8s的网络访问，因此可以对目前myblog访问mysql的方式做改造：\n 为mysql创建一个固定clusterIp的Service，把clusterIp配置在myblog的环境变量中 利用集群服务发现的能力，组件之间通过service name来访问  服务发现 在k8s集群中，组件之间可以通过定义的Service名称实现通信。\n演示服务发现：\n## 演示思路：在myblog的容器中直接通过service名称访问服务，观察是否可以访问通  # 先查看服务 $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 59m mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 35m  # 进入myblog容器 $ kd exec -ti myblog-5c97d79cdb-j485f bash [root@myblog-5c97d79cdb-j485f myblog]# curl mysql:3306 5.7.29 )→ (mysql_native_password ot packets out of order [root@myblog-5c97d79cdb-j485f myblog]# curl myblog/blog/index/ 我的博客列表 虽然podip和clusterip都不固定，但是service name是固定的，而且具有完全的跨集群可移植性，因此组件之间调用的同时，完全可以通过service name去通信，这样避免了大量的ip维护成本，使得服务的yaml模板更加简单。因此可以对mysql和myblog的部署进行优化改造：\n mysql可以去掉hostNetwork部署，使得服务只暴漏在k8s集群内部网络 configMap中数据库地址可以换成Service名称，这样跨环境的时候，配置内容基本上可以保持不用变化  修改deploy-mysql.yaml\n spec:  hostNetwork: true\t# 去掉此行  volumes:  - name: mysql-data  hostPath:  path: /opt/mysql/data 修改configmap.yaml\napiVersion: v1 kind: ConfigMap metadata:  name: myblog  namespace: luffy data:  MYSQL_HOST: \u0026#34;mysql\u0026#34;\t# 此处替换为mysql  MYSQL_PORT: \u0026#34;3306\u0026#34; 应用修改：\n$ kubectl delete -f deployment-mysql.yaml  ## myblog不用动，会自动因健康检测不过而重启 服务发现实现：\nCoreDNS是一个Go语言实现的链式插件DNS服务端，是CNCF成员，是一个高性能、易扩展的DNS服务端。\n$ kubectl -n kube-system get po -o wide|grep dns coredns-d4475785-2w4hk 1/1 Running 0 4d22h 10.244.0.64 coredns-d4475785-s49hq 1/1 Running 0 4d22h 10.244.0.65  # 查看myblog的pod解析配置 $ kubectl -n luffy exec -ti myblog-5c97d79cdb-j485f bash [root@myblog-5c97d79cdb-j485f myblog]# cat /etc/resolv.conf nameserver 10.96.0.10 search luffy.svc.cluster.local svc.cluster.local cluster.local options ndots:5  ## 10.96.0.10 从哪来 $ kubectl -n kube-system get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kube-dns ClusterIP 10.96.0.10 \u0026lt;none\u0026gt; 53/UDP,53/TCP 51d  ## 启动pod的时候，会把kube-dns服务的cluster-ip地址注入到pod的resolve解析配置中，同时添加对应的namespace的search域。 因此跨namespace通过service name访问的话，需要添加对应的namespace名称， service_name.namespace $ kubectl get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE kubernetes ClusterIP 10.96.0.1 \u0026lt;none\u0026gt; 443/TCP 26h Service负载均衡之NodePort cluster-ip为虚拟地址，只能在k8s集群内部进行访问，集群外部如果访问内部服务，实现方式之一为使用NodePort方式。NodePort会默认在 30000-32767 ，不指定的会随机使用其中一个。\nmyblog/deployment/svc-myblog-nodeport.yaml\napiVersion: v1 kind: Service metadata:  name: myblog-np  namespace: luffy spec:  ports:  - port: 80  protocol: TCP  targetPort: 8002  selector:  app: myblog  type: NodePort 查看并访问服务：\n$ kd create -f svc-myblog-nodeport.yaml service/myblog-np created $ kd get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE myblog ClusterIP 10.99.174.93 \u0026lt;none\u0026gt; 80/TCP 102m myblog-np NodePort 10.105.228.101 \u0026lt;none\u0026gt; 80:30647/TCP 4s mysql ClusterIP 10.108.214.84 \u0026lt;none\u0026gt; 3306/TCP 77m  #集群内每个节点的NodePort端口都会进行监听 $ curl 172.21.51.67:30647/blog/index/ 我的博客列表 $ curl 172.21.51.68:30647/blog/index/ 我的博客列表 ## 浏览器访问 思考：\n  NodePort的端口监听如何转发到对应的Pod服务？\n  CLUSTER-IP为虚拟IP，集群内如何通过虚拟IP访问到具体的Pod服务？\n  kube-proxy 运行在每个节点上，监听 API Server 中服务对象的变化，再通过创建流量路由规则来实现网络的转发。参照\n有三种模式：\n User space, 让 Kube-Proxy 在用户空间监听一个端口，所有的 Service 都转发到这个端口，然后 Kube-Proxy 在内部应用层对其进行转发 ， 所有报文都走一遍用户态，性能不高，k8s v1.2版本后废弃。 Iptables， 当前默认模式，完全由 IPtables 来实现， 通过各个node节点上的iptables规则来实现service的负载均衡，但是随着service数量的增大，iptables模式由于线性查找匹配、全量更新等特点，其性能会显著下降。 IPVS， 与iptables同样基于Netfilter，但是采用的hash表，因此当service数量达到一定规模时，hash查表的速度优势就会显现出来，从而提高service的服务性能。 k8s 1.8版本开始引入，1.11版本开始稳定，需要开启宿主机的ipvs模块。  IPtables模式示意图：\n$ iptables-save |grep -v myblog-np|grep \u0026#34;luffy/myblog\u0026#34; -A KUBE-SERVICES ! -s 10.244.0.0/16 -d 10.99.174.93/32 -p tcp -m comment --comment \u0026#34;demo/myblog: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-MARK-MASQ -A KUBE-SERVICES -d 10.99.174.93/32 -p tcp -m comment --comment \u0026#34;demo/myblog: cluster IP\u0026#34; -m tcp --dport 80 -j KUBE-SVC-WQNGJ7YFZKCTKPZK  $ iptables-save |grep KUBE-SVC-WQNGJ7YFZKCTKPZK -A KUBE-SVC-WQNGJ7YFZKCTKPZK -m statistic --mode random --probability 0.50000000000 -j KUBE-SEP-GB5GNOM5CZH7ICXZ -A KUBE-SVC-WQNGJ7YFZKCTKPZK -j KUBE-SEP-7GWC3FN2JI5KLE47  $ iptables-save |grep KUBE-SEP-GB5GNOM5CZH7ICXZ -A KUBE-SEP-GB5GNOM5CZH7ICXZ -p tcp -m tcp -j DNAT --to-destination 10.244.1.158:8002  $ iptables-save |grep KUBE-SEP-7GWC3FN2JI5KLE47 -A KUBE-SEP-7GWC3FN2JI5KLE47 -p tcp -m tcp -j DNAT --to-destination 10.244.1.159:8002 Kubernetes服务访问之Ingress 对于Kubernetes的Service，无论是Cluster-Ip和NodePort均是四层的负载，集群内的服务如何实现七层的负载均衡，这就需要借助于Ingress，Ingress控制器的实现方式有很多，比如nginx, Contour, Haproxy, trafik, Istio。几种常用的ingress功能对比和选型可以参考这里\nIngress-nginx是7层的负载均衡器 ，负责统一管理外部对k8s cluster中Service的请求。主要包含：\n  ingress-nginx-controller：根据用户编写的ingress规则（创建的ingress的yaml文件），动态的去更改nginx服务的配置文件，并且reload重载使其生效（是自动化的，通过lua脚本来实现）；\n  Ingress资源对象：将Nginx的配置抽象成一个Ingress对象\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata:  name: simple-example spec:  rules:  - host: foo.bar.com  http:  paths:  - path: /  backend:  serviceName: service1  servicePort: 8080   示意图： 实现逻辑 1）ingress controller通过和kubernetes api交互，动态的去感知集群中ingress规则变化 2）然后读取ingress规则(规则就是写明了哪个域名对应哪个service)，按照自定义的规则，生成一段nginx配置 3）再写到nginx-ingress-controller的pod里，这个Ingress controller的pod里运行着一个Nginx服务，控制器把生成的nginx配置写入/etc/nginx/nginx.conf文件中 4）然后reload一下使配置生效。以此达到域名分别配置和动态更新的问题。\n安装 官方文档\n$ wget https://raw.githubusercontent.com/kubernetes/ingress-nginx/nginx-0.30.0/deploy/static/mandatory.yaml ## 或者使用myblog/deployment/ingress/mandatory.yaml ## 修改部署节点 $ grep -n5 nodeSelector mandatory.yaml 212- spec: 213- hostNetwork: true #添加为host模式 214- # wait up to five minutes for the drain of connections 215- terminationGracePeriodSeconds: 300 216- serviceAccountName: nginx-ingress-serviceaccount 217: nodeSelector: 218- ingress: \u0026#34;true\u0026#34;\t#替换此处，来决定将ingress部署在哪些机器 219- containers: 220- - name: nginx-ingress-controller 221- image: quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 222- args: 创建ingress\n# 为k8s-master节点添加label $ kubectl label node k8s-master ingress=true  $ kubectl create -f mandatory.yaml 使用示例：myblog/deployment/ingress.yaml \napiVersion: extensions/v1beta1 kind: Ingress metadata:  name: myblog  namespace: luffy spec:  rules:  - host: myblog.luffy.com  http:  paths:  - path: /  backend:  serviceName: myblog  servicePort: 80 ingress-nginx动态生成upstream配置：\n...  server_name myblog.luffy.com ;   listen 80 ;  listen [::]:80 ;  listen 443 ssl http2 ;  listen [::]:443 ssl http2 ;   set $proxy_upstream_name \u0026#34;-\u0026#34;;   ssl_certificate_by_lua_block {  certificate.call()  }   location / {   set $namespace \u0026#34;luffy\u0026#34;;  set $ingress_name \u0026#34;myblog\u0026#34;;  ... 访问 域名解析服务，将 myblog.luffy.com解析到ingress的地址上。ingress是支持多副本的，高可用的情况下，生产的配置是使用lb服务（内网F5设备，公网elb、slb、clb，解析到各ingress的机器，如何域名指向lb地址）\n本机，添加如下hosts记录来演示效果。\n172.21.51.67 myblog.luffy.com 然后，访问 http://myblog.luffy.com/blog/index/\nHTTPS访问：\n#自签名证书 $ openssl req -x509 -nodes -days 2920 -newkey rsa:2048 -keyout tls.key -out tls.crt -subj \u0026#34;/CN=*.luffy.com/O=ingress-nginx\u0026#34;  # 证书信息保存到secret对象中，ingress-nginx会读取secret对象解析出证书加载到nginx配置中 $ kubectl -n luffy create secret tls https-secret --key tls.key --cert tls.crt 修改yaml\napiVersion: extensions/v1beta1 kind: Ingress metadata:  name: myblog-tls  namespace: luffy spec:  rules:  - host: myblog.luffy.com  http:  paths:  - path: /  backend:  serviceName: myblog  servicePort: 80  tls:  - hosts:  - myblog.luffy.com  secretName: https-secret 然后，访问 https://myblog.luffy.com/blog/index/\n多路径转发及重写的实现   多path转发示例：\n目标：\n  myblog.luffy.com -\u0026gt; 172.21.51.67 -\u0026gt; /foo service1:4200 /bar service2:8080 /\tmyblog:80 ​\t实现：\napiVersion: networking.k8s.io/v1beta1 kind: Ingress metadata:  name: simple-fanout-example  namespace: luffy spec:  rules:  - host: myblog.luffy.com  http:  paths:  - path: /foo  backend:  serviceName: service1  servicePort: 4200  - path: /bar  backend:  serviceName: service2  servicePort: 8080  - path: /  backend:  serviceName: myblog  servicePort: 80  nginx的URL重写\n目标：\nmyblog.luffy.com -\u0026gt; 172.21.51.67 -\u0026gt; /foo/ myblog:80/admin/   实现：\n apiVersion: networking.k8s.io/v1beta1  kind: Ingress  metadata:  name: rewrite-path  namespace: luffy  annotations:  nginx.ingress.kubernetes.io/rewrite-target: /admin/$1  spec:  rules:  - host: myblog.luffy.com  http:  paths:  - path: /foo/(.*)  backend:  serviceName: myblog  servicePort: 80 小结  核心讲如何通过k8s管理业务应用 介绍k8s的架构、核心组件和工作流程，使用kubeadm快速安装k8s集群 定义Pod.yaml，将myblog和mysql打包在同一个Pod中，myblog使用localhost访问mysql mysql数据持久化，为myblog业务应用添加了健康检查和资源限制 将myblog与mysql拆分，使用独立的Pod管理 yaml文件中的环境变量存在账号密码明文等敏感信息，使用configMap和Secret来统一配置，优化部署 只用Pod去直接管理业务应用，对于多副本的需求，很难实现，因此使用Deployment Workload 有了多副本，多个Pod如何去实现LB入口，因此引入了Service的资源类型，有CLusterIp和NodePort ClusterIP是四层的IP地址，不固定，不具备跨环境迁移，因此利用coredns实现集群内服务发现，组件之间直接通过Service名称通信，实现配置的去IP化 对Django应用做改造，django直接使用mysql:3306实现数据库访问 为了实现在集群外部对集群内服务的访问，因此创建NodePort类型的Service 介绍了Service的实现原理，通过kube-proxy利用iptables或者ipvs维护服务访问规则，实现虚拟IP转发到具体Pod的需求 为了实现集群外使用域名访问myblog，因此引入Ingress资源，通过定义访问规则，实现七层代理 考虑真实的场景，对Ingress的使用做了拓展，介绍多path转发及nginx URL重写的实现  ","permalink":"https://iblog.zone/archives/kubernetes%E8%90%BD%E5%9C%B0%E5%AE%9E%E8%B7%B5%E4%B9%8B%E6%97%85/","summary":"第二天 Kubernetes落地实践之旅 本章学习kubernetes的架构及工作流程，重点介绍如何使用Workload管理业务应用的生命周期，实现服务不中断的滚动更新，通过服务发现和集群内负载均衡来实现集群内部的服务间访问，并通过ingress实现外部使用域名访问集群内部的服务。\n学习过程中会逐步对Django项目做k8s改造，从零开始编写所需的资源文件。通过本章的学习，学员会掌握高可用k8s集群的搭建，同时Django demo项目已经可以利用k8s的控制器、服务发现、负载均衡、配置管理等特性来实现生命周期的管理。\n纯容器模式的问题  业务容器数量庞大，哪些容器部署在哪些节点，使用了哪些端口，如何记录、管理，需要登录到每台机器去管理？ 跨主机通信，多个机器中的容器之间相互调用如何做，iptables规则手动维护？ 跨主机容器间互相调用，配置如何写？写死固定IP+端口？ 如何实现业务高可用？多个容器对外提供服务如何实现负载均衡？ 容器的业务中断了，如何可以感知到，感知到以后，如何自动启动新的容器? 如何实现滚动升级保证业务的连续性？ \u0026hellip;\u0026hellip;  容器调度管理平台 Docker Swarm Mesos Google Kubernetes\n2017年开始Kubernetes凭借强大的容器集群管理功能, 逐步占据市场,目前在容器编排领域一枝独秀\nhttps://kubernetes.io/\n架构图 分布式系统，两类角色：管理节点和工作节点\n核心组件   ETCD：分布式高性能键值数据库,存储整个集群的所有元数据\n  ApiServer: API服务器,集群资源访问控制入口,提供restAPI及安全访问控制\n  Scheduler：调度器,负责把业务容器调度到最合适的Node节点\n  Controller Manager：控制器管理,确保集群资源按照期望的方式运行\n Replication Controller Node controller ResourceQuota Controller Namespace Controller ServiceAccount Controller Token Controller Service Controller Endpoints Controller    kubelet：运行在每个节点上的主要的“节点代理”，脏活累活\n pod 管理：kubelet 定期从所监听的数据源获取节点上 pod/container 的期望状态（运行什么容器、运行的副本数量、网络或者存储如何配置等等），并调用对应的容器平台接口达到这个状态。 容器健康检查：kubelet 创建了容器之后还要查看容器是否正常运行，如果容器运行出错，就要根据 pod 设置的重启策略进行处理.","title":"Kubernetes落地实践之旅"},{"content":"ETCD 简介 ETCD 是用于共享配置和服务发现的分布式，一致性的KV存储系统。ETCD是CoreOS公司发起的一个开源项目，授权协议为Apache。\nETCD 使用场景 ETCD 有很多使用场景，包括但不限于：\n 配置管理 服务注册于发现 选主 应用调度 分布式队列 分布式锁  ETCD 存储 k8s 所有数据信息 ETCD 是k8s集群极为重要的一块服务，存储了集群所有的数据信息。同理，如果发生灾难或者 etcd 的数据丢失，都会影响集群数据的恢复。所以，本文重点讲如何备份和恢复数据。\nETCD 一些查询操作  查看集群状态  $ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.pem --cert=/opt/kubernetes/ssl/server.pem --key=/opt/kubernetes/ssl/server-key.pem --endpoints=https://192.168.1.36:2379,https://192.168.1.37:2379,https://192.168.1.38:2379 endpoint health  https://192.168.1.36:2379 is healthy: successfully committed proposal: took = 1.698385ms https://192.168.1.37:2379 is healthy: successfully committed proposal: took = 1.577913ms https://192.168.1.38:2379 is healthy: successfully committed proposal: took = 5.616079ms  获取某个 key 信息  $ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.pem --cert=/opt/kubernetes/ssl/server.pem --key=/opt/kubernetes/ssl/server-key.pem --endpoints=https://192.168.1.36:2379,https://192.168.1.37:2379,https://192.168.1.38:2379 get /registry/apiregistration.k8s.io/apiservices/v1.apps  获取 etcd 版本信息  $ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.pem --cert=/opt/kubernetes/ssl/server.pem --key=/opt/kubernetes/ssl/server-key.pem --endpoints=https://192.168.1.36:2379,https://192.168.1.37:2379,https://192.168.1.38:2379 version  获取 ETCD 所有的 key  $ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.pem --cert=/opt/kubernetes/ssl/server.pem --key=/opt/kubernetes/ssl/server-key.pem --endpoints=https://192.168.1.36:2379,https://192.168.1.37:2379,https://192.168.1.38:2379 get / --prefix --keys-only 环境 主机 IP k8s-master1 192.168.1.36 k8s-master2 192.168.1.37 k8s-master3 192.168.1.38  ETCD version 3.2.12 Kubernetes version v1.15.6 二进制安装  备份 注意：ETCD 不同的版本的 etcdctl 命令不一样，但大致差不多，本文备份使用 napshot save , 每次备份一个节点就行。\n命令备份（k8s-master1 机器上备份）：\n$ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.pem --cert=/opt/kubernetes/ssl/server.pem --key=/opt/kubernetes/ssl/server-key.pem --endpoints=https://192.168.1.36:2379 snapshot save /data/etcd_backup_dir/etcd-snapshot-`date +%Y%m%d`.db 备份脚本（k8s-master1 机器上备份）：\n#!/usr/bin/env bash  date;  CACERT=\u0026#34;/opt/kubernetes/ssl/ca.pem\u0026#34; CERT=\u0026#34;/opt/kubernetes/ssl/server.pem\u0026#34; EKY=\u0026#34;/opt/kubernetes/ssl/server-key.pem\u0026#34; ENDPOINTS=\u0026#34;192.168.1.36:2379\u0026#34;  ETCDCTL_API=3 etcdctl \\ --cacert=\u0026#34;${CACERT}\u0026#34; --cert=\u0026#34;${CERT}\u0026#34; --key=\u0026#34;${EKY}\u0026#34; \\ --endpoints=${ENDPOINTS} \\ snapshot save /data/etcd_backup_dir/etcd-snapshot-`date +%Y%m%d`.db  # 备份保留30天 find /data/etcd_backup_dir/ -name *.db -mtime +30 -exec rm -f {} \\; 恢复 准备工作  停止所有 Master 上 kube-apiserver 服务  $ systemctl stop kube-apiserver  # 确认 kube-apiserver 服务是否停止  $ ps -ef | grep kube-apiserver  停止集群中所有 ETCD 服务  $ systemctl stop etcd  移除所有 ETCD 存储目录下数据  $ mv /var/lib/etcd/default.etcd /var/lib/etcd/default.etcd.bak  拷贝 ETCD 备份快照  # 从 k8s-master1 机器上拷贝备份  $ scp /data/etcd_backup_dir/etcd-snapshot-20191222.db root@k8s-master2:/data/etcd_backup_dir/ $ scp /data/etcd_backup_dir/etcd-snapshot-20191222.db root@k8s-master3:/data/etcd_backup_dir/ 恢复备份 # k8s-master1 机器上操作 $ ETCDCTL_API=3 etcdctl snapshot restore /data/etcd_backup_dir/etcd-snapshot-20191222.db \\  --name etcd-0 \\  --initial-cluster \u0026#34;etcd-0=https://192.168.1.36:2380,etcd-1=https://192.168.1.37:2380,etcd-2=https://192.168.1.38:2380\u0026#34; \\  --initial-cluster-token etcd-cluster \\  --initial-advertise-peer-urls https://192.168.1.36:2380 \\  --data-dir=/var/lib/etcd/default.etcd  # k8s-master2 机器上操作 $ ETCDCTL_API=3 etcdctl snapshot restore /data/etcd_backup_dir/etcd-snapshot-20191222.db \\  --name etcd-1 \\  --initial-cluster \u0026#34;etcd-0=https://192.168.1.36:2380,etcd-1=https://192.168.1.37:2380,etcd-2=https://192.168.1.38:2380\u0026#34; \\  --initial-cluster-token etcd-cluster \\  --initial-advertise-peer-urls https://192.168.1.37:2380 \\  --data-dir=/var/lib/etcd/default.etcd  # k8s-master3 机器上操作 $ ETCDCTL_API=3 etcdctl snapshot restore /data/etcd_backup_dir/etcd-snapshot-20191222.db \\  --name etcd-2 \\  --initial-cluster \u0026#34;etcd-0=https://192.168.1.36:2380,etcd-1=https://192.168.1.37:2380,etcd-2=https://192.168.1.38:2380\u0026#34; \\  --initial-cluster-token etcd-cluster \\  --initial-advertise-peer-urls https://192.168.1.38:2380 \\  --data-dir=/var/lib/etcd/default.etcd 上面三台 ETCD 都恢复完成后，依次登陆三台机器启动 ETCD\n$ systemctl start etcd 三台 ETCD 启动完成，检查 ETCD 集群状态\n$ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.pem --cert=/opt/kubernetes/ssl/server.pem --key=/opt/kubernetes/ssl/server-key.pem --endpoints=https://192.168.1.36:2379,https://192.168.1.37:2379,https://192.168.1.38:2379 endpoint health 三台 ETCD 全部健康，分别到每台 Master 启动 kube-apiserver\n$ systemctl start kube-apiserver 检查 Kubernetes 集群是否恢复正常\n$ kubectl get cs 总结 Kubernetes 集群备份主要是备份 ETCD 集群。而恢复时，主要考虑恢复整个顺序：\n停止kube-apiserver --\u0026gt; 停止ETCD --\u0026gt; 恢复数据 --\u0026gt; 启动ETCD --\u0026gt; 启动kube-apiserve 注意：备份ETCD集群时，只需要备份一个ETCD就行，恢复时，拿同一份备份数据恢复。\n","permalink":"https://iblog.zone/archives/etcd-v3%E5%A4%87%E4%BB%BD%E4%B8%8E%E6%81%A2%E5%A4%8D/","summary":"ETCD 简介 ETCD 是用于共享配置和服务发现的分布式，一致性的KV存储系统。ETCD是CoreOS公司发起的一个开源项目，授权协议为Apache。\nETCD 使用场景 ETCD 有很多使用场景，包括但不限于：\n 配置管理 服务注册于发现 选主 应用调度 分布式队列 分布式锁  ETCD 存储 k8s 所有数据信息 ETCD 是k8s集群极为重要的一块服务，存储了集群所有的数据信息。同理，如果发生灾难或者 etcd 的数据丢失，都会影响集群数据的恢复。所以，本文重点讲如何备份和恢复数据。\nETCD 一些查询操作  查看集群状态  $ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.pem --cert=/opt/kubernetes/ssl/server.pem --key=/opt/kubernetes/ssl/server-key.pem --endpoints=https://192.168.1.36:2379,https://192.168.1.37:2379,https://192.168.1.38:2379 endpoint health  https://192.168.1.36:2379 is healthy: successfully committed proposal: took = 1.698385ms https://192.168.1.37:2379 is healthy: successfully committed proposal: took = 1.577913ms https://192.168.1.38:2379 is healthy: successfully committed proposal: took = 5.616079ms  获取某个 key 信息  $ ETCDCTL_API=3 etcdctl --cacert=/opt/kubernetes/ssl/ca.","title":"Etcd v3备份与恢复"},{"content":"第一天 走进Docker的世界 介绍docker的前世今生，了解docker的实现原理，以Django项目为例，带大家如何编写最佳的Dockerfile构建镜像。通过本章的学习，大家会知道docker的概念及基本操作，并学会构建自己的业务镜像，并通过抓包的方式掌握Docker最常用的bridge网络模式的通信。\n认识docker  why what how  为什么出现docker 需要一种轻量、高效的虚拟化能力\nDocker 公司位于旧金山,原名dotCloud，底层利用了Linux容器技术（LXC）（在操作系统中实现资源隔离与限制）。为了方便创建和管理这些容器，dotCloud 开发了一套内部工具，之后被命名为“Docker”。Docker就是这样诞生的。\nHypervisor： 一种运行在基础物理服务器和操作系统之间的中间软件层，可允许多个操作系统和应用共享硬件 。常见的VMware的 Workstation 、ESXi、微软的Hyper-V或者思杰的XenServer。\nContainer Runtime：通过Linux内核虚拟化能力管理多个容器，多个容器共享一套操作系统内核。因此摘掉了内核占用的空间及运行所需要的耗时，使得容器极其轻量与快速。\n什么是docker 基于操作系统内核，提供轻量级虚拟化功能的CS架构的软件产品。\n基于轻量的特性，解决软件交付过程中的环境依赖\ndocker能做什么  可以把应用程序代码及运行依赖环境打包成镜像，作为交付介质，在各环境部署 可以将镜像（image）启动成为容器(container)，并且提供多容器的生命周期进行管理（启、停、删） container容器之间相互隔离，且每个容器可以设置资源限额 提供轻量级虚拟化功能，容器就是在宿主机中的一个个的虚拟的空间，彼此相互隔离，完全独立  版本管理  Docker 引擎主要有两个版本：企业版（EE）和社区版（CE） 每个季度(1-3,4-6,7-9,10-12)，企业版和社区版都会发布一个稳定版本(Stable)。社区版本会提供 4 个月的支持，而企业版本会提供 12 个月的支持 每个月社区版还会通过 Edge 方式发布月度版 从 2017 年第一季度开始，Docker 版本号遵循 YY.MM-xx 格式，类似于 Ubuntu 等项目。例如，2018 年 6 月第一次发布的社区版本为 18.06.0-ce  发展史 13年成立，15年开始，迎来了飞速发展。\nDocker 1.8之前，使用LXC，Docker在上层做了封装， 把LXC复杂的容器创建与使用方式简化为自己的一套命令体系。\n之后，为了实现跨平台等复杂的场景，Docker抽出了libcontainer项目，把对namespace、cgroup的操作封装在libcontainer项目里，支持不同的平台类型。\n2015年6月，Docker牵头成立了 OCI（Open Container Initiative开放容器计划）组织，这个组织的目的是建立起一个围绕容器的通用标准 。 容器格式标准是一种不受上层结构绑定的协议，即不限于某种特定操作系统、硬件、CPU架构、公有云等 ， 允许任何人在遵循该标准的情况下开发应用容器技术，这使得容器技术有了一个更广阔的发展空间。\nOCI成立后，libcontainer 交给OCI组织来维护，但是libcontainer中只包含了与kernel交互的库，因此基于libcontainer项目，后面又加入了一个CLI工具，并且项目改名为runC (https://github.com/opencontainers/runc )， 目前runC已经成为一个功能强大的runtime工具。\nDocker也做了架构调整。将容器运行时相关的程序从docker daemon剥离出来，形成了containerd。containerd向上为Docker Daemon提供了gRPC接口，使得Docker Daemon屏蔽下面的结构变化，确保原有接口向下兼容。向下通过containerd-shim结合runC，使得引擎可以独立升级，避免之前Docker Daemon升级会导致所有容器不可用的问题。\n也就是说\n runC（libcontainer）是符合OCI标准的一个实现，与底层系统交互 containerd是实现了OCI之上的容器的高级功能，比如镜像管理、容器执行的调用等 Dockerd目前是最上层与CLI交互的进程，接收cli的请求并与containerd协作  小结  为了提供一种更加轻量的虚拟化技术，docker出现了 借助于docker容器的轻、快等特性，解决了软件交付过程中的环境依赖问题，使得docker得以快速发展 Docker是一种CS架构的软件产品，可以把代码及依赖打包成镜像，作为交付介质，并且把镜像启动成为容器，提供容器生命周期的管理 docker-ce，每季度发布stable版本。18.06，18.09，19.03 发展至今，docker已经通过制定OCI标准对最初的项目做了拆分，其中runC和containerd是docker的核心项目，理解docker整个请求的流程，对我们深入理解docker有很大的帮助  安装 配置宿主机网卡转发 ## 若未配置，需要执行如下 $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/docker.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 EOF $ sysctl -p /etc/sysctl.d/docker.conf Yum安装配置docker ## 下载阿里源repo文件 $ curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo $ curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo  $ yum clean all \u0026amp;\u0026amp; yum makecache ## yum安装 $ yum install docker-ce-20.10.6 -y ## 查看源中可用版本 $ yum list docker-ce --showduplicates | sort -r ## 安装旧版本 ##yum install -y docker-ce-18.09.9  ## 配置源加速 ## https://cr.console.aliyun.com/cn-hangzhou/instances/mirrors mkdir -p /etc/docker vi /etc/docker/daemon.json {  \u0026#34;registry-mirrors\u0026#34; : [  \u0026#34;https://8xpk5wnt.mirror.aliyuncs.com\u0026#34;  ] }  ## 设置开机自启 systemctl enable docker systemctl daemon-reload  ## 启动docker systemctl start docker  ## 查看docker信息 docker info  ## docker-client which docker ## docker daemon ps aux |grep docker ## containerd ps aux|grep containerd systemctl status containerd 核心要素及常用操作详解 三大核心要素：镜像(Image)、容器(Container)、仓库(Registry)\n镜像（Image） 打包了业务代码及运行环境的包，是静态的文件，不能直接对外提供服务。\n容器（Container） 镜像的运行时，可以对外提供服务。\n仓库（Registry） 存放镜像的地方\n 公有仓库，Docker Hub，阿里，网易\u0026hellip; 私有仓库，企业内部搭建  Docker Registry，Docker官方提供的镜像仓库存储服务 Harbor, 是Docker Registry的更高级封装，它除了提供友好的Web UI界面，角色和用户权限管理，用户操作审计等功能   镜像访问地址形式 registry.devops.com/demo/hello:latest,若没有前面的url地址，则默认寻找Docker Hub中的镜像，若没有tag标签，则使用latest作为标签。 比如，docker pull nginx，会被解析成docker.io/library/nginx:latest 公有的仓库中，一般存在这么几类镜像  操作系统基础镜像（centos，ubuntu，suse，alpine） 中间件（nginx，redis，mysql，tomcat） 语言编译环境（python，java，golang） 业务镜像（django-demo\u0026hellip;）    容器和仓库不会直接交互，都是以镜像为载体来操作。\n  查看镜像列表\n$ docker images   如何获取镜像\n  从远程仓库拉取\n$ docker pull nginx:alpine $ docker images   使用tag命令\n$ docker tag nginx:alpine 172.21.51.143:5000/nginx:alpine $ docker images   本地构建\n$ docker build . -t my-nginx:ubuntu -f Dockerfile     如何通过镜像启动容器\n$ docker run --name my-nginx-alpine -d nginx:alpine   如何知道容器内部运行了什么程序？\n# 进入容器内部,分配一个tty终端 $ docker exec -ti my-nginx-alpine /bin/sh # ps aux   docker怎么知道容器启动后该执行什么命令？\n通过docker build来模拟构建一个nginx的镜像，\n  创建Dockerfile\n# 告诉docker使用哪个基础镜像作为模板，后续命令都以这个镜像为基础  FROM ubuntu # RUN命令会在上面指定的镜像里执行命令  RUN apt-get update \u0026amp;\u0026amp; apt install -y nginx #告诉docker，启动容器时执行如下命令 CMD [\u0026#34;/usr/sbin/nginx\u0026#34;, \u0026#34;-g\u0026#34;,\u0026#34;daemon off;\u0026#34;]   构建本地镜像\n$ docker build . -t my-nginx:ubuntu -f Dockerfile       使用新镜像启动容器\n $ docker run --name my-nginx-ubuntu -d my-nginx:ubuntu   进入容器查看进程\n $ docker exec -ti my-nginx-ubuntu /bin/sh  # ps aux     如何访问容器内服务\n# 进入容器内部 $ docker exec -ti my-nginx-alpine /bin/sh # ps aux|grep nginx # curl localhost:80   宿主机中如何访问容器服务\n# 删掉旧服务,重新启动 $ docker rm -f my-nginx-alpine $ docker run --name my-nginx-alpine -d -p 8080:80 nginx:alpine $ curl 172.21.51.143:8080   docker client如何与daemon通信\n# /var/run/docker.sock $ docker run --name portainer -d -p 9001:9000 -v /var/run/docker.sock:/var/run/docker.sock portainer/portainer   操作演示  查看所有镜像：  $ docker images  拉取镜像:  $ docker pull nginx:alpine  如何唯一确定镜像:   image_id repository:tag  $ docker images REPOSITORY TAG IMAGE ID CREATED SIZE nginx alpine 377c0837328f 2 weeks ago 19.7MB   导出镜像到文件中\n$ docker save -o nginx-alpine.tar nginx:alpine   从文件中加载镜像\n$ docker load -i nginx-alpine.tar   部署镜像仓库\nhttps://docs.docker.com/registry/\n## 使用docker镜像启动镜像仓库服务 $ docker run -d -p 5000:5000 --restart always --name registry registry:2  ## 默认仓库不带认证，若需要认证，参考https://docs.docker.com/registry/deploying/#restricting-access   推送本地镜像到镜像仓库中\n$ docker tag nginx:alpine localhost:5000/nginx:alpine $ docker push localhost:5000/nginx:alpine ## 查看仓库内元数据 $ curl -X GET http://172.21.51.143:5000/v2/_catalog $ curl -X GET http://172.21.51.143:5000/v2/nginx/tags/list ## 镜像仓库给外部访问，不能通过localhost，尝试使用内网地址172.21.51.143:5000/nginx:alpine $ docker tag nginx:alpine 172.21.51.143:5000/nginx:alpine $ docker push 172.21.51.143:5000/nginx:alpine The push refers to repository [172.21.51.143:5000/nginx] Get https://172.21.51.143:5000/v2/: http: server gave HTTP response to HTTPS client ## docker默认不允许向http的仓库地址推送，如何做成https的，参考：https://docs.docker.com/registry/deploying/#run-an-externally-accessible-registry ## 我们没有可信证书机构颁发的证书和域名，自签名证书需要在每个节点中拷贝证书文件，比较麻烦，因此我们通过配置daemon的方式，来跳过证书的验证： $ cat /etc/docker/daemon.json {  \u0026#34;registry-mirrors\u0026#34;: [  \u0026#34;https://8xpk5wnt.mirror.aliyuncs.com\u0026#34;  ],  \u0026#34;insecure-registries\u0026#34;: [  \u0026#34;172.21.51.143:5000\u0026#34;  ] } $ systemctl restart docker $ docker push 172.21.51.143:5000/nginx:alpine $ docker images # IMAGE ID相同，等于起别名或者加快捷方式 REPOSITORY TAG IMAGE ID CREATED SIZE 172.21.51.143:5000/nginx alpine 377c0837328f 4 weeks ago nginx alpine 377c0837328f 4 weeks ago localhost:5000/nginx alpine 377c0837328f 4 weeks ago registry 2 708bc6af7e5e 2 months ago   删除镜像\n$ docker rmi nginx:alpine   查看容器列表\n## 查看运行状态的容器列表 $ docker ps ## 查看全部状态的容器列表 $ docker ps -a   启动容器\n## 后台启动 $ docker run --name nginx -d nginx:alpine ## 映射端口,把容器的端口映射到宿主机中,-p \u0026lt;host_port\u0026gt;:\u0026lt;container_port\u0026gt; $ docker run --name nginx -d -p 8080:80 nginx:alpine ## 资源限制,最大可用内存500M $ docker run --memory=500m nginx:alpine   容器数据持久化\n## 挂载主机目录 $ docker run --name nginx -d -v /opt:/opt nginx:alpine $ docker run --name mysql -e MYSQL_ROOT_PASSWORD=123456 -d -v /opt/mysql/:/var/lib/mysql mysql:5.7   进入容器或者执行容器内的命令\n $ docker exec -ti \u0026lt;container_id_or_name\u0026gt; /bin/sh  $ docker exec \u0026lt;container_id_or_name\u0026gt; hostname   主机与容器之间拷贝数据\n ## 主机拷贝到容器  $ echo \u0026#39;123\u0026#39;\u0026gt;/tmp/test.txt  $ docker cp /tmp/test.txt nginx:/tmp  $ docker exec -ti nginx cat /tmp/test.txt  123  ## 容器拷贝到主机  $ docker cp nginx:/tmp/test.txt ./   挂载已有的数据，重新创建镜像仓库容器\n## 解压离线镜像文件 $ tar zxf registry.tar.gz -C /opt ## 删除当前镜像仓库容器 $ docker rm -f registry ## 使用docker镜像启动镜像仓库服务 $ docker run -d -p 5000:5000 --restart always -v /opt/registry:/var/lib/registry --name registry registry:2 假设启动镜像仓库服务的主机地址为172.21.51.143，该目录中已存在的镜像列表：\n   现镜像仓库地址 原镜像仓库地址     172.21.51.143:5000/coreos/flannel:v0.11.0-amd64 quay.io/coreos/flannel:v0.11.0-amd64   172.21.51.143:5000/mysql:5.7 mysql:5.7   172.21.51.143:5000/nginx:alpine nginx:alpine   172.21.51.143:5000/centos:centos7.5.1804 centos:centos7.5.1804   172.21.51.143:5000/elasticsearch/elasticsearch:7.4.2 docker.elastic.co/elasticsearch/elasticsearch:7.4.2   172.21.51.143:5000/fluentd-es-root:v1.6.2-1.0 quay.io/fluentd_elasticsearch/fluentd:v2.5.2   172.21.51.143:5000/kibana/kibana:7.4.2 docker.elastic.co/kibana/kibana:7.4.2   172.21.51.143:5000/kubernetesui/dashboard:v2.0.0-beta5 kubernetesui/dashboard:v2.0.0-beta5   172.21.51.143:5000/kubernetesui/metrics-scraper:v1.0.1 kubernetesui/metrics-scraper:v1.0.1   172.21.51.143:5000/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0 quay.io/kubernetes-ingress-controller/nginx-ingress-controller:0.30.0   172.21.51.143:5000/jenkinsci/blueocean:latest jenkinsci/blueocean:latest   172.21.51.143:5000/sonarqube:7.9-community sonarqube:7.9-community   172.21.51.143:5000/postgres:11.4 postgres:11.4      查看容器日志\n ## 查看全部日志  $ docker logs nginx  ## 实时查看最新日志  $ docker logs -f nginx  ## 从最新的100条开始查看  $ docker logs --tail=100 -f nginx   停止或者删除容器\n ## 停止运行中的容器  $ docker stop nginx  ## 启动退出容器  $ docker start nginx  ## 删除非运行中状态的容器  $ docker rm nginx  ## 删除运行中的容器  $ docker rm -f nginx   查看容器或者镜像的明细\n ## 查看容器详细信息，包括容器IP地址等  $ docker inspect nginx  ## 查看镜像的明细信息  $ docker inspect nginx:alpine   Dockerfile使用 $ docker build . -t ImageName:ImageTag -f Dockerfile Dockerfile是一堆指令，在docker build的时候，按照该指令进行操作，最终生成我们期望的镜像\n  FROM 指定基础镜像，必须为第一个命令\n格式： FROM \u0026lt;image\u0026gt; FROM \u0026lt;image\u0026gt;:\u0026lt;tag\u0026gt;示例： FROM mysql:5.7注意： tag是可选的，如果不使用tag时，会使用latest版本的基础镜像  MAINTAINER 镜像维护者的信息\n格式：\tMAINTAINER \u0026lt;name\u0026gt;示例：\tMAINTAINER Yongxin Li MAINTAINER inspur_lyx@hotmail.com MAINTAINER Yongxin Li \u0026lt;inspur_lyx@hotmail.com\u0026gt;  COPY|ADD 添加本地文件到镜像中\n格式： COPY \u0026lt;src\u0026gt;... \u0026lt;dest\u0026gt;示例： ADD hom* /mydir/ # 添加所有以\u0026#34;hom\u0026#34;开头的文件 ADD test relativeDir/ # 添加 \u0026#34;test\u0026#34; 到 `WORKDIR`/relativeDir/ ADD test /absoluteDir/ # 添加 \u0026#34;test\u0026#34; 到 /absoluteDir/  WORKDIR 工作目录\n格式：\tWORKDIR /path/to/workdir 示例： WORKDIR /a (这时工作目录为/a)注意： 通过WORKDIR设置工作目录后，Dockerfile中其后的命令RUN、CMD、ENTRYPOINT、ADD、COPY等命令都会在该目录下执行  RUN 构建镜像过程中执行命令\n格式： RUN \u0026lt;command\u0026gt;示例： RUN yum install nginx RUN pip install django RUN mkdir test \u0026amp;\u0026amp; rm -rf /var/lib/unusedfiles注意： RUN指令创建的中间镜像会被缓存，并会在下次构建中使用。如果不想使用这些缓存镜像，可以在构建时指定--no-cache参数，如：docker build --no-cache  CMD 构建容器后调用，也就是在容器启动时才进行调用\n格式： CMD [\u0026#34;executable\u0026#34;,\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (执行可执行文件，优先) CMD [\u0026#34;param1\u0026#34;,\u0026#34;param2\u0026#34;] (设置了ENTRYPOINT，则直接调用ENTRYPOINT添加参数) CMD command param1 param2 (执行shell内部命令)示例： CMD [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;] CMD ping www.baidu.com注意： CMD不同于RUN，CMD用于指定在容器启动时所要执行的命令，而RUN用于指定镜像构建时所要执行的命令。  ENTRYPOINT 设置容器初始化命令，使其可执行化\n格式： ENTRYPOINT [\u0026#34;executable\u0026#34;, \u0026#34;param1\u0026#34;, \u0026#34;param2\u0026#34;] (可执行文件, 优先) ENTRYPOINT command param1 param2 (shell内部命令)示例： ENTRYPOINT [\u0026#34;/usr/bin/wc\u0026#34;,\u0026#34;--help\u0026#34;]注意：\tENTRYPOINT与CMD非常类似，不同的是通过docker run执行的命令不会覆盖ENTRYPOINT，而docker run命令中指定的任何参数，都会被当做参数再次传递给ENTRYPOINT。Dockerfile中只允许有一个ENTRYPOINT命令，多指定时会覆盖前面的设置，而只执行最后的ENTRYPOINT指令  ENV\n格式： ENV \u0026lt;key\u0026gt; \u0026lt;value\u0026gt; ENV \u0026lt;key\u0026gt;=\u0026lt;value\u0026gt;示例： ENV myName John ENV myCat=fluffy  EXPOSE\n格式： EXPOSE \u0026lt;port\u0026gt; [\u0026lt;port\u0026gt;...]示例： EXPOSE 80 443 EXPOSE 8080 EXPOSE 11211/tcp 11211/udp注意： EXPOSE并不会让容器的端口访问到主机。要使其可访问，需要在docker run运行容器时通过-p来发布这些端口，或通过-P参数来发布EXPOSE导出的所有端口  基础环境镜像\nFROMjava:8-alpineRUN apk add --update ca-certificates \u0026amp;\u0026amp; rm -rf /var/cache/apk/* \u0026amp;\u0026amp; \\  find /usr/share/ca-certificates/mozilla/ -name \u0026#34;*.crt\u0026#34; -exec keytool -import -trustcacerts \\  -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts -storepass changeit -noprompt \\  -file {} -alias {} \\; \u0026amp;\u0026amp; \\  keytool -list -keystore /usr/lib/jvm/java-1.8-openjdk/jre/lib/security/cacerts --storepass changeitENV MAVEN_VERSION 3.5.4ENV MAVEN_HOME /usr/lib/mvnENV PATH $MAVEN_HOME/bin:$PATHRUN wget http://archive.apache.org/dist/maven/maven-3/$MAVEN_VERSION/binaries/apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026amp;\u0026amp; \\  tar -zxvf apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026amp;\u0026amp; \\  rm apache-maven-$MAVEN_VERSION-bin.tar.gz \u0026amp;\u0026amp; \\  mv apache-maven-$MAVEN_VERSION /usr/lib/mvnRUN mkdir -p /usr/src/appWORKDIR/usr/src/app  前端镜像\nFROMnginx:1.19.0-alpineLABEL maintainer=\u0026#34;mritd \u0026lt;mritd@linux.com\u0026gt;\u0026#34;ARG TZ=\u0026#39;Asia/Shanghai\u0026#39;ENV TZ ${TZ}RUN apk upgrade --update \\  \u0026amp;\u0026amp; apk add bash tzdata curl wget ca-certificates \\  \u0026amp;\u0026amp; ln -sf /usr/share/zoneinfo/${TZ} /etc/localtime \\  \u0026amp;\u0026amp; echo ${TZ} \u0026gt; /etc/timezone \\  \u0026amp;\u0026amp; rm -rf /usr/share/nginx/html /var/cache/apk/*COPY landscape-animation-experiment /usr/share/nginx/htmlEXPOSE80 443CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;]  java镜像\nFROMjava:8u111ENV JAVA_OPTS \u0026#34;\\ -Xmx4096m \\ -XX:MetaspaceSize=256m \\ -XX:MaxMetaspaceSize=256m\u0026#34;ENV JAVA_HOME /usr/java/jdkENV PATH ${PATH}:${JAVA_HOME}/binCOPY target/myapp.jar myapp.jarRUN ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtimeRUN echo \u0026#39;Asia/Shanghai\u0026#39; \u0026gt;/etc/timezoneEXPOSE9000CMD java ${JAVA_OPTS} -jar myapp.jar  golang镜像\n多阶段构建\n  多阶构建 https://gitee.com/agagin/href-counter.git\n原始构建：\nFROMgolang:1.13WORKDIR/go/src/github.com/alexellis/href-counter/COPY vendor vendorCOPY app.go .ENV GOPROXY https://goproxy.cnRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .$ docker build . -t href-counter:v1 -f Dockerfile多阶构建：\nFROMgolang:1.13 AS builderWORKDIR/go/src/github.com/alexellis/href-counter/COPY vendor vendorCOPY app.go .ENV GOPROXY https://goproxy.cnRUN CGO_ENABLED=0 GOOS=linux go build -a -installsuffix cgo -o app .FROMalpine:3.10RUN apk --no-cache add ca-certificatesWORKDIR/root/COPY --from=builder /go/src/github.com/alexellis/href-counter/app .CMD [\u0026#34;./app\u0026#34;]$ docker build . -t href-counter:v2 -f Dockerfile.multi原则：\n 不必要的内容不要放在镜像中 减少不必要的层文件 减少网络传输操作 可以适当的包含一些调试命令  通过1号进程理解容器的本质 $ docker exec -ti my-nginx-alpine /bin/sh #/ ps aux 容器启动的时候可以通过命令去覆盖默认的CMD\n$ docker run -d --name xxx nginx:alpine \u0026lt;自定义命令\u0026gt; # \u0026lt;自定义命令\u0026gt;会覆盖镜像中指定的CMD指令，作为容器的1号进程启动。  $ docker run -d --name test-3 nginx:alpine echo 123  $ docker run -d --name test-4 nginx:alpine ping www.luffycity.com 本质上讲容器是利用namespace和cgroup等技术在宿主机中创建的独立的虚拟空间，这个空间内的网络、进程、挂载等资源都是隔离的。\n$ docker exec -ti my-nginx /bin/sh #/ ip addr #/ ls -l / #/ apt install xxx #/ #安装的软件对宿主机和其他容器没有任何影响，和虚拟机不同的是，容器间共享一个内核，所以容器内没法升级内核 Django应用容器化实践 django项目介绍  项目地址：https://gitee.com/agagin/python-demo.git python3 + django + uwsgi + nginx + mysql 内部服务端口8002  容器化Django项目 dockerfiles/myblog/Dockerfile\n# This my first django Dockerfile# Version 1.0# Base images 基础镜像FROMcentos:centos7.5.1804#MAINTAINER 维护者信息LABEL maintainer=\u0026#34;inspur_lyx@hotmail.com\u0026#34;#ENV 设置环境变量ENV LANG en_US.UTF-8ENV LC_ALL en_US.UTF-8#RUN 执行以下命令RUN curl -so /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo \u0026amp;\u0026amp; rpm -Uvh http://nginx.org/packages/centos/7/noarch/RPMS/nginx-release-centos-7-0.el7.ngx.noarch.rpmRUN yum install -y python36 python3-devel gcc pcre-devel zlib-devel make net-tools nginx#工作目录WORKDIR/opt/myblog#拷贝文件至工作目录COPY . .# 拷贝nginx配置文件COPY myblog.conf /etc/nginx#安装依赖的插件RUN pip3 install -i http://mirrors.aliyun.com/pypi/simple/ --trusted-host mirrors.aliyun.com -r requirements.txtRUN chmod +x run.sh \u0026amp;\u0026amp; rm -rf ~/.cache/pip#EXPOSE 映射端口EXPOSE8002#容器启动时执行命令CMD [\u0026#34;./run.sh\u0026#34;]执行构建：\n$ docker build . -t myblog:v1 -f Dockerfile 运行mysql $ docker run -d -p 3306:3306 --name mysql -v /opt/mysql:/var/lib/mysql -e MYSQL_DATABASE=myblog -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 --character-set-server=utf8mb4 --collation-server=utf8mb4_unicode_ci  ## 参数传递 ## 查看数据库 $ docker exec -ti mysql bash #/ mysql -uroot -p123456 #/ show databases;  ## navicator连接 启动Django应用 ## 启动容器 $ docker run -d -p 8002:8002 --name myblog -e MYSQL_HOST=172.21.51.143 -e MYSQL_USER=root -e MYSQL_PASSWD=123456 myblog:v1  ## migrate $ docker exec -ti myblog bash #/ python3 manage.py makemigrations #/ python3 manage.py migrate #/ python3 manage.py createsuperuser  ## 创建超级用户 $ docker exec -ti myblog python3 manage.py createsuperuser  ## 收集静态文件 ## $ docker exec -ti myblog python3 manage.py collectstatic 访问172.21.51.143:8002/admin\n实现原理 docker优势：\n  轻量级的虚拟化\n  容器快速启停\n  虚拟化核心需要解决的问题：资源隔离与资源限制\n 虚拟机硬件虚拟化技术， 通过一个 hypervisor 层实现对资源的彻底隔离。 容器则是操作系统级别的虚拟化，利用的是内核的 Cgroup 和 Namespace 特性，此功能完全通过软件实现。  Namespace 资源隔离 命名空间是全局资源的一种抽象，将资源放到不同的命名空间中，各个命名空间中的资源是相互隔离的。\n   分类 系统调用参数 相关内核版本     Mount namespaces CLONE_NEWNS Linux 2.4.19   UTS namespaces CLONE_NEWUTS Linux 2.6.19   IPC namespaces CLONE_NEWIPC Linux 2.6.19   PID namespaces CLONE_NEWPID Linux 2.6.24   Network namespaces CLONE_NEWNET 始于Linux 2.6.24 完成于 Linux 2.6.29   User namespaces CLONE_NEWUSER 始于 Linux 2.6.23 完成于 Linux 3.8    我们知道，docker容器对于操作系统来讲其实是一个进程，我们可以通过原始的方式来模拟一下容器实现资源隔离的基本原理：\nlinux系统中，通常可以通过clone()实现进程创建的系统调用 ，原型如下：\nint clone(int (*child_func)(void *), void *child_stack, int flags, void *arg);  child_func : 传入子进程运行的程序主函数。 child_stack : 传入子进程使用的栈空间。 flags : 表示使用哪些 CLONE_* 标志位。 args : 用于传入用户参数。  示例一：实现进程独立的UTS空间\n#define _GNU_SOURCE #include \u0026lt;sys/mount.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/wait.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;sched.h\u0026gt;#include \u0026lt;signal.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#define STACK_SIZE (1024 * 1024) static char container_stack[STACK_SIZE]; char* const container_args[] = {  \u0026#34;/bin/bash\u0026#34;,  NULL };  int container_main(void* arg) {  printf(\u0026#34;Container - inside the container!\\n\u0026#34;);  sethostname(\u0026#34;container\u0026#34;,10); /* 设置hostname */  execv(container_args[0], container_args);  printf(\u0026#34;Something\u0026#39;s wrong!\\n\u0026#34;);  return 1; }  int main() {  printf(\u0026#34;Parent - start a container!\\n\u0026#34;);  int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWUTS | SIGCHLD , NULL);  waitpid(container_pid, NULL, 0);  printf(\u0026#34;Parent - container stopped!\\n\u0026#34;);  return 0; } 执行编译并测试：\n$ gcc -o ns_uts ns_uts.c $ ./ns_uts $ hostname 示例二：实现容器独立的进程空间\n#define _GNU_SOURCE #include \u0026lt;sys/mount.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/wait.h\u0026gt;#include \u0026lt;stdio.h\u0026gt;#include \u0026lt;sched.h\u0026gt;#include \u0026lt;signal.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#define STACK_SIZE (1024 * 1024) static char container_stack[STACK_SIZE]; char* const container_args[] = {  \u0026#34;/bin/bash\u0026#34;,  NULL };  int container_main(void* arg) {  printf(\u0026#34;Container [%5d] - inside the container!\\n\u0026#34;, getpid());  sethostname(\u0026#34;container\u0026#34;,10); /* 设置hostname */  execv(container_args[0], container_args);  printf(\u0026#34;Something\u0026#39;s wrong!\\n\u0026#34;);  return 1; }  int main() {  printf(\u0026#34;Parent [%5d] - start a container!\\n\u0026#34;, getpid());  int container_pid = clone(container_main, container_stack+STACK_SIZE, CLONE_NEWUTS | CLONE_NEWPID | SIGCHLD , NULL);  waitpid(container_pid, NULL, 0);  printf(\u0026#34;Parent - container stopped!\\n\u0026#34;);  return 0; } 执行编译并测试：\n$ gcc -o ns_pid ns_pid.c $ ./ns_pid $ echo $$ 如何确定进程是否属于同一个namespace：\n$ ./ns_pid Parent [ 8061] - start a container! $ pstree -p 8061 pid1(8061)───bash(8062)───pstree(8816) $ ls -l /proc/8061/ns lrwxrwxrwx 1 root root 0 Jun 24 12:51 ipc -\u0026gt; ipc:[4026531839] lrwxrwxrwx 1 root root 0 Jun 24 12:51 mnt -\u0026gt; mnt:[4026531840] lrwxrwxrwx 1 root root 0 Jun 24 12:51 net -\u0026gt; net:[4026531968] lrwxrwxrwx 1 root root 0 Jun 24 12:51 pid -\u0026gt; pid:[4026531836] lrwxrwxrwx 1 root root 0 Jun 24 12:51 user -\u0026gt; user:[4026531837] lrwxrwxrwx 1 root root 0 Jun 24 12:51 uts -\u0026gt; uts:[4026531838] $ ls -l /proc/8062/ns lrwxrwxrwx 1 root root 0 Jun 24 12:51 ipc -\u0026gt; ipc:[4026531839] lrwxrwxrwx 1 root root 0 Jun 24 12:51 mnt -\u0026gt; mnt:[4026531840] lrwxrwxrwx 1 root root 0 Jun 24 12:51 net -\u0026gt; net:[4026531968] lrwxrwxrwx 1 root root 0 Jun 24 12:51 pid -\u0026gt; pid:[4026534845] lrwxrwxrwx 1 root root 0 Jun 24 12:51 user -\u0026gt; user:[4026531837] lrwxrwxrwx 1 root root 0 Jun 24 12:51 uts -\u0026gt; uts:[4026534844]  ## 发现pid和uts是和父进程使用了不同的ns，其他的则是继承了父进程的命名空间 综上：通俗来讲，docker在启动一个容器的时候，会调用Linux Kernel Namespace的接口，来创建一块虚拟空间，创建的时候，可以支持设置下面这几种（可以随意选择）,docker默认都设置。\n pid：用于进程隔离（PID：进程ID） net：管理网络接口（NET：网络） ipc：管理对 IPC 资源的访问（IPC：进程间通信（信号量、消息队列和共享内存）） mnt：管理文件系统挂载点（MNT：挂载） uts：隔离主机名和域名 user：隔离用户和用户组  CGroup 资源限制 通过namespace可以保证容器之间的隔离，但是无法控制每个容器可以占用多少资源， 如果其中的某一个容器正在执行 CPU 密集型的任务，那么就会影响其他容器中任务的性能与执行效率，导致多个容器相互影响并且抢占资源。如何对多个容器的资源使用进行限制就成了解决进程虚拟资源隔离之后的主要问题。\nControl Groups（简称 CGroups）\n cgroups是Linux内核提供的一种机制，这种机制可以根据需求吧一系列系统任务及其子任务整合(或分隔)到按资源划分等级的不同组中，从而为系统资源管理提供一个统一的框架。\n CGroups能够隔离宿主机器上的物理资源，例如 CPU、内存、磁盘 I/O 。每一个 CGroup 都是一组被相同的标准和参数限制的进程。而我们需要做的，其实就是把容器这个进程加入到指定的Cgroup中。深入理解CGroup，请点此。\nUnionFS 联合文件系统 Linux namespace和cgroup分别解决了容器的资源隔离与资源限制，那么容器是很轻量的，通常每台机器中可以运行几十上百个容器， 这些个容器是共用一个image，还是各自将这个image复制了一份，然后各自独立运行呢？ 如果每个容器之间都是全量的文件系统拷贝，那么会导致至少如下问题：\n 运行容器的速度会变慢 容器和镜像对宿主机的磁盘空间的压力  怎么解决这个问题\u0026mdash;\u0026mdash;Docker的存储驱动\n 镜像分层存储 UnionFS  Docker 镜像是由一系列的层组成的，每层代表 Dockerfile 中的一条指令，比如下面的 Dockerfile 文件：\nFROMubuntu:15.04COPY . /appRUN make /appCMD python /app/app.py这里的 Dockerfile 包含4条命令，其中每一行就创建了一层，下面显示了上述Dockerfile构建出来的镜像运行的容器层的结构：\n镜像就是由这些层一层一层堆叠起来的，镜像中的这些层都是只读的，当我们运行容器的时候，就可以在这些基础层至上添加新的可写层，也就是我们通常说的容器层，对于运行中的容器所做的所有更改（比如写入新文件、修改现有文件、删除文件）都将写入这个容器层。\n对容器层的操作，主要利用了写时复制（CoW）技术。CoW就是copy-on-write，表示只在需要写时才去复制，这个是针对已有文件的修改场景。 CoW技术可以让所有的容器共享image的文件系统，所有数据都从image中读取，只有当要对文件进行写操作时，才从image里把要写的文件复制到自己的文件系统进行修改。所以无论有多少个容器共享同一个image，所做的写操作都是对从image中复制到自己的文件系统中的复本上进行，并不会修改image的源文件，且多个容器操作同一个文件，会在每个容器的文件系统里生成一个复本，每个容器修改的都是自己的复本，相互隔离，相互不影响。使用CoW可以有效的提高磁盘的利用率。\n镜像中每一层的文件都是分散在不同的目录中的，如何把这些不同目录的文件整合到一起呢？\nUnionFS 其实是一种为 Linux 操作系统设计的用于把多个文件系统联合到同一个挂载点的文件系统服务。 它能够将不同文件夹中的层联合（Union）到了同一个文件夹中，整个联合的过程被称为联合挂载（Union Mount）。\n上图是AUFS的实现，AUFS是作为Docker存储驱动的一种实现，Docker 还支持了不同的存储驱动，包括 aufs、devicemapper、overlay2、zfs 和 Btrfs 等等，在最新的 Docker 中，overlay2 取代了 aufs 成为了推荐的存储驱动，但是在没有 overlay2 驱动的机器上仍然会使用 aufs 作为 Docker 的默认驱动。\nDocker网络 docker容器是一块具有隔离性的虚拟系统，容器内可以有自己独立的网络空间，\n 多个容器之间是如何实现通信的呢？ 容器和宿主机之间又是如何实现的通信呢？ 使用-p参数是怎么实现的端口映射?  带着这些问题，我们来学习一下docker的网络模型，最后我会通过抓包的方式，给大家演示一下数据包在容器和宿主机之间的转换过程。\n网络模式 我们在使用docker run创建Docker容器时，可以用\u0026ndash;net选项指定容器的网络模式，Docker有以下4种网络模式：\n  bridge模式，使用\u0026ndash;net=bridge指定，默认设置\n  host模式，使用\u0026ndash;net=host指定，容器内部网络空间共享宿主机的空间，效果类似直接在宿主机上启动一个进程，端口信息和宿主机共用\n  container模式，使用\u0026ndash;net=container:NAME_or_ID指定\n指定容器与特定容器共享网络命名空间\n  none模式，使用\u0026ndash;net=none指定\n网络模式为空，即仅保留网络命名空间，但是不做任何网络相关的配置(网卡、IP、路由等)\n  bridge模式 那我们之前在演示创建docker容器的时候其实是没有指定的网络模式的，如果不指定的话默认就会使用bridge模式，bridge本意是桥的意思，其实就是网桥模式。\n那我们怎么理解网桥，如果需要做类比的话，我们可以把网桥看成一个二层的交换机设备，我们来看下这张图：\n交换机通信简图\n交换机网络通信流程：\n网桥模式示意图\nLinux 中，能够起到虚拟交换机作用的网络设备，是网桥（Bridge）。它是一个工作在数据链路层（Data Link）的设备，主要功能是根据 MAC 地址将数据包转发到网桥的不同端口上。 网桥在哪，查看网桥\n$ yum install -y bridge-utils $ brctl show bridge name bridge id STP enabled interfaces docker0 8000.0242b5fbe57b no veth3a496ed 有了网桥之后，那我们看下docker在启动一个容器的时候做了哪些事情才能实现容器间的互联互通\nDocker 创建一个容器的时候，会执行如下操作：\n 创建一对虚拟接口/网卡，也就是veth pair； veth pair的一端桥接 到默认的 docker0 或指定网桥上，并具有一个唯一的名字，如 vethxxxxxx； veth paid的另一端放到新启动的容器内部，并修改名字作为 eth0，这个网卡/接口只在容器的命名空间可见； 从网桥可用地址段中（也就是与该bridge对应的network）获取一个空闲地址分配给容器的 eth0 配置容器的默认路由  那整个过程其实是docker自动帮我们完成的，清理掉所有容器，来验证。\n## 清掉所有容器 $ docker rm -f `docker ps -aq` $ docker ps $ brctl show # 查看网桥中的接口，目前没有  ## 创建测试容器test1 $ docker run -d --name test1 nginx:alpine $ brctl show # 查看网桥中的接口，已经把test1的veth端接入到网桥中 $ ip a |grep veth # 已在宿主机中可以查看到 $ docker exec -ti test1 sh / # ifconfig # 查看容器的eth0网卡及分配的容器ip  # 再来启动一个测试容器，测试容器间的通信 $ docker run -d --name test2 nginx:alpine $ docker exec -ti test2 sh / # sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories / # apk add curl / # curl 172.17.0.8:80  ## 为啥可以通信？ / # route -n #  Kernel IP routing table Destination Gateway Genmask Flags Metric Ref Use Iface 0.0.0.0 172.17.0.1 0.0.0.0 UG 0 0 0 eth0 172.17.0.0 0.0.0.0 255.255.0.0 U 0 0 0 eth0  # eth0 网卡是这个容器里的默认路由设备；所有对 172.17.0.0/16 网段的请求，也会被交给 eth0 来处理（第二条 172.17.0.0 路由规则），这条路由规则的网关（Gateway）是 0.0.0.0，这就意味着这是一条直连规则，即：凡是匹配到这条规则的 IP 包，应该经过本机的 eth0 网卡，通过二层网络(数据链路层)直接发往目的主机。  # 而要通过二层网络到达 test1 容器，就需要有 172.17.0.8 这个 IP 地址对应的 MAC 地址。所以test2容器的网络协议栈，就需要通过 eth0 网卡发送一个 ARP 广播，来通过 IP 地址查找对应的 MAC 地址。  #这个 eth0 网卡，是一个 Veth Pair，它的一端在这个 test2 容器的 Network Namespace 里，而另一端则位于宿主机上（Host Namespace），并且被“插”在了宿主机的 docker0 网桥上。网桥设备的一个特点是插在桥上的网卡都会被当成桥上的一个端口来处理，而端口的唯一作用就是接收流入的数据包，然后把这些数据包的“生杀大权”（比如转发或者丢弃），全部交给对应的网桥设备处理。  # 因此ARP的广播请求也会由docker0来负责转发，这样网桥就维护了一份端口与mac的信息表，因此针对test2的eth0拿到mac地址后发出的各类请求，同样走到docker0网桥中由网桥负责转发到对应的容器中。  # 网桥会维护一份mac映射表，我们可以大概通过命令来看一下， $ brctl showmacs docker0 ## 这些mac地址是主机端的veth网卡对应的mac，可以查看一下 $ ip a 我们如何知道网桥上的这些虚拟网卡与容器端是如何对应？\n通过ifindex，网卡索引号\n## 查看test1容器的网卡索引 $ docker exec -ti test1 cat /sys/class/net/eth0/ifindex  ## 主机中找到虚拟网卡后面这个@ifxx的值，如果是同一个值，说明这个虚拟网卡和这个容器的eth0网卡是配对的。 $ ip a |grep @if 整理脚本，快速查看对应：\nfor container in $(docker ps -q); do  iflink=`docker exec -it $container sh -c \u0026#39;cat /sys/class/net/eth0/iflink\u0026#39;`  iflink=`echo $iflink|tr -d \u0026#39;\\r\u0026#39;`  veth=`grep -l $iflink /sys/class/net/veth*/ifindex`  veth=`echo $veth|sed -e \u0026#39;s;^.*net/\\(.*\\)/ifindex$;\\1;\u0026#39;`  echo $container:$veth done 上面我们讲解了容器之间的通信，那么容器与宿主机的通信是如何做的？\n添加端口映射：\n## 启动容器的时候通过-p参数添加宿主机端口与容器内部服务端口的映射 $ docker run --name test -d -p 8088:80 nginx:alpine $ curl localhost:8088 端口映射如何实现的？先来回顾iptables链表图\n访问本机的8088端口，数据包会从流入方向进入本机，因此涉及到PREROUTING和INPUT链，我们是通过做宿主机与容器之间加的端口映射，所以肯定会涉及到端口转换，那哪个表是负责存储端口转换信息的呢，就是nat表，负责维护网络地址转换信息的。因此我们来查看一下PREROUTING链的nat表：\n$ iptables -t nat -nvL PREROUTING Chain PREROUTING (policy ACCEPT 159 packets, 20790 bytes)  pkts bytes target prot opt in out source destination  3 156 DOCKER all -- * * 0.0.0.0/0 0.0.0.0/0 ADDRTYPE match dst-type LOCAL 规则利用了iptables的addrtype拓展，匹配网络类型为本地的包，如何确定哪些是匹配本地，\n$ ip route show table local type local 127.0.0.0/8 dev lo proto kernel scope host src 127.0.0.1 127.0.0.1 dev lo proto kernel scope host src 127.0.0.1 172.17.0.1 dev docker0 proto kernel scope host src 172.17.0.1 172.21.51.143 dev eth0 proto kernel scope host src 172.21.51.143 也就是说目标地址类型匹配到这些的，会转发到我们的TARGET中，TARGET是动作，意味着对符合要求的数据包执行什么样的操作，最常见的为ACCEPT或者DROP，此处的TARGET为DOCKER，很明显DOCKER不是标准的动作，那DOCKER是什么呢？我们通常会定义自定义的链，这样把某类对应的规则放在自定义链中，然后把自定义的链绑定到标准的链路中，因此此处DOCKER 是自定义的链。那我们现在就来看一下DOCKER这个自定义链上的规则。\n$ iptables -t nat -nvL DOCKER Chain DOCKER (2 references)  pkts bytes target prot opt in out source destination  0 0 RETURN all -- docker0 * 0.0.0.0/0 0.0.0.0/0  0 0 DNAT tcp -- !docker0 * 0.0.0.0/0 0.0.0.0/0 tcp dpt:8088 to:172.17.0.2:80 此条规则就是对主机收到的目的端口为8088的tcp流量进行DNAT转换，将流量发往172.17.0.2:80，172.17.0.2地址是不是就是我们上面创建的Docker容器的ip地址，流量走到网桥上了，后面就走网桥的转发就ok了。 所以，外界只需访问172.21.51.143:8088就可以访问到容器中的服务了。\n数据包在出口方向走POSTROUTING链，我们查看一下规则：\n$ iptables -t nat -nvL POSTROUTING Chain POSTROUTING (policy ACCEPT 1099 packets, 67268 bytes)  pkts bytes target prot opt in out source destination  86 5438 MASQUERADE all -- * !docker0 172.17.0.0/16 0.0.0.0/0  0 0 MASQUERADE tcp -- * * 172.17.0.4 172.17.0.4 tcp dpt:80 大家注意MASQUERADE这个动作是什么意思，其实是一种更灵活的SNAT，把源地址转换成主机的出口ip地址，那解释一下这条规则的意思:\n这条规则会将源地址为172.17.0.0/16的包（也就是从Docker容器产生的包），并且不是从docker0网卡发出的，进行源地址转换，转换成主机网卡的地址。大概的过程就是ACK的包在容器里面发出来，会路由到网桥docker0，网桥根据宿主机的路由规则会转给宿主机网卡eth0，这时候包就从docker0网卡转到eth0网卡了，并从eth0网卡发出去，这时候这条规则就会生效了，把源地址换成了eth0的ip地址。\n 注意一下，刚才这个过程涉及到了网卡间包的传递，那一定要打开主机的ip_forward转发服务，要不然包转不了，服务肯定访问不到。\n 抓包演示 我们先想一下，我们要抓哪个网卡的包\n  首先访问宿主机的8088端口，我们抓一下宿主机的eth0\n$ tcpdump -i eth0 port 8088 -w host.cap   然后最终包会流入容器内，那我们抓一下容器内的eth0网卡\n# 容器内安装一下tcpdump $ sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.tuna.tsinghua.edu.cn/g\u0026#39; /etc/apk/repositories $ apk add tcpdump $ tcpdump -i eth0 port 80 -w container.cap   到另一台机器访问一下，\n$ curl 172.21.51.143:8088/ 停止抓包，拷贝容器内的包到宿主机\n$ docker cp test:/root/container.cap /root/ 把抓到的内容拷贝到本地，使用wireshark进行分析。\n$ scp root@172.21.51.143:/root/*.cap /d/packages （wireshark合并包进行分析）\n进到容器内的包做DNAT，出去的包做SNAT，这样对外面来讲，根本就不知道机器内部是谁提供服务，其实这就和一个内网多个机器公用一个外网IP地址上网的效果是一样的，那这也属于NAT功能的一个常见的应用场景。\nHost模式 容器内部不会创建网络空间，共享宿主机的网络空间。比如直接通过host模式创建mysql容器：\n$ docker run --net host -d --name mysql -e MYSQL_ROOT_PASSWORD=123456 mysql:5.7 容器启动后，会默认监听3306端口，由于网络模式是host，因为可以直接通过宿主机的3306端口进行访问服务，效果等同于在宿主机中直接启动mysqld的进程。\nConatiner模式 这个模式指定新创建的容器和已经存在的一个容器共享一个 Network Namespace，而不是和宿主机共享。新创建的容器不会创建自己的网卡，配置自己的 IP，而是和一个指定的容器共享 IP、端口范围等。同样，两个容器除了网络方面，其他的如文件系统、进程列表等还是隔离的。两个容器的进程可以通过 lo 网卡设备通信。\n## 启动测试容器，共享mysql的网络空间 $ docker run -ti --rm --net=container:mysql busybox sh / # ip a / # netstat -tlp|grep 3306 / # telnet localhost 3306 在一些特殊的场景中非常有用，例如，kubernetes的pod，kubernetes为pod创建一个基础设施容器，同一pod下的其他容器都以container模式共享这个基础设施容器的网络命名空间，相互之间以localhost访问，构成一个统一的整体。\nNone模式 只会创建对应的网络空间，不会配置网络堆栈（网卡、路由等）。\n# 创建none的容器 $ docker run -it --name=network-none --net=none nginx:alpine sh # ifconfig 在宿主机中操作：\n# 创建虚拟网卡对 $ ip link add A type veth peer name B # A端插入到docker0网桥 $ brctl addif docker0 A $ ip link set A up  # B端插入到network-none容器中，需要借助ip netns,因此需要显示的创建命名network namespace $ PID=$(docker inspect -f \u0026#39;{{.State.Pid}}\u0026#39; network-none) $ mkdir -p /var/run/netns $ ln -s /proc/$PID/ns/net /var/run/netns/$PID  # B端放到容器的命名空间 $ ip link set B netns $PID $ ip netns exec $PID ip link set dev B name eth0 # 修改设备名称为eth0，和docker默认行为一致 $ ip netns exec $PID ip link set eth0 up  # 设置ip $ ip netns exec $PID ip addr add 172.17.0.100/16 dev eth0 # 添加默认路由，指定给docker0网桥 $ ip netns exec $PID ip route add default via 172.17.0.1  # 测试容器间通信 前置知识：\n ip netns 命令用来管理 network namespace。它可以创建命名的 network namespace，然后通过名字来引用 network namespace network namespace 在逻辑上是网络堆栈的一个副本，它有自己的路由、防火墙规则和网络设备。 默认情况下，子进程继承其父进程的 network namespace。也就是说，如果不显式创建新的 network namespace，所有进程都从 init 进程继承相同的默认 network namespace。 根据约定，命名的 network namespace 是可以打开的 /var/run/netns/ 目录下的一个对象。比如有一个名称为 net1 的 network namespace 对象，则可以由打开 /var/run/netns/net1 对象产生的文件描述符引用 network namespace net1。通过引用该文件描述符，可以修改进程的 network namespace。  实用技巧   清理主机上所有退出的容器\n$ docker rm $(docker ps -aq)   调试或者排查容器启动错误\n## 若有时遇到容器启动失败的情况，可以先使用相同的镜像启动一个临时容器，先进入容器 $ docker run --rm -ti \u0026lt;image_id\u0026gt; sh ## 进入容器后，手动执行该容器对应的ENTRYPOINT或者CMD命令，这样即使出错，容器也不会退出，因为bash作为1号进程，我们只要不退出容器，该容器就不会自动退出   本章小结  为了解决软件交付过程中的环境依赖，同时提供一种更加轻量的虚拟化技术，Docker出现了。 2013年诞生，15年开始迅速发展，从17.03月开始，使用时间日期管理版本，稳定版以每季度为准。 Docker是一种CS架构的软件产品，可以把代码及依赖打包成镜像，作为交付介质，并且把镜像启动成为容器，提供容器生命周期的管理。 使用yum部署docker，启动后通过操作docker这个命令行，自动调用docker daemon完成容器相关操作。 常用操作，围绕镜像|容器|仓库三大核心要素  systemctl start|stop|restart docker docker build | pull -\u0026gt; docker tag -\u0026gt; docker push docker run \u0026ndash;name my-demo -d -p 8080:80 -v /opt/data:/data demo:v20200327 ping xx.com docker cp /path/a.txt mycontainer:/opt docker exec -ti mycontainer /bin/sh docker logs -f \u0026ndash;tail=100 mycontainer   通过dockerfile构建业务镜像，先使用基础镜像，然后通过一系列的指令把我们的业务应用所需要的运行环境和依赖都打包到镜像中，然后通过CMD或者ENTRYPOINT指令把镜像启动时的入口制定好，完成封装即可。有点类似于，先找来一个集装箱模板(基础镜像)，然后把项目依赖的服务都扔到集装箱中，然后设置好服务的启动入口，关闭箱门，即完成了业务镜像的制作。 容器的实现依赖于内核模块提供的namespace和control-group的功能，通过namespace创建一块虚拟空间，空间内实现了各类资源(进程、网络、文件系统)的隔离，提供control-group实现了对隔离的空间的资源使用的限制。 docker镜像使用分层的方式进行存储，根据主机的存储驱动的不同，实现方式会不同，kernel在3.10.0-514以上自动支持overlay2 存储驱动，也是目前Docker推荐的方式。 得益于分层存储的模式，多个容器可以通过copy-on-write的策略，在镜像的最上层加一个可写层，同时利用存储驱动的UnionFS的能力，实现一个镜像快速启动多个容器的场景。 docker的网络模式分为4种，最常用的为bridge和host模式。bridge模式通过docker0网桥，启动容器的时候通过创建一对虚拟网卡，将容器连接在桥上，同时维护了虚拟网卡与网桥端口的关系，实现容器间的通信。容器与宿主机之间的通信通过iptables端口映射的方式，docker利用iptables的PREROUTING和POSTROUTING的nat功能，实现了SNAT与DNAT，使得容器内部的服务被完美的保护起来。 本章重点内容是docker的核心要素及基础的操作，实现原理以及docker的网络模式为选修包，目的为了帮助有docker基础及经验的同学更好的进一步理解docker。  ","permalink":"https://iblog.zone/archives/%E8%B5%B0%E8%BF%9Bdocker%E7%9A%84%E4%B8%96%E7%95%8C/","summary":"第一天 走进Docker的世界 介绍docker的前世今生，了解docker的实现原理，以Django项目为例，带大家如何编写最佳的Dockerfile构建镜像。通过本章的学习，大家会知道docker的概念及基本操作，并学会构建自己的业务镜像，并通过抓包的方式掌握Docker最常用的bridge网络模式的通信。\n认识docker  why what how  为什么出现docker 需要一种轻量、高效的虚拟化能力\nDocker 公司位于旧金山,原名dotCloud，底层利用了Linux容器技术（LXC）（在操作系统中实现资源隔离与限制）。为了方便创建和管理这些容器，dotCloud 开发了一套内部工具，之后被命名为“Docker”。Docker就是这样诞生的。\nHypervisor： 一种运行在基础物理服务器和操作系统之间的中间软件层，可允许多个操作系统和应用共享硬件 。常见的VMware的 Workstation 、ESXi、微软的Hyper-V或者思杰的XenServer。\nContainer Runtime：通过Linux内核虚拟化能力管理多个容器，多个容器共享一套操作系统内核。因此摘掉了内核占用的空间及运行所需要的耗时，使得容器极其轻量与快速。\n什么是docker 基于操作系统内核，提供轻量级虚拟化功能的CS架构的软件产品。\n基于轻量的特性，解决软件交付过程中的环境依赖\ndocker能做什么  可以把应用程序代码及运行依赖环境打包成镜像，作为交付介质，在各环境部署 可以将镜像（image）启动成为容器(container)，并且提供多容器的生命周期进行管理（启、停、删） container容器之间相互隔离，且每个容器可以设置资源限额 提供轻量级虚拟化功能，容器就是在宿主机中的一个个的虚拟的空间，彼此相互隔离，完全独立  版本管理  Docker 引擎主要有两个版本：企业版（EE）和社区版（CE） 每个季度(1-3,4-6,7-9,10-12)，企业版和社区版都会发布一个稳定版本(Stable)。社区版本会提供 4 个月的支持，而企业版本会提供 12 个月的支持 每个月社区版还会通过 Edge 方式发布月度版 从 2017 年第一季度开始，Docker 版本号遵循 YY.MM-xx 格式，类似于 Ubuntu 等项目。例如，2018 年 6 月第一次发布的社区版本为 18.06.0-ce  发展史 13年成立，15年开始，迎来了飞速发展。\nDocker 1.8之前，使用LXC，Docker在上层做了封装， 把LXC复杂的容器创建与使用方式简化为自己的一套命令体系。\n之后，为了实现跨平台等复杂的场景，Docker抽出了libcontainer项目，把对namespace、cgroup的操作封装在libcontainer项目里，支持不同的平台类型。\n2015年6月，Docker牵头成立了 OCI（Open Container Initiative开放容器计划）组织，这个组织的目的是建立起一个围绕容器的通用标准 。 容器格式标准是一种不受上层结构绑定的协议，即不限于某种特定操作系统、硬件、CPU架构、公有云等 ， 允许任何人在遵循该标准的情况下开发应用容器技术，这使得容器技术有了一个更广阔的发展空间。","title":"走进Docker的世界"},{"content":"nginx本身不能处理PHP，它只是个web服务器，当接收到请求后，如果是php请求，则发给php解释器处理，并把结果返回给客户端。\nnginx一般是把请求发fastcgi管理进程处理，fascgi管理进程选择cgi子进程处理结果并返回被nginx\n本文以php-fpm为例介绍如何使nginx支持PHP\n一、编译安装php-fpm\n什么是PHP-FPM\nPHP-FPM是一个PHP FastCGI管理器，是只用于PHP的,可以在 http://php-fpm.org/download下载得到.\nPHP-FPM其实是PHP源代码的一个补丁，旨在将FastCGI进程管理整合进PHP包中。必须将它patch到你的PHP源代码中，在编译安装PHP后才可以使用。\n新版PHP已经集成php-fpm了，不再是第三方的包了，推荐使用。PHP-FPM提供了更好的PHP进程管理方式，可以有效控制内存和进程、可以平滑重载PHP配置，比spawn-fcgi具有更多优点，所以被PHP官方收录了。在./configure的时候带 –enable-fpm参数即可开启PHP-FPM，其它参数都是配置php的，具体选项含义可以查看这里。\n安装前准备 centos下执行\nyum -y install gcc automake autoconf libtool make yum -y install gcc gcc-c++ glibc yum -y install libmcrypt-devel mhash-devel libxslt-devel \\ libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel \\ zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel \\ ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel \\ krb5 krb5-devel libidn libidn-devel openssl openssl-devel 新版php-fpm安装(推荐安装方式)\nwget http://cn2.php.net/distributions/php-5.4.7.tar.gz tar zvxf php-5.4.7.tar.gz cd php-5.4.7 ./configure --prefix=/usr/local/php --enable-fpm --with-mcrypt \\ --enable-mbstring --disable-pdo --with-curl --disable-debug --disable-rpath \\ --enable-inline-optimization --with-bz2 --with-zlib --enable-sockets \\ --enable-sysvsem --enable-sysvshm --enable-pcntl --enable-mbregex \\ --with-mhash --enable-zip --with-pcre-regex --with-mysql --with-mysqli \\ --with-gd --with-jpeg-dir make all install 安装后内容放在/usr/local/php目录下\n以上就完成了php-fpm的安装。\n下面是对php-fpm运行用户进行设置\ncd /usr/local/php cp etc/php-fpm.conf.default etc/php-fpm.conf vi etc/php-fpm.conf 修改\nuser = www-data group = www-data 如果www-data用户不存在，那么先添加www-data用户\ngroupadd www-data useradd -g www-data www-data 二、编译安装nginx\n然后按照http://www.nginx.cn/install 安装nginx\n三、修改nginx配置文件以支持php-fpm\nnginx安装完成后，修改nginx配置文件为,nginx.conf\n其中server段增加如下配置，注意标红内容配置，否则会出现No input file specified.错误\n# pass the PHP scripts to FastCGI server listening on 127.0.0.1:9000 # location ~ \\.php$ { root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; } 四、创建测试php文件\n创建php文件\n在/usr/local/nginx/html下创建index.php文件，输入如下内容\n\u0026lt;?php echo phpinfo(); ?\u0026gt; 五、启动服务\n启动php-fpm和nginx\n/usr/local/php/sbin/php-fpm #手动打补丁的启动方式/usr/local/php/sbin/php-fpm start sudo /usr/local/nginx/nginx php-fpm关闭重启见文章结尾\n六、浏览器访问\n访问http://你的服务器ip/index.php，皆可以见到php信息了。\n安装php-fpm时可能遇到的错误：\n php configure时出错  configure: error: XML configuration could not be found\napt-get install libxml2 libxml2-dev (ubuntu下) yum -y install libxml2 libxml2-devel（centos下)  Please reinstall the BZip2 distribution  wget http://www.bzip.org/1.0.5/bzip2-1.0.5.tar.gz tar -zxvf bzip2-1.0.5.tar.gz cd bzip2-1.0.5 make make install  php的配置文件中有一行\u0026ndash;with-mysql=/usr。  安装的时候提示：\nconfigure: error: Cannot find MySQL header files under yes. Note that the MySQL client library is not bundled anymore. 这是由于安装mysql时没有安装mysql头文件，或者是路径指定不正确,php找不到mysql的头文件引起的错误提示。\n解决方法。\n(1.) 查看你的系统有没有安装mysql header\nfind / -name mysql.h 如果有。请指定\u0026ndash;with-mysql=/跟你的正常路径。\n如果没有。请看下一步。\n(2.)redhat安装\nrpm -ivh MySQL-devel-4.1.12-1.i386.rpm (3.)ubuntu安装\napt-get install libmysqlclient15-dev (4.)最后一步php的配置选项添加\u0026ndash;with-mysql=/usr即可！\n4.No input file specified.\nlocation ~ \\.php$ { root html; fastcgi_pass 127.0.0.1:9000; fastcgi_index index.php; fastcgi_param SCRIPT_FILENAME $document_root$fastcgi_script_name; include fastcgi_params; }  如果php configure时缺库，可以先安装库(ubuntu下)  sudo apt-get install make bison flex gcc patch autoconf subversion locate sudo apt-get install libxml2-dev libbz2-dev libpcre3-dev libssl-dev zlib1g-dev libmcrypt-dev libmhash-dev libmhash2 libcurl4-openssl-dev libpq-dev libpq5 libsyck0-dev  mcrypt.h not found. Please reinstall libmcrypt  apt-get install libmcrypt-dev 或者\ncd /usr/local/src wget http://softlayer.dl.sourceforge.net/sourceforge/mcrypt/libmcrypt-2.5.8.tar.gz tar -zxvf libmcrypt-2.5.8.tar.gz cd /usr/local/src/libmcrypt-2.5.8 ./configure --prefix=/usr/local make make install  php-fpm 5.4.7 如何关闭 重启？  php 5.4.7 下的php-fpm 不再支持 php-fpm 以前具有的 /usr/local/php/sbin/php-fpm (start|stop|reload)等命令，需要使用信号控制：\nmaster进程可以理解以下信号\nINT, TERM 立刻终止 QUIT 平滑终止 USR1 重新打开日志文件 USR2 平滑重载所有worker进程并重新载入配置和二进制模块\n示例：\nphp-fpm 关闭：\nkill -INT `cat /usr/local/php/var/run/php-fpm.pid` php-fpm 重启：\nkill -USR2 `cat /usr/local/php/var/run/php-fpm.pid` 查看php-fpm进程数：\nps aux | grep -c php-fpm 8.命令行下执行php，提示找不到命令\n-bash: /usr/bin/php: No such file or directory vi /etc/profile 在文件底部增加一行配置\nexport PATH=/usr/local/php/bin:$PATH 保存退出\nsource /etc/profile ","permalink":"https://iblog.zone/archives/nginx-php-fpm%E5%AE%89%E8%A3%85%E9%85%8D%E7%BD%AE/","summary":"nginx本身不能处理PHP，它只是个web服务器，当接收到请求后，如果是php请求，则发给php解释器处理，并把结果返回给客户端。\nnginx一般是把请求发fastcgi管理进程处理，fascgi管理进程选择cgi子进程处理结果并返回被nginx\n本文以php-fpm为例介绍如何使nginx支持PHP\n一、编译安装php-fpm\n什么是PHP-FPM\nPHP-FPM是一个PHP FastCGI管理器，是只用于PHP的,可以在 http://php-fpm.org/download下载得到.\nPHP-FPM其实是PHP源代码的一个补丁，旨在将FastCGI进程管理整合进PHP包中。必须将它patch到你的PHP源代码中，在编译安装PHP后才可以使用。\n新版PHP已经集成php-fpm了，不再是第三方的包了，推荐使用。PHP-FPM提供了更好的PHP进程管理方式，可以有效控制内存和进程、可以平滑重载PHP配置，比spawn-fcgi具有更多优点，所以被PHP官方收录了。在./configure的时候带 –enable-fpm参数即可开启PHP-FPM，其它参数都是配置php的，具体选项含义可以查看这里。\n安装前准备 centos下执行\nyum -y install gcc automake autoconf libtool make yum -y install gcc gcc-c++ glibc yum -y install libmcrypt-devel mhash-devel libxslt-devel \\ libjpeg libjpeg-devel libpng libpng-devel freetype freetype-devel libxml2 libxml2-devel \\ zlib zlib-devel glibc glibc-devel glib2 glib2-devel bzip2 bzip2-devel \\ ncurses ncurses-devel curl curl-devel e2fsprogs e2fsprogs-devel \\ krb5 krb5-devel libidn libidn-devel openssl openssl-devel 新版php-fpm安装(推荐安装方式)\nwget http://cn2.php.net/distributions/php-5.4.7.tar.gz tar zvxf php-5.","title":"nginx php-fpm安装配置"},{"content":"一：删除之前的python和yum 1、删除python rpm -qa|grep python|xargs rpm -ev --allmatches --nodeps ##强制删除已安装程序及其关联 whereis python |xargs rm -frv ##删除所有残余文件 ##xargs，允许你对输出执行其他某些命令 whereis python ##验证删除，返回无结果 2、删除现有的yum rpm -qa|grep yum|xargs rpm -ev --allmatches --nodeps whereis yum |xargs rm -frv whereis yum ##验证删除，返回无结果 二：下载安装包 https://mirrors.ustc.edu.cn/centos/7.9.2009/os/x86_64/Packages/\n三：安装 rpm -Uvh --replacepkgs *.rpm #意思是安装当前目录下所有的rpm文件 ","permalink":"https://iblog.zone/archives/centos7.9.2009-%E5%8D%B8%E8%BD%BD%E8%87%AA%E5%B8%A6%E7%9A%84python%E5%8F%8Ayum%E5%B9%B6%E9%87%8D%E8%A3%85/","summary":"一：删除之前的python和yum 1、删除python rpm -qa|grep python|xargs rpm -ev --allmatches --nodeps ##强制删除已安装程序及其关联 whereis python |xargs rm -frv ##删除所有残余文件 ##xargs，允许你对输出执行其他某些命令 whereis python ##验证删除，返回无结果 2、删除现有的yum rpm -qa|grep yum|xargs rpm -ev --allmatches --nodeps whereis yum |xargs rm -frv whereis yum ##验证删除，返回无结果 二：下载安装包 https://mirrors.ustc.edu.cn/centos/7.9.2009/os/x86_64/Packages/\n三：安装 rpm -Uvh --replacepkgs *.rpm #意思是安装当前目录下所有的rpm文件 ","title":"CentOS7.9.2009 卸载自带的python及yum并重装"},{"content":"CentOS7中自带的python版本是python-2.7.5，由于新开的虚拟机需要使用python3，于是便升级一下版本。\n安装Python3.7.3 官网下载地址：https://www.python.org/downlo\u0026hellip;\n这里选择下载python 3.7.3。\n# 下载 wget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz # 解压 tar -zxf Python-3.7.3.tgz # 安装依赖包 yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc libffi-devel # 进入python目录 cd Python-3.7.3 # 编译 ./configure --prefix=/usr/local/python3.7 #安装 make \u0026amp;\u0026amp; make install 关于Python3.7以上的版本，需要多安装一个依赖包:\n yum install -y libffi-devel 否则会出现ModuleNotFoundError: No module named '_ctypes'的报错。\n在make install后执行echo $?，为0表示没有出错。如果没有报错，在/usr/local会生成python3.7目录。\n然后将系统默认的python2备份\nmv /usr/bin/python /usr/bin/python.bak 创建新的软连接\nln -s /usr/local/python/bin/python3.7 /usr/bin/python 查看版本\n[root@moli-linux03 src]# python -V Python 3.7.3 升级完成了。\n更改yum配置 因为yum需要使用python2，将/usr/bin/python改为python3后，yum就不能正常运行了，因此需要更改一下yum的配置。\nvim /usr/bin/yum vim /usr/libexec/urlgrabber-ext-down 编辑这两个文件，将文件头的#!/usr/bin/python改为#!/usr/bin/python2即可。\n","permalink":"https://iblog.zone/archives/centos7%E5%8D%87%E7%BA%A7python2.7.5%E5%88%B0python3.7%E4%BB%A5%E4%B8%8A%E7%89%88%E6%9C%AC/","summary":"CentOS7中自带的python版本是python-2.7.5，由于新开的虚拟机需要使用python3，于是便升级一下版本。\n安装Python3.7.3 官网下载地址：https://www.python.org/downlo\u0026hellip;\n这里选择下载python 3.7.3。\n# 下载 wget https://www.python.org/ftp/python/3.7.3/Python-3.7.3.tgz # 解压 tar -zxf Python-3.7.3.tgz # 安装依赖包 yum install zlib-devel bzip2-devel openssl-devel ncurses-devel sqlite-devel readline-devel tk-devel gcc libffi-devel # 进入python目录 cd Python-3.7.3 # 编译 ./configure --prefix=/usr/local/python3.7 #安装 make \u0026amp;\u0026amp; make install 关于Python3.7以上的版本，需要多安装一个依赖包:\n yum install -y libffi-devel 否则会出现ModuleNotFoundError: No module named '_ctypes'的报错。\n在make install后执行echo $?，为0表示没有出错。如果没有报错，在/usr/local会生成python3.7目录。\n然后将系统默认的python2备份\nmv /usr/bin/python /usr/bin/python.bak 创建新的软连接\nln -s /usr/local/python/bin/python3.7 /usr/bin/python 查看版本\n[root@moli-linux03 src]# python -V Python 3.7.3 升级完成了。","title":"CentOS7升级python2.7.5到python3.7以上版本"},{"content":"简介 通常生产环境由于安全原因都无法访问互联网。此时就需要进行离线安装，主要有两种方式：源码编译、rpm包安装。源码编译耗费时间长且缺乏编译环境，所以一般都选择使用离线 rpm 包安装。\n验证环境 Centos 7.2\n查看依赖包 可以使用“yum deplist”命令来查找 rpm 包的依赖列表。例如，要查找“ansible”rpm的依赖包：\n$ yum deplist ansible 软件包：ansible.noarch 2.9.3-1.el7  依赖：/usr/bin/env  provider: coreutils.x86_64 8.22-24.el7  依赖：/usr/bin/python2  provider: python.x86_64 2.7.5-86.el7  依赖：PyYAML  provider: PyYAML.x86_64 3.10-11.el7  依赖：python(abi) = 2.7  provider: python.x86_64 2.7.5-86.el7  依赖：python-httplib2  provider: python-httplib2.noarch 0.9.2-1.el7  依赖：python-jinja2  provider: python-jinja2.noarch 2.7.2-4.el7  依赖：python-paramiko  provider: python-paramiko.noarch 2.1.1-9.el7  依赖：python-setuptools  provider: python-setuptools.noarch 0.9.8-7.el7  依赖：python-six  provider: python-six.noarch 1.9.0-2.el7  依赖：python2-cryptography  provider: python2-cryptography.x86_64 1.7.2-2.el7  依赖：python2-jmespath  provider: python2-jmespath.noarch 0.9.0-3.el7  依赖：sshpass  provider: sshpass.x86_64 1.06-2.el7 方案一（推荐）：repotrack # 安装yum-utils $ yum -y install yum-utils  # 下载 ansible 全量依赖包 $ repotrack ansible 方案二：yumdownloader # 安装yum-utils $ yum -y install yum-utils  # 下载 ansible 依赖包 $ yumdownloader --resolve --destdir=/tmp ansible 参数说明：\n —destdir：指定 rpm 包下载目录（不指定时，默认为当前目录） —resolve：下载依赖的 rpm 包。  注意\n仅会将主软件包和基于你现在的操作系统所缺少的依赖关系包一并下载。 方案三：yum 的 downloadonly 插件 # 安装插件 $ yum -y install yum-download  # 下载 ansible 依赖包 $ yum -y install ansible --downloadonly --downloaddir=/tmp 注意\n与 yumdownloader 命令一样，也是仅会将主软件包和基于你现在的操作系统所缺少的依赖关系包一并下载。 离线安装 rpm # 离线安装 $ rpm -Uvh --force --nodeps *.rpm 参考资料  https://serverfault.com/questions/470964/yumdownloader-vs-repotrack  ","permalink":"https://iblog.zone/archives/yum%E4%B8%8B%E8%BD%BD%E5%85%A8%E9%87%8Frpm%E4%BE%9D%E8%B5%96%E5%8C%85%E5%8F%8A%E7%A6%BB%E7%BA%BF%E5%AE%89%E8%A3%85/","summary":"简介 通常生产环境由于安全原因都无法访问互联网。此时就需要进行离线安装，主要有两种方式：源码编译、rpm包安装。源码编译耗费时间长且缺乏编译环境，所以一般都选择使用离线 rpm 包安装。\n验证环境 Centos 7.2\n查看依赖包 可以使用“yum deplist”命令来查找 rpm 包的依赖列表。例如，要查找“ansible”rpm的依赖包：\n$ yum deplist ansible 软件包：ansible.noarch 2.9.3-1.el7  依赖：/usr/bin/env  provider: coreutils.x86_64 8.22-24.el7  依赖：/usr/bin/python2  provider: python.x86_64 2.7.5-86.el7  依赖：PyYAML  provider: PyYAML.x86_64 3.10-11.el7  依赖：python(abi) = 2.7  provider: python.x86_64 2.7.5-86.el7  依赖：python-httplib2  provider: python-httplib2.noarch 0.9.2-1.el7  依赖：python-jinja2  provider: python-jinja2.noarch 2.7.2-4.el7  依赖：python-paramiko  provider: python-paramiko.noarch 2.1.1-9.el7  依赖：python-setuptools  provider: python-setuptools.noarch 0.9.8-7.el7  依赖：python-six  provider: python-six.","title":"yum下载全量rpm依赖包及离线安装"},{"content":"注意事项 本文的环境都是系统自带的openssh，若是手动编译安装的，不保证成功。若是自带的，则升级过程中不需要卸载旧版本openssh。\n安装之前可以先试试yum更新,若是可以更新，就不需要往下看了\n# centos8  $ yum update openssh -y # 重启sshd $ systemctl restart sshd 准备工作 系统说明  系统版本：CentOS Linux release 7.7.1908 (Core) openssh：OpenSSH_7.4p1, OpenSSL 1.0.2k-fips 26 Jan 2017 openssl: OpenSSL 1.0.2k-fips 26 Jan 2017  下载最新包  openssh openssl  本文选择的是: openssh-8.2p1.tar.gz openssl-1.1.1g.tar.gz\n$ wget https://cdn.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-8.2p1.tar.gz  $ wget https://ftp.openssl.org/source/openssl-1.1.1g.tar.gz 安装telnet备用（可选） 安装新的ssh之后，只要配置好启动，就可以做到无缝切换，但是中途断开就不能连接了，为了防止这种情况，我们可以安装telnet当作备用，若是你能保证中途不会断开，此步骤可以忽略\n1.安装\n$ yum install telnet telnet-server -y 2.启动\n$ systemctl enable telnet.socket $ systemctl start telnet.socket 3.连接\n# telnet 默认禁止root用户连接，我们先生成一个普通用户  $ useradd testuser $ passwd testuser  # 本地测试 $ telnet 127.0.0.1 VM_0_6_centos login: testuser Password: [testuser@VM_0_6_centos ~]$ # 切换root [testuser@VM_0_6_centos ~]$ su 升级openssl 备份 $ mv /usr/bin/openssl /usr/bin/openssl_old 安装 $ tar xzvf openssl-1.1.1g.tar.gz $ cd openssl-1.1.1g/ $ ./config shared \u0026amp;\u0026amp; make \u0026amp;\u0026amp; make install 配置软连接 $ ln -s /usr/local/bin/openssl /usr/bin/openssl 如果执行openssl version报下面的错误\nopenssl: error while loading shared libraries: libssl.so.1.1: cannot open shared object file: No such file or directory 则执行下面命令解决：\n$ ln -s /usr/local/lib64/libssl.so.1.1 /usr/lib64/ $ ln -s /usr/local/lib64/libcrypto.so.1.1 /usr/lib64/ 旧版本:\n$ openssl_old version OpenSSL 1.0.2k-fips 26 Jan 2017 升级openssh 安装所需依赖 $ yum install zlib-devel openssl-devel pam-devel -y 备份 $ mkdir /etc/ssh_old $ mv /etc/ssh/* /etc/ssh_old/ 解压、编译安装 $ tar xzvf openssh-8.2p1.tar.gz $ cd openssh-8.2p1/  $ ./configure --prefix=/usr/ --sysconfdir=/etc/ssh --with-ssl-dir=/usr/local/lib64/ --with-zlib --with-pam --with-md5-password --with-ssl-engine --with-selinux  # 安装 $ make \u0026amp;\u0026amp; make install  # 验证 $ ssh -V OpenSSH_8.2p1, OpenSSL 1.1.1g 21 Apr 2020  $ ls /etc/ssh moduli ssh_config sshd_config ssh_host_dsa_key ssh_host_dsa_key.pub ssh_host_ecdsa_key ssh_host_ecdsa_key.pub ssh_host_ed25519_key ssh_host_ed25519_key.pub ssh_host_rsa_key ssh_host_rsa_key.pub 配置 1.修改sshd_config\n$ vim /etc/ssh/sshd_config  # 例子：配置root登录, 根据你以前的配置来 PermitRootLogin yes 2.启动\n# 移走以前的ssh服务, 防止与新的冲突 $ mv /usr/lib/systemd/system/sshd.service /etc/ssh_old/sshd.service $ mv /usr/lib/systemd/system/sshd.socket /etc/ssh_old/sshd.socket  # 在解压包中拷贝一些文件 $ cp -a contrib/redhat/sshd.init /etc/init.d/sshd    # 重新启动 $ /etc/init.d/sshd restart $ systemctl daemon-reload  # 添加自启动 $ chkconfig --add sshd $ chkconfig sshd on 可能碰到的问题：  /etc/init.d/sshd restart之后报错  Reloading systemd: [ 确定 ] Restarting sshd (via systemctl): Job for sshd.service failed because the control process exited with error code. See \u0026#34;systemctl status sshd.service\u0026#34; and \u0026#34;journalctl -xe\u0026#34; for details.  [失败] 是selinux导致，需要关闭\n# 临时修改 setenforce 0 # 永久修改 sed -i \u0026#39;s/SELINUX=.*/SELINUX=disabled/g\u0026#39; /etc/selinux/config # 重启 /etc/init.d/sshd restart ","permalink":"https://iblog.zone/archives/centos7-openssh%E5%8D%87%E7%BA%A7%E5%88%B0%E6%9C%80%E6%96%B0%E7%89%88%E6%9C%AC/","summary":"注意事项 本文的环境都是系统自带的openssh，若是手动编译安装的，不保证成功。若是自带的，则升级过程中不需要卸载旧版本openssh。\n安装之前可以先试试yum更新,若是可以更新，就不需要往下看了\n# centos8  $ yum update openssh -y # 重启sshd $ systemctl restart sshd 准备工作 系统说明  系统版本：CentOS Linux release 7.7.1908 (Core) openssh：OpenSSH_7.4p1, OpenSSL 1.0.2k-fips 26 Jan 2017 openssl: OpenSSL 1.0.2k-fips 26 Jan 2017  下载最新包  openssh openssl  本文选择的是: openssh-8.2p1.tar.gz openssl-1.1.1g.tar.gz\n$ wget https://cdn.openbsd.org/pub/OpenBSD/OpenSSH/portable/openssh-8.2p1.tar.gz  $ wget https://ftp.openssl.org/source/openssl-1.1.1g.tar.gz 安装telnet备用（可选） 安装新的ssh之后，只要配置好启动，就可以做到无缝切换，但是中途断开就不能连接了，为了防止这种情况，我们可以安装telnet当作备用，若是你能保证中途不会断开，此步骤可以忽略\n1.安装\n$ yum install telnet telnet-server -y 2.启动\n$ systemctl enable telnet.socket $ systemctl start telnet.","title":"centos7 openssh升级到最新版本"},{"content":"小版本升级\n1. 查看当前和可升级版本\n[root@jksb_qz ~]# yum list kernel Loaded plugins: fastestmirror, langpacks Loading mirror speeds from cached hostfile  * base: mirrors.aliyun.com  * extras: mirrors.aliyun.com  * updates: mirrors.aliyun.com Installed Packages kernel.x86_64 3.10.0-693.el7 kernel.x86_64 3.10.0-1160.49.1.el7 2. 升级\n[root@jksb_qz ~]# yum update kernel -y  3. 重启并检查\n [root@jksb_qz ~]# reboot    [root@jksb_qz ~]# uname -r  大版本升级\n1. 载入公钥\n[root@jksb_qz ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org 2. 升级安装ELRepo\n[root@jksb_qz ~]# rpm -Uvh http://www.elrepo.org/elrepo-release-7.0-3.el7.elrepo.noarch.rpm 3. 载入elrepo-kernel元数据\n[root@jksb_qz ~]# yum --disablerepo=\\* --enablerepo=elrepo-kernel repolist 4. 查看可用的rpm包\n[root@jksb_qz ~]# yum --disablerepo=\\* --enablerepo=elrepo-kernel list kernel* Loaded plugins: fastestmirror, langpacks Loading mirror speeds from cached hostfile  * elrepo-kernel: mirrors.tuna.tsinghua.edu.cn Installed Packages kernel.x86_64 3.10.0-693.el7 kernel.x86_64 3.10.0-1160.49.1.el7 kernel-devel.x86_64 3.10.0-693.el7 kernel-devel.x86_64 3.10.0-1160.49.1.el7 kernel-headers.x86_64 3.10.0-1160.49.1.el7 kernel-tools.x86_64 3.10.0-1160.49.1.el7 kernel-tools-libs.x86_64 3.10.0-1160.49.1.el7 Available Packages kernel-lt.x86_64 5.4.168-1.el7.elrepo kernel-lt-devel.x86_64 5.4.168-1.el7.elrepo kernel-lt-doc.noarch 5.4.168-1.el7.elrepo kernel-lt-headers.x86_64 5.4.168-1.el7.elrepo kernel-lt-tools.x86_64 5.4.168-1.el7.elrepo kernel-lt-tools-libs.x86_64 5.4.168-1.el7.elrepo kernel-lt-tools-libs-devel.x86_64 5.4.168-1.el7.elrepo kernel-ml.x86_64 5.15.11-1.el7.elrepo kernel-ml-devel.x86_64 5.15.11-1.el7.elrepo kernel-ml-doc.noarch 5.15.11-1.el7.elrepo kernel-ml-headers.x86_64 5.15.11-1.el7.elrepo kernel-ml-tools.x86_64 5.15.11-1.el7.elrepo kernel-ml-tools-libs.x86_64 5.15.11-1.el7.elrepo kernel-ml-tools-libs-devel.x86_64 5.15.11-1.el7.elrepo 说明：\n lt ：long term support，长期支持版本；\nml：mainline，主线版本；\n 5. 安装最新版本的kernel\n[root@jksb_qz ~]# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml.x86_64 -y 6. 删除旧版本工具包\n[root@jksb_qz ~]# yum remove kernel-tools-libs.x86_64 kernel-tools.x86_64 -y 7. 安装新版本工具包\n[root@jksb_qz ~]# yum --disablerepo=\\* --enablerepo=elrepo-kernel install kernel-ml-tools.x86_64 -y 8. 查看内核插入顺序\n[root@jksb_qz ~]# awk -F \\\u0026#39; \u0026#39;$1==\u0026#34;menuentry \u0026#34; {print i++ \u0026#34; : \u0026#34; $2}\u0026#39; /etc/grub2.cfg 0 : CentOS Linux (5.15.11-1.el7.elrepo.x86_64) 7 (Core) 1 : CentOS Linux (3.10.0-1160.49.1.el7.x86_64) 7 (Core) 2 : CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core) 3 : CentOS Linux (0-rescue-3492312c4af1455a9be8092117be49a4) 7 (Core) 说明：默认新内核是从头插入，默认启动顺序也是从0开始（当前顺序还未生效），或者使用：\n[root@jksb_qz ~]# grep \u0026#34;^menuentry\u0026#34; /boot/grub2/grub.cfg | cut -d \u0026#34;\u0026#39;\u0026#34; -f2 CentOS Linux (5.15.11-1.el7.elrepo.x86_64) 7 (Core) CentOS Linux (3.10.0-1160.49.1.el7.x86_64) 7 (Core) CentOS Linux (3.10.0-693.el7.x86_64) 7 (Core) CentOS Linux (0-rescue-3492312c4af1455a9be8092117be49a4) 7 (Core) 其中文件 /etc/grub2.cfg 和 /boot/grub2/grub.cfg 内容一致。\n9. 查看当前实际启动顺序\n[root@jksb_qz ~]# grub2-editenv list saved_entry=CentOS Linux (3.10.0-1160.49.1.el7.x86_64) 7 (Core) 10. 设置默认启动\n[root@jksb_qz ~]# grub2-set-default \u0026#39;CentOS Linux (5.15.11-1.el7.elrepo.x86_64) 7 (Core)\u0026#39; [root@jksb_qz ~]# grub2-editenv list saved_entry=CentOS Linux (5.15.11-1.el7.elrepo.x86_64) 7 (Core) 或者直接设置数值\n[root@jksb_qz ~]# grub2-set-default 0　// 0代表当前第一行，也就是上面的5.15.11版本那一行内容 [root@jksb_qz ~]# grub2-editenv list saved_entry=0 11. 重启并检查\n [root@jksb_qz ~]# reboot    [root@jksb_qz ~]# uname -r  ","permalink":"https://iblog.zone/archives/centos7%E5%8D%87%E7%BA%A7%E5%86%85%E6%A0%B8%E7%89%88%E6%9C%AC/","summary":"小版本升级\n1. 查看当前和可升级版本\n[root@jksb_qz ~]# yum list kernel Loaded plugins: fastestmirror, langpacks Loading mirror speeds from cached hostfile  * base: mirrors.aliyun.com  * extras: mirrors.aliyun.com  * updates: mirrors.aliyun.com Installed Packages kernel.x86_64 3.10.0-693.el7 kernel.x86_64 3.10.0-1160.49.1.el7 2. 升级\n[root@jksb_qz ~]# yum update kernel -y  3. 重启并检查\n [root@jksb_qz ~]# reboot    [root@jksb_qz ~]# uname -r  大版本升级\n1. 载入公钥\n[root@jksb_qz ~]# rpm --import https://www.elrepo.org/RPM-GPG-KEY-elrepo.org 2. 升级安装ELRepo\n[root@jksb_qz ~]# rpm -Uvh http://www.","title":"Centos7升级内核版本"},{"content":"//显示所有 portproxy 参数，包括 v4tov4、v4tov6、v6tov4 和 v6tov6 的端口/地址对。 C:\\\u0026gt;netsh interface portproxy show all //因为没有配置过它，所以没有东西可以显示。  //添加配置: 本机监听10022端口,当有socket连接到10022端口时,本机就连接到192.168.2.53的22端口,本机的10022端口可以接受的连接地址为\u0026#34;*\u0026#34;,使用的协议为tcp,当前仅支持传输控制协议 (TCP)。 C:\\\u0026gt;netsh interface portproxy add v4tov4 listenport=10022 connectaddress=192.168.2.53 connectport=22 listenaddress=* protocol=tcp //添加完毕。  //显示所有。 C:\\\u0026gt;netsh interface portproxy show all  侦听 ipv4: 连接到 ipv4:  地址 端口 地址 端口 --------------- ---------- --------------- ---------- * 10022 192.168.2.53 22  //删除配置: 本机的监听端口为10022,10022端口接受的连接地址为\u0026#34;*\u0026#34;,使用的协议为tcp,当前仅支持TCP协议。 C:\\\u0026gt;netsh interface portproxy delete v4tov4 listenport=10022 listenaddress=* protocol=tcp //删除完毕。  //显示所有。 C:\\\u0026gt;netsh interface portproxy show all //因为所有的配置均已删除，所以没有东西可以显示。  //查看帮助信息。 C:\\\u0026gt;netsh interface portproxy /? //略。  //查看帮助信息。 C:\\\u0026gt;netsh interface /? //略。  //查看帮助信息。 C:\\\u0026gt;netsh /? //略。 ","permalink":"https://iblog.zone/archives/windows%E9%85%8D%E7%BD%AE%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/","summary":"//显示所有 portproxy 参数，包括 v4tov4、v4tov6、v6tov4 和 v6tov6 的端口/地址对。 C:\\\u0026gt;netsh interface portproxy show all //因为没有配置过它，所以没有东西可以显示。  //添加配置: 本机监听10022端口,当有socket连接到10022端口时,本机就连接到192.168.2.53的22端口,本机的10022端口可以接受的连接地址为\u0026#34;*\u0026#34;,使用的协议为tcp,当前仅支持传输控制协议 (TCP)。 C:\\\u0026gt;netsh interface portproxy add v4tov4 listenport=10022 connectaddress=192.168.2.53 connectport=22 listenaddress=* protocol=tcp //添加完毕。  //显示所有。 C:\\\u0026gt;netsh interface portproxy show all  侦听 ipv4: 连接到 ipv4:  地址 端口 地址 端口 --------------- ---------- --------------- ---------- * 10022 192.168.2.53 22  //删除配置: 本机的监听端口为10022,10022端口接受的连接地址为\u0026#34;*\u0026#34;,使用的协议为tcp,当前仅支持TCP协议。 C:\\\u0026gt;netsh interface portproxy delete v4tov4 listenport=10022 listenaddress=* protocol=tcp //删除完毕。  //显示所有。 C:\\\u0026gt;netsh interface portproxy show all //因为所有的配置均已删除，所以没有东西可以显示。  //查看帮助信息。 C:\\\u0026gt;netsh interface portproxy /?","title":"Windows配置端口转发"},{"content":"1. 安装密码生成工具 $ yum -y install httpd-tools 2. 生成用户和密码文件 生成用户和密码\n$ htpasswd -c /usr/local/nginx/password username # 回车后输入密码 # -c 创建一个加密文件 查看生成的用户和密码\n如果要修改密码，或者删除密码，请参考下面操作\n删除用户和密码\n$ htpasswd -D /usr/local/nginx/password username # -D 删除指定的用户 修改用户和密码\n$ htpasswd -D /usr/local/nginx/password username $ htpasswd -b /usr/local/nginx/password username pass # -D 删除指定的用户 # -b htpassswd命令行中一并输入用户名和密码而不是根据提示输入密码 # -p htpassswd命令不对密码进行进行加密，即明文密码 3. 配置Nginx认证 找到 nginx 配置文件，通常默认的配置文件在/usr/local/nginx/conf/nginx.conf，要对整个站点开启验证，需在配置文件中的server加上认证配置 auth_basic 和 auth_basic_user_file\nserver {  listen 80;  server_name localhost;  # ...   auth_basic \u0026#34;请输入用户和密码\u0026#34;; # 验证时的提示信息  auth_basic_user_file /usr/local/nginx/password; # 认证文件   location / {  root /var/www;  index index.html index.htm;  }  # ... } 4. 重启/重载Nginx使站点的认证生效 /usr/local/nginx/sbin/nginx -s reload ","permalink":"https://iblog.zone/archives/%E9%85%8D%E7%BD%AEnginx%E8%AE%BF%E9%97%AE%E7%BD%91%E9%A1%B5%E9%9C%80%E8%A6%81%E5%AF%86%E7%A0%81/","summary":"1. 安装密码生成工具 $ yum -y install httpd-tools 2. 生成用户和密码文件 生成用户和密码\n$ htpasswd -c /usr/local/nginx/password username # 回车后输入密码 # -c 创建一个加密文件 查看生成的用户和密码\n如果要修改密码，或者删除密码，请参考下面操作\n删除用户和密码\n$ htpasswd -D /usr/local/nginx/password username # -D 删除指定的用户 修改用户和密码\n$ htpasswd -D /usr/local/nginx/password username $ htpasswd -b /usr/local/nginx/password username pass # -D 删除指定的用户 # -b htpassswd命令行中一并输入用户名和密码而不是根据提示输入密码 # -p htpassswd命令不对密码进行进行加密，即明文密码 3. 配置Nginx认证 找到 nginx 配置文件，通常默认的配置文件在/usr/local/nginx/conf/nginx.conf，要对整个站点开启验证，需在配置文件中的server加上认证配置 auth_basic 和 auth_basic_user_file\nserver {  listen 80;  server_name localhost;  # .","title":"配置Nginx访问网页需要密码"},{"content":"集群信息 1. 节点规划 部署k8s集群的节点按照用途可以划分为如下2类角色：\n master：集群的master节点，集群的初始化节点，基础配置不低于2C4G slave：集群的slave节点，可以多台，基础配置不低于2C4G  本例为了演示slave节点的添加，会部署一台master+2台slave，节点规划如下：\n   主机名 节点ip 角色 部署组件     k8s-master 192.168.136.128 master etcd, kube-apiserver, kube-controller-manager, kubectl, kubeadm, kubelet, kube-proxy, flannel   k8s-slave1 192.168.136.131 slave kubectl, kubelet, kube-proxy, flannel   k8s-slave2 192.168.136.132 slave kubectl, kubelet, kube-proxy, flannel    2. 组件版本    组件 版本 说明     CentOS 7.6.1810    Kernel Linux 3.10.0-1062.9.1.el7.x86_64    etcd 3.3.15 使用容器方式部署，默认数据挂载到本地路径   coredns 1.6.2    kubeadm v1.16.2    kubectl v1.16.2    kubelet v1.16.2    kube-proxy v1.16.2    flannel v0.11.0     安装前准备工作 1. 设置hosts解析 操作节点：所有节点（k8s-master，k8s-slave）均需执行\n 修改hostname hostname必须只能包含小写字母、数字、\u0026quot;,\u0026quot;、\u0026quot;-\u0026quot;，且开头结尾必须是小写字母或数字  # 在master节点 $ hostnamectl set-hostname k8s-master #设置master节点的hostname  # 在slave-1节点 $ hostnamectl set-hostname k8s-slave1 #设置slave1节点的hostname  # 在slave-2节点 $ hostnamectl set-hostname k8s-slave2 #设置slave2节点的hostname  添加hosts解析  $ cat \u0026gt;\u0026gt;/etc/hosts\u0026lt;\u0026lt;EOF 192.168.136.128 k8s-master 192.168.136.131 k8s-slave1 192.168.136.132 k8s-slave2 EOF 2. 调整系统配置 操作节点： 所有的master和slave节点（k8s-master,k8s-slave）需要执行\n 本章下述操作均以k8s-master为例，其他节点均是相同的操作（ip和hostname的值换成对应机器的真实值）\n  设置安全组开放端口  如果节点间无安全组限制（内网机器间可以任意访问），可以忽略，否则，至少保证如下端口可通： k8s-master节点：TCP：6443，2379，2380，60080，60081UDP协议端口全部打开 k8s-slave节点：UDP协议端口全部打开\n 设置iptables  iptables -P FORWARD ACCEPT  关闭swap  swapoff -a # 防止开机自动挂载 swap 分区 sed -i \u0026#39;/ swap / s/^\\(.*\\)$/#\\1/g\u0026#39; /etc/fstab  关闭selinux和防火墙  sed -ri \u0026#39;s#(SELINUX=).*#\\1disabled#\u0026#39; /etc/selinux/config setenforce 0 systemctl disable firewalld \u0026amp;\u0026amp; systemctl stop firewalld  修改内核参数  cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/sysctl.d/k8s.conf net.bridge.bridge-nf-call-ip6tables = 1 net.bridge.bridge-nf-call-iptables = 1 net.ipv4.ip_forward=1 vm.max_map_count=262144 EOF modprobe br_netfilter sysctl -p /etc/sysctl.d/k8s.conf  设置yum源  $ curl -o /etc/yum.repos.d/Centos-7.repo http://mirrors.aliyun.com/repo/Centos-7.repo $ curl -o /etc/yum.repos.d/docker-ce.repo http://mirrors.aliyun.com/docker-ce/linux/centos/docker-ce.repo $ cat \u0026lt;\u0026lt;EOF \u0026gt; /etc/yum.repos.d/kubernetes.repo [kubernetes] name=Kubernetes baseurl=http://mirrors.aliyun.com/kubernetes/yum/repos/kubernetes-el7-x86_64 enabled=1 gpgcheck=0 repo_gpgcheck=0 gpgkey=http://mirrors.aliyun.com/kubernetes/yum/doc/yum-key.gpg http://mirrors.aliyun.com/kubernetes/yum/doc/rpm-package-key.gpg EOF $ yum clean all \u0026amp;\u0026amp; yum makecache 3. 安装docker 操作节点： 所有节点\n ## 查看所有的可用版本 $ yum list docker-ce --showduplicates | sort -r $ yum install docker-ce-18.09.9  ## 配置docker加速 $ mkdir -p /etc/docker vi /etc/docker/daemon.json {  \u0026#34;insecure-registries\u0026#34;: [  \u0026#34;172.21.0.10:5000\u0026#34;  ],  \u0026#34;registry-mirrors\u0026#34; : [  \u0026#34;https://dockerhub.azk8s.cn\u0026#34;,  \u0026#34;https://registry.docker-cn.com\u0026#34;,  \u0026#34;https://ot2k4d59.mirror.aliyuncs.com/\u0026#34;  ] } ## 启动docker $ systemctl enable docker \u0026amp;\u0026amp; systemctl start docker 部署kubernetes 1. 安装 kubeadm, kubelet 和 kubectl 操作节点： 所有的master和slave节点(k8s-master,k8s-slave) 需要执行\n$ yum install -y kubelet-1.16.2 kubeadm-1.16.2 kubectl-1.16.2 --disableexcludes=kubernetes ## 查看kubeadm 版本 $ kubeadm version ## 设置kubelet开机启动 $ systemctl enable kubelet 2. 初始化配置文件 操作节点： 只在master节点（k8s-master）执行\n$ kubeadm config print init-defaults \u0026gt; kubeadm.yaml $ cat kubeadm.yaml apiVersion: kubeadm.k8s.io/v1beta2 bootstrapTokens: - groups:  - system:bootstrappers:kubeadm:default-node-token  token: abcdef.0123456789abcdef  ttl: 24h0m0s  usages:  - signing  - authentication kind: InitConfiguration localAPIEndpoint:  advertiseAddress: 172.21.0.10 # apiserver地址，因为单master，所以配置master的节点内网IP  bindPort: 6443 nodeRegistration:  criSocket: /var/run/dockershim.sock  name: k8s-master # 默认读取当前master节点的hostname  taints:  - effect: NoSchedule  key: node-role.kubernetes.io/master --- apiServer:  timeoutForControlPlane: 4m0s apiVersion: kubeadm.k8s.io/v1beta2 certificatesDir: /etc/kubernetes/pki clusterName: kubernetes controllerManager: {} dns:  type: CoreDNS etcd:  local:  dataDir: /var/lib/etcd imageRepository: gcr.azk8s.cn/google_containers # 修改成微软镜像源,或者registry.cn-shanghai.aliyuncs.com/gcr-k8s kind: ClusterConfiguration kubernetesVersion: v1.16.2 networking:  dnsDomain: cluster.local  podSubnet: 10.244.0.0/16 # Pod 网段，flannel插件需要使用这个网段  serviceSubnet: 10.96.0.0/12 scheduler: {}  对于上面的资源清单的文档比较杂，要想完整了解上面的资源对象对应的属性，可以查看对应的 godoc 文档，地址: https://godoc.org/k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/v1beta2。\n 3. 提前下载镜像 操作节点：只在master节点（k8s-master）执行\n# 查看需要使用的镜像列表,若无问题，将得到如下列表 $ kubeadm config images list --config kubeadm.yaml gcr.azk8s.cn/google_containers/kube-apiserver:v1.16.2 gcr.azk8s.cn/google_containers/kube-controller-manager:v1.16.2 gcr.azk8s.cn/google_containers/kube-scheduler:v1.16.2 gcr.azk8s.cn/google_containers/kube-proxy:v1.16.2 gcr.azk8s.cn/google_containers/pause:3.1 gcr.azk8s.cn/google_containers/etcd:3.3.15-0 gcr.azk8s.cn/google_containers/coredns:1.6.2  # 提前下载镜像到本地 $ kubeadm config images pull --config kubeadm.yaml 4. 初始化master节点 操作节点：只在master节点（k8s-master）执行\nkubeadm init --config kubeadm.yaml 若初始化成功后，最后会提示如下信息：\n... Your Kubernetes master has initialized successfully!  To start using your cluster, you need to run the following as a regular user:   mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/config  You should now deploy a pod network to the cluster. Run \u0026#34;kubectl apply -f [podnetwork].yaml\u0026#34; with one of the options listed at:  https://kubernetes.io/docs/concepts/cluster-administration/addons/  Then you can join any number of worker nodes by running the following on each as root:  kubeadm join 172.21.0.10:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:1c4305f032f4bf534f628c32f5039084f4b103c922ff71b12a5f0f98d1ca9a4f 接下来按照上述提示信息操作，配置kubectl客户端的认证\n mkdir -p $HOME/.kube  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config  sudo chown $(id -u):$(id -g) $HOME/.kube/config  ⚠️注意：此时使用 kubectl get nodes查看节点应该处于notReady状态，因为还未配置网络插件\n若执行初始化过程中出错，根据错误信息调整后，执行kubeadm reset后再次执行init操作即可\n 5. 添加slave节点到集群中 操作节点：所有的slave节点（k8s-slave）需要执行 在每台slave节点，执行如下命令，该命令是在kubeadm init成功后提示信息中打印出来的，需要替换成实际init后打印出的命令。\nkubeadm join 172.21.0.10:6443 --token abcdef.0123456789abcdef \\  --discovery-token-ca-cert-hash sha256:1c4305f032f4bf534f628c32f5039084f4b103c922ff71b12a5f0f98d1ca9a4f 6. 安装flannel插件 操作节点：只在master节点（k8s-master）执行\n 下载flannel的yaml文件  wget https://raw.githubusercontent.com/coreos/flannel/2140ac876ef134e0ed5af15c65e414cf26827915/Documentation/kube-flannel.yml  修改配置，指定网卡名称，大概在文件的190行，添加一行配置：  $ vi kube-flannel.yml ...  containers:  - name: kube-flannel  image: quay.io/coreos/flannel:v0.11.0-amd64  command:  - /opt/bin/flanneld  args:  - --ip-masq  - --kube-subnet-mgr  - --iface=eth0 # 如果机器存在多网卡的话，指定内网网卡的名称，默认不指定的话会找第一块网  resources:  requests:  cpu: \u0026#34;100m\u0026#34; ...  执行安装flannel网络插件  # 先拉取镜像,此过程国内速度比较慢 $ docker pull quay.io/coreos/flannel:v0.11.0-amd64 # 执行flannel安装 $ kubectl create -f kube-flannel.yaml 7. 设置master节点是否可调度（可选） 操作节点：k8s-master\n默认部署成功后，master节点无法调度业务pod，如需设置master节点也可以参与pod的调度，需执行：\n$ kubectl taint node k8s-master node-role.kubernetes.io/master:NoSchedule- 8. 验证集群 操作节点： 在master节点（k8s-master）执行\n$ kubectl get nodes #观察集群节点是否全部Ready NAME STATUS ROLES AGE VERSION k8s-master Ready master 22h v1.13.3 k8s-slave Ready \u0026lt;none\u0026gt; 22h v1.13.3 创建测试nginx服务\n$ kubectl run test-nginx --image=nginx:alpine 查看pod是否创建成功，并访问pod ip测试是否可用\n$ kubectl get po -o wide NAME READY STATUS RESTARTS AGE IP NODE NOMINATED NODE READINESS GATES test-nginx-5bd8859b98-5nnnw 1/1 Running 0 9s 10.244.1.2 k8s-slave1 \u0026lt;none\u0026gt; \u0026lt;none\u0026gt; $ curl 10.244.1.2 ... \u0026lt;h1\u0026gt;Welcome to nginx!\u0026lt;/h1\u0026gt; \u0026lt;p\u0026gt;If you see this page, the nginx web server is successfully installed and working. Further configuration is required.\u0026lt;/p\u0026gt;  \u0026lt;p\u0026gt;For online documentation and support please refer to \u0026lt;a href=\u0026#34;http://nginx.org/\u0026#34;\u0026gt;nginx.org\u0026lt;/a\u0026gt;.\u0026lt;br/\u0026gt; Commercial support is available at \u0026lt;a href=\u0026#34;http://nginx.com/\u0026#34;\u0026gt;nginx.com\u0026lt;/a\u0026gt;.\u0026lt;/p\u0026gt;  \u0026lt;p\u0026gt;\u0026lt;em\u0026gt;Thank you for using nginx.\u0026lt;/em\u0026gt;\u0026lt;/p\u0026gt; \u0026lt;/body\u0026gt; \u0026lt;/html\u0026gt; 9. 部署dashboard  部署服务  # 推荐使用下面这种方式 $ wget https://raw.githubusercontent.com/kubernetes/dashboard/v2.0.0-beta5/aio/deploy/recommended.yaml $ vi recommended.yaml # 修改Service为NodePort类型 ...... kind: Service apiVersion: v1 metadata:  labels:  k8s-app: kubernetes-dashboard  name: kubernetes-dashboard  namespace: kubernetes-dashboard spec:  ports:  - port: 443  targetPort: 8443  selector:  k8s-app: kubernetes-dashboard  type: NodePort # 加上type=NodePort变成NodePort类型的服务 ......  查看访问地址，本例为30133端口  kubectl -n kubernetes-dashboard get svc NAME TYPE CLUSTER-IP EXTERNAL-IP PORT(S) AGE dashboard-metrics-scraper ClusterIP 10.105.62.124 \u0026lt;none\u0026gt; 8000/TCP 31m kubernetes-dashboard NodePort 10.103.74.46 \u0026lt;none\u0026gt; 443:30133/TCP 31m  使用浏览器访问 https://62.234.133.177:30133，其中62.234.133.177为master节点的外网ip地址，chrome目前由于安全限制，测试访问不了，使用firefox可以进行访问。 创建ServiceAccount进行访问  $ vi admin.conf kind: ClusterRoleBinding apiVersion: rbac.authorization.k8s.io/v1beta1 metadata:  name: admin  annotations:  rbac.authorization.kubernetes.io/autoupdate: \u0026#34;true\u0026#34; roleRef:  kind: ClusterRole  name: cluster-admin  apiGroup: rbac.authorization.k8s.io subjects: - kind: ServiceAccount  name: admin  namespace: kubernetes-dashboard  --- apiVersion: v1 kind: ServiceAccount metadata:  name: admin  namespace: kubernetes-dashboard  $ kubectl -n kubernetes-dashboard get secret |grep admin-token admin-token-fqdpf kubernetes.io/service-account-token 3 7m17s # 使用该命令拿到token，然后粘贴到 $ kubectl -n kubernetes-dashboard get secret admin-token-fqdpf -o jsonpath={.data.token}|base64 -d eyJhbGciOiJSUzI1NiIsImtpZCI6Ik1rb2xHWHMwbWFPMjJaRzhleGRqaExnVi1BLVNRc2txaEhETmVpRzlDeDQifQ.eyJpc3MiOiJrdWJlcm5ldGVzL3NlcnZpY2VhY2NvdW50Iiwia3ViZXJuZXRlcy5pby9zZXJ2aWNlYWNjb3VudC9uYW1lc3BhY2UiOiJrdWJlcm5ldGVzLWRhc2hib2FyZCIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VjcmV0Lm5hbWUiOiJhZG1pbi10b2tlbi1mcWRwZiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50Lm5hbWUiOiJhZG1pbiIsImt1YmVybmV0ZXMuaW8vc2VydmljZWFjY291bnQvc2VydmljZS1hY2NvdW50LnVpZCI6IjYyNWMxNjJlLTQ1ZG... 10. 清理环境 如果你的集群安装过程中遇到了其他问题，我们可以使用下面的命令来进行重置：\n$ kubeadm reset $ ifconfig cni0 down \u0026amp;\u0026amp; ip link delete cni0 $ ifconfig flannel.1 down \u0026amp;\u0026amp; ip link delete flannel.1 $ rm -rf /var/lib/cni/ 11. 组件位置   etcd、apiserver、controller-manager、kube-scheduler\n静态Pod的方式\n$ kubectl -n kube-system get po  kubelet kubectl    ","permalink":"https://iblog.zone/archives/kubernetes1.16%E5%AE%89%E8%A3%85kubadm%E6%96%B9%E5%BC%8F/","summary":"集群信息 1. 节点规划 部署k8s集群的节点按照用途可以划分为如下2类角色：\n master：集群的master节点，集群的初始化节点，基础配置不低于2C4G slave：集群的slave节点，可以多台，基础配置不低于2C4G  本例为了演示slave节点的添加，会部署一台master+2台slave，节点规划如下：\n   主机名 节点ip 角色 部署组件     k8s-master 192.168.136.128 master etcd, kube-apiserver, kube-controller-manager, kubectl, kubeadm, kubelet, kube-proxy, flannel   k8s-slave1 192.168.136.131 slave kubectl, kubelet, kube-proxy, flannel   k8s-slave2 192.168.136.132 slave kubectl, kubelet, kube-proxy, flannel    2. 组件版本    组件 版本 说明     CentOS 7.6.1810    Kernel Linux 3.","title":"Kubernetes1.16安装[kubadm方式]"},{"content":" 部署一个Service  vim consul-server-service.yaml apiVersion: v1 kind: Service metadata:  name: consul-server  labels:  name: consul-server spec:  selector:  name: consul-server  ports:  - name: http  port: 8500  targetPort: 8500  - name: https  port: 8443  targetPort: 8443  - name: rpc  port: 8400  targetPort: 8400  - name: serf-lan-tcp  protocol: \u0026#34;TCP\u0026#34;  port: 8301  targetPort: 8301  - name: serf-lan-udp  protocol: \u0026#34;UDP\u0026#34;  port: 8301  targetPort: 8301  - name: serf-wan-tcp  protocol: \u0026#34;TCP\u0026#34;  port: 8302  targetPort: 8302  - name: serf-wan-udp  protocol: \u0026#34;UDP\u0026#34;  port: 8302  targetPort: 8302  - name: server  port: 8300  targetPort: 8300  - name: consul-dns  port: 8600  targetPort: 8600 kubect create -f consul-server-service.yaml 用于通过dns查找agent pod\n2.以StatefulSet方式部署3个有状态的Consul Server\nvim consul-server.yaml apiVersion: apps/v1 kind: StatefulSet metadata:  name: consul-server  labels:  name: consul-server spec:  serviceName: consul-server  selector:  matchLabels:  name: consul-server  replicas: 3  template:  metadata:  labels:  name: consul-server  spec:  terminationGracePeriodSeconds: 10  containers:  - name: consul  image: consul:latest  imagePullPolicy: IfNotPresent  args:  - \u0026#34;agent\u0026#34;  - \u0026#34;-server\u0026#34;  - \u0026#34;-bootstrap-expect=3\u0026#34;  - \u0026#34;-ui\u0026#34;  - \u0026#34;-data-dir=/consul/data\u0026#34;  - \u0026#34;-bind=0.0.0.0\u0026#34;  - \u0026#34;-client=0.0.0.0\u0026#34;  - \u0026#34;-advertise=$(POD_IP)\u0026#34;  - \u0026#34;-retry-join=consul-server-0.consul-server.$(NAMESPACE).svc.cluster.local\u0026#34;  - \u0026#34;-retry-join=consul-server-1.consul-server.$(NAMESPACE).svc.cluster.local\u0026#34;  - \u0026#34;-retry-join=consul-server-2.consul-server.$(NAMESPACE).svc.cluster.local\u0026#34;  - \u0026#34;-domain=cluster.local\u0026#34;  - \u0026#34;-disable-host-node-id\u0026#34;  env:  - name: POD_IP  valueFrom:  fieldRef:  fieldPath: status.podIP  - name: NAMESPACE  valueFrom:  fieldRef:  fieldPath: metadata.namespace  ports:  - containerPort: 8500  name: http  - containerPort: 8400  name: rpc  - containerPort: 8443  name: https-port  - containerPort: 8301  name: serf-lan  - containerPort: 8302  name: serf-wan  - containerPort: 8600  name: consul-dns  - containerPort: 8300  name: server kubect create -f consul-server.yaml 3个consul server正常运行\n3.对外NodePort方式对外暴露服务端口\nvim consul-server-http.yaml apiVersion: v1 kind: Service metadata:  name: consul-server-http spec:  selector:  name: consul-server  type: NodePort  ports:  - protocol: TCP  port: 8500  targetPort: 8500  nodePort: 30098  name: consul-server-tcp kubectl create -f consul-server-http.yaml 对外暴露节点端口是30098\n通过控制台访问\nconsul-server-0是leader节点，共有3个Server节点\n4.以daemonSet方式在工作节点上部署client\nvim consul-client.yaml apiVersion: apps/v1 kind: DaemonSet metadata:  name: consul-client  labels:  name: consul-client spec:  selector:  matchLabels:  name: consul-client  template:  metadata:  labels:  name: consul-client  spec:  containers:  - name: consul  image: consul:latest  imagePullPolicy: IfNotPresent  args:  - \u0026#34;agent\u0026#34;  - \u0026#34;-data-dir=/consul/data\u0026#34;  - \u0026#34;-bind=0.0.0.0\u0026#34;  - \u0026#34;-client=0.0.0.0\u0026#34;  - \u0026#34;-advertise=$(POD_IP)\u0026#34;  - \u0026#34;-retry-join=consul-server-0.consul-server.$(NAMESPACE).svc.cluster.local\u0026#34;  - \u0026#34;-retry-join=consul-server-1.consul-server.$(NAMESPACE).svc.cluster.local\u0026#34;  - \u0026#34;-retry-join=consul-server-2.consul-server.$(NAMESPACE).svc.cluster.local\u0026#34;  - \u0026#34;-domain=cluster.local\u0026#34;  - \u0026#34;-disable-host-node-id\u0026#34;  env:  - name: POD_IP  valueFrom:  fieldRef:  fieldPath: status.podIP  - name: NAMESPACE  valueFrom:  fieldRef:  fieldPath: metadata.namespace  lifecycle:  postStart:  exec:  command:  - /bin/sh  - -c  - consul reload  preStop:  exec:  command:  - /bin/sh  - -c  - consul leave  volumeMounts:  - name: consul-data-dir  mountPath: /consul/data  ports:  - containerPort: 8500  hostPort: 8500  name: http  - containerPort: 8400  name: rpc  - containerPort: 8443  name: https  - containerPort: 8301  name: serf-lan  - containerPort: 8302  name: serf-wan  - containerPort: 8600  name: consul  - containerPort: 8300  name: server  volumes:  - name: consul-data-dir  hostPath:  path: /Users/xuan/desktop/consul/data  type: DirectoryOrCreate kubectl create -f consul-client.yaml 创建client\n通过控制台可以看到有两个client加入了集群\n","permalink":"https://iblog.zone/archives/k8s%E6%90%AD%E5%BB%BAconsul%E9%9B%86%E7%BE%A4/","summary":"部署一个Service  vim consul-server-service.yaml apiVersion: v1 kind: Service metadata:  name: consul-server  labels:  name: consul-server spec:  selector:  name: consul-server  ports:  - name: http  port: 8500  targetPort: 8500  - name: https  port: 8443  targetPort: 8443  - name: rpc  port: 8400  targetPort: 8400  - name: serf-lan-tcp  protocol: \u0026#34;TCP\u0026#34;  port: 8301  targetPort: 8301  - name: serf-lan-udp  protocol: \u0026#34;UDP\u0026#34;  port: 8301  targetPort: 8301  - name: serf-wan-tcp  protocol: \u0026#34;TCP\u0026#34;  port: 8302  targetPort: 8302  - name: serf-wan-udp  protocol: \u0026#34;UDP\u0026#34;  port: 8302  targetPort: 8302  - name: server  port: 8300  targetPort: 8300  - name: consul-dns  port: 8600  targetPort: 8600 kubect create -f consul-server-service.","title":"k8s搭建consul集群"},{"content":"本文包括容器生命周期管理命令、容器操作命令、容器rootfs命令、镜像仓库命令、本地镜像管理命令和基础版本信息命令。\n容器生命周期管理命令 run 创建一个新的容器。\n# 使用docker镜像nginx:latest以后台模式启动一个容器, # 并将容器命名为mynginx。  docker run --name mynginx -d nginx:latest  # 使用镜像 nginx:latest，以后台模式启动一个容器, # 将容器的 80 端口映射到主机的 80 端口, # 主机的目录 /data 映射到容器的 /data。  docker run -p 80:80 -v /data:/data -d nginx:latest  # 使用镜像nginx:latest以交互模式启动一个容器, # 在容器内执行/bin/bash命令。  docker run -it nginx:latest /bin/bash start/stop/restart  docker start : 启动一个或多个已经被停止的容器。 docker stop : 停止一个运行中的容器。 docker restart : 重启容器。  # 启动已被停止的容器mynginx  docker start mynginx  # 停止运行中的容器mynginx  docker stop mynginx  # 重启容器mynginx  docker restart mynginx kill 杀掉一个运行中的容器。可选参数：\n -s : 发送什么信号到容器，默认 KILL  # 根据容器名字杀掉容器  docker kill tomcat7  # 根据容器ID杀掉容器  docker kill 65d4a94f7a39 rm 删除一个或多个容器。\n# 强制删除容器 db01、db02：  docker rm -f db01 db02  # 删除容器 nginx01, 并删除容器挂载的数据卷：  docker rm -v nginx01  # 删除所有已经停止的容器：  docker rm $(docker ps -a -q) create 创建一个新的容器但不启动它。\n# 使用docker镜像nginx:latest创建一个容器,并将容器命名为mynginx  docker create --name mynginx nginx:latest exec 在运行的容器中执行命令。可选参数：\n -d : 分离模式: 在后台运行 -i : 即使没有附加也保持STDIN 打开 -t : 分配一个伪终端  # 在容器 mynginx 中以交互模式执行容器内 /root/nginx.sh 脚本  docker exec -it mynginx /bin/sh /root/nginx.sh  # 在容器 mynginx 中开启一个交互模式的终端  docker exec -i -t mynginx /bin/bash  # 也可以通过 docker ps -a 命令查看已经在运行的容器，然后使用容器 ID 进入容器。  docker ps -a docker exec -it 9df70f9a0714 /bin/bash pause/unpause  docker pause :暂停容器中所有的进程。 docker unpause :恢复容器中所有的进程。  # 暂停数据库容器db01提供服务。  docker pause db01  # 恢复数据库容器 db01 提供服务  docker unpause db0 容器操作命令 ps 列出容器。可选参数：\n -a : 显示所有的容器，包括未运行的。 -f : 根据条件过滤显示的内容。 –format : 指定返回值的模板文件。 -l : 显示最近创建的容器。 -n : 列出最近创建的n个容器。 –no-trunc : 不截断输出。 -q : 静默模式，只显示容器编号。 -s : 显示总的文件大小。  # 列出所有在运行的容器信息。  docker ps  # 列出最近创建的5个容器信息。  docker ps -n 5  # 列出所有创建的容器ID。  docker ps -a -q  补充说明：\n容器的7种状态：created（已创建）、restarting（重启中）、running（运行中）、removing（迁移中）、paused（暂停）、exited（停止）、dead（死亡）。\n inspect 获取容器/镜像的元数据。可选参数：\n -f : 指定返回值的模板文件。 -s : 显示总的文件大小。 –type : 为指定类型返回JSON。  # 获取镜像mysql:5.7的元信息。  docker inspect mysql:5.7  # 获取正在运行的容器mymysql的 IP。  docker inspect --format=\u0026#39;{{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}\u0026#39; mymysql top 查看容器中运行的进程信息，支持 ps 命令参数。\n# 查看容器mymysql的进程信息。  docker top mymysql  # 查看所有运行容器的进程信息。  for i in `docker ps |grep Up|awk \u0026#39;{print $1}\u0026#39;`;do echo \\ \u0026amp;\u0026amp;docker top $i; done events 获取实时事件。参数说明：\n -f ： 根据条件过滤事件； –since ： 从指定的时间戳后显示所有事件； –until ： 流水时间显示到指定的时间为止；  # 显示docker 2016年7月1日后的所有事件。  docker events --since=\u0026#34;1467302400\u0026#34;  # 显示docker 镜像为mysql:5.6 2016年7月1日后的相关事件。  docker events -f \u0026#34;image\u0026#34;=\u0026#34;mysql:5.6\u0026#34; --since=\u0026#34;1467302400\u0026#34;  说明：如果指定的时间是到秒级的，需要将时间转成时间戳。如果时间为日期的话，可以直接使用，如–since=“2016-07-01”。\n logs 获取容器的日志。参数说明：\n -f : 跟踪日志输出 –since : 显示某个开始时间的所有日志 -t : 显示时间戳 –tail : 仅列出最新N条容器日志  # 跟踪查看容器mynginx的日志输出。  docker logs -f mynginx  # 查看容器mynginx从2016年7月1日后的最新10条日志。  docker logs --since=\u0026#34;2016-07-01\u0026#34; --tail=10 mynginx export 将文件系统作为一个tar归档文件导出到STDOUT。参数说明：\n -o : 将输入内容写到文件。  # 将id为a404c6c174a2的容器按日期保存为tar文件。  docker export -o mysql-`date +%Y%m%d`.tar a404c6c174a2  ls mysql-`date +%Y%m%d`.tar port 列出指定的容器的端口映射。\n# 查看容器mynginx的端口映射情况。  docker port mymysql 容器rootfs命令 commit 从容器创建一个新的镜像。参数说明：\n -a : 提交的镜像作者； -c : 使用Dockerfile指令来创建镜像； -m : 提交时的说明文字； -p : 在commit时，将容器暂停。  # 将容器a404c6c174a2 保存为新的镜像, # 并添加提交人信息和说明信息。  docker commit -a \u0026#34;guodong\u0026#34; -m \u0026#34;my db\u0026#34; a404c6c174a2 mymysql:v1 cp 用于容器与主机之间的数据拷贝。参数说明：\n -L : 保持源目标中的链接  # 将主机/www/runoob目录拷贝到容器96f7f14e99ab的/www目录下。  docker cp /www/runoob 96f7f14e99ab:/www/  # 将主机/www/runoob目录拷贝到容器96f7f14e99ab中，目录重命名为www。  docker cp /www/runoob 96f7f14e99ab:/www  # 将容器96f7f14e99ab的/www目录拷贝到主机的/tmp目录中。  docker cp 96f7f14e99ab:/www /tmp/ diff 检查容器里文件结构的更改。\n# 查看容器mymysql的文件结构更改。  docker diff mymysql 镜像仓库命令 login/logout docker login : 登陆到一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub**docker logout :**登出一个Docker镜像仓库，如果未指定镜像仓库地址，默认为官方仓库 Docker Hub参数说明：\n -u : 登陆的用户名 -p : 登陆的密码  # 登陆到Docker Hub  docker login -u 用户名 -p 密码  # 登出Docker Hub  docker logout pull 从镜像仓库中拉取或者更新指定镜像。参数说明：\n -a : 拉取所有 tagged 镜像 –disable-content-trust : 忽略镜像的校验,默认开启  # 从Docker Hub下载java最新版镜像。  docker pull java  # 从Docker Hub下载REPOSITORY为java的所有镜像。  docker pull -a java push 将本地的镜像上传到镜像仓库,要先登陆到镜像仓库。参数说明：\n –disable-content-trust : 忽略镜像的校验,默认开启  # 上传本地镜像myapache:v1到镜像仓库中。  docker push myapache:v1 search 从Docker Hub查找镜像。参数说明：\n –automated : 只列出 automated build类型的镜像； –no-trunc : 显示完整的镜像描述； -f \u0026lt;过滤条件\u0026gt;: 列出指定条件的镜像。  # 从 Docker Hub 查找所有镜像名包含 java，并且收藏数大于 10 的镜像  docker search -f stars=10 java  NAME DESCRIPTION STARS OFFICIAL AUTOMATED java Java is a concurrent, class-based... 1037 [OK] anapsix/alpine-java Oracle Java 8 (and 7) with GLIBC ... 115 [OK] develar/java 46 [OK] 每列参数说明：\n NAME: 镜像仓库源的名称 DESCRIPTION: 镜像的描述 OFFICIAL: 是否 docker 官方发布 stars: 类似 Github 里面的 star，表示点赞、喜欢的意思 AUTOMATED: 自动构建  本地镜像管理命令 images 列出本地镜像。参数说明：\n -a : 列出本地所有的镜像（含中间映像层，默认情况下，过滤掉中间映像层）； –digests : 显示镜像的摘要信息； -f : 显示满足条件的镜像； –format : 指定返回值的模板文件； –no-trunc : 显示完整的镜像信息； -q : 只显示镜像ID。  # 查看本地镜像列表。  docker images  # 列出本地镜像中REPOSITORY为ubuntu的镜像列表。  docker images ubuntu rmi 删除本地一个或多个镜像。参数说明：\n -f : 强制删除； –no-prune : 不移除该镜像的过程镜像，默认移除；  # 强制删除本地镜像 guodong/ubuntu:v4。  docker rmi -f guodong/ubuntu:v4 tag 标记本地镜像，将其归入某一仓库。\n# 将镜像ubuntu:15.10标记为 runoob/ubuntu:v3 镜像。  docker tag ubuntu:15.10 runoob/ubuntu:v3 build 用于使用 Dockerfile 创建镜像。参数说明：\n –build-arg=[] : 设置镜像创建时的变量； –cpu-shares : 设置 cpu 使用权重； –cpu-period : 限制 CPU CFS周期； –cpu-quota : 限制 CPU CFS配额； –cpuset-cpus : 指定使用的CPU id； –cpuset-mems : 指定使用的内存 id； –disable-content-trust : 忽略校验，默认开启； -f : 指定要使用的Dockerfile路径； –force-rm : 设置镜像过程中删除中间容器； –isolation : 使用容器隔离技术； –label=[] : 设置镜像使用的元数据； -m : 设置内存最大值； –memory-swap : 设置Swap的最大值为内存+swap，\u0026quot;-1\u0026quot;表示不限swap； –no-cache : 创建镜像的过程不使用缓存； –pull : 尝试去更新镜像的新版本； –quiet, -q : 安静模式，成功后只输出镜像 ID； –rm : 设置镜像成功后删除中间容器； –shm-size : 设置/dev/shm的大小，默认值是64M； –ulimit : Ulimit配置。 –squash : 将 Dockerfile 中所有的操作压缩为一层。 –tag, -t: 镜像的名字及标签，通常 name:tag 或者 name 格式；可以在一次构建中为一个镜像设置多个标签。 –network: 默认 default。在构建期间设置RUN指令的网络模式  # 使用当前目录的 Dockerfile 创建镜像，标签为 runoob/ubuntu:v1  docker build -t runoob/ubuntu:v1 .  # 使用URL github.com/creack/docker-firefox 的 Dockerfile 创建镜像  docker build github.com/creack/docker-firefox  # 通过 -f Dockerfile文件的位置 创建镜像  docker build -f /path/to/a/Dockerfile . history 查看指定镜像的创建历史。参数说明：\n -H : 以可读的格式打印镜像大小和日期，默认为true； –no-trunc : 显示完整的提交记录； -q : 仅列出提交记录ID。  # 查看本地镜像 guodong/ubuntu:v3 的创建历史。  docker history guodong/ubuntu:v3 save 将指定镜像保存成 tar 归档文件。参数说明：\n -o : 输出到的文件。  # 将镜像 runoob/ubuntu:v3 生成 my_ubuntu_v3.tar 文档  docker save -o my_ubuntu_v3.tar runoob/ubuntu:v3 load 导入使用 docker save 命令导出的镜像。参数说明：\n –input , -i : 指定导入的文件，代替 STDIN。 –quiet , -q : 精简输出信息。  # 导入镜像  docker load --input fedora.tar import 从归档文件中创建镜像。参数说明：\n -c : 应用docker 指令创建镜像； -m : 提交时的说明文字；  # 从镜像归档文件my_ubuntu_v3.tar创建镜像，命名为runoob/ubuntu:v4  docker import my_ubuntu_v3.tar runoob/ubuntu:v4 基础版本信息命令 info 显示 Docker 系统信息，包括镜像和容器数。\n# 查看docker系统信息。  docker info version 显示 Docker 版本信息。\ndocker version ","permalink":"https://iblog.zone/archives/docker-%E5%B8%B8%E7%94%A8%E5%91%BD%E4%BB%A4%E5%A4%A7%E5%85%A8/","summary":"本文包括容器生命周期管理命令、容器操作命令、容器rootfs命令、镜像仓库命令、本地镜像管理命令和基础版本信息命令。\n容器生命周期管理命令 run 创建一个新的容器。\n# 使用docker镜像nginx:latest以后台模式启动一个容器, # 并将容器命名为mynginx。  docker run --name mynginx -d nginx:latest  # 使用镜像 nginx:latest，以后台模式启动一个容器, # 将容器的 80 端口映射到主机的 80 端口, # 主机的目录 /data 映射到容器的 /data。  docker run -p 80:80 -v /data:/data -d nginx:latest  # 使用镜像nginx:latest以交互模式启动一个容器, # 在容器内执行/bin/bash命令。  docker run -it nginx:latest /bin/bash start/stop/restart  docker start : 启动一个或多个已经被停止的容器。 docker stop : 停止一个运行中的容器。 docker restart : 重启容器。  # 启动已被停止的容器mynginx  docker start mynginx  # 停止运行中的容器mynginx  docker stop mynginx  # 重启容器mynginx  docker restart mynginx kill 杀掉一个运行中的容器。可选参数：","title":"Docker 常用命令大全"},{"content":"Java中的OOM（Out of Memory）指java.lang.OutOfMemoryError错误。了解JVM的基本原理后，很容易理解以下几种常见的OOM。\njava.lang.OutOfMemoryError:Java heap space 这是最常见的OOM原因。\n堆中主要存放各种对象实例，还有常量池等结构。当JVM发现堆中没有足够的空间分配给新对象时，抛出该异常。具体来讲，在刚发现空间不足时，会先进行一次Full GC，如果GC后还是空间不足，再抛出异常。\n引起空间不足的原因主要有：\n 业务高峰，创建对象过多 内存泄露 内存碎片严重，无法分配给大对象  java.lang.OutOfMemoryError:Metaspace 方法区主要存储类的元信息，实现在元数据区。当JVM发现元数据区没有足够的空间分配给加载的类时，抛出该异常。\n引起元数据区空间不足的原因主要有：\n 加载的类太多，常见于Tomcat等容器中  但是元数据区被实现在堆外，主要受到进程本身的内存限制，这种实现下很难溢出。\njava.lang.OutOfMemoryError:Permgen space jdk7中，方法区被实现在永久代中，错误原因同上。\n永久代非常小，而且不会被回收，很容易溢出，因此，jdk8彻底废除了永久代，将方法区实现在元数据区。\njava.lang.OutOfMemoryError:Unable to create new native thread 以Linux系统为例，JVM创建的线程与操作系统中的线程一一对应，受到以下限制：\n 进程和操作系统的内存资源限制。其中，一个JVM线程至少要占用OS的线程栈+JVM的虚拟机栈 = 8MB + 1MB = 9MB（当然JVM实现可以选择不使用这1MB的JVM虚拟机栈）。 进程和操作系统的线程数限制。 Linux中的线程被实现为轻量级进程，因此，还受到pid数量的限制。  当无法在操作系统中继续创建线程时，抛出上述异常。\n解决办法从原因中找：\n 内存资源：调小OS的线程栈、JVM的虚拟机栈。 线程数：增大线程数限制。 pid：增大pid范围。  其他异常 java.lang.OutOfMemoryError:GC overhead limit exceeded 默认配置下，如果GC花费了98%的时间，回收的内存都不足2%的话，抛出该异常。\njava.lang.OutOfMemoryError:Out of swap space 如果JVM申请的内存大于可用物理内存，操作系统会将内存中的数据交换到磁盘上去（交换区）。如果交换区空间不足，抛出该异常。\n","permalink":"https://iblog.zone/archives/java%E4%B8%AD%E7%9A%84%E5%B8%B8%E8%A7%81oom%E5%8F%8A%E5%8E%9F%E5%9B%A0/","summary":"Java中的OOM（Out of Memory）指java.lang.OutOfMemoryError错误。了解JVM的基本原理后，很容易理解以下几种常见的OOM。\njava.lang.OutOfMemoryError:Java heap space 这是最常见的OOM原因。\n堆中主要存放各种对象实例，还有常量池等结构。当JVM发现堆中没有足够的空间分配给新对象时，抛出该异常。具体来讲，在刚发现空间不足时，会先进行一次Full GC，如果GC后还是空间不足，再抛出异常。\n引起空间不足的原因主要有：\n 业务高峰，创建对象过多 内存泄露 内存碎片严重，无法分配给大对象  java.lang.OutOfMemoryError:Metaspace 方法区主要存储类的元信息，实现在元数据区。当JVM发现元数据区没有足够的空间分配给加载的类时，抛出该异常。\n引起元数据区空间不足的原因主要有：\n 加载的类太多，常见于Tomcat等容器中  但是元数据区被实现在堆外，主要受到进程本身的内存限制，这种实现下很难溢出。\njava.lang.OutOfMemoryError:Permgen space jdk7中，方法区被实现在永久代中，错误原因同上。\n永久代非常小，而且不会被回收，很容易溢出，因此，jdk8彻底废除了永久代，将方法区实现在元数据区。\njava.lang.OutOfMemoryError:Unable to create new native thread 以Linux系统为例，JVM创建的线程与操作系统中的线程一一对应，受到以下限制：\n 进程和操作系统的内存资源限制。其中，一个JVM线程至少要占用OS的线程栈+JVM的虚拟机栈 = 8MB + 1MB = 9MB（当然JVM实现可以选择不使用这1MB的JVM虚拟机栈）。 进程和操作系统的线程数限制。 Linux中的线程被实现为轻量级进程，因此，还受到pid数量的限制。  当无法在操作系统中继续创建线程时，抛出上述异常。\n解决办法从原因中找：\n 内存资源：调小OS的线程栈、JVM的虚拟机栈。 线程数：增大线程数限制。 pid：增大pid范围。  其他异常 java.lang.OutOfMemoryError:GC overhead limit exceeded 默认配置下，如果GC花费了98%的时间，回收的内存都不足2%的话，抛出该异常。\njava.lang.OutOfMemoryError:Out of swap space 如果JVM申请的内存大于可用物理内存，操作系统会将内存中的数据交换到磁盘上去（交换区）。如果交换区空间不足，抛出该异常。","title":"Java中的常见OOM及原因"},{"content":"一、安装宝塔面板 yum install -y wget \u0026amp;\u0026amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh \u0026amp;\u0026amp; sh install.sh 安装成功后，显示如下内容\n外网面板地址: http://xx:8888/xx 内网面板地址: http://10.0.16.13:8888/xx username: xx password: xx If you cannot access the panel, release the following panel port [8888] in the security group 若无法访问面板，请检查防火墙/安全组是否有放行面板[8888]端口 在宝塔面板网站管理中，添加站点（第一次进入需要安装nginx，点击快速安装即可）\n二、Git仓库创建 1、服务端增加git用户\nadduser git passwd git 2、给git用户授权\nvim /etc/sudoers  #在root ALL=(ALL) ALL 下方添加一行  git ALL=(ALL) ALL 3、用户端跟服务端做免密登录\n用户端生成密钥对\nssh-keygen 将秘钥传到服务端\nssh-copy-id git@server_ip # 输入git刚创建的git用户的密码 验证\nssh git@server_ip # 如果没有要求输入密码就登录进去了 说明免密成功 4、创建git仓库\nsudo mkdir /home/git/repos cd /home/git/repos sudo git init --bare iblog.git 5、配置钩子实现自动部署\ncd /home/git/repos/iblog.git/hooks/ mv post-update.sample post-update vim post-update\n#!/bin/sh git --work-tree=/www/wwwroot/iblog.zone --git-dir=/home/git/repos/iblog.git checkout -f 6、授权\ncd /home/git/repos/iblog.git/hooks/ sudo chmod +x post-update #赋予其可执行权限 sudo chown -R git:git /home/git/repos/ #仓库所有者改为git 7、测试仓库是否可用\n# 用户端使用git命令拉取 git clone git@server_ip:/home/git/repos/iblog.git 三、配置hexo 修改本地Hexo博客文件夹中的_config.yml文件\ndeploy:  type: git  repo: git@server_ip:/home/git/repos/iblog.git  branch: maste 部署hexo\nhexo generate #生成静态页面 hexo delopy #将本地静态页面目录部署到云服务器 四、测试 配置域名解析\n访问创建站点时输入的域名，如果可以访问则配置成功\n","permalink":"https://iblog.zone/archives/%E4%BD%BF%E7%94%A8%E5%AE%9D%E5%A1%94%E9%9D%A2%E6%9D%BF%E5%B0%86hexo%E9%83%A8%E7%BD%B2%E5%88%B0%E8%85%BE%E8%AE%AF%E8%BD%BB%E9%87%8F%E7%BA%A7%E5%BA%94%E7%94%A8%E6%9C%8D%E5%8A%A1%E5%99%A8/","summary":"一、安装宝塔面板 yum install -y wget \u0026amp;\u0026amp; wget -O install.sh http://download.bt.cn/install/install_6.0.sh \u0026amp;\u0026amp; sh install.sh 安装成功后，显示如下内容\n外网面板地址: http://xx:8888/xx 内网面板地址: http://10.0.16.13:8888/xx username: xx password: xx If you cannot access the panel, release the following panel port [8888] in the security group 若无法访问面板，请检查防火墙/安全组是否有放行面板[8888]端口 在宝塔面板网站管理中，添加站点（第一次进入需要安装nginx，点击快速安装即可）\n二、Git仓库创建 1、服务端增加git用户\nadduser git passwd git 2、给git用户授权\nvim /etc/sudoers  #在root ALL=(ALL) ALL 下方添加一行  git ALL=(ALL) ALL 3、用户端跟服务端做免密登录\n用户端生成密钥对\nssh-keygen 将秘钥传到服务端\nssh-copy-id git@server_ip # 输入git刚创建的git用户的密码 验证\nssh git@server_ip # 如果没有要求输入密码就登录进去了 说明免密成功 4、创建git仓库","title":"使用宝塔面板将Hexo部署到腾讯轻量级应用服务器"},{"content":"前面我们是使用openvpn 秘钥的方式登录，这种登录安全性比较高。但是运维操作起来比较麻烦，如果有ldap的也推荐使用ldap集成openvpn。但是这里我们偷个懒，通过设置账号密码的方式连接vpn\n配置OpenVPN 首先我们需要编写一个用户认证的脚本 (脚本是由openvpn官网提供的)\nvim /etc/openvpn/checkpsw.sh #!/bin/sh ########################################################### # checkpsw.sh (C) 2004 Mathias Sundman  # # This script will authenticate OpenVPN users against # a plain text file. The passfile should simply contain # one row per user with the username first followed by # one or more space(s) or tab(s) and then the password.  PASSFILE=\u0026#34;/etc/openvpn/psw-file\u0026#34; LOG_FILE=\u0026#34;/etc/openvpn/openvpn-password.log\u0026#34; TIME_STAMP=`date \u0026#34;+%Y-%m-%d %T\u0026#34;`  ###########################################################  if [ ! -r \u0026#34;${PASSFILE}\u0026#34; ]; then  echo \u0026#34;${TIME_STAMP}: Could not open password file \\\u0026#34;${PASSFILE}\\\u0026#34; for reading.\u0026#34; \u0026gt;\u0026gt; ${LOG_FILE}  exit 1 fi  CORRECT_PASSWORD=`awk \u0026#39;!/^;/\u0026amp;\u0026amp;!/^#/\u0026amp;\u0026amp;$1==\u0026#34;\u0026#39;${username}\u0026#39;\u0026#34;{print $2;exit}\u0026#39; ${PASSFILE}`  if [ \u0026#34;${CORRECT_PASSWORD}\u0026#34; = \u0026#34;\u0026#34; ]; then  echo \u0026#34;${TIME_STAMP}: User does not exist: username=\\\u0026#34;${username}\\\u0026#34;, password=\\\u0026#34;${password}\\\u0026#34;.\u0026#34; \u0026gt;\u0026gt; ${LOG_FILE}  exit 1 fi  if [ \u0026#34;${password}\u0026#34; = \u0026#34;${CORRECT_PASSWORD}\u0026#34; ]; then  echo \u0026#34;${TIME_STAMP}: Successful authentication: username=\\\u0026#34;${username}\\\u0026#34;.\u0026#34; \u0026gt;\u0026gt; ${LOG_FILE}  exit 0 fi  echo \u0026#34;${TIME_STAMP}: Incorrect password: username=\\\u0026#34;${username}\\\u0026#34;, password=\\\u0026#34;${password}\\\u0026#34;.\u0026#34; \u0026gt;\u0026gt; ${LOG_FILE} exit 1 接下来给脚本执行权限\nchmod 755 /etc/openvpn/checkpsw.sh 现在我们配置用户密码文件\ncat /etc/openvpn/psw-file abcdocker 123456 abc 123456 test test  #前面为用户名，后面为密码。 中间使用空格分开 接下来我们需要openvpn的server.conf\ncat \u0026gt;\u0026gt;/etc/openvpn/server.conf\u0026lt;\u0026lt;EOF script-security 3 auth-user-pass-verify /etc/openvpn/checkpsw.sh via-env #指定用户认证脚本 username-as-common-name verify-client-cert none EOF  #在service.conf最后一行添加 接下来我们需要修改client.ovpn\nclient dev tun proto tcp remote 192.168.0.11 1194 resolv-retry infinite nobind persist-key persist-tun ca ca.crt ;cert cyh.crt #注释 ;key cyh.key #注释 tls-auth ta.key 1 cipher AES-256-CBC comp-lzo verb 3 auth-user-pass #使用用户名密码登录openvpn服务器   #主要是注释crt和key路径，以及添加一行auth-user-pass 接下来我们重启一下openvpn即可\n[root@vpn ~]# ps -ef|grep openvpn root 54047 53844 0 14:38 pts/0 00:00:00 grep --color=auto openvpn [root@vpn ~]# /usr/local/openvpn/sbin/openvpn --daemon --config /etc/openvpn/server.conf 然后我们导出client.ovpn，从新启动客户端就可以了\n","permalink":"https://iblog.zone/archives/openvpn-%E8%AE%BE%E7%BD%AE%E8%B4%A6%E5%8F%B7%E5%AF%86%E7%A0%81%E7%99%BB%E5%BD%95/","summary":"前面我们是使用openvpn 秘钥的方式登录，这种登录安全性比较高。但是运维操作起来比较麻烦，如果有ldap的也推荐使用ldap集成openvpn。但是这里我们偷个懒，通过设置账号密码的方式连接vpn\n配置OpenVPN 首先我们需要编写一个用户认证的脚本 (脚本是由openvpn官网提供的)\nvim /etc/openvpn/checkpsw.sh #!/bin/sh ########################################################### # checkpsw.sh (C) 2004 Mathias Sundman  # # This script will authenticate OpenVPN users against # a plain text file. The passfile should simply contain # one row per user with the username first followed by # one or more space(s) or tab(s) and then the password.  PASSFILE=\u0026#34;/etc/openvpn/psw-file\u0026#34; LOG_FILE=\u0026#34;/etc/openvpn/openvpn-password.log\u0026#34; TIME_STAMP=`date \u0026#34;+%Y-%m-%d %T\u0026#34;`  ###########################################################  if [ ! -r \u0026#34;${PASSFILE}\u0026#34; ]; then  echo \u0026#34;${TIME_STAMP}: Could not open password file \\\u0026#34;${PASSFILE}\\\u0026#34; for reading.","title":"OpenVPN 设置账号密码登录"},{"content":"以centos为例(以root身份登录)\n目录约定  安装文件下载目录：/data/software mysql目录安装位置：/usr/local/mysql 数据库保存位置：/data/mysql 日志保存位置：/data/log/mysql  #如果这3个文件夹不存在，先创建 mkdir -p /data/software mkdir -p /data/mysql mkdir -p /data/log/mysql  #创建错误日志文件 cd /data/log/mysql touch error.log  #/usr/local/mysql这个目录待会解压安装包的时候一并创建 下载并解压 在官网选择mysql5.7.24 Linux - Generic版本下载\n 文件较大推荐服务器使用wget命令下载，或者本地使用迅雷下载，然后上传到服务器\n cd /data/software wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz  tar -zxvf mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz #这里注意mysql后面没有/ mv mysql-5.7.24-linux-glibc2.12-x86_64 /usr/local/mysql 新建mysql组、用户 groupadd mysql #新建msyql用户禁止登录shell useradd -r -g mysql mysql -s /sbin/nologin  #更改目录权限 chown -R mysql:mysql /usr/local/mysql chown -R mysql:mysql /data/log/mysql 配置参数 bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql  bin/mysql_ssl_rsa_setup --datadir=/data/mysql  cd /usr/local/mysql/support-files cp mysql.server /etc/init.d/mysql  vim /etc/init.d/mysql #修改以下内容 basedir=/usr/local/mysql datadir=/data/mysql  网上很多教程有这一步 cp my-default.cnf /etc/my.cnf，不过官网说：从5.7.18开始不在二进制包中提供my-default.cnf文件。参考：https://dev.mysql.com/doc/refman/5.7/en/binary-installation.html\n vim /etc/my.conf\n[mysqld] basedir=/usr/local/mysql datadir=/data/mysql socket=/tmp/mysql.sock character_set_server=utf8 user=mysql port=3306 tmpdir=/tmp # Disabling symbolic-links is recommended to prevent assorted security risks symbolic-links=0  [mysqld_safe] log-error=/data/log/mysql/error.log pid-file=/data/mysql/mysql.pid  # include all files from the config directory !includedir /etc/my.cnf.d 启动mysql 启动  /etc/init.d/mysql start   mysql -hlocalhost -uroot -p   #如果出现：-bash: mysql: command not found,就执行  ln -s /usr/local/mysql/bin/mysql /usr/bin 修改密码 set password=password(\u0026#39;root\u0026#39;); 设置root账户的host地址（修改了才可以远程连接） 这里就可以使用远程连接测试了；\nmysql\u0026gt;grant all privileges on *.* to \u0026#39;root\u0026#39;@\u0026#39;%\u0026#39; identified by \u0026#39;root\u0026#39;; mysql\u0026gt;flush privileges; 查看表 use mysql; select host,user from user; 配置mysql开机自启 chmod 755 /etc/init.d/mysql chkconfig --add mysql chkconfig --level 345 mysql on 补充 #退出mysql命令窗口 exit  #查看mysql状态 service mysql status  #停止mysql service mysql stop  #启动mysql service mysql start ","permalink":"https://iblog.zone/archives/centos7.4-%E5%AE%89%E8%A3%85mysql-5.7/","summary":"以centos为例(以root身份登录)\n目录约定  安装文件下载目录：/data/software mysql目录安装位置：/usr/local/mysql 数据库保存位置：/data/mysql 日志保存位置：/data/log/mysql  #如果这3个文件夹不存在，先创建 mkdir -p /data/software mkdir -p /data/mysql mkdir -p /data/log/mysql  #创建错误日志文件 cd /data/log/mysql touch error.log  #/usr/local/mysql这个目录待会解压安装包的时候一并创建 下载并解压 在官网选择mysql5.7.24 Linux - Generic版本下载\n 文件较大推荐服务器使用wget命令下载，或者本地使用迅雷下载，然后上传到服务器\n cd /data/software wget https://dev.mysql.com/get/Downloads/MySQL-5.7/mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz  tar -zxvf mysql-5.7.24-linux-glibc2.12-x86_64.tar.gz #这里注意mysql后面没有/ mv mysql-5.7.24-linux-glibc2.12-x86_64 /usr/local/mysql 新建mysql组、用户 groupadd mysql #新建msyql用户禁止登录shell useradd -r -g mysql mysql -s /sbin/nologin  #更改目录权限 chown -R mysql:mysql /usr/local/mysql chown -R mysql:mysql /data/log/mysql 配置参数 bin/mysqld --initialize-insecure --user=mysql --basedir=/usr/local/mysql --datadir=/data/mysql  bin/mysql_ssl_rsa_setup --datadir=/data/mysql  cd /usr/local/mysql/support-files cp mysql.","title":"centos7.4 安装mysql 5.7"},{"content":"一、问题出现原因 使用gitlab-runner部署前端js项目时，下载chromedriver@2.46.0有时候相当的慢。\n主要原因是npm install老去国外github下载\n\u0026gt; chromedriver@2.45.0 install /app/node_modules/chromedriver \u0026gt; node install.js  Current existing ChromeDriver binary is unavailable, proceding with download and extraction. Downloading from file: https://chromedriver.storage.googleapis.com/2.45/chromedriver_linux64.zip Saving to file: /app/node_modules/chromedriver/chromedriver/chromedriver_linux64.zip ... 后来去放置gitlab-runner那台主机wget一下，结果一直响应中\ncuiyf@gitlabrunner:~$ wget https://chromedriver.storage.googleapis.com/2.46/chromedriver_linux64.zip --2019-08-05 17:46:32-- https://chromedriver.storage.googleapis.com/2.46/chromedriver_linux64.zip Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 216.58.200.48, 2404:6800:4008:801::2010 Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|216.58.200.48|:443... connected. HTTP request sent, awaiting response... 二、解决思路 设法使其去淘宝镜像源地址下载\nhttps://registry.npm.taobao.org/ 三、解决方案 由于我是用docker部署的，修改Dockerfile添加如下配置 npm install chromedriver --chromedriver_cdnurl=http://cdn.npm.taobao.org/dist/chromedriver\nFROMcuiyf/node:8.12.0-alpine as build-stageCOPY . /appWORKDIR/appRUN npm config set registry http://172.18.0.1:9000/repository/node-public \\  \u0026amp;\u0026amp; npm install chromedriver --chromedriver_cdnurl=http://cdn.npm.taobao.org/dist/chromedriver\\  \u0026amp;\u0026amp; npm installRUN npm run buildFROMcuiyf/nginx:1.13.12-alpine as production-stageCOPY --from=build-stage /app/dist /usr/share/nginx/htmlEXPOSE80CMD [\u0026#34;nginx\u0026#34;, \u0026#34;-g\u0026#34;, \u0026#34;daemon off;\u0026#34;]最终效果，快速的去淘宝源下载去了\n\u0026gt; chromedriver@2.46.0 install /app/node_modules/chromedriver \u0026gt; node install.js  Current existing ChromeDriver binary is unavailable, proceding with download and extraction. Downloading from file: http://cdn.npm.taobao.org/dist/chromedriver/2.46/chromedriver_linux64.zip Saving to file: /app/node_modules/chromedriver/2.46/chromedriver/chromedriver_linux64.zip Received 808K... Received 1652K... Received 2459K... Received 3258K... Received 4092K... Received 4922K... Received 5277K total. Extracting zip contents Copying to target path /app/node_modules/chromedriver/lib/chromedriver Fixing file permissions Done. ChromeDriver binary available at /app/node_modules/chromedriver/lib/chromedriver/chromedriver 四、终极方案 新建一个镜像吧，包括下载node-sass、phantomjs、electron等\nFROMnode:8.12.0-alpineLABEL QianJia.cuiyufeng \u0026lt;1627999813@qq.com\u0026gt;ENV SASS_BINARY_SITE=http://172.17.3.95:9000/repository/node-saas/ \\  PHANTOMJS_CDNURL=http://172.17.3.95:9000/repository/phantomjs/ \\  ELECTRON_MIRROR=http://172.17.3.95:9000/repository/electron/ \\  CHROMEDRIVER_CDNURL=http://172.17.3.95:9000/repository/chromedriver/RUN sed -i \u0026#39;s/dl-cdn.alpinelinux.org/mirrors.aliyun.com/g\u0026#39; /etc/apk/repositories \\  \u0026amp;\u0026amp; apk add --no-cache tzdata \\  \u0026amp;\u0026amp; ln -sf /usr/share/zoneinfo/Asia/Shanghai /etc/localtime \\  \u0026amp;\u0026amp; echo \u0026#34;Asia/Shanghai\u0026#34; \u0026gt; /etc/timezone \\  \u0026amp;\u0026amp; rm -rf /var/cache/apk/* /tmp/* /var/tmp/* $HOME/.cache ","permalink":"https://iblog.zone/archives/%E8%A7%A3%E5%86%B3npm-install%E4%B8%8B%E8%BD%BDchromedriver2.46.0%E4%BE%9D%E8%B5%96%E5%8C%85%E8%B6%85%E7%BA%A7%E6%85%A2%E9%97%AE%E9%A2%98/","summary":"一、问题出现原因 使用gitlab-runner部署前端js项目时，下载chromedriver@2.46.0有时候相当的慢。\n主要原因是npm install老去国外github下载\n\u0026gt; chromedriver@2.45.0 install /app/node_modules/chromedriver \u0026gt; node install.js  Current existing ChromeDriver binary is unavailable, proceding with download and extraction. Downloading from file: https://chromedriver.storage.googleapis.com/2.45/chromedriver_linux64.zip Saving to file: /app/node_modules/chromedriver/chromedriver/chromedriver_linux64.zip ... 后来去放置gitlab-runner那台主机wget一下，结果一直响应中\ncuiyf@gitlabrunner:~$ wget https://chromedriver.storage.googleapis.com/2.46/chromedriver_linux64.zip --2019-08-05 17:46:32-- https://chromedriver.storage.googleapis.com/2.46/chromedriver_linux64.zip Resolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 216.58.200.48, 2404:6800:4008:801::2010 Connecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|216.58.200.48|:443... connected. HTTP request sent, awaiting response... 二、解决思路 设法使其去淘宝镜像源地址下载\nhttps://registry.npm.taobao.org/ 三、解决方案 由于我是用docker部署的，修改Dockerfile添加如下配置 npm install chromedriver --chromedriver_cdnurl=http://cdn.npm.taobao.org/dist/chromedriver\nFROMcuiyf/node:8.12.0-alpine as build-stageCOPY . /appWORKDIR/appRUN npm config set registry http://172.","title":"解决npm install下载chromedriver@2.46.0依赖包超级慢问题"},{"content":"OpenVPN的工作原理 在Linux2.4版本以上，操作系统支持一个名为tun的设备，tun设备的驱动程序中包含两个部分，一部分是字符设备驱动，一部分是网卡驱动。网卡的驱动把从TCP/IP协议栈收到的数据包结构skb放于tun设备的读取队列，用户进程通过调用字符设备接口read获得完整的IP数据包，字符驱动read函数的功能是从设备的读取队列读取数据，将核心态的skb传递给用户；反过来字符驱动write函数给用户提供了把用户态的数据写入核心态的接口，write函数把用户数据写入核心空间并穿入TCP/IP协议栈。该设备既能以字符设备的方式被读写，作为系统的虚拟网卡，也具有和物理网卡相同的特点：能够配置IP地址和路由。对虚拟网卡的使用是OpenVPN实现其SSL VPN功能的关键。\nOpenVPN服务器一般需要配置一个虚拟IP地址池和一个自用的静态虚拟IP地址（静态地址和地址池必须在同一个子网中），然后为每一个成功建立SSL连接的客户端动态分配一个虚拟IP地址池中未分配的地址。这样，物理网络中的客户端和OpenVPN服务器就连接成一个虚拟网络上的星型结构局域网，OpenVPN服务器成为每个客户端在虚拟网络上的网关。OpenVPN服务器同时提供对客户端虚拟网卡的路由管理。当客户端对OpenVPN服务器后端的应用服务器的任何访问时，数据包都会经过路由流经虚拟网卡，OpenVPN程序在虚拟网卡上截获数据IP报文，然后使用SSL协议将这些IP报文封装起来，再经过物理网卡发送出去。OpenVPN的服务器和客户端在虚拟网卡之上建立起一个虚拟的局域网络，这个虚拟的局域网对系统的用户来说是透明的。\nOpenVPN的服务器和客户端支持tcp和udp两种连接方式，只需在服务端和客户端预先定义好使用的连接方式（tcp或udp）和端口号，客户端和服务端在这个连接的基础上进行SSL握手。连接过程包括SSL的握手以及虚拟网络上的管理信息，OpenVPN将虚拟网上的网段、地址、路由发送给客户端。连接成功后，客户端和服务端建立起SSL安全连接，客户端和服务端的数据都流入虚拟网卡做SSL的处理，再在tcp或udp的连接上从物理网卡发送出去\n环境说明 192.168.0.10外网 10.4.82.10 内网  系统环境 [root@vpn ~]# cat /etc/redhat-release CentOS Linux release 7.7.1908 (Core) [root@vpn ~]# uname -r 3.10.0-1062.9.1.el7.x86_64  网卡为双网卡 [root@vpn ~]# ifconfig eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500  inet 192.168.0.11 netmask 255.255.255.0 broadcast 192.168.0.255  inet6 fe80::6b5a:9ab8:1bb:5f8d prefixlen 64 scopeid 0x20  ether 00:0c:29:32:b2:36 txqueuelen 1000 (Ethernet)  RX packets 15104 bytes 16993218 (16.2 MiB)  RX errors 0 dropped 0 overruns 0 frame 0  TX packets 8661 bytes 889872 (869.0 KiB)  TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0  eth1: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500  inet 10.4.82.11 netmask 255.255.255.0 broadcast 10.4.82.255  inet6 fe80::20c:29ff:fe32:b240 prefixlen 64 scopeid 0x20  ether 00:0c:29:32:b2:40 txqueuelen 1000 (Ethernet)  RX packets 202 bytes 18735 (18.2 KiB)  RX errors 0 dropped 0 overruns 0 frame 0  TX packets 8 bytes 656 (656.0 B)  TX errors 0 dropped 0 overruns 0 carrier 0 collisions 0 首先我们先使用easy-rsa制作openVPN证书\n下载并解压easy-rsa软件包\nmkdir /data/tools -p wget -P /data/tools http://down.i4t.com/easy-rsa.zip unzip -d /usr/local /data/tools/easy-rsa.zip 在开始制作CA证书之前，我们还需要编辑vars文件，修改如下相关选项\ncd /usr/local/easy-rsa-old-master/easy-rsa/2.0/  vim vars export KEY_COUNTRY=\u0026#34;cn\u0026#34; export KEY_PROVINCE=\u0026#34;BJ\u0026#34; export KEY_CITY=\u0026#34;BJ\u0026#34; export KEY_ORG=\u0026#34;abcdocker\u0026#34; export KEY_EMAIL=\u0026#34;cyh@i4t.com\u0026#34; export KEY_CN=abc export KEY_NAME=abc export KEY_OU=abc  #行数大约67行开始,主要是修改默认的注册信息，比如注册公司、公司名称、部门、国家城市等  注意：以上内容，我们也可以使用系统默认的，也就是说不进行修改也是可以使用的\n 然后使用使环境变量生效\n#初始化环境边看 source vars ./clean-all  #注意：执行clean-all命令会在当前目录下创建一个名词为keys的目录 接下来开始正式制作CA证书，命令如下\n./build-ca  # 生成根证书ca.crt和根密钥ca.key #因为在vars中填写了证书的基本信息，所以这里一路回车即可 这时我们可以查看keys目录，已经帮我们生成ca.crt和ca.key两个文件，其中ca.crt就是我们的证书文件\n[root@abc01 2.0]# ls keys ca.crt ca.key index.txt serial  制作Server端证书 为服务端生成证书和密钥\n#一直回车，2个Y  [root@abc01 2.0]# ./build-key-server server .... An optional company name []: Using configuration from /usr/local/easy-rsa-old-master/easy-rsa/2.0/openssl-1.0.0.cnf Check that the request matches the signature Signature ok The Subject\u0026#39;s Distinguished Name is as follows countryName :PRINTABLE:\u0026#39;cn\u0026#39; stateOrProvinceName :PRINTABLE:\u0026#39;BJ\u0026#39; localityName :PRINTABLE:\u0026#39;BJ\u0026#39; organizationName :PRINTABLE:\u0026#39;abcdocker\u0026#39; organizationalUnitName:PRINTABLE:\u0026#39;abc\u0026#39; commonName :PRINTABLE:\u0026#39;abc\u0026#39; name :PRINTABLE:\u0026#39;abc\u0026#39; emailAddress :IA5STRING:\u0026#39;cyh@i4t.com\u0026#39; Certificate is to be certified until Jan 31 14:01:35 2030 GMT (3650 days) Sign the certificate? [y/n]:y   1 out of 1 certificate requests certified, commit? [y/n]y Write out database with 1 new entries Data Base Updated  #这里的server就是我们server端的证书 查看新生成的证书\n[root@abc01 2.0]# ls keys 01.pem abc.key index.txt serial server.crt ca.crt index.txt.attr serial.old server.csr ca.key index.txt.old 这里我们已经生成了server.crt、server.key、server.csr三个文件，其中server.crt和server.key两个文件是我们需要使用的\n 制作Client端证书 这里我们创建2个用户，分别为client1和client2\n#每一个登陆的VPN客户端需要有一个证书，每个证书在同一时刻只能供一个客户端连接，下面建立2份 #为客户端生成证书和密钥（一路按回车，直到提示需要输入y/n时，输入y再按回车，一共两次） ./build-key client1 ./build-key client2 每一个登陆的VPN客户端需要有一个证书，每个证书在同一时刻只可以一个客户端连接(可以修改配置文件)\n现在为服务器生成加密交换时的Diffie-Hellman文件\n./build-dh # 创建迪菲·赫尔曼密钥，会生成dh2048.pem文件（生成过程比较慢，在此期间不要去中断它）  证书生成完毕\n[root@abc01 2.0]# ll keys total 84 -rw-r--r-- 1 root root 7997 Feb 3 09:01 01.pem -rw-r--r-- 1 root root 7880 Feb 3 09:09 02.pem -rw-r--r-- 1 root root 7997 Feb 3 09:01 abc.crt -rw-r--r-- 1 root root 1765 Feb 3 09:01 abc.csr -rw------- 1 root root 3272 Feb 3 09:01 abc.key -rw-r--r-- 1 root root 2293 Feb 3 09:01 ca.crt -rw------- 1 root root 3272 Feb 3 09:01 ca.key -rw-r--r-- 1 root root 424 Feb 3 09:06 dh2048.pem -rw-r--r-- 1 root root 211 Feb 3 09:09 index.txt -rw-r--r-- 1 root root 21 Feb 3 09:09 index.txt.attr -rw-r--r-- 1 root root 21 Feb 3 09:01 index.txt.attr.old -rw-r--r-- 1 root root 105 Feb 3 09:01 index.txt.old -rw-r--r-- 1 root root 3 Feb 3 09:09 serial -rw-r--r-- 1 root root 3 Feb 3 09:01 serial.old -rw-r--r-- 1 root root 7880 Feb 3 09:09 test.crt -rw-r--r-- 1 root root 1765 Feb 3 09:09 test.csr -rw------- 1 root root 3272 Feb 3 09:09 test.key 其中包含了一个test用户和abc用户的证书\n 其中只有*.crt和*.key文件是我们需要使用的\n  安装OpenVPN 安装vpn的方法有2种，一种是使用yum安装，另外一种是编译安装。这两个我们选择一个就可以\n编译安装\n#安装依赖包 curl -o /etc/yum.repos.d/CentOS-Base.repo http://mirrors.aliyun.com/repo/Centos-7.repo wget -O /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum makecache yum install -y lzo lzo-devel openssl openssl-devel pam pam-devel net-tools git lz4-devel   #下载openVPN软件包 wget -P /data/tools http://down.i4t.com/openvpn-2.4.7.tar.gz cd /data/tools  #安装openVPN tar zxf openvpn-2.4.7.tar.gz cd openvpn-2.4.7 ./configure --prefix=/usr/local/openvpn-2.4.7 make make install  # 创建软连接 ln -s /usr/local/openvpn-2.4.7 /usr/local/openvpn yum安装\nOpenVPN需要安装EPEL\ncurl -o /etc/yum.repos.d/epel.repo http://mirrors.aliyun.com/repo/epel-7.repo yum clean all \u0026amp;\u0026amp; yum makecache yum 安装openVPN\nyum install -y openvpn  配置OpenVPN服务端 我们需要创建openVPN文件目录和证书目录\n# openVPN配置文件目录，yum安装默认存在 mkdir /etc/openvpn  #openvpn证书目录 mkdir /etc/openvpn/keys 生成tls-auth key并将其拷贝到证书目录中（防DDos攻击、UDP淹没等恶意攻击）\n#编译安装执行此句 /usr/local/openvpn/sbin/openvpn --genkey --secret ta.key  # yum安装执行此句 openvpn --genkey --secret ta.key  #将本地的ta.key移动到openVPN证书目录 mv ./ta.key /etc/openvpn/keys/ 将我们上面生成的CA证书和服务端证书拷贝到证书目录中\n$ cp /usr/local/easy-rsa-old-master/easy-rsa/2.0/keys/{server.crt,server.key,ca.crt,dh2048.pem} /etc/openvpn/keys/  [root@k8s-01 ~]# ll /etc/openvpn/keys/ total 24 -rw-r--r-- 1 root root 2342 Feb 3 12:48 ca.crt -rw-r--r-- 1 root root 424 Feb 3 12:48 dh2048.pem -rw-r--r-- 1 root root 8089 Feb 3 12:48 server.crt -rw------- 1 root root 3272 Feb 3 12:48 server.key -rw------- 1 root root 636 Feb 3 12:47 ta.key   #abc和test为我们client端用户的证书 拷贝OpenVPN配置文件\n# 编译安装 cp /data/tools/openvpn-2.4.7/sample/sample-config-files/server.conf /etc/openvpn/  # yum安装 cp /usr/share/doc/openvpn-2.4.7/sample/sample-config-files/server.conf /etc/openvpn/ 接下来我们来配置服务端的配置文件\n$ cat /etc/openvpn/server.conf port 1194 #openVPN端口 proto tcp #tcp连接 dev tun #生成tun0虚拟网卡  ca keys/ca.crt #相关证书配置路径(可以修改为全路径/etc/openvpn/keys) cert keys/server.crt key keys/server.key # This file should be kept secret dh keys/dh2048.pem  server 10.4.82.0 255.255.255.0 #默认虚拟局域网网段，不要和实际的局域网冲突就可以 ifconfig-pool-persist ipp.txt  push \u0026#34;route 10.4.82.0 255.255.255.0\u0026#34; #可以通过iptables进行路由的转发 push \u0026#34;route 172.17.88.0 255.255.255.0\u0026#34; #vpn机器内网 push \u0026#34;route 172.17.65.0 255.255.255.0\u0026#34; #目的网段内网  client-to-client #如果客户端都是用一个证书和密钥连接VPN，需要打开这个选项 duplicate-cn keepalive 10 120 tls-auth keys/ta.key 0 # This file is secret comp-lzo  persist-key persist-tun  status openvpn-status.log #状态日志路径 log-append openvpn.log #运行日志 verb 3 #调试信息级别    #如果需要接入ldap，需要在server.conf下添加如下2行 plugin /usr/lib64/openvpn/plugin/lib/openvpn-auth-ldap.so \u0026#34;/etc/openvpn/auth/ldap.conf cn=%u\u0026#34; client-cert-not-required  #如何环境和我相同，可以直接cp我的配置文件 开启内核路由转发功能\necho \u0026#34;net.ipv4.ip_forward = 1\u0026#34; \u0026gt;\u0026gt;/etc/sysctl.conf sysctl -p 如果有iptables可以开启iptables策略\niptables -P FORWARD ACCEPT iptables -I INPUT -p tcp --dport 1194 -m comment --comment \u0026#34;openvpn\u0026#34; -j ACCEPT iptables -t nat -A POSTROUTING -s 10.4.82.0/24 -j MASQUERADE 启动openvpn服务\n$ cd /etc/openvpn/ $ /usr/local/openvpn/sbin/openvpn --daemon --config /etc/openvpn/server.conf 检查服务\n$ netstat -lntup|grep 1194 tcp 0 0 0.0.0.0:1194 0.0.0.0:* LISTEN 48091/openvpn 设置开机启动\necho \u0026#34;/usr/local/openvpn/sbin/openvpn --daemon --config /etc/openvpn/server.conf \u0026gt; /dev/null 2\u0026gt;\u0026amp;1 \u0026amp;\u0026#34; \u0026gt;\u0026gt; /etc/rc.local  客户端连接测试 无论我们是在Windows还是Linux OS上Client端的配置，都需要将Client证书、CA证书以及Client配置文件下载下来\n现在我们需要先配置一下client.conf\ncp /data/tools/openvpn-2.4.7/sample/sample-config-files/client.conf /root/  #修改如下，并将client.conf修改为client.ovpn $ cat /root/client.conf client dev tun proto tcp remote 192.168.0.10 1194 #openvpn服务器的外网IP和端口(可以写多个做到高可用) resolv-retry infinite nobind persist-key persist-tun ca ca.crt cert client1.crt #用户的证书 key client1.key  tls-auth ta.key 1 cipher AES-256-CBC comp-lzo verb 3   #比较重点的就是修改remote 地址，这里的地址为server cert key，我们这里使用用户的证书，所以证书也应当修改为client1.crt和client1.key tls-auth 因为使用加密协议，所以ta.key也需要下载下来 修改后缀并导出\n[root@vpn ~]# mv client.conf client.ovpn [root@vpn ~]# sz client.ovpn  #同时还需要导出几个证书 mv client.conf client.ovpn sz /root/client.ovpn sz /etc/openvpn/keys/ca.crt sz /etc/openvpn/keys/ta.key sz /usr/local/easy-rsa-old-master/easy-rsa/2.0/keys/client1.crt sz /usr/local/easy-rsa-old-master/easy-rsa/2.0/keys/client1.key 添加用户\n以后我们如果想添加用户只需要到cd /usr/local/easy-rsa-old-master/easy-rsa/2.0目录下执行./build-key 用户名，在将keys目录下生成的用户名.crt和key导出，修改一下client.ovpn的用户key名称即可\n Windows\n客户端需要证书如下\nWindows客户端下载\nhttp://down.i4t.com/openvpn-install-2.4.7-I606-Win10.exe\nhttp://down.i4t.com/openvpn-install-2.4.7-I606-Win7.exe\n 打开浏览器即可下载\n 安装步骤，直接C盘就可以了，文件很小~\n疯狂下一步即可\n安装完成后\n接下来我们需要替换一下证书\n点击桌面的logo，右击属性\n点击打开文件位置\n点击上方OpenVPN从新选择目录\n选择config目录\n将config目录下文件全部删除\n然后将我们导出的5个证书复制过去\n 其中client1.*为client1用户的证书\n 现在我们进行启动openvpn客户端，进行连接\n双击桌面OpenVPN 右击图标，选择连接\n这里可以看到已经连接成功\nMac\n复制链接到浏览器，下载软件包\nhttp://down.i4t.com/Tunnelblick_3.8.1_build_5400.dmg\n我们直接点开client.ovpn就可以自动链接\n成功后会有下面的流量图\n配置文件打开方式\n这时候网络已经通了\n配置完成后，可能路由存在问题，可以尝试添加下面的iptables规则\niptables -t nat -A POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to 172.17.88.56 iptables -I INPUT -p tcp --dport 1194 -j ACCEPT iptables -I FORWARD -s 10.8.0.0/24 -j ACCEPT iptables -I FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT iptables -t nat -D POSTROUTING -s 10.8.0.0/24 ! -d 10.8.0.0/24 -j SNAT --to 172.17.88.56 iptables -D INPUT -p tcp --dport 1194 -j ACCEPT iptables -D FORWARD -s 10.8.0.0/24 -j ACCEPT iptables -D FORWARD -m state --state RELATED,ESTABLISHED -j ACCEPT ","permalink":"https://iblog.zone/archives/centos-7-%E6%90%AD%E5%BB%BAopenvpn%E6%9C%8D%E5%8A%A1%E5%99%A8/","summary":"OpenVPN的工作原理 在Linux2.4版本以上，操作系统支持一个名为tun的设备，tun设备的驱动程序中包含两个部分，一部分是字符设备驱动，一部分是网卡驱动。网卡的驱动把从TCP/IP协议栈收到的数据包结构skb放于tun设备的读取队列，用户进程通过调用字符设备接口read获得完整的IP数据包，字符驱动read函数的功能是从设备的读取队列读取数据，将核心态的skb传递给用户；反过来字符驱动write函数给用户提供了把用户态的数据写入核心态的接口，write函数把用户数据写入核心空间并穿入TCP/IP协议栈。该设备既能以字符设备的方式被读写，作为系统的虚拟网卡，也具有和物理网卡相同的特点：能够配置IP地址和路由。对虚拟网卡的使用是OpenVPN实现其SSL VPN功能的关键。\nOpenVPN服务器一般需要配置一个虚拟IP地址池和一个自用的静态虚拟IP地址（静态地址和地址池必须在同一个子网中），然后为每一个成功建立SSL连接的客户端动态分配一个虚拟IP地址池中未分配的地址。这样，物理网络中的客户端和OpenVPN服务器就连接成一个虚拟网络上的星型结构局域网，OpenVPN服务器成为每个客户端在虚拟网络上的网关。OpenVPN服务器同时提供对客户端虚拟网卡的路由管理。当客户端对OpenVPN服务器后端的应用服务器的任何访问时，数据包都会经过路由流经虚拟网卡，OpenVPN程序在虚拟网卡上截获数据IP报文，然后使用SSL协议将这些IP报文封装起来，再经过物理网卡发送出去。OpenVPN的服务器和客户端在虚拟网卡之上建立起一个虚拟的局域网络，这个虚拟的局域网对系统的用户来说是透明的。\nOpenVPN的服务器和客户端支持tcp和udp两种连接方式，只需在服务端和客户端预先定义好使用的连接方式（tcp或udp）和端口号，客户端和服务端在这个连接的基础上进行SSL握手。连接过程包括SSL的握手以及虚拟网络上的管理信息，OpenVPN将虚拟网上的网段、地址、路由发送给客户端。连接成功后，客户端和服务端建立起SSL安全连接，客户端和服务端的数据都流入虚拟网卡做SSL的处理，再在tcp或udp的连接上从物理网卡发送出去\n环境说明 192.168.0.10外网 10.4.82.10 内网  系统环境 [root@vpn ~]# cat /etc/redhat-release CentOS Linux release 7.7.1908 (Core) [root@vpn ~]# uname -r 3.10.0-1062.9.1.el7.x86_64  网卡为双网卡 [root@vpn ~]# ifconfig eth0: flags=4163\u0026lt;UP,BROADCAST,RUNNING,MULTICAST\u0026gt; mtu 1500  inet 192.168.0.11 netmask 255.255.255.0 broadcast 192.168.0.255  inet6 fe80::6b5a:9ab8:1bb:5f8d prefixlen 64 scopeid 0x20  ether 00:0c:29:32:b2:36 txqueuelen 1000 (Ethernet)  RX packets 15104 bytes 16993218 (16.2 MiB)  RX errors 0 dropped 0 overruns 0 frame 0  TX packets 8661 bytes 889872 (869.","title":"CentOS 7 搭建OpenVPN服务器"}]